
Strepsirrhini or Strepsirhini ( (listen); STREP-sə-RY-nee) is a suborder of primates that includes the lemuriform primates, which consist of the lemurs of Madagascar, galagos ("bushbabies") and pottos from Africa, and the lorises from India and southeast Asia. Collectively they are referred to as strepsirrhines. Also belonging to the suborder are the extinct adapiform primates that thrived during the Eocene in Europe, North America, and Asia, but disappeared from most of the Northern Hemisphere as the climate cooled. Adapiforms are sometimes referred to as being "lemur-like", although the diversity of both lemurs and adapiforms does not support this comparison.
Strepsirrhines are defined by their "wet" (moist) rhinarium (the tip of the snout) - hence the colloquial but inaccurate term "wet-nosed" - similar to the rhineria of dogs and cats. They also have a smaller brain than comparably sized simians, large olfactory lobes for smell, a vomeronasal organ to detect pheromones, and a bicornuate uterus with an epitheliochorial placenta. Their eyes contain a reflective layer to improve their night vision, and their eye sockets include a ring of bone around the eye, but they lack a wall of thin bone behind it. Strepsirrhine primates produce their own vitamin C, whereas haplorhine primates must obtain it from their diets. Lemuriform primates are characterized by a toothcomb, a specialized set of teeth in the front, lower part of the mouth mostly used for combing fur during grooming.
Many of today's living strepsirrhines are endangered due to habitat destruction, hunting for bushmeat, and live capture for the exotic pet trade. Both living and extinct strepsirrhines are behaviorally diverse, although all are primarily arboreal (tree-dwelling). Most living lemuriforms are nocturnal, while most adapiforms were diurnal. Both living and extinct groups primarily fed on fruit, leaves, and insects.
The taxonomic name Strepsirrhini derives from the Greek στρεψίς (strepsis or "a turning [inward])" and ῥινός (rhinos or "nose"), which refers to the appearance of the sinuous (comma-shaped) nostrils on the rhinarium or wet nose. The name was first used by French naturalist Étienne Geoffroy Saint-Hilaire in 1812 as a subordinal rank comparable to Platyrrhini (New World monkeys) and Catarrhini (Old World monkeys). In his description, he mentioned "Les narines terminales et sinueuses" ("Nostrils terminal and winding").When British zoologist Reginald Innes Pocock revived Strepsirrhini and defined Haplorhini in 1918, he omitted the second "r" from both ("Strepsirhini" and "Haplorhini" instead of "Strepsirrhini" and "Haplorrhini"), although he did not remove the second "r" from Platyrrhini or Catarrhini, both of which were also named by É. Geoffroy in 1812. Following Pocock, many researchers continued to spell Strepsirrhini with a single "r" until primatologists Paulina Jenkins and Prue Napier pointed out the error in 1987.
Strepsirrhines include the extinct adapiforms and the lemuriform primates, which include lemurs and lorisoids (lorises, pottos, and galagos). Strepsirrhines diverged from the haplorhine primates near the beginning of the primate radiation between 55 and 90 mya. Older divergence dates are based on genetic analysis estimates, while younger dates are based on the scarce fossil record. Lemuriform primates may have evolved from either cercamoniines or sivaladapids, both of which were adapiforms that may have originated in Asia. They were once thought to have evolved from adapids, a more specialized and younger branch of adapiform primarily from Europe.
Lemurs rafted from Africa to Madagascar between 47 and 54 mya, whereas the lorises split from the African galagos around 40 mya and later colonized Asia. The lemuriforms, and particularly the lemurs of Madagascar, are often portrayed inappropriately as "living fossils" or as examples of "basal", or "inferior" primates. These views have historically hindered the understanding of mammalian evolution and the evolution of strepsirrhine traits, such as their reliance on smell (olfaction), characteristics of their skeletal anatomy, and their brain size, which is relatively small. In the case of lemurs, natural selection has driven this isolated population of primates to diversify significantly and fill a rich variety of ecological niches, despite their smaller and less complex brains compared to simians.
The divergence between strepsirrhines, simians, and tarsiers likely followed almost immediately after primates first evolved. Although few fossils of living primate groups – lemuriforms, tarsiers, and simians – are known from the Early to Middle Eocene, evidence from genetics and recent fossil finds both suggest they may have been present during the early adaptive radiation.The origin of the earliest primates that the simians (strepsirrhines) and tarsiers (haplorhines) both evolved from is a mystery. Both their place of origin and the group from which they emerged are uncertain. Although the fossil record demonstrating their initial radiation across the Northern Hemisphere is very detailed, the fossil record from the tropics (where primates most likely first developed) is very sparse, particularly around the time that primates and other major clades of eutherian mammals first appeared.
Lacking detailed tropical fossils, geneticists and primatologists have used genetic analyses to determine the relatedness between primate lineages and the amount of time since they diverged. Using this molecular clock, divergence dates for the major primate lineages have suggested that primates evolved more than 80–90 mya, nearly 40 million years before the first examples appear in the fossil record.
The early primates include both nocturnal and diurnal small-bodied species, and all were arboreal, with hands and feet specially adapted for maneuvering on small branches. Plesiadapiforms from the early Paleocene are sometimes considered "archaic primates", because their teeth resembled those of early primates and because they possessed adaptations to living in trees, such as a divergent big toe (hallux). Although plesiadapiforms were closely related to primates, they may represent a paraphyletic group from which primates may or may not have directly evolved, and some genera may have been more closely related to colugos, which are thought to be more closely related to primates.The first true primates (euprimates) do not appear in the fossil record until the early Eocene (~55 mya), at which point they radiated across the Northern Hemisphere during a brief period of rapid global warming known as the Paleocene–Eocene Thermal Maximum. These first primates included Cantius, Donrussellia, Altanius, and Teilhardina on the northern continents, as well as the more questionable (and fragmentary) fossil Altiatlasius from Paleocene Africa. These earliest fossil primates are often divided into two groups, adapiforms and omomyiforms. Both appeared suddenly in the fossil record without transitional forms to indicate ancestry, and both groups were rich in diversity and were widespread throughout the Eocene.
The last branch to develop were the adapiforms, a diverse and widespread group that thrived during the Eocene (56 to 34 million years ago [mya]) in Europe, North America, and Asia. They disappeared from most of the Northern Hemisphere as the climate cooled: The last of the adapiforms died out at the end of the Miocene (~7 mya).
Adapiform primates are extinct strepsirrhines that shared many anatomical similarities with lemurs. They are sometimes referred to as lemur-like primates, although the diversity of both lemurs and adapiforms do not support this analogy.Like the living strepsirrhines, adapiforms were extremely diverse, with at least 30 genera and 80 species known from the fossil record as of the early 2000s. They diversified across Laurasia during the Eocene, some reaching North America via a land bridge.They were among the most common mammals found in the fossil beds from that time. A few rare species have also been found in northern Africa. The most basal of the adapiforms include the genera Cantius from North America and Europe and Donrussellia from Europe. The latter bears the most ancestral traits, so it is often considered a sister group or stem group of the other adapiforms.Adapiforms are often divided into three major groups:
Adapids were most commonly found in Europe, although the oldest specimens (Adapoides from middle Eocene China) indicate that they most likely evolved in Asia and immigrated. They died out in Europe during the Grande Coupure, part of a significant extinction event at the end of the Eocene.
Notharctids, which most closely resembled some of Madagascar's lemurs, come from Europe and North America. The European branch is often referred to as cercamoniines. The North American branch thrived during the Eocene, but did not survive into the Oligocene. Like the adapids, the European branch were also extinct by the end of the Eocene.
Sivaladapids of southern and eastern Asia are best known from the Miocene, and the only adapiforms to survive past the Eocene/Oligocene boundary (~34 mya). Their relationship to the other adapiforms remains unclear. They had vanished before the end of the Miocene (~7 mya).The relationship between adapiform and lemuriform primates has not been clearly demonstrated, so the position of adapiforms as a paraphyletic stem group is questionable. Both molecular clock data and new fossil finds suggest that the lemuriform divergence from the other primates and the subsequent lemur-lorisoid split both predate the appearance of adapiforms in the early Eocene. New calibration methods may reconcile the discrepancies between the molecular clock and the fossil record, favoring more recent divergence dates. The fossil record suggests that the strepsirrhine adapiforms and the haplorhine omomyiforms had been evolving independently before the early Eocene, although their most basal members share enough dental similarities to suggest that they diverged during the Paleocene (66–55 mya).
Lemuriform origins are unclear and debated. American paleontologist Philip Gingerich proposed that lemuriform primates evolved from one of several genera of European adapids based on similarities between the front lower teeth of adapids and the toothcomb of extant lemuriforms; however, this view is not strongly supported due to a lack of clear transitional fossils. Instead, lemuriforms may be descended from a very early branch of Asian cercamoniines or sivaladapids that migrated to northern Africa.Until discoveries of three 40-million-year-old fossil lorisoids (Karanisia, Saharagalago, and Wadilemur) in the El Fayum deposits of Egypt between 1997 and 2005, the oldest known lemuriforms had come from the early Miocene (~20 mya) of Kenya and Uganda. These newer finds demonstrate that lemuriform primates were present during the middle Eocene in Afro-Arabia and that the lemuriform lineage and all other strepsirrhine taxa had diverged before then. Djebelemur from Tunisia dates to the late early or early middle Eocene (52 to 46 mya) and has been considered a cercamoniine, but also may have been a stem lemuriform. Azibiids from Algeria date to roughly the same time and may be a sister group of the djebelemurids. Together with Plesiopithecus from the late Eocene Egypt, the three may qualify as the stem lemuriforms from Africa.Molecular clock estimates indicate that lemurs and the lorisoids diverged in Africa during the Paleocene, approximately 62 mya. Between 47 and 54 mya, lemurs dispersed to Madagascar by rafting. In isolation, the lemurs diversified and filled the niches often filled by monkeys and apes today. In Africa, the lorises and galagos diverged during the Eocene, approximately 40 mya. Unlike the lemurs in Madagascar, they have had to compete with monkeys and apes, as well as other mammals.
The taxonomy of strepsirrhines is controversial and has a complicated history. Confused taxonomic terminology and oversimplified anatomical comparisons have created misconceptions about primate and strepsirrhine phylogeny, illustrated by the media attention surrounding the single "Ida" fossil in 2009.
Strepsirrhine primates were first grouped under the genus Lemur by Swedish taxonomist Carl Linnaeus in the 10th edition of Systema Naturae published in 1758. At the time, only three species were recognized, one of which (the colugo) is no longer recognized as a primate. In 1785, Dutch naturalist Pieter Boddaert divided the genus Lemur into two genera: Prosimia for the lemurs, colugos, and tarsiers and Tardigradus for the lorises. Ten years later, É. Geoffroy and Georges Cuvier grouped the tarsiers and galagos due to similarities in their hindlimb morphology, a view supported by German zoologist Johann Karl Wilhelm Illiger, who placed them in the family Macrotarsi while placing the lemurs and tarsiers in the family Prosimia (Prosimii) in 1811. The use of the tarsier-galago classification continued for many years until 1898, when Dutch zoologist Ambrosius Hubrecht demonstrated two different types of placentation (formation of a placenta) in the two groups.English comparative anatomist William Henry Flower created the suborder Lemuroidea in 1883 to distinguish these primates from the simians, which were grouped under English biologist St. George Jackson Mivart's suborder Anthropoidea (=Simiiformes). According to Flower, the suborder Lemuroidea contained the families Lemuridae (lemurs, lorises, and galagos), Chiromyidae (aye-aye), and Tarsiidae (tarsiers). Lemuroidea was later replaced by Illiger's suborder Prosimii. Many years earlier, in 1812, É. Geoffroy first named the suborder Strepsirrhini, in which he included the tarsiers. This taxonomy went unnoticed until 1918, when Pocock compared the structure of the nose and reinstated the use of the suborder Strepsirrhini, while also moving the tarsiers and the simians into a new suborder, Haplorhini. It was not until 1953, when British anatomist William Charles Osman Hill wrote an entire volume on strepsirrhine anatomy, that Pocock's taxonomic suggestion became noticed and more widely used. Since then, primate taxonomy has shifted between Strepsirrhini-Haplorhini and Prosimii-Anthropoidea multiple times.Most of the academic literature provides a basic framework for primate taxonomy, usually including several potential taxonomic schemes. Although most experts agree upon phylogeny, many disagree about nearly every level of primate classification.
The most commonly recurring debate in primatology during the 1970s, 1980s, and early 2000s concerned the phylogenetic position of tarsiers compared to both simians and the other prosimians. Tarsiers are most often placed in either the suborder Haplorhini with the simians or in the suborder Prosimii with the strepsirrhines. Prosimii is one of the two traditional primate suborders and is based on evolutionary grades (groups united by anatomical traits) rather than phylogenetic clades, while the Strepsirrhini-Haplorrhini taxonomy was based on evolutionary relationships. Yet both systems persist because the Prosimii-Anthropoidea taxonomy is familiar and frequently seen in the research literature and textbooks.Strepsirrhines are traditionally characterized by several symplesiomorphic (ancestral) traits not shared with the simians, particularly the rhinarium. Other symplesiomorphies include long snouts, convoluted maxilloturbinals, relatively large olfactory bulbs, and smaller brains. The toothcomb is a synapomorphy (shared, derived trait) seen among lemuriforms, although it is frequently and incorrectly used to define the strepsirrhine clade. Strepsirrhine primates are also united in possessing an epitheliochorial placenta. Unlike the tarsiers and simians, strepsirrhines are capable of producing their own vitamin C and do not need it supplied in their diet. Further genetic evidence for the relationship between tarsiers and simians as a haplorhine clade is the shared possession of three SINE markers.Because of their historically mixed assemblages which included tarsiers and close relatives of primates, both Prosimii and Strepsirrhini have been considered wastebasket taxa for "lower primates". Regardless, the strepsirrhine and haplorrhine clades are generally accepted and viewed as the preferred taxonomic division. Yet tarsiers still closely resemble both strepsirrhines and simians in different ways, and since the early split between strepsirrhines, tarsiers and simians is ancient and hard to resolve, a third taxonomic arrangement with three suborders is sometimes used: Prosimii, Tarsiiformes, and Anthropoidea. More often, the term "prosimian" is no longer used in official taxonomy, but is still used to illustrate the behavioral ecology of tarsiers relative to the other primates.In addition to the controversy over tarsiers, the debate over the origins of simians once called the strepsirrhine clade into question. Arguments for an evolutionary link between adapiforms and simians made by paleontologists Gingerich, Elwyn L. Simons, Tab Rasmussen, and others could have potentially excluded adapiforms from Strepsirrhini. In 1975, Gingerich proposed a new suborder, Simiolemuriformes, to suggest that strepsirrhines are more closely related to simians than tarsiers. However, no clear relationship between the two had been demonstrated by the early 2000s. The idea reemerged briefly in 2009 during the media attention surrounding Darwinius masillae (dubbed "Ida"), a cercamoniine from Germany that was touted as a "missing link between humans and earlier primates" (simians and adapiforms). However, the cladistic analysis was flawed and the phylogenetic inferences and terminology were vague. Although the authors noted that Darwinius was not a "fossil lemur", they did emphasize the absence of a toothcomb, which adapiforms did not possess.
Within Strepsirrhini, two common classifications include either two infraorders (Adapiformes and Lemuriformes) or three infraorders (Adapiformes, Lemuriformes, Lorisiformes). A less common taxonomy places the aye-aye (Daubentoniidae) in its own infraorder, Chiromyiformes. In some cases, plesiadapiforms are included within the order Primates, in which case Euprimates is sometimes treated as a suborder, with Strepsirrhini becoming an infraorder, and the Lemuriformes and others become parvorders. Regardless of the infraordinal taxonomy, Strepsirrhini is composed of three ranked superfamilies and 14 families, seven of which are extinct. Three of these extinct families included the recently extinct giant lemurs of Madagascar, many of which died out within the last 1,000 years following human arrival on the island.
When Strepsirrhini is divided into two infraorders, the clade containing all toothcombed primates can be called "lemuriforms". When it is divided into three infraorders, the term "lemuriforms" refers only to Madagascar's lemurs, and the toothcombed primates are referred to as either "crown strepsirrhines" or "extant strepsirrhines". Confusion of this specific terminology with the general term "strepsirrhine", along with oversimplified anatomical comparisons and vague phylogenetic inferences, can lead to misconceptions about primate phylogeny and misunderstandings about primates from the Eocene, as seen with the media coverage of Darwinius. Because the skeletons of adapiforms share strong similarities with those of lemurs and lorises, researchers have often referred to them as "primitive" strepsirrhines, lemur ancestors, or a sister group to the living strepsirrhines. They are included in Strepsirrhini, and are considered basal members of the clade. Although their status as true primates is not questioned, the questionable relationship between adapiforms and other living and fossil primates leads to multiple classifications within Strepsirrhini. Often, adapiforms are placed in their own infraorder due to anatomical differences with lemuriforms and their unclear relationship. When shared traits with lemuriforms (which may or may not be synapomorphic) are emphasized, they are sometimes reduced to families within the infraorder Lemuriformes (or superfamily Lemuroidea).The first fossil primate described was the adapiform Adapis parisiensis by French naturalist Georges Cuvier in 1821, who compared it to a hyrax ("le Daman"), then considered a member of a now obsolete group called pachyderms. It was not recognized as a primate until it was reevaluated in the early 1870s. Originally, adapiforms were all included under the family Adapidae, which was divided into two or three subfamilies: Adapinae, Notharctinae, and sometimes Sivaladapinae. All North American adapiforms were lumped under Notharctinae, while the Old World forms were usually assigned to Adapinae. Around the 1990s, two distinct groups of European "adapids" began to emerge, based on differences in the postcranial skeleton and the teeth. One of these two European forms was identified as cercamoniines, which were allied with the notharctids found mostly in North America, while the other group falls into the traditional adapid classification. The three major adapiform divisions are now typically regarded as three families within Adapiformes (Notharctidae, Adapidae and Sivaladapidae), but other divisions ranging from one to five families are used as well.
All lemuriforms possess a specialized dental structure called a “toothcomb”, with the exception of the aye-aye, in which the structure has been modified into two continually growing (hypselodont) incisors (or canine teeth), similar to those of rodents. Often, the toothcomb is incorrectly used to characterize all strepsirrhines. Instead, it is unique to lemuriforms and is not seen among adapiforms.
Lemuriforms groom orally, and also possess a grooming claw on the second toe of each foot for scratching in areas that are inaccessible to the mouth and tongue. It is unclear whether adapiforms possessed grooming claws. The toothcomb consists of either two or four procumbent lower incisors and procumbent lower canine teeth followed by a canine-shaped premolar. It is used to comb the fur during oral grooming. Shed hairs that accumulate between the teeth of the toothcomb are removed by the sublingua or "under-tongue". Lemuriforms also possess a grooming claw on the second digit of each foot for scratching. Adapiforms did not possess a toothcomb. Instead, their lower incisors varied in orientation—from somewhat procumbent to somewhat vertical—and the lower canines were projected upwards and were often prominent. Adapiforms may have had a grooming claw, but there is little evidence of this.
Like all primates, strepsirrhine orbits (eye sockets) have a postorbital bar, a protective ring of bone created by a connection between the frontal and zygomatic bones. Both living and extinct strepsirrhines lack a thin wall of bone behind the eye, referred to as postorbital closure, which is only seen in haplorhine primates. Although the eyes of strepsirrhines point forward, giving stereoscopic vision, the orbits do not face fully forward. Among living strepsirrhines, most or all species are thought to possess a reflective layer behind the retina of the eye, called a tapetum lucidum (consisting of riboflavin crystals), which improves vision in low light, but they lack a fovea, which improves day vision. This differs from tarsiers, which lack a tapetum lucidum but possess a fovea.
Strepsirrhine primates have a brain relatively comparable to or slightly larger in size than most mammals.  Compared to simians, however, they have a relatively small brain-to-body size ratio. Strepsirrhines are also traditionally noted for their unfused mandibular symphysis (two halves of the lower jaw), however, fusion of the mandibular symphysis was common in adapiforms, notably Notharctus. Also, several extinct giant lemurs exhibited a fused mandibular symphysis.
Many nocturnal species have large, independently movable ears, although there are significant differences in sizes and shapes of the ear between species. The structure of the middle and inner ear of strepsirrhines differs between the lemurs and lorisoids. In lemurs, the tympanic cavity, which surrounds the middle ear, is expanded. This leaves the ectotympanic ring, which supports the eardrum, free within the auditory bulla. This trait is also seen in adapiforms. In lorisoids, however, the tympanic cavity is smaller and the ectotympanic ring becomes attached to the edge of the auditory bulla. The tympanic cavity in lorisoids also has two accessory air spaces, which are not present in lemurs.
Both lorisoids and cheirogaleid lemurs have replaced the internal carotid artery with an enlarged ascending pharyngeal artery.
Strepsirrhines also possess distinctive features in their tarsus (ankle bones) that differentiate them from haplorhines, such as a sloping talo-fibular facet (the face where the talus bone and fibula meet) and a difference in the location of the position of the flexor fibularis tendon on the talus. These differences give strepsirrhines the ability to make more complex rotations of the ankle and indicate that their feet are habitually inverted, or turned inward, an adaptation for grasping vertical supports.
Sexual dichromatism (different coloration patterns between males and females) can be seen in most brown lemur species, but otherwise lemurs show very little if any difference in body size or weight between sexes. This lack of sexual dimorphism is not characteristic of all strepsirrhines. Some adapiforms were sexually dimorphic, with males bearing a larger sagittal crest (a ridge of bone on the top of the skull to which jaw muscles attach) and canine teeth. Lorisoids exhibit some sexual dimorphism, but males are typically no more than 20 percent larger than females.
Strepsirrhines have a long snout that ends in a moist and touch-sensitive rhinarium, similar to that of dogs and many other mammals. The rhinarium is surrounded by vibrissae that are also sensitive to touch. Convoluted maxilloturbinals on the inside of their nose filter, warm, and moisten the incoming air, while olfactory receptors of the main olfactory system lining the ethmoturbinals detect airborne smells. The olfactory bulbs of lemurs are comparable in size to those of other arboreal mammals.The surface of the rhinarium does not have any olfactory receptors, so it is not used for smell in terms of detecting volatile substances. Instead, it has sensitive touch receptors (Merkel cells). The rhinarium, upper lip, and gums are tightly connected by a fold of mucous membrane called the philtrum, which runs from the tip of the nose to the mouth. The upper lip is constrained by this connection and has fewer nerves to control movement, which leaves it less mobile than the upper lips of simians. The philtrum creates a gap (diastema) between the roots of the first two upper incisors.The strepsirrhine rhinarium can collect relatively non-volatile, fluid-based chemicals (traditionally categorized as pheromones) and transmit them to the vomeronasal organ (VNO), which is located below and in front of the nasal cavity, above the mouth. The VNO is an encased duct-like structure made of cartilage and is isolated from the air passing through the nasal cavity. The VNO is connected to the mouth through nasopalatine ducts (which communicate via the incisive foramen), which pass through the hard palate at the top, front of the mouth. Fluids traveling from the rhinarium to the mouth and then up the nasopalatine ducts to the VNO are detected, and information is relayed to the accessory olfactory bulb, which is relatively large in strepsirrhines. From the accessory olfactory bulb, information is sent to the amygdala, which handles emotions, and then to the hypothalamus, which handles basic body functions and metabolic processes. This neural pathway differs from that used by the main olfactory system.All lemuriforms have a VNO, as do tarsiers and some New World monkeys. Adapiforms exhibit the gap between the upper incisors, which indicates the presence of a VNO, but there is some disagreement over whether or not they possessed a rhinarium.
Extant strepsirrhines have an epitheliochorial placenta, where the maternal blood does not come in direct contact with the fetal chorion like it does in the hemochorial placenta of haplorhines. The strepsirrhine uterus has two distinct chambers (bicornuate). Despite having similar gestation periods to comparably sized haplorhines, fetal growth rates are generally slower in strepsirrhines, which results in newborn offspring that are as little as one-third the size of haplorhine newborns. Extant strepsirrhines also have a lower basal metabolic rate, which elevates in females during gestation, putting greater demands on the mother.Most primates have two mammary glands, but the number and positions vary between species within strepsirrhines. Lorises have two pairs, while others, like the ring-tailed lemur, have one pair on the chest (pectoral). The aye-aye also has two mammary glands, but they are located near the groin (inguinal). In females, the clitoris is sometimes enlarged and pendulous, resembling the male penis, which can make sex identification difficult for human observers. The clitoris may also have a bony structure in it, similar to the baculum (penis bone) in males. Most male primates have a baculum, but it is typically larger in strepsirrhines and usually forked at the tip.
Approximately three-quarters of all extant strepsirrhine species are nocturnal, sleeping in nests made from dead leaves or tree hollows during the day. All of the lorisoids from continental Africa and Asia are nocturnal, a circumstance that minimizes their competition with the simian primates of the region, which are diurnal. The lemurs of Madagascar, living in the absence of simians, are more variable in their activity cycles. The aye-aye, mouse lemurs, woolly lemurs, and sportive lemurs are nocturnal, while ring-tailed lemurs and most of their kin, sifakas, and indri are diurnal. Yet some or all of the brown lemurs (Eulemur) are cathemeral, which means that they may be active during the day or night, depending on factors such as temperature and predation. Many extant strepsirrhines are well adapted for nocturnal activity due to their relatively large eyes; large, movable ears; sensitive tactile hairs; strong sense of smell; and the tapetum lucidum behind the retina. Among the adapiforms, most are considered diurnal, with the exception of Pronycticebus and Godinotia from Middle Eocene Europe, both of which had large orbits that suggest nocturnality.Reproduction in most strepsirrhine species tends to be seasonal, particularly in lemurs. Key factors that affect seasonal reproduction include the length of the wet season, subsequent food availability, and the maturation time of the species. Like other primates, strepsirrhines are relatively slow breeders compared to other mammals. Their gestation period and interbirth intervals are usually long, and the young develop slowly, just like in haplorhine primates. Unlike simians, some strepsirrhines produce two or three offspring, although some produce only a single offspring. Those that produce multiple offspring tend to build nests for their young. These two traits are thought to be plesiomorphic (ancestral) for primates. The young are precocial (relatively mature and mobile) at birth, but not as coordinated as ungulates (hoofed mammals). Infant care by the mother is relatively prolonged compared to many other mammals, and in some cases, the infants cling to the mother's fur with their hands and feet.Despite their relatively smaller brains compared to other primates, lemurs have demonstrated levels of technical intelligence in problem solving that are comparable to those seen in simians. However, their social intelligence differs, often emphasizing within-group competition over cooperation, which may be due to adaptations for their unpredictable environment. Although lemurs have not been observed using objects as tools in the wild, they can be trained to use objects as tools in captivity and demonstrate a basic understanding about the functional properties of the objects they are using.
The nocturnal strepsirrhines have been traditionally described as "solitary", although this term is no longer favored by the researchers who study them. Many are considered "solitary foragers", but many exhibit complex and diverse social organization, often overlapping home ranges, initiating social contact at night, and sharing sleeping sites during the day. Even the mating systems are variable, as seen in woolly lemurs, which live in monogamous breeding pairs. Because of this social diversity among these solitary but social primates, whose level of social interaction is comparable to that of diurnal simians, alternative classifications have been proposed to emphasize their gregarious, dispersed, or solitary nature.Among extant strepsirrhines, only the diurnal and cathemeral lemurs have evolved to live in multi-male/multi-female groups, comparable to most living simians. This social trait, seen in two extant lemur families (Indriidae and Lemuridae), is thought to have evolved independently. Group sizes are smaller in social lemurs than in simians, and despite the similarities, the community structures differ. Female dominance, which is rare in simians, is fairly common in lemurs. Strepsirrhines spend a considerable amount of time grooming each other (allogrooming). When lemuriform primates groom, they lick the fur and then comb it with their toothcomb. They also use their grooming claw to scratch places they cannot reach with their mouth.Like New World monkeys, strepsirrhines rely on scent marking for much of their communication. This involves smearing secretions from epidermal scent glands on tree branches, along with urine and feces. In some cases, strepsirrhines may anoint themselves with urine (urine washing). Body postures and gestures may be used, although the long snout, non-mobile lips, and reduced facial enervation restrict the use of facial expressions in strepsirrhines. Short-range calls, long-range calls, and alarm calls are also used. Nocturnal species are more constrained by the lack of light, so their communication systems differ from those of diurnal species, often using long-range calls to claim their territory.
Living strepsirrhines are predominantly arboreal, with only the ring-tailed lemur spending considerable time on the ground. Most species move around quadrupedally (on four legs) in the trees, including five genera of smaller, nocturnal lemurs. Galagos, indriids, sportive lemurs, and bamboo lemurs leap from vertical surfaces, and the indriids are highly specialized for vertical clinging and leaping. Lorises are slow-moving, deliberate climbers.Analyses of extinct adapiforms postcranial skeletons suggest a variety of locomotor behavior. The European adapids Adapis, Palaeolemur, and Leptadapis shared adaptations for slow climbing like the lorises, although they may have been quadrupedal runners like small New World monkeys. Both Notharctus and Smilodectes from North America and Europolemur from Europe exhibit limb proportions and joint surfaces comparable to vertical clinging and leaping lemurs, but were not as specialized as indriids for vertical clinging, suggesting that they ran along branches and did not leap as much. Notharctids Cantius and Pronycticebus appear to have been agile arboreal quadrupeds, with adaptations comparable to the brown lemurs.
Primates primarily feed on fruits (including seeds), leaves (including flowers), and animal prey (arthropods, small vertebrates, and eggs). Diets vary markedly between strepsirrhine species. Like other leaf-eating (folivorous) primates, some strepsirrhines can digest cellulose and hemicellulose. Some strepsirrhines, such as the galagos, slender lorises, and angwantibos, are primarily insectivorous. Other species, such as fork-marked lemurs and needle-clawed bushbabies, specialize on tree gum, while indriids, sportive lemurs, and bamboo lemurs are folivores. Many strepsirrhines are frugivores (fruit eaters), and others, like the ring-tailed lemur and mouse lemurs, are omnivores, eating a mix of fruit, leaves, and animal matter.Among the adapiforms, frugivory seems to have been the most common diet, particularly for medium-sized to large species, such as Cantius, Pelycodus and Cercamonius. Folivory was also common among the medium and large-sized adapiforms, including Smilodectes, Notharctus, Adapis and Leptadapis. Sharp cusps on the teeth of some of the smaller adapiforms, such as Anchomomys and Donrussellia, indicate that they were either partly or primarily insectivorous.
The now extinct adapiform primates were primarily found across North America, Asia, and Europe, with a few species in Africa. They flourished during the Eocene when those regions were more tropical in nature, and they disappeared when the climate became cooler and drier. Today, the lemuriforms are confined in the tropics, ranging between 28° S to 26° N latitude. Lorises are found both in equatorial Africa and Southeast Asia, while the galagos are limited to the forests and woodlands of sub-Saharan Africa. Lemurs are endemic to Madagascar, although much of their diversity and habitat has been lost due to recent human activity.As with nearly all primates, strepsirrhines typically reside in tropical rainforests. These habitats allow strepsirrhines and other primates to evolve diverse communities of sympatric species. In the eastern rainforests of Madagascar, as many as 11 or 12 species share the same forests, and prior to human arrival, some forests had nearly double that diversity. Several species of lemur are found in drier, seasonal forests, including the spiny forest on the southern tip of the island, although the lemur communities in these regions are not as rich.
Like all other non-human primates, strepsirrhines face an elevated risk of extinction due to human activity, particularly deforestation in tropical regions. Much of their habitat has been converted for human use, such as agriculture and pasture. The threats facing strepsirrhine primates fall into three main categories: habitat destruction, hunting (for bushmeat or traditional medicine), and live capture for export or local exotic pet trade. Although hunting is often prohibited, the laws protecting them are rarely enforced. In Madagascar, local taboos known as fady sometimes help protect lemur species, although some are still hunted for traditional medicine.
In 2012, the International Union for Conservation of Nature (IUCN) announced that lemurs were the "most endangered mammals", due largely to elevated illegal logging and hunting following a political crisis in 2009. In Southeast Asia, slow lorises are threatened by the exotic pet trade and traditional medicine, in addition to habitat destruction. Both lemurs and slow lorises are protected from commercial international trade under CITES Appendix I.

Stretford is a town in Trafford, Greater Manchester, England, on flat ground between the River Mersey and the Manchester Ship Canal,  3.8 miles (6.1 km) southwest of Manchester city centre, 3.0 miles (4.8 km) south of Salford and 4.2 miles (6.8 km) northeast of Altrincham. Stretford borders Chorlton-cum-Hardy to the east, Urmston to the west, Salford to the north, and Sale to the south. The Bridgewater Canal bisects the town.
Historically in Lancashire, in the 19th century Stretford was an agricultural village, known locally as Porkhampton due to the large number of pigs produced for the Manchester market. It was also an extensive market-gardening area, producing more than 500 long tons (508 t) of vegetables each week for sale in Manchester by 1845. The arrival of the Manchester Ship Canal in 1894, and the subsequent development of the Trafford Park industrial estate, accelerated the industrialisation that had begun in the late 19th century. By 2001 less than one per cent of Stretford's population was employed in agriculture.
Stretford has been the home of Manchester United Football Club since 1910, and of Lancashire County Cricket Club since 1864. Notable residents have included the industrialist, philanthropist, and Manchester's first multi-millionaire John Rylands, the suffragette Emmeline Pankhurst, the painter L. S. Lowry, Morrissey, Joy Division frontman Ian Curtis and Jay Kay of Jamiroquai.
The origin of the name Stretford is "street" (Old English stræt) on a ford across the River Mersey. The principal road through Stretford, the A56 Chester Road, follows the line of the old Roman road from Deva Victrix (Chester) to Mancunium (Manchester), crossing the Mersey into Stretford at Crossford Bridge, built at the location of the ancient ford.
The earliest evidence of human occupation around Stretford comes from Neolithic stone axes found in the area, dating from about 2000 BC. Stretford was part of the land occupied by the Celtic Brigantes tribe before and during the Roman occupation, and lay on their border with the Cornovii on the southern side of the Mersey. By 1212, there were two manors in the area now called Stretford. The land in the south, close to the River Mersey, was held by Hamon de Mascy, while the land in the north, closer to the River Irwell, was held by Henry de Trafford. In about 1250, a later Hamon de Mascy gave the Stretford manor to his daughter, Margery. She in turn, in about 1260, granted Stretford to Richard de Trafford at a rent of one penny. The de Mascy family shortly afterwards released all rights to their lands in Stretford to Henry de Trafford, the Trafford family thus acquiring the whole of Stretford, since when the two manors descended together.The de Trafford family leased out large parts of the land, much of it to tenants who farmed at subsistence levels. Although there is known to have been a papermill operating in 1765, the area remained largely rural until the early 20th-century development of Trafford Park in the Old Trafford district north of the town. Until then Stretford "remained in the background of daily life in England", except for a brief cameo role during the Jacobite rising of 1745, when Crossford Bridge was destroyed to prevent a crossing by Bonnie Prince Charlie's army during its abortive advance on London; the bridge was quickly rebuilt.Until the 1820s one of Stretford's main cottage industries was the hand-weaving of cotton. There were reported at one time to have been 302 handlooms operating in Stretford, providing employment for 780 workers, but by 1826 only four were still in use, as the mechanised cotton mills of nearby Manchester replaced handlooms. As Manchester continued to grow, it offered a good and easily accessible market for Stretford's agricultural products, in particular rhubarb, once known locally as Stretford beef. By 1836 market gardening had become so extensive around Stretford that one writer described it as the "garden of Lancashire"; in 1845 more than 500 tons of vegetables were being produced for the Manchester market each week. Stretford also became well known for its pig market and the production of black puddings, leading to the village being given the nickname of Porkhampton. A local dish, known as Stretford goose, was made from pork stuffed with sage and onions. During the 1830s, between 800 and 1,000 pigs a week were being slaughtered for the Manchester market.Situated on the border with Manchester, Stretford became a fashionable place to live in the mid-19th century. Large recreation areas were established, such as the Royal Botanical Gardens, opened in 1831. The gardens were sited in Old Trafford on the advice of scientist John Dalton, because the prevailing southwesterly wind kept the area clear of the city's airborne pollution. In 1857, the gardens hosted the Art Treasures Exhibition, the largest art exhibition ever held in the United Kingdom. A purpose-built iron and glass building was constructed at a cost of £38,000 to house the 16,000 exhibits. The gardens were also chosen as a site for the Royal Jubilee Exhibition of 1887, celebrating Queen Victoria's 50-year reign. The exhibition ran for more than six months and was attended by more than 4.75 million visitors. The gardens were converted into an entertainment resort in 1907, and hosted the first speedway meeting in Greater Manchester on 16 June 1928. There was also greyhound racing from 1930, and an athletics track. The complex was demolished in the late 1980s, and all that remains is the entrance gates, close to what is now the White City Retail Park. The gates were designated a Grade II listed structure in 1987.
The arrival of the Manchester Ship Canal in 1894, and the subsequent development of the Trafford Park industrial estate in the north of the town – the first planned industrial estate in the world – had a substantial effect on Stretford's growth. The population in 1891 was 21,751, but by 1901 it had increased by 40% to 30,436 as people were drawn to the town by the promise of work in the new industries at Trafford Park.During the Second World War Trafford Park was largely turned over to the production of matériel, including the Avro Manchester heavy bomber, and the Rolls-Royce Merlin engines used to power both the Spitfire and the Lancaster. That resulted in Stretford being the target for heavy bombing, particularly during the Manchester Blitz of 1940. On the nights of 22/23 and 23/24 December 1940 alone, 124 incendiaries and 120 high-explosive bombs fell on the town, killing 73 people and injuring many more. Among the buildings damaged or destroyed during the war were Manchester United's Old Trafford football ground, All Saints' Church, St Hilda's Church, and the children's library in King Street. Smoke generators were set up in the north of the town close to Trafford Park in an effort to hide it from enemy aircraft, and 11,900 children were evacuated to safer areas in Lancashire, Cheshire, Derbyshire, and Staffordshire, along with their teachers and supervisors. A memorial to those residents who lost their lives in the bombing was erected in Stretford Cemetery in 1948, over the communal grave of the 17 unidentified people who were killed in the blitz of December 1940.Between 1972 and 1975, what is now a closed B&Q store in Great Stone Road was the 3,000-capacity Hardrock Theatre and Village Discothèque, hosting some of that period's major artists in their prime. Led Zeppelin, David Bowie, Bob Marley, Elton John, Hawkwind, Yes, Chaka Khan, Curved Air and Lou Reed were amongst those who appeared. Tangerine Dream was the last band to perform at the Hardrock, on 19 October 1975. In more recent years, Lancashire Cricket Club's Old Trafford ground, next door, has provided a concert venue for bands such as Oasis, Foo Fighters, The Cure, Radiohead, Coldplay, Arctic Monkeys and Pixies.
Stretford's growth was fuelled by the transport revolutions of the 18th and especially the 19th century: the Bridgewater Canal reached Stretford in 1761, and the railway in 1849. The completion of the Manchester South Junction and Altrincham Railway (MSJAR) in 1849, passing through Stretford, led to the population of the town nearly doubling in a decade, from 4,998 in 1851 to 8,757 by 1861.Because Stretford is situated on the main A56 road between Chester and Manchester many travellers passed through the village, and as this traffic increased, more inns were built to provide travellers with stopping places. One of the earliest forms of public transport through Stretford was the stagecoach; the Angel Hotel, on the present day site of what used to be the Bass Drum public house, was one of the main stopping places for stagecoaches in Stretford, and the Trafford Arms was another. Horse-drawn omnibuses replaced the stagecoach service through Stretford in 1845. The Manchester Carriage Company's tramway from Manchester to Stretford was built in 1879, terminating at the Old Cock Hotel on the A56 road, next to which a small depot was built to house the cars and horses. A 1900 timetable shows that trams left for Manchester every 10 minutes between 8:00 am and 10:15 pm. The horse-drawn trams were replaced with electric trams in 1902, and after the Second World War the trams were replaced by buses.The MSJAR railway line through Stretford was electrified in 1931 and converted to light rail operation in 1992, when it became part of the Manchester Metrolink tram network. The first Metrolink tram through Stretford ran on 15 June 1992.
Stretford was part of the ancient parish of Manchester, within the historic county boundaries of Lancashire. Following the Poor Law Amendment Act of 1834, a national scheme for dealing with the relief of the poor, Stretford joined the Chorlton Poor Law Union in 1837, one of three such unions in Manchester, before transferring to the Barton-upon-Irwell Poor Law Union in 1849. In 1867, Stretford Local Board of Health was established, assuming responsibility for the local government of the area in 1868. The board's responsibilities included sanitation and the maintenance of the highways, and it had the authority to levy rates to pay for those services. The local board continued in that role until it was superseded by the creation of Stretford Urban District Council in 1894, as a result of the Local Government Act 1894.
Stretford Urban District became the Municipal Borough of Stretford in 1933, giving it borough status in the United Kingdom. Stretford Borough Council was granted its arms on 20 February 1933. The roses are the red roses of Lancashire, and the lion in the centre represents John of Gaunt, 1st Duke of Lancaster. Above the lion are a crossed flail and scythe; the flail comes from the arms of the de Trafford family; the scythe is a reminder of the agricultural history of the area; the thunderbolts above represent the importance of electricity in Stretford's industrial development. The boat at the bottom represents Stretford's links to the sea via the Manchester Ship Canal.In 1974, as a result of the Local Government Act 1972, the Municipal Borough of Stretford was abolished and Stretford has, since 1 April 1974, formed part of the Metropolitan Borough of Trafford, in Greater Manchester. Trafford Town Hall – previously Stretford Town Hall – is the administrative centre of Trafford.
The constituency of Stretford was created in 1885, and existed until 1997, when it was replaced by the present constituency of Stretford and Urmston. Kate Green, a member of the Labour Party, became the MP at the 2010 General Election, with a majority of 8,935, representing 48.6% of the vote. She retained the seat at the 2017 General Election with an increased majority of 19,705, which represents 66.8% of the vote. The Conservatives took 27.0% of the vote, UKIP 2.2%, the Liberal Democrats 2.0%, the Green Party 1.3%, and the Christian Party 0.2%.Stretford is one of the four major urban areas in Trafford; the other three are Altrincham, Sale and Urmston. The area historically known as Stretford, between the River Irwell in the north and the River Mersey in the south, has since 2004 been divided between the Trafford local government wards of Clifford, Longford, Gorse Hill, and Stretford. Each ward is represented by three local councillors, giving Stretford 12 of the 63 seats on Trafford Council. The wards elect in thirds on a four yearly cycle. As of the 2014 local elections, all 12 councillors representing the Stretford area are members of the Labour Party.
Stretford occupies an area of 4.1 square miles (10.6 km2), just north of the River Mersey, at 53°26′48″N 2°18′31″W (53.4466, −2.3086). The area is generally flat, sloping slightly southwards towards the river valley, and is approximately 150 feet (46 m) above sea level at its highest point. The most southerly part of Stretford lies within the flood plain of the River Mersey, and so has historically been prone to flooding. A great deal of flood mitigation work has been carried out in the Mersey Valley since the 1970s, with the stretch of the Mersey through Stretford canalised to speed up the passage of floodwater. Emergency floodbasins have also been constructed, Sale Water Park being a prominent local example, lying immediately to the south of Stretford.
Stretford comprises the local areas of Old Trafford, Gorse Hill, Trafford Park and Firswood. Its climate is generally temperate, with few extremes of temperature or weather. The mean temperature is slightly above average for the United Kingdom. Annual rainfall and average amount of sunshine are both slightly below the average for the UK.Stretford's built environment developed along the A56 road in two separate sections, corresponding to the original two manors. The area in the south, near to the border with Sale, grew around the church of St Matthew – an old alternative name for the town was Stretford St. Matthew. The northern part of Stretford was centred on Old Trafford, with undeveloped countryside separating them. During the 19th century, the sections merged.
The western terminus of the early medieval linear earthwork Nico Ditch is in Hough Moss, just to the east of Stretford; it was probably used as an administrative boundary and dates from the 8th or 9th century.
As at the 2011 UK census, the Stretford area wards of Clifford, Gorse Hill, Longford and Stretford had a total population of 46,910 and a population density of 8,907 persons per square mile (3,439 per km²).Stretford residents had an average age of 36 years, younger than the 39.3 Trafford average. For every 100 females, there were 97.8 males. Of all residents, 42% were single (never married): in Trafford, 33% were single. Of the 19,209 households, 33% were one-person households, 56% were married couples with dependent children, and 15% were lone parents with dependent children. Of those aged 16–74 in Stretford, 23% had no academic qualifications, higher than the 18% in all of Trafford.With 78% of residents born in the United Kingdom, there is a relatively high proportion of foreign-born residents reported. There is also a high proportion of non-white people, as 63% of residents were recorded as white. The largest minority group was Asian, at 21% of the population.In 1931, 19% of Stretford's population was middle class and 20% working class compared to 14% middle class and 36% working class nationally. The rest of the population was made up of clerical workers and skilled manual workers. By 1971, the middle class in Stretford had declined steadily to 15% whilst the working class had grown to 31% compared to 24% middle class and 26% working class nationally.
Until the end of the 19th century Stretford was a largely agricultural village. The development of the Trafford Park industrial estate in the north of the town, beginning in the late 19th century, had a significant effect on Stretford's subsequent development. At its peak in 1945 the park employed an estimated 75,000 workers; housing and other amenities had to be constructed on what had previously been agricultural land. Trafford Park is still a very significant source of employment, containing an estimated 1,400 companies and employing about 44,000 people.The main shopping centre is Stretford Mall in the commercial centre of Stretford, previously known as Stretford Arndale. It was opened in 1969 and changed its name in 2003. Stretford Mall was built on the site of the original shopping centre in the former King Street. The Trafford Centre, a large shopping and leisure complex opened in September 1998, lies to the northwest of Stretford. Frequent shuttle buses run between Stretford Metrolink tram station and The Trafford Centre, about 2.5 miles (4.0 km) away.
According to the 2011 UK census, the industry of employment of residents in Stretford was 17% retail and wholesale, 11% health and social work, 11% education, 7% manufacturing, 6% transport and storage, 6% public administration and defence, 6% professional, scientific and technical activities, 5% hotels and restaurants, 5% construction, 5% finance, 1% energy and water supply, 0.06% agriculture and 5% other. This is roughly in line with national figures, except for the town's relatively low percentage of agricultural workers.
The 2011 census recorded the economic activity of residents aged 16–74 as 39.3% in full-time employment, 13.6% in part-time employment, 7.5% self-employed, 5.7% unemployed, 5.9% students, 9.5% retired, 5.6% looking after home or family, and 6.1% permanently sick or disabled. The 5.7% unemployment rate in Stretford was high compared with the national rate of 3.2%. According to the Office for National Statistics estimates, between April 2001 and March 2002 the average gross income of households in Stretford was £415 per week (£21,664 per year).
Longford Cinema, opposite Stretford Mall, on the eastern side of the A56 Chester Road, is perhaps the most visually striking building in the town. Designed by the architect Henry Elder, it was the height of Art Deco fashion when it was opened by the Mayor of Stretford in 1936. Its unusual "cash register" frontage was intended to symbolise the business aspect of show business.The building incorporated many modern features, such as sound-proofing and under-seat heating, and it was also the first cinema in Britain to make use of concealed neon lighting. It had a seating capacity of 1,400 in the stalls and 600 in the circle, with a further 146 seats in the café area. When built, the cinema had a short pedestrian approach to the facade, which was removed when the A56 was widened. During the Second World War the building was used for concerts, including one given by a young Julie Andrews. It also played host to the Hallé Orchestra after the orchestra's own home, the Free Trade Hall, was bombed and severely damaged during the Manchester Blitz of 1940.
After a change of ownership in 1950, the cinema was renamed the Stretford Essoldo. It continued to operate as a cinema until 1965, when it was converted into a bingo hall, which it remained until its closure in 1995. The building has been unused since then. It was designated a Grade II listed building in 1994.In 2017 a proposal was put forward by Trafford Council to bring the Essoldo back into use as part of the new University Academy 92, to provide student amenities and other community facilities such as an enhanced library.
The Great Stone, which gave its name to the Great Stone Road, where it was located until being moved in 1925, is one of Stretford's most easily overlooked landmarks. The stone is composed of millstone grit and was probably deposited as a glacial erratic. It is rectangular in shape, about 5 feet (2 m) wide, 2 feet (1 m) deep, and 3 feet (1 m) tall, with two 7-inch (18 cm) deep rectangular slots cut into its upper surface.
Several suggestions have been made for the history of the Great Stone. There was a succession of plagues in Manchester from the 14th century onwards, and during the Great Plague of 1655–56 the holes in the top of the stone were filled with vinegar or holy water, through which coins were passed in the belief that would halt the spread of the disease. But the holes are probably too deep for that to have been the stone's original purpose. It may have been a marker on the Roman road between Northwich and Manchester, or some kind of boundary marker. The Great Stone is also thought to have been the base of an Anglo-Saxon cross shaft. A local legend had it that the stone was slowly sinking into the earth, and that its ultimate disappearance would mark the end of the world.When the Great Stone Road was widened in the late 19th century, the stone was moved back from the road slightly. In 1925, the stone was moved again, to its current location outside the North Lodge of Gorse Hill Park, about 328 feet (100 m) from its historical location. The stone is a Grade II listed structure.
Stretford Cenotaph, opposite the Chester Road entrance to Gorse Hill Park, was built as a memorial to the 580 Stretford men who lost their lives in the First World War. Their names and regiments are listed on a large bronze plaque on the wall behind the cenotaph. It was formally unveiled in 1923, by the Earl of Derby, Secretary of State for War.The cenotaph is 24 feet (7 m) high and 11 feet (3 m) wide at its base. It cost £2,000 to build, raised by public subscription and a donation from the Stretford Red Cross. The memorial bears the legend "They died that we might live" on one side, and "In memory of the heroic dead" on the other. It is a Grade II listed structure.
Longford Park is the largest park in Trafford, at 54 acres (22 ha). It includes a pets' corner, wildlife garden, bowling greens, tennis and basketball courts, children's play areas, disc golf course, and an athletics stadium, and was the finishing point of the annual Stretford Pageant. It was the site of a royal garden party in 1977, the Silver Jubilee of Elizabeth II. As the Manchester–Stretford boundary ran across the park until the Boundary Commission moved it in 1987, part of it (including the Athletics Stadium) was in Chorlton-cum-Hardy, although it was always administered by Trafford MBC.
Longford Park was the home of John Rylands, industrialist, philanthropist, and Manchester's first multi-millionaire, from 1855 until his death in 1888 and of his widow Enriqueta Augustina Rylands until her death in 1908. The house Rylands constructed in the park in 1857, Longford Hall, was demolished in 1995. It replaced an earlier house of the same name that had been the residence of Thomas Walker (died 1817) and subsequently of his sons Thomas (died 1836) and Charles. Today only the front porch, coach house, and the stable buildings remain. The estate and hall were sold to Stretford Council in 1911 after a poll of ratepayers, and the park was opened to the public the following year.Longford Park is the home of Stretford parkrun; a free, weekly 5k running event.
Stretford Public Hall was built in 1878 by John Rylands. It was designed by N. Lofthouse and is on the western side of the A56 Chester Road, opposite the Longford Cinema. Stretford's first public lending library was established in the building in 1883. On the death of Rylands in 1888, his widow placed the building at the disposal of the local authority for a nominal rent, and on her own death in 1908, the building was bought by Stretford Council for £5,000.Public baths were built to the rear of the building, accessed via Cyprus Street. In 1940 the new Stretford Library was opened on King Street, and the public hall was rendered surplus. The building re-opened in March 1949 as the Stretford Civic Theatre, with a well-equipped stage for the use of local groups. After the Stretford Leisure Centre opened in 1983 now Stretford Sports Village, the Cyprus Street Baths wing fell into disuse, and was demolished. The remainder of the building began to fall into disrepair, despite being designated a Grade II listed structure in 1987, until Trafford Council refurbished and converted the hall to serve as council offices in the mid-1990s. It was re-opened in 1997, once again named Stretford Public Hall.
Stretford Cemetery was designed by John Shaw and opened in 1885. Its chapel is in the Decorated style, designed by architects Bellamy & Hardy, and quite elaborate. On the western side is a memorial to the casualties of the Second World War and to the east a newer section of the cemetery.
Trafford Town Hall stands in a large site at the junction of Talbot Road and Warwick Road, directly opposite the Old Trafford Cricket Ground. Work on the building, designed by architects Bradshaw Gass & Hope of Bolton, began on 21 August 1931.The town hall officially came into use as Stretford Town Hall on the granting of Stretford's charter, on 16 September 1933. In 1974, on the formation of the new Trafford Metropolitan Borough, Stretford Town Hall was adopted as the base for the new council and renamed Trafford Town Hall; it was designated a Grade II listed building in 2007.
The Union Church was formed in 1862, with John Rylands as its patron; he laid the foundation stone of its building in Edge Lane, close to Longford Park's southern entrance, in 1867. In the latter part of the 20th century the church was converted into office accommodation but by the early 21st century was standing empty. In 2012 it was restored to its original use a church having been bought by the Church of Christ – Iglesia ni Cristo.
Stretford Metrolink station is part of the Manchester Metrolink tram system, and lies on the Altrincham to Bury line. Trams leave about every six minutes between 7:15 and 18:30, and every 12 minutes at other times of the day. The nearest main line railway station is Trafford Park, on the Liverpool to Manchester line. Services are roughly every two hours in each direction, with extra services calling during the peak hours. The 20-acre (8 ha) Trafford Park Euroterminal rail freight terminal, opened in 1993, is in the Gorse Hill area of Stretford. It cost £11 million and has the capacity to deal with 100,000 containers a year. The containers are handled by two huge gantry cranes, the noise from which has led to complaints from some local residents.The town has good access to the motorway network. Junction 7 of the M60 is just to the north of Stretford's boundary with Sale, and the A56 road gives easy access to the south as well as to Manchester city centre in the other direction. Cycle paths exist as part of the Trafford cycle initiative.
Manchester Airport, the busiest in the UK outside London, is about nine miles (14 km) to the south of Stretford.
Along with the rest of Trafford, Stretford maintains a selective education system assessed by the 11-plus examination.
The proportion of pupils leaving Stretford Grammar School with five or more GCSEs at grades A*–C in 2006, was 98.3%, compared to an average of 66.7% for all secondary schools in Trafford and a national UK average of 61.3%. The proportion of students from minority ethnic backgrounds, and for whom English is an additional language, is much higher than the average. Stretford Grammar was awarded specialist Science College status in September 2005. The school was assessed as "good" in its April 2015 Ofsted report.Stretford High School Community Languages College, like Stretford Grammar, has a much higher proportion than the national average of pupils with a first language other than English, many of them being either asylum seekers or refugees. In 2004 Stretford High School was made subject to special measures, as it was considered not to be providing an adequate education for its pupils. Substantial improvement has taken place since then; the school was assessed as "satisfactory" in its November 2005 Ofsted report and was removed from special measures. Further improvements saw Stretford High School gain an "outstanding" assessment from Ofsted, following its February 2008 inspection. GCSE results also placed the school in the top 1% of schools in the country for adding value to its students.Stretford also has the specialist Arts College, Lostock College.
Plans to build a new university in the town, to be known as University Academy 92, were announced in September 2017. A branch of Lancaster University, it hopes to welcome the first of its anticipated 6500 students in September 2019. The campus is to be built on the Kelloggs headquarters site on Talbot Road, which has already been acquired by Trafford Council for £12 million.
The date of the first church to be built in Stretford is unrecorded, but in a lease dated 1413, land is described as lying next to a chapel. Many of the present day churches in the area were constructed during the late 19th and early 20th century, as the population of Stretford began to grow.
Methodism was a significant influence in 19th-century Stretford, but of the seventeen churches in the town today, only one is Methodist whereas five are Roman Catholic. The Catholic mission in Stretford was begun in 1859, in a small chapel on Herbert Street.As at the 2011 UK census, 51% of Stretford residents reported themselves as being Christian, 18% as Muslim, and 3% as Sikh. No other religion was represented at higher than 1% of the population, with 20% reporting themselves as having no religion.
Stretford is in the Roman Catholic Diocese of Salford, and the Anglican Diocese of Manchester.There are two Grade II listed churches in Stretford: the Church of St Ann and the Church of St Matthew. St Ann's is a Roman Catholic church, built in 1862–67 by E. W. Pugin for Sir Humphrey and Lady Annette de Trafford. It was officially opened by Bishop William Turner on 22 November 1863, and was consecrated in June 1867. Features include a historic organ built by Jardine & Co (1867) and a good number of fine stained glass windows by Hardman & Co of Birmingham. St Matthew's church was built in 1842 by W. Hayley in the Gothic Revival style, with additional phases in 1869, 1906 and 1922.
Stretford has been the home of Manchester United Football Club since 1910, when the club moved to its present Old Trafford ground, the western end of which is still unofficially called the Stretford End.
Old Trafford was originally the home of Manchester Cricket Club, but became the home of Lancashire County Cricket Club in 1864 upon that club's formation. The ground is on Talbot Road, Stretford, where it has been since 1856. Similar to its counterpart, one end of the Old Trafford cricket ground is called the Stretford End. It has been a test venue since 1884 and has hosted three World Cup semi-finals. After the 2005 Ashes Test, when more than 20,000 fans had to be turned away, the decision was made to increase the ground's capacity from 20,000 to 25,000. Initial plans included building a new stadium on the site of Trafford Town Hall, opposite the present ground. Trafford Council voted against the demolition of the town hall and instead, in 2007, signed an agreement jointly with Lancashire County Cricket Club, Ask Developments, and Tesco, to redevelop the ground on its present site.  The new cricket ground will be at the heart of a 750,000 square feet (69,677 m2) development that will also include business space, residential, retail, hotel and leisure facilities. More than £25 million is expected to be invested in the redevelopments at Old Trafford.Stretford Stadium, adjoining Longford Park, is the home of Trafford Athletic Club. Trafford is one of the UK's top athletic clubs, with more than 100 members having competed at international level.The Stretford Sports Village run by Trafford Community Leisure Trust comprises two main centres: the original Stretford Leisure Centre, now called the Chester Centre, and the facility at Stretford High School called the Talbot Centre. The Stretford Sports Village is between Manchester United Football Club's Old Trafford Stadium and Lancashire County Cricket Club's Old Trafford Ground. The centres have a 25-metre main pool, a 20-metre children's pool, four gyms, a table tennis room, twelve badminton courts, two five-a-side courts, a spinning studio, practice hall, training rooms, community room, a cafe, an outdoor full-size floodlight artificial turf pitch and a full-size grass pitch. Trafford Water Sports Centre lies just across Stretford's southern border with Sale, about one mile (1.6 km) from Stretford town centre.
Although Stretford town centre is busy during the day, there is very little in the way of a night-time economy. There are no restaurants or other entertainments except for a number of public houses and members-only social clubs. There are two public libraries: Greatstone Library, part of Stretford Sports Village, and Stretford Library, both run by Trafford Council.
The Stretford Pageant is an annual Rose Queen festival held on the last Saturday of June; the inaugural pageant was staged in 1919. There is a procession of decorated floats through the streets, collecting money for local charities and ending at Longford Park, where the Rose Queen is crowned. The tradition of the Rose Queen derives from an earlier event organised by St Peter's Church from 1909 until the pageant began in 1919. Various other entertainments are provided in the park on the day of the pageant, such as a fun fair and a car boot sale. Stretford Pageant, along with similar events in other parts of Trafford, is under threat because of the council's proposals to reduce funding and support for such events in the future.The Stretford Wives is a television drama that was broadcast by the BBC in August 2002, watched by 5.7 million viewers. Written by Danny Brocklehurst, it is the story of three sisters living in Stretford, although most of the filming took place in nearby Salford. The programme received a mixed critical reception.
The Stretford process was developed at the North-Western Gas Board's laboratories in Stretford during the 1940s. It was the first liquid phase oxidation process for removing hydrogen sulphide (H2S) from town gas to be commercially successful. Many Stretford plants were built worldwide.
Policing in Stretford is the responsibility of the Greater Manchester Police, who have their headquarters in the town. The force's "M" Division, responsible for policing in Trafford, is also based in Stretford, close to Trafford Town Hall.Waste management is co-ordinated by the local authority via the Greater Manchester Waste Disposal Authority.
Perhaps fittingly for an area so close to Trafford Park, the world's first planned industrial estate, one of the world's first industrial espionage agents, John Holker, was born in Stretford, in 1719.Two of Stretford's famous residents were the suffragette Emmeline Pankhurst and the painter L. S. Lowry, who was born in Stretford in 1887. Manchester's first multi-millionaire John Rylands and his wife Enriqueta Augustina Rylands lived at Longford Hall in Stretford during the later parts of their lives. The radical firebrand socialist, and later post-war politician Herschel Lewis Austin (1911–1974) served Stretford as a Labour Member of Parliament between 1945 and 1950.
Sir Walter Baldwin Spencer KCMG (1860–1929) a British-Australian biologist and anthropologist was born in Stretford, as were ABC's lead singer Martin Fry, rock climber Derek Hersey and television actor John Comer, best known for his role as café owner Sid in the BBC sitcom Last of the Summer Wine.Musicians who have lived in the area include Morrissey, the front man of 1980s alternative rock band the Smiths, whose family moved to King's Road in Stretford when he was 10 years old. Ian Curtis, the lead singer of Joy Division, was born in Stretford. Jay Kay, lead singer and songwriter of Jamiroquai, was born in Stretford in 1969. Oscar-nominated film maker Mark Gill was raised in Stretford, where in 2016 he made a film about Morrissey's early life.A number of Manchester United players, including some of those who died in the Munich air disaster of February 1958, lived in lodgings at 19 Gorse Avenue. A blue plaque was unveiled at the house by former lodger and Munich survivor Sir Bobby Charlton in recognition of the house's association with Manchester United. The owner of the house during the 1950s was Margaret Watson, but by the time of the plaque's unveiling more than 50 years later it was occupied by a different family.
Contemporary documentary on the Borough of Stretford, including footage of the 1933 Charter Day celebrations

The structural history of the Roman military concerns the major transformations in the organization and constitution of ancient Rome's armed forces, "the most effective and long-lived military institution known to history." From its origins around 800 BC to its final dissolution in AD 476 with the demise of the Western Roman Empire, Rome's military organization underwent substantial structural change. At the highest level of structure, the forces were split into the Roman army and the Roman navy, although these two branches were less distinct than in many modern national defense forces. Within the top levels of both army and navy, structural changes occurred as a result of both positive military reform and organic structural evolution. These changes can be divided into four distinct phases.
The army was derived from obligatory annual military service levied on the citizenry, as part of their duty to the state. During this period, the Roman army would wage seasonal campaigns against largely local adversaries.Phase II
As the extent of the territories falling under Roman control expanded and the size of the forces increased, the soldiery gradually became salaried professionals. As a consequence, military service at the lower (non-salaried) levels became progressively longer-term. Roman military units of the period were largely homogeneous and highly regulated. The army consisted of units of citizen infantry known as legions (Latin: legiones) as well as non-legionary allied troops known as auxilia. The latter were most commonly called upon to provide light infantry, logistical, or cavalry support.Phase III
At the height of the Roman Empire's power, forces were tasked with manning and securing the borders of the vast provinces which had been brought under Roman control. Serious strategic threats were less common in this period and emphasis was placed on preserving gained territory. The army underwent changes in response to these new needs and became more dependent on fixed garrisons than on march-camps and continuous field operations.Phase IV
As Rome began to struggle to keep control over its sprawling territories, military service continued to be salaried and professional for Rome's regular troops. However, the trend of employing allied or mercenary elements was expanded to such an extent that these troops came to represent a substantial proportion of the armed forces. At the same time, the uniformity of structure found in Rome's earlier military disappeared. Soldiery of the era ranged from lightly armed mounted archers to heavy infantry, in regiments of varying size and quality. This was accompanied by a trend in the late empire of an increasing predominance of cavalry rather than infantry troops, as well as a requirement for more mobile operations. In this period there was more focus (on all frontiers but the east) on smaller units of independently-operating troops, engaging less in set-piece battles and more in low-intensity, guerilla actions.
According to the historians Livy and Dionysius of Halicarnassus, writing at a far later date, the earliest Roman army existed in the 8th century BC. During this period Rome itself was probably little more than a fortified hilltop settlement and its army a relatively small force, whose activities were limited "mainly [to] raiding and cattle rustling with the occasional skirmish-like battle". Historian Theodor Mommsen referred to it as Rome's curiate army, named for its presumed subdivision along the boundaries of Rome's three founding tribes (Latin: curiae), the Ramnians, Tities and Luceres.  This army's exact structure is not known, but it is probable that it loosely resembled a warrior band or group of bodyguards led by a chieftain or king. Mommsen believes that Roman military organization of this period was regimented by the "Laws of [the apocryphal] King [V]Italus" but these laws, though referred to by Aristotle, have been lost.
The army (Latin: legio) consisted, according to Livy, of exactly 3,000 infantry and 300 horsemen, one third from each of Rome's three founding tribes.  Warriors served under six "leaders of division" (Latin: tribuni) who in turn served under a general, usually in the person of the reigning King. Mommsen uses philological arguments and references from Livy and others to suggest that the greater mass of foot-soldiers probably consisted of pilumni (javelin-throwers), with a smaller number possibly serving as arquites (archers). The cavalry was far smaller in number and probably consisted solely of the town's richest citizens. The army may also have contained the earliest form of chariots, hinted at by references to the flexuntes ("the wheelers").By the beginning of the 7th century BC, the Iron-Age Etruscan civilization (Latin: Etrusci) was dominant in the region. Like most of the other peoples in the region, the Romans warred against the Etruscans. By the close of the century, the Romans had lost their struggle for independence, and the Etruscans had conquered Rome, establishing a military dictatorship, or kingdom, in the city.
Although several Roman sources including Livy and Polybius talk extensively about the Roman army of the Roman Kingdom period that followed the Etruscan capture of the city, no contemporary accounts survive. Polybius, for example, was writing some 300 years after the events in question, and Livy some 500 years later. Additionally, what records were kept by the Romans at this time were later destroyed when the city was sacked. The sources for this period cannot therefore be seen as reliable, as they can be for later military history, e.g. from the First Punic War onwards.
According to our surviving narratives, the three kings of Rome during the Etruscan occupation were Tarquinius Priscus, Servius Tullius, and Tarquinius Superbus. During this period the army underwent a reformation into a centurial army based on socio-economic class. This reformation is traditionally attributed to Servius Tullius, the second of the Etruscan kings. Tullius had earlier carried out the first Roman census of all citizens. Livy tells us that Tullius reformed the army by transplanting onto it the structure derived originally for civil life as a result of this census. At all levels, military service was, at this time, considered to be a civic responsibility and a way of advancing one's status within society.However, Rome's social classes were qualified rather than created by the census. It is perhaps more accurate to say therefore that the army's structure was slightly refined during this period rather than radically reformed. Prior to these reforms, the infantry was divided into the classis of rich citizens and the infra classem of poorer citizens. The latter were excluded from the regular line of battle on the basis that their equipment was of poor quality. During the reforms, this crude division of poorer and richer citizens was further stratified. The army thereafter consisted of a number of troop types based upon the social class of propertied citizens, collectively known as adsidui. From the poorest in the "fifth class" to the richest in the "first class" and the equestrians above them, military service was compulsory for all. However, Roman citizens at this time generally viewed military service as a proper undertaking of duty to the state, in contrast to later views of military service as an unwelcome and unpleasant burden. Whereas there are accounts of Romans in the late empire mutilating their own bodies in order to exempt themselves from military service, there seems to have been no such reluctance to serve in the military of early Rome. This may in part be due to the generally lower intensity of conflict in this era; to the fact that men were fighting close to and often in protection of their own homes, or due to—as posited by later Roman writers—a greater martial spirit in antiquity.The equestrians, the highest social class of all, served in mounted units known as equites. The first class of the richest citizens served as heavy infantry with swords and long spears (resembling hoplites), and provided the first line of the battle formation. The second class were armed similarly to the first class, but without a breastplate for protection, and with an oblong rather than a round shield. The second class stood immediately behind the first class when the army was drawn up in battle formation. The third and fourth classes were more lightly armed and carried a thrusting-spear and javelins. The third class stood behind the second class in battle formation, normally providing javelin support. The poorest of the propertied men of the city comprised the fifth class. They were generally too poor to afford much equipment at all and were armed as skirmishers with slings and stones. They were deployed in a screen in front of the main army, covering its approach and masking its manoeuvres.
Men without property, who were thereby excluded from the qualifying social classes of the adsidui, were exempted from military service on the grounds that they were too poor to provide themselves with any arms whatsoever. However, in the most pressing circumstances, even these proletarii were pressed into service, though their military worth was probably questionable. Troops in all of these classes would fight together on the battlefield, with the exception of the most senior troops, who were expected to guard the city.The army is said to have increased from 3,000 to 4,000 men in the 5th century BC, and then again from 4,000 to 6,000 men sometime before 400 BC. This later army of 6,000 men were then divided into 60 centuries of 100 men each.
The army of the early Republic continued to evolve, and although there was a tendency among Romans to attribute such changes to great reformers, it is more likely that changes were the product of slow evolution rather than singular and deliberate policy of reform. The manipular formation was probably copied from Rome's Samnite enemies to the south, perhaps as a result of Roman defeats in the Second Samnite War.During this period, a military formation of around 5,000 men was known as a legion (Latin: legio). However, in contrast to later legionary formations of exclusively heavy infantry, the legions of the early and middle Republic consisted of both light and heavy infantry. The term manipular legion, a legion based on units called maniples, is therefore used to contrast the later cohortal legion of the Empire that was based around a system of cohort units. The manipular legion was based partially upon social class and partially upon age and military experience. It therefore represents a theoretical compromise between the earlier class-based army and the class-free armies of later years. In practice, even slaves were at one time pressed into the army of the Republic out of necessity. Normally a single legion was raised each year, but in 366 BC two legions were raised in a single year for the first time.Maniples were units of 120 men each drawn from a single infantry class. The maniples were small enough to permit tactical movement of individual infantry units on the battlefield within the framework of the greater army. The maniples were typically deployed into three discrete lines (Latin: triplex acies) based on the three heavy infantry types of hastati, principes and triarii. The first type, the hastati, typically formed the first rank in battle formation.  They typically wore a brass chest plate (though some could afford mail), a helmet called a galea, and occasionally, greaves (shin guards). They carried an iron bossed wooden shield, 120 cm (4 ft) tall and rectangular in shape with a curved front to partially protect the sides. Traditionally they were armed with a sword known as a gladius and two throwing spears known as pila: one the heavy pilum of popular imagination and one a slender javelin. However the exact introduction of the Gladius and the replacement of the spear with the sword as the primary weapon of the Roman Legions is uncertain, and it's possible that the early manipular legions still fought with the Hastati and Principes wielding the Hasta or Spear.
The second type, the principes, typically formed the second rank of soldiers back from the front of a battle line. They were heavy infantry soldiers armed and armoured as per the hastati.  The triarii, who typically formed the third rank when the army was arrayed for battle, were the last remnant of hoplite-style troops in the Roman army. They were armed and armoured as per the principes, with the exception that they carried a pike rather than two pila. A 600-man triarii maniple was divided into two formations each six men across by 10 men deep. A manipular legion typically contained between 1,200-2400 hastati, 1,200-2400 principes and 600-1200 triarii. The three classes of unit may have retained some slight parallel to social divisions within Roman society, but at least officially the three lines were based upon age and experience rather than social class. Young, unproven men would serve as hastati, older men with some military experience as principes, and veteran troops of advanced age and experience as triarii.
The heavy infantry of the maniples were supported by a number of light infantry (Latin: velites) and cavalry (Latin: equites) troops, typically 300 horsemen per manipular legion. The cavalry was drawn primarily from the richest class of equestrians, but additional cavalry and light infantry were drawn at times from the socii and Latini of the Italian mainland. The equites were still drawn from the wealthier classes in Roman society. There was an additional class of troops (Latin: accensi, also adscripticii and later supernumerarii) who followed the army without specific martial roles and were deployed to the rear of the triarii. Their role in accompanying the army was primarily to supply any vacancies that might occur in the maniples, but they also seem to have acted occasionally as orderlies to the officers.
The light infantry of 1,200 velites consisted of unarmoured skirmishing troops drawn from the youngest and lower social classes. They were armed with a sword and shield (90 cm (3 ft) diameter), as well as several light javelins, each with a 90 cm (3 ft) wooden shaft the diameter of a finger, with a c. 25 cm (10 in) narrow metal point. Their numbers were swollen by the addition of allied light infantry and irregular rorarii.
The Roman levy of 403 BC was the first to be requested to campaign for longer than a single season, and from this point on such a practice became gradually more common, if still not typical.
A small navy had operated at a fairly low level after the Second Samnite War, but it was massively upgraded during this period, expanding from a few primarily river- and coastal-based patrol craft to a full maritime unit. After a period of frenetic construction, the navy mushroomed to a size of more than 400 ships on the Carthaginian pattern. Once completed, it could accommodate up to 100,000 sailors and embarked troops for battle. The navy thereafter declined in size. This was partially because a pacified Roman Mediterranean called for little naval policing, and partially because the Romans chose to rely during this period on ships provided by Greek cities, whose peoples had greater maritime experience.
The extraordinary demands of the Punic Wars, in addition to a shortage of manpower, exposed the tactical weaknesses of the manipular legion, at least in the short term. In 217 BC, Rome was forced to effectively ignore its long-standing principle that its soldiers must be both citizens and property owners when slaves were pressed into naval service; around 213 BC, the property requirement was reduced from 11,000 to 4,000 asses. Since the Romans are unlikely to have preferred to employ slaves over poor citizens in their armies, it must be assumed that, at this point, the proletarii of the poorest citizens must also have been pressed into service despite their lack of legal qualification. By 123 BC, the financial requirement for military service was slashed again from 4,000 asses to just 1,500 asses. By this time, therefore, it is clear that many of the property-less former proletarii had been nominally admitted into the adsidui.During the 2nd century BC, Roman territory saw an overall decline in population, partially due to the huge losses incurred during various wars. This was accompanied by severe social stresses and the greater collapse of the middle classes into lower classes of the census and the proletarii. As a result, both the Roman society and its military became increasingly proletarianised. The Roman state was forced to arm its soldiers at the expense of the state, since many of the soldiers who made up its lower classes were now impoverished proletarii in all but name, and were too poor to afford their own equipment.The distinction between the heavy infantry types of hastati, principes and triarii began to blur, perhaps because the state was now assuming the responsibility of providing standard-issue equipment to all but the first class of troops, who alone were able to afford their own equipment. By the time of Polybius, the triarii or their successors still represented a distinct heavy infantry type armed with a unique style of cuirass, but the hastati and principes had become indistinguishable.In addition, the shortage of available manpower led to a greater burden being placed upon its allies (socii) for the provision of allied troops. Where accepted allies could not provide the required force types, the Romans were not averse during this period to hiring mercenaries to fight alongside the legions.
In a process known as the Marian reforms, Roman consul Gaius Marius carried out a programme of reform of the Roman military. In 107 BC, all citizens, regardless of their wealth or social class, were made eligible for entry into the Roman army. This move formalised and concluded a gradual process that had been growing for centuries, of removing property requirements for military service. The distinction between hastati, principes and triarii, which had already become blurred, was officially removed, and the legionary infantry of popular imagination was created. Legionary infantry formed a homogeneous force of heavy infantry. These legionaries were drawn from citizen stock; by this time, Roman or Latin citizenship had been regionally expanded over much of ancient Italy and Cisalpine Gaul. Lighter citizen infantry, such as the velites and equites, were replaced by non-citizen auxilia that could consist of foreign mercenaries. Due to the concentration of the citizen legions into a force of heavy infantry Rome's armies depended on auxiliary cavalry attachments for support. As a tactical necessity, legions were almost always accompanied by an equal or greater number of lighter auxiliary troops, which were drawn from the non-citizens of the Empire's territories. One known exception of legions being formed from non-citizen provinces during this period was the legion that was raised in the province of Galatia.After Marius, the legions were drawn largely from volunteer citizens rather than citizens conscripted for duty. Volunteers came forward and were accepted not from citizens of the city of Rome itself but from the surrounding countryside and smaller towns falling under Roman control. Whereas some long-term military professionals were classed as veterans, they were outnumbered by civilians with limited military experience who were in active service perhaps only for a few campaigns. The legions of the late Republic remained, unlike the legions of the later Empire, predominantly Roman in origin, although some small number of ex-auxiliary troops were probably incorporated. The army's higher-level officers and commanders were still drawn exclusively from the Roman aristocracy.Unlike earlier in the Republic, legionaries were no longer fighting on a seasonal basis to protect their land. Instead, they received standard pay, and were employed by the state on a fixed-term basis. As a consequence, military duty began to appeal most to the poorest sections of society, to whom a guaranteed salary was attractive. The army therefore consisted of a far higher proportion of the poor—particularly the rural poor—than it had previously. A destabilising consequence of this development was that the proletariat "acquired a stronger and more elevated position" within the state. This professionalisation of the military was necessary to provide permanent garrisons for newly acquired and distant territories such as Hispania, something not possible under an army of seasonal citizen militia.
Historian R. E. Smith notes that there was a need to raise additional legions in an emergency to repel specific strategic threats. He argues that this may have resulted in two types of legion. Long-standing legions deployed overseas were probably professional troops forming a standing army. Quickly-formed new legions, in contrast, consisted of younger men, perhaps with little or no military experience, who hoped for adventure and plunder. However, no distinction in basic pay, discipline or armour is known of between the two types of legion. The practice of veteran troops signing up again voluntarily into newly raised legions must have meant that no one army conformed exactly to one or other of these theoretical archetypes.
The legions of the late Republic were, structurally, almost entirely heavy infantry. The legion's main sub-unit was called a cohort and consisted of approximately 480 infantrymen. The cohort was therefore a much larger unit than the earlier maniple sub-unit, and was divided into six centuriae of 80 men each. Each centuria was separated further into 10 "tent groups" (Latin: contubernia) of 8 men each. Legions additionally consisted of a small body, typically 120 men, of Roman legionary cavalry (Latin: equites legionis). The equites were used as scouts and dispatch riders rather than battlefield cavalry. Legions also contained a dedicated artillery crew of perhaps 60 men, who would operate devices such as ballistae.Each legion was normally partnered with an approximately equal number of allied (non-Roman) auxiliae troops. The addition of allied troops to the Roman army was a formalisation of the earlier arrangement of using light troops from the Socii and Latini, who had received Roman citizenship after the Social War. Auxiliary troops could be formed from either auxiliary light cavalry known as alae, auxiliary light infantry known as cohors auxiliae, or a flexible mixture of the two known as cohors equitata. Cavalry types included mounted archers (Latin: sagittarii) and heavy shock cavalry (Latin: cataphracti or clibanarii). Infantry could be armed with bows, slings, throwing spears, long swords, or thrusting spears. Auxiliary units were originally led by their own chiefs, and, in this period, their internal organisation was left to their commanders.However, "the most obvious deficiency" of the Roman army remained its shortage of cavalry, especially heavy cavalry; even auxiliary troops were predominantly infantry. Luttwak argues that auxiliary forces largely consisted of Cretan archers, Balearic slingers and Numidian infantry, all of whom fought on foot. As Rome's borders expanded and its adversaries changed from largely infantry-based to largely cavalry-based troops, the infantry-based Roman army began to find itself at a tactical disadvantage, particularly in the East.
After having declined in size following the subjugation of the Mediterranean, the Roman navy underwent short-term upgrading and revitalisation in the late Republic to meet several new demands. Under Caesar, an invasion fleet was assembled in the English Channel to allow the invasion of Britain; under Pompey, a large fleet was raised in the Mediterranean Sea to clear the sea of Cilician pirates. During the civil war that followed, as many as a thousand ships were either constructed or pressed into service from Greek cities.
By the time of Julius Caesar in 54 BC, regular legionary units were supplemented by exploratores, a body of scouts, and speculatores, spies who infiltrated enemy camps. Due to the demands of the civil war, the extraordinary measure of recruiting legions from non-citizens was taken by Caesar in Transalpine Gaul (Latin: Gallia Transalpina), by Brutus in Macedonia, and by Pompey in Pharsalus. This irregular and extraordinary recruitment was not, however, typical of recruitment during this period, and Roman law still officially required that legions were recruited from Roman citizens only.
By the turn of the millennium, Emperor Augustus' primary military concern was to prevent Roman generals from further usurping the imperial throne. The experience of Caesar and, earlier, Marius and Sulla, had demonstrated the willingness of "emergency" (re-activated previously decommissioned) legions containing troops keen for plunder to follow their generals against the state. Augustus therefore removed the need for such emergency armies by increasing the size of the standing armies to a size sufficient to provide territorial defence on their own. Perhaps due to similar concerns, the legions and auxiliaries of the army were supplemented under the Emperor Augustus by an elite formation of guards dedicated to the protection of the Emperor. The first such unit was based in Rome and were known as the Praetorian Guard, and a second similar formation were known as the Cohortes urbanae.The legions, which had been a mix of life professionals and civilian campaigners, was altered into a standing army of professionals only. The actual structure of the cohort army remained much the same as in the late Republic, although around the 1st century AD the first cohort of each legion was increased in size to a total of 800 soldiers. However, while the structure of the legions remained much the same, their make-up gradually changed. Whereas early Republican legions had been raised by a draft from eligible Roman citizens, imperial legions were recruited solely on a voluntary basis and from a much wider base of manpower. Likewise, whereas Republican legions had been recruited almost exclusively in Italy, early Imperial legions drew most of their recruits from Roman colonies in the provinces from 68 AD onwards. One estimate places the proportion of Italian troops at 65% under Augustus in c. 1 AD, falling to around 49% by the end of Nero's reign.
Since the legions were officially open only to Roman citizens, Max Cary and Howard Hayes Scullard argue that at least in some provinces at this time "many provincials must have been recruited who lacked any genuine claim to Roman citizenship but received it unofficially on enlistment," a practice that was to increase in the 2nd century. This is most likely in those provinces where the pool of Roman citizens was not large enough to fulfill the provincial army's recruitment needs. One possible example is Britain, where one estimate puts the citizen pool in the 1st century at only 50,000 out of a total provincial population of around two million.At the same time as the legions underwent these transformations, the auxilia were reorganized and a number of allied troops were formalised into standing units similar to legions. Rather than being raised re-actively when required, the process of raising auxiliary troops was carried out in advance of conflicts according to annual targets. Whereas the internal organisation of the auxilia had previously been left up to their commanders, in the early empire they were organised into standardised units known as turmae (for cavalry alae) and centuriae (for infantry cohortes). Although never becoming as standardised in their equipment as the legions, and often retaining some national flavour, the size of the units at least was standardised to some degree. Cavalry were formed into either an ala quingenaria of 512 horsemen, or an ala millaria of 1,000 horsemen. Likewise, infantry auxilia could be formed into a cohors quingenaria of 500 men or a cohors millaria of 1,000 men. Mixed cavalry/infantry auxiliaries were typically formed with a larger proportion of foot than horse troops: the cohors equitata quingenaria consisted of 380 foot and 120 horsemen, and the cohors equitata millaria consisted of 760 foot and 240 horsemen.The vitality of the empire at this point was such that the use of native auxilia in the Roman army did not apparently barbarise the military as some scholars claim was to happen in the late empire. On the contrary, those serving in the auxilia  during this period frequently strove to Romanise themselves. They were granted Roman citizenship on retirement, granting them several social advantages, and their sons became eligible for service in the legions.As with the army, many non-Italians were recruited into the Roman Navy, partly because  the Romans had never readily taken to the sea. It appears that the navy was considered to be slightly less prestigious than the auxilia but, like the auxilia, troops could gain citizenship on discharge upon retirement. In terms of structure, each ship was staffed by a group of men approximately equivalent to a century, with ten ships forming a naval squadron.
Through the final years of the 1st century AD, the legions remained the backbone of the Roman army, although the auxilia in fact outnumbered them by up to half as much again. Within the legions, the proportion of troops recruited from within Italy fell gradually after 70 AD. By the close of the 1st century, this proportion had fallen to as low as 22 percent, with the remainder drawn from conquered provinces. Since technically only citizens were allowed to enlist in the legions, where recruits did not possess citizenship then, at least in some instances, citizenship "was simply given [to] them on enlistment". During this time, the borders of the Empire had remained relatively fixed to the extent originally reached under the Emperor Trajan. Because of this, the army was increasingly responsible for protecting existing frontiers rather than expanding into foreign territory, the latter of which had characterised the army's earlier existence. As a result, legions became stationed in largely fixed locations. Although entire legions were occasionally transferred into theatres of war, they remained largely rooted in one or more legionary bases in a province, detaching into smaller bodies of troops (Latin: vexillationes) on demand. This policy eventually led to a split of the military's land-based forces into mobile and fixed troops in the later Empire. In general, the best troops were dispatched as vexillationes, and the remainder left to guard border defences were of lower quality, perhaps those with injuries or near retirement.
By the time of the emperor Hadrian the proportion of Italians in the legions had fallen to just ten percent and provincial citizens now dominated. This low figure is probably a direct result of the changing needs of military staffing: a system of fixed border defences (Latin: limes) were established around the Empire's periphery under Hadrian, consolidating Trajan's territorial gains. These called for troops to be stationed permanently in the provinces, a prospect more attractive to locally raised rather than Italian troops. The higher prestige and pay to be found in the Italian dominated Praetorian Guard must also have played a role. The majority of the troops in the legions at the start of the 3rd century AD were from the more Romanised (though non-Italian) provinces, especially Illyria. As the century progressed, more and more barbarians (Latin: barbari) were permitted to settle inside of, and tasked with aiding in the defence of, Rome's borders. As a result, greater numbers of barbarous and semi-barbarous peoples were gradually admitted to the army.
However, whether this regionalisation of the legions was partnered by a drop in the professionalism of the troops is contested. Antonio Santosuosso argues that the strict discipline and high motivation of the days of Marius had lapsed, but Andrew Alfoldi states that the Illyrian troops were both valiant and warlike, and Tacitus described German recruits as being natural mercenaries (Latin: vivi ad arma nati). It seems that discipline in the legions did slacken, with soldiers granted permission to live with wives outside of military lodgings and permitted to adopt a more lavish and comfortable lifestyle, in contrast to the strict military regimen of earlier years. However, it is by no means certain that this led to any reduction in the effectiveness of the legions, due to the greater ferocity and stature of the barbari recruits.  The flavour of the Roman military, however, was now dictated by the increasing number of regional recruits, leading to a partial barbarisation of Rome's military forces beginning in this period. The barbarisation of the lower ranks was paralleled by a concurrent barbarisation of its command structure, with the Roman senators who had traditionally provided its commanders becoming entirely excluded from the army. By 235 AD the Emperor himself, the figurehead of the entire military, was a man born outside of Italy to non-Italian parents.
The gradual inclusion of greater numbers of non-citizen troops into the military was taken a further step by the creation under Hadrian of a new type of force in addition to the legions and auxilia, known as numeri. Formed in bodies of around 300 irregular troops, the numeri were drawn from subjugate provinces and peoples of client-states or even from beyond the borders of the empire. They were both less regimented and less Romanised than auxiliary troops, with a "pronounced national character," including native dress and native war cries. The introduction of the numeri was a response to the need for cheap troops, who were nevertheless fierce and provided a force balance of light infantry and cavalry. They were therefore largely less well armed and less well trained than auxilia or legions, although more prestigious elite irregular native troops were also utilised. However, the legions still made up around one half of the Roman army at this point.
By the late Empire, enemy forces in both the East and West were "sufficiently mobile and sufficiently strong to pierce [the Roman] defensive perimeter on any selected axis of penetration"; from the 3rd century onwards, both Germanic tribes and Persian armies pierced the frontiers of the Roman Empire. In response, the Roman army underwent a series of changes, more organic and evolutionary than the deliberate military reforms of the Republic and early Empire. A stronger emphasis was placed upon ranged combat ability of all types, such as field artillery, hand-held ballistae, archery and darts. Roman forces also gradually became more mobile, with one cavalryman for every three infantryman, compared to one in forty in the early Empire. Additionally, the Emperor Gallienus took the revolutionary step of forming an entirely cavalry field army, which was kept as a mobile reserve at the city of Milan in northern Italy. It is believed that Gallienus facilitated this concentration of cavalry by stripping the legions of their integral mounted element. A diverse range of cavalry regiments existed, including catafractarii or clibanarii, scutarii, and legionary cavalry known as promoti. Collectively, these regiments were known as equites. Around 275 AD, the proportion of catafractarii was also increased. There is some disagreement over exactly when the relative proportion of cavalry increased, whether Gallienus' reforms occurred contemporaneously with an increased reliance on cavalry, or whether these are two distinct events. Alfoldi appears to believe that Gallienus' reforms were contemporaneous with an increase in cavalry numbers. He argues that, by 258, Gallienus had made cavalry the predominant troop type in the Roman army in place of heavy infantry, which dominated earlier armies. According to Warren Treadgold, however, the proportion of cavalry did not change between the early 3rd and early 4th centuries.Larger groups of barbari began to settle in Rome's territories around this time, and the troops they were contracted to provide to the Roman army were no longer organised as numeri but rather were the forerunners of the later rented native armies known as federated troops (Latin: foederati). Though they served under Roman officers, the troops of these units were far more barbarised than the numeri, lacked Romanisation of either military structure or personal ideology, and were ineligible for Roman citizenship upon discharge.  These native troops were not permitted to fight in native war bands under their own leaders, unlike the later foederati; instead, these troops were split into small groups attached to other Roman units. They existed therefore as a halfway house between numeri, who were encouraged to be Romanised, and the foederati, who raised officers from their own ranks and were almost entirely self-dependent.
A distinction between frontier guard troops and more mobile reserve forces had emerged with the use of certain troops to permanently man frontiers such as Hadrian's Wall in Britannia in the 2nd century AD. The competing demands of manned frontiers and strategic reserve forces had led to the division of the military into four types of troops by the early 4th century:
The limitanei or riparienses patrolled the border and defended the border fortifications. According to some older theories, the limitanei were "settled and hereditary" militia that were "tied to their posts." But according to most recent research, the limitanei were originally regular soldiers, including infantry, cavalry, and river flotillas, although they eventually became settled militia. According to Luttwak, the cunei of cavalry, and auxilia of infantry alone by this time, were local provincial reserves that may have evolved from earlier auxiliary units. According to Pat Southern and Karen Dixon, the legiones, auxilia, and cunei of the border armies were part of the limitanei, but higher-status than the older cohortes and alae in the same armies.
The comitatenses, and later the palatini were strategic reserves, usually in the rear. After their division into palatini and comitatenses, the latter were usually associated with the praesental armies, and the former were usually associated with the regional armies, but both types could be moved between the two.
The emperor Constantine I created the scholae to replace the old praetorian guard. The scholae were his personal guard, and were mainly equipped as cavalry. Vogt suggests that the scholae formed two small central reserves (Latin: scholae) held to the strategic rear even of the comitatenses, one each in the presence of the emperors of West and East respectively.The permanent field armies of the palatini and comitatenses ultimately derived from the temporary field armies of the earlier sacer comitatus.
Created and expanded from the core troops of the Emperor's personal bodyguards, the central field armies by 295 AD seem to have been too large to be accounted for as simple bodyguard forces, but were still too small to be able to campaign independently of legionary or vexillation support.Of the four troop types, the limitanei (border guards) were once considered to have been of the lowest quality, consisting largely of peasant-soldiers that were both "grossly inferior" to the earlier legions and inferior also to their counterparts in the mobile field armies. However, more recent work establishes that the limitanei were regular soldiers.While the limitanei were supposed to deal with policing actions and low-intensity incursions, the duty of responding to more serious incidents fell upon the provincial troops. The countering of the very largest scale incursions on a strategic scale was the task of the comitatenses and palatini or mobile field troops, possibly accompanied by the emperor's scholae. Both border and field armies consisted of a mix of infantry and cavalry units although the weight of cavalry was, according to some authorities, greater in the mobile field armies. Overall, approximately one quarter of the army consisted of cavalry troops but their importance is uncertain. Older works such as the Eleventh Edition of the Encyclopædia Britannica (1911) state that the Roman military of the late Empire was "marked by that predominance of the horseman which characterised the earlier centuries of the Middle Ages," but many more recent authors believe that the infantry remained predominant.There is some dispute about whether this new military structure was put into place under the Emperor Diocletian or Constantine since both reorganised the Roman Army in the late 3rd and early 4th centuries to some degree. Both Diocletian and even his predecessor of thirty years Gallienus may already have controlled mobile strategic reserves to assist the empire's border forces; either Diocletian or Constantine expanded this nascent force into permanent field armies.Recruitment from amongst Roman citizens had become greatly curtailed as a consequence of a declining population, "cripplingly numerous" categories of those exempted from military service and the spread of Christianity with its pacifist message. Together, these factors culminated in "the withdrawal of the urban class from all forms of military activity." In their place, much of Rome's military were now recruited from non-Italian peoples living within the empire's borders. Many of these people were barbarians or semi-barbarians recently settled from lands beyond the empire, including several colonies of Carpi, Bastarnae and Sarmatians.Although units described as legiones existed as late as the 5th century in both the border and field armies, the legionary system was very different from that of the principate and early empire. Since the term legion continued to be used, it is unclear exactly when the structure and role of the legions changed. In the 3rd or 4th century, however, the legions' role as elite heavy infantry was substantially reduced and may have evaporated entirely. Instead, those "legions" that remained were no longer drawn exclusively (and perhaps hardly at all) from Roman citizens. Either Diocletian or Constantine reorganised the legions into smaller infantry units who, according to some sources, were more lightly armoured than their forebears. Their lighter armament may have been either because they "would not consent to wear the same weight of body armour as the legionaries of old" or, as in at least one documented instance, because they were prohibited from wearing heavy armour by their general in order to increase their mobility. 4th-century legions were at times only one sixth the size of early imperial legions, and they were armed with some combination of spears, bows, slings, darts and swords, reflecting a greater contemporary emphasis on ranged fighting. The auxilia and numeri had also largely disappeared. Constantine further increased the proportion of German troops in the regular army; their cultural impact was so great that even legionaries began wearing German dress. At the start of Diocletian's reign, the Roman army numbered about 390,000 men, but by the end of his reign he successfully increased the number to 581,000 men.
By the late 4th century, the Empire had become chronically deficient in raising sufficient troops from amongst its own population. As an alternative, taxation raised internally was increasingly used to subsidise growing numbers of barbarian recruits. The Romans had, for some time, recruited individual non-Roman soldiers into regular military units. In 358 AD, this practice was accelerated by the wholescale adoption of the entire Salian Franks people into the Empire, providing a ready pool of such recruits. In return for being allowed to settle as foederatii in northern Gallia on the near side of the Rhine, the Franks were expected to defend the Empire's borders in their territory and provide troops to serve in Roman units.
In 376, a large band of Goths asked Emperor Valens for permission to settle on the southern bank of the Danube River on terms similar to the Franks. The Goths were also accepted into the empire as foederati; however, they rebelled later that year and defeated the Romans at the Battle of Adrianople. The heavy losses that the Roman military suffered during this defeat ironically forced the Roman Empire to rely still further on such foederati troops to supplement its forces. In 382, the practice was radically extended when federated troops were signed up en masse as allied contingents of laeti and foederatii troops separate from existing Roman units. Near-constant civil wars during the period 408 and 433 between various Roman usurpers, emperors and their supposed deputies such as Constantine III, Constantius III, Aetius and Bonifacius resulted in further losses, necessitating the handing over of more taxable land to foederati.
The size and composition of these allied forces remains in dispute. Santosuosso argues that foederati regiments consisted mostly of cavalry that were raised both as a temporary levy for a specific campaign need and, in some cases, as a permanent addition to the army. Hugh Elton believes that the importance of foederati has been overstated in traditional accounts by historians such as A.H.M. Jones. Elton argues that the majority of soldiers were probably non-Italian Roman citizens,  while Santosuosso believes that the majority of troops were almost certainly non-citizen barbari.
The non-federated mobile field army, known as the comitatenses, was eventually split into a number of smaller field armies: a central field army under the emperor's direct control, known as the comitatensis palatina or praesentalis, and several regional field armies. Historians Santosuosso and Vogt agree that the latter gradually degraded into low-quality garrison units similar to the limitanei that they either supplemented or replaced. By the 5th century, a significant portion of Western Rome's main military strength lay in rented barbarian mercenaries known as foederati.As the 5th century progressed, many of the Empire's original borders had been either wholly or partially denuded of troops to support the central field army. In 395, the Western Roman Empire had several regional field armies in Italy, Illyricum, Gallia, Britannia and Africa, and about twelve border armies. By about 430, two more field armies were established in Hispania and Tingitania but the central government had lost control of Britannia as well as much of Gaul, Hispania, and Africa. In the same period, the Eastern Roman Empire had two palatine field armies (at Constantinople), three regional field armies (in the East, in Thrace, and in Illyricum) and fifteen frontier armies.
As Roman troops were spread increasingly thin over its long border, the Empire's territory continued to dwindle in size as the population of the empire declined. Barbarian war bands increasingly began to penetrate the Empire's vulnerable borders, both as settlers and invaders. In 451, the Romans defeated Attila the Hun, but only with assistance from a confederation of foederatii troops, which included Visigoths, Franks and Alans. As barbarian incursions continued, some advancing as far as the heart of Italy, Rome's borders began to collapse, with frontier forces swiftly finding themselves cut off deep in the enemy's rear.Simultaneously, barbarian troops in Rome's pay came to be "in a condition of almost perpetual turbulence and revolt" from 409 onwards. In 476 these troops finally unseated the last emperor of the Western Roman Empire. The Eastern Roman forces continued to defend the Eastern Roman (Byzantine) Empire until its fall in 1453.The former Oxford University historian Adrian Goldsworthy has argued that the cause of the fall of the Roman Empire in the West should not be blamed on barbarization of the late Roman Army, but on its recurrent civil wars, which led to its inability to repel or defeat invasions from outside its frontiers. The East Roman or Byzantine empire on the other hand had fewer civil wars to contend with in the late fourth and early fifth centuries, or in the years from 383-432 A.D.
Livy, From the Founding of the City on Wikisource (print: Book 1 as The Rise of Rome, Oxford University Press, 1998, ISBN 0-19-282296-9)
Polybius: The Rise of the Roman Empire at LacusCurtius print: Harvard University Press, 1927. (Translation by W. R. Paton).
Alfoldi, Andrew, The Crisis of the Empire (AD 249–270), in S A Cook et al. (eds.), The Cambridge Ancient History, Vol. XII: The Imperial Crisis and Recovery (AD 193–324), pp. 208ff., ISBN 0-521-30199-8.
Campbell, Brian, The Army, in The Crisis of Empire, AD 193–337, in The Cambridge Ancient History, Second Edition, Vol. XII, ISBN 0-521-30199-8.
Elton, Hugh, Warfare in Roman Europe AD 350–425, Oxford University Press, 1996, ISBN 0-19-815241-8.
Gabba, Emilio, Republican Rome, The Army and The Allies, University of California Press, 1976, ISBN 0-520-03259-4.
Gibbon, Edward: The Decline and Fall of the Roman Empire (print: Penguin Books, 1985, ISBN 0-14-043189-6).
Goldsworthy, Adrian, In the Name of Rome: The Men Who Won the Roman Empire, Weidenfeld & Nicolson, 2003, ISBN 0-297-84666-3.
Hassall, Mark, The Army, in The High Empire, AD 70–192, in The Cambridge Ancient History, Second Edition, Vol. XI, ISBN 0-521-26335-2.
Heather, Peter, The Fall of the Roman Empire: A New History, Macmillan Publishers, 2005, ISBN 0-330-49136-9.
Keppie, Lawrence, The Making of the Roman Army, Barnes & Noble Books, 1984, ISBN 978-0-389-20447-3.
Luttwak, Edward, The Grand Strategy of the Roman Empire, Johns Hopkins University Press, ISBN 0-8018-2158-4.
Mattingly, David, An Imperial Possession-Britain in the Roman Empire, Allen Lane, 2006, ISBN 0-14-014822-1.
Runciman, Steven, The Fall of Constantinople: 1453. Cambridge University Press, 1965, ISBN 0-521-39832-0.
Santosusso, Antonio, Storming the Heavens: Soldiers, Emperors and Civilians in the Roman Empire, Westview Press, 2001, ISBN 0-8133-3523-X.
Smith, Richard, Service in the Post-Marian Roman Army, Manchester University Press, 1958, ASIN B0000CK67F.

Stuyvesant High School (pronounced ) commonly referred to as Stuy (pronounced ) is a specialized high school in New York City, United States. Operated by the New York City Department of Education, these specialized schools offer tuition-free accelerated academics to city residents. Stuyvesant is a college-preparatory high school.
Stuyvesant was established as an all-boys school in the East Village of Manhattan in 1904. An entrance examination was mandated for all applicants starting in 1934, and the school started accepting female students in 1969. Stuyvesant moved to its current location at Battery Park City in 1992 because the school had become too large. The old building now houses several high schools.
Admission to Stuyvesant involves passing the Specialized High Schools Admissions Test. Each November, the 900 to 950 applicants with the highest exam scores out of around 30,000 eighth- and ninth-graders are accepted to Stuyvesant. The school has a wide range of extracurricular activities, including a theater competition called SING! and two student publications. Notable alumni include physicists Brian Greene and Lisa Randall, mathematician Paul Cohen, and genome researcher Eric Lander.
New York City's Superintendent of Schools, William Henry Maxwell, had first written about the need to construct manual trade schools in New York City in 1887. At the time, C. B. J. Snyder was designing many of the city's public school buildings using multiple architectural styles. The first trade school in the city was Manual Training High School in Brooklyn, which opened in 1893. By 1899, Maxwell was advocating for a manual trade school in Manhattan.In January 1903, Maxwell and Snyder submitted a report to the New York City Board of Education in which they suggested the creation of a trade school in Manhattan. The Board of Education approved the plans in April 1904. They suggested that the school occupy a plot on East 15th Street, west of First Avenue, but that plot did not yet contain a school building, and so the new trade school was initially housed within P.S. 47's former building at 225 East 23rd Street. The Board of Education also wrote that the new trade school would be "designated as the Stuyvesant High School, as being reminiscent of the locality." Stuyvesant Square, Stuyvesant Street, and later Stuyvesant Town (which was built in 1947) are all located near the proposed 15th Street school building. All of these locations were named after Peter Stuyvesant, the last Dutch governor of New Netherland. The appellation was selected in order to avoid confusion with Brooklyn's manual Training High School, Stuyvesant High School opened in September 1904 as Manhattan's first manual trade school for boys. At the time of its opening, the school consisted of 155 students and 12 teachers.At first, the school provided a core curriculum of "English, Latin, modern languages, history, mathematics, physics, chemistry, [and] music", as well as a physical education program and a more specialized track of "woodworking, metalworking, mechanical drawing, [and] freehand drawing". However, in June 1908, Maxwell announced that the trade school curriculum would be separated from the core curriculum, and a discrete trade school would operate in the Stuyvesant building during the evening. Thereafter, Stuyvesant became renowned for excellence in math and science. In 1909, eighty percent of the school's alumni went to college, compared to other schools, which only sent 25% to 50% of their graduates to college.By 1919, officials started restricting admission based on scholastic achievement. Stuyvesant implemented a double session plan in 1919 to accommodate the rising number of students: some students would attend in the morning, while others would take classes in the afternoon and early evening. All students studied a full set of courses. These double sessions ran until 1956. The school implemented a system of entrance examinations in 1934. The examination program, developed with the assistance of Columbia University, was expanded in 1938 to include the newly founded Bronx High School of Science.In 1956, a team of six students designed and began construction of a cyclotron. A low-power test of the device succeeded six years later. A later attempt at full-power operation, however, knocked out the power to the school and surrounding buildings.In 1967, Alice de Rivera filed a lawsuit against the Board of Education, alleging that she had been banned from taking Stuyvesant's entrance exam because of her gender. The lawsuit was decided in the student's favor, and Stuyvesant was required to accept female students. The first female students were accepted in September 1969, when Stuyvesant offered admission to 14 girls and enrolled 12 of them. The next year, 223 female students were accepted to Stuyvesant. By 2015, the last year that enrollment reports are available, females represented 43% of the total student body.In 1972, the New York State Legislature passed the Hecht–Calandra Act, which designated Brooklyn Technical High School, Bronx High School of Science, Stuyvesant High School, and the High School of Music & Art (now Fiorello H. LaGuardia High School) as specialized high schools of New York City. The act called for a uniform exam to be administered for admission to Brooklyn Tech, Bronx Science, and Stuyvesant. The exam, named the Specialized High Schools Admissions Test (SHSAT), tested the mathematical and verbal abilities of students who were applying to any of the specialized high schools. The only exception was for applicants to LaGuardia High School, who were accepted by audition rather than examination.The current school building is 0.5 miles (0.80 km) away from the site of the World Trade Center, which was destroyed in the terrorist attacks on September 11, 2001. The school was evacuated during the attack. Although the smoke cloud coming from the World Trade Center engulfed the building at one point, there was no structural damage to the building, and there were no reports of physical injuries. Less than an hour after the collapse of the second World Trade Center tower, concern over a bomb threat at the school prompted an evacuation of the surrounding area, as reported live on the Today show. When classes resumed on September 21, 2001, students were moved to Brooklyn Technical High School while the Stuyvesant building served as a base of operations for rescue and recovery workers. This caused serious congestion at Brooklyn Tech, and required the students to attend in two shifts, with the Stuyvesant students attending the evening shift. Normal classes resumed nearly a month after the attack, on October 9.
Because Stuyvesant was so close to the World Trade Center site, there were concerns of asbestos exposure. The U.S. EPA indicated at that time that Stuyvesant was safe from asbestos, and conducted a thorough cleaning of the Stuyvesant building, but the Stuyvesant High School Parents' Association contested the accuracy of the assessment. Some problems, including former teacher Mark Bodenheimer's respiratory problems, have been reported—he accepted a transfer to The Bronx High School of Science after having difficulty continuing his work at Stuyvesant. Other isolated cases include Stuyvesant's 2002 class president Amit Friedlander, who received local press coverage in September 2006 after he was diagnosed with cancer. While there have been other cases linked to the same dust cloud that emanated from Ground Zero, there is no definitive evidence that such cases have directly affected the Stuyvesant community. Stuyvesant students did spend a full year in the building before the theater and air systems were cleaned, however, and a group of Stuyvesant alumni is currently lobbying for health insurance as a result.Nine alumni were killed in the World Trade Center attack. Another alumnus, Richard Ben-Veniste of the class of 1960, was on the 9/11 Commission. On October 2, 2001, the school newspaper, The Spectator, created a special 24-page full-color 9/11 insert containing student photos, reflections and stories. On November 20, 2001, the magazine was distributed for free to the greater metropolitan area, enclosed within 830,000 copies of The New York Times. In the months after the attacks, Annie Thoms, an English teacher at Stuyvesant and the theater adviser at the time, suggested that the students take accounts of staff and students' reactions during and after September 11, 2001, and turn them into a series of monologues. Thoms then published these monologues as With Their Eyes: September 11—The View from a High School at Ground Zero.During the 2003–2004 school year, Stuyvesant celebrated the 100th anniversary of its founding with a full year of activities. Events included a procession from the 15th Street building to the Chambers Street one, a meeting of the National Consortium for Specialized Secondary Schools of Mathematics, Science and Technology, an all-class reunion, and visits and speeches from notable alumni.
In August 1904, the Board of Education authorized Snyder to design a new facility for Stuyvesant High School at 15th Street. The new school would be shaped like the letter "H" in order to maximize the number of windows on the building. The cornerstone for the new building was laid in September 1905. Approximately $1.5 million was spent on constructing the school, including $600,000 for the exterior alone. In 1907, Stuyvesant moved to the new building on 15th Street. The new building had a capacity of 2,600 students, more than double that of the existing school building at 23rd Street. It contained 25 classrooms devoted to skilled industrial trades such as joinery, as well as 53 regular classrooms and a 1,600-seat auditorium.
During the 1950s, the building underwent a $2 million renovation to update its classrooms, shops, libraries, and cafeterias.Through the 1970s and 1980s, when New York City public schools in general were marked by violence and low grades among their students, Stuyvesant had a reputation for being a top-notch school. However, the school building was deteriorating due to overuse and lack of maintenance. A New York Times report stated that the building had "held out into old age with minimal maintenance and benign neglect until its peeling paint, creaking floorboards and antiquated laboratories became an embarrassment." The five-story building could not cater adequately to the several thousand students, leading the New York City Board of Education to make plans to move the school to a new building in Battery Park City, near lower Manhattan's Financial District.The 15th Street building remains in use as the "Old Stuyvesant Campus", housing three schools: the Institute for Collaborative Education, the High School for Health Professions and Human Services, and P.S. 226. In recent years, keynote graduation speakers have included Attorney General Eric Holder (2001), former President Bill Clinton (2002), United Nations Secretary General Kofi Annan (2004), Late Night comedian Conan O'Brien (2006), the founder of Humans of New York, Brandon Stanton (2015), and astrophysicist Neil deGrasse Tyson (2018).
In 1987, New York City Mayor Ed Koch and New York State Governor Mario Cuomo jointly announced the construction of a new Stuyvesant High School building in Battery Park City. The Battery Park City Authority donated 1.5 acres (0.61 ha) of land for the new building.  The authority was not required to hire the lowest bidder, which meant that the construction process could be accelerated in return for a higher cost. The building was designed by the architectural firms of Gruzen Samton Steinglass and Cooper, Robertson & Partners. The structure's main architect, Alexander Cooper of Cooper, Robertson & Partners, had also designed much of Battery Park City.Stuyvesant's principal at the time, Abraham Baumel, visited the country's most advanced laboratories to gather ideas about what to include in the new Stuyvesant building's 12 laboratory rooms. The new 10-story building also included banks of escalators, glass-walled studios on the roof, and a four-story northern wing with a swimming pool, five gymnasiums, and an auditorium. Construction began in 1989. When it opened in 1992, the building was New York City's first new high school building in ten years. The new Stuyvesant Campus cost $150 million, making it the most expensive high school building ever built in the city at the time. The library has a capacity of 40,000 volumes and overlooks Battery Park City.
Shortly after the building was completed, the $10-million Tribeca Bridge was built to allow students to enter the building without having to cross the busy West Street. The building was designed to be fully compliant with the Americans with Disabilities Act, and is listed as such by the New York City Department of Education. As a result, the building is one of the 5 additional sites of P721M, a school for students with multiple disabilities who are between the ages of 15 and 21.In 1997, the eastern end of the mathematics floor was dedicated to Richard Rothenberg, the math department chairman who had died from a sudden heart attack earlier that year. Sculptor Madeleine Segall-Marx was commissioned to create the Rothenberg Memorial in his honor. She created a mathematics wall entitled "Celebration", consisting of 50 wooden boxes—one for each year of his life—behind a glass wall, featuring mathematical concepts and reflections on Rothenberg.In 2006, Robert Ira Lewy of the class of 1960 made a gift worth $1,000,000 to found the Dr. Robert Ira Lewy M.D. Multimedia Center. and donated his personal library in 2007. In late 2010, the school library merged with the New York Public Library (NYPL) network in a four-year pilot program, in which all students of the school received a student library card so they could check books out of the school library or any other public library in the NYPL system.An escalator collapse at Stuyvesant High School on September 13, 2018, injured 10 people, including 8 students.
During construction, the Battery Park City Authority, the Percent for Art Program of the City of New York, the Department of Cultural Affairs, and the New York City Board of Education commissioned Mnemonics, an artwork by public artists Kristin Jones and Andrew Ginzel. Four hundred hollow glass blocks were dispersed randomly from the basement to the tenth floor of the new Stuyvesant High School building. Each block contains relics providing evidence of geographical, natural, cultural and social worlds, from antiquity to the present time.The blocks are set into the hallway walls and scattered throughout the building. Each block is inscribed with a brief description of its contents or context. The items displayed include a section of the Great Wall of China, fragments of the Mayan pyramids, leaves from the sacred Bo tree, water from the Nile and Ganges Rivers, a Revolutionary War button, pieces of the 15th Street Stuyvesant building, a report card of a student who studied in the old building, and fragments of monuments from around the world, various chemical compounds, and memorabilia from each of the 88 years' history of the 15th Street building. Empty blocks were also installed so that they could be filled with items chosen by each of the graduating classes up through 2080. The installation received the Award for Excellence in Design from the Art Commission of the City of New York.
The New York City Subway's Chambers Street station, served by the 1, ​2, and ​3 trains, is located nearby. Additionally, New York City Bus's M9, M20 and M22 routes stop near Stuyvesant.  Students residing a certain distance from the school are provided full-fare or half-fare student MetroCards for public transportation at the start of each term, based on how far away the student resides from the school.
Stuyvesant has a total enrollment of over 3,000 students and is open to residents of New York City entering ninth or tenth grade. Enrollment is based solely on performance on the three-hour Specialized High Schools Admissions Test, which is administered annually. Approximately 28,000 students took the test in 2017. The list of schools using the SHSAT has since grown to include eight of New York's nine specialized high schools. The test score necessary for admission to Stuyvesant has consistently been higher than that needed for admission to the other schools using the test. Admission is currently based on an individual's score on the examination and the pre-submitted ranking of Stuyvesant among the other specialized schools. Ninth- and rising tenth–grade students are also eligible to take the test for enrollment, but far fewer students are admitted that way. The test covers math (word problems and computation) and verbal (reading comprehension) skills. Former Mayor John Lindsay and community activist group Association of Community Organizations for Reform Now (ACORN) have argued that the exam may be biased against African and Hispanic Americans.
For most of the 20th century, the student body at Stuyvesant was heavily Jewish. A significant influx of Asian students began in the 1970s; by 2019, 74% of the students in attendance were Asian-American (most from families making low incomes). In the 2013 academic year, the student body was 72.43% Asian, 21.44% Caucasian, 1.03% African American, 2.34% Hispanic, and 3% unknown/other. The paucity of Black and Hispanic students at Stuyvesant has often been an issue for some city administrators.  In 1971, Mayor John Lindsay argued that the test was culturally biased against black and Hispanic students and sought to implement an affirmative action program. However, protests by parents forced the plan to be scrapped and led to the passage of the Hecht-Calandra Act, which preserved admissions by examination only. A small number of students judged to be economically disadvantaged and who come within a few points of the cut-off score were given an extra chance to pass the test.Community activist group ACORN published two reports in 1996, titled Secret Apartheid and Secret Apartheid II. In these reports, ACORN called the SHSAT "permanently suspect" and described it as a "product of an institutional racism", saying that black and Hispanic students did not have access to proper test preparation materials. Along with Schools Chancellor Rudy Crew, they began an initiative for more diversity in the city's gifted and specialized schools, in particular demanding the SHSAT be suspended altogether until the Board of Education was able to show all children have had access to appropriate materials to prepare themselves. Students published several editorials in response to ACORN's claims, stating the admissions system at the school was based on student merit, not race.A number of students take preparatory courses offered by private companies such as The Princeton Review and Kaplan in order to perform better on the SHSAT, often leaving those unable to afford such classes at a disadvantage. To bridge this gap and boost minority admissions, the Board of Education started the Math Science Institute in 1995, a free program to prepare students for the admissions test. Students attend preparatory classes through the program, now known as the Specialized High School Institute (also known as DREAM), at several schools around the city from the summer after sixth grade until the eighth-grade exam. After implementation of these free programs for improving underprivileged children's enrollment, black and Hispanic enrollment continued to decline. After further expansion of those free test prep programs, there was still no increase in percentages to the attendance of black and Hispanic children. As of  2019, fewer than 1% of freshman openings were given to black students, while over 66% were given to Asian-American children.The New York City Department of Education reported in 2003 that public per student spending at Stuyvesant is slightly lower than the city average. Stuyvesant also receives private contributions.
Stuyvesant students undertake a college-preparatory curriculum that mostly includes four years of English, history, and laboratory-based sciences. The sciences courses include requisite biology, chemistry, and physics classes. Students also take four years of mathematics. Students also take three years of a single foreign language; a semester each of introductory art, music, health, and technical drawing; two semesters of computer science; and two lab-based technology courses. Several exemptions from technology education exist for seniors.  Stuyvesant offers students a broad selection of elective courses. Some of the more unusual offerings include astronomy, New York City history, Women's Voices, and the mathematics of financial markets. Most students complete the New York City Regents courses by junior year and take calculus during their senior year. However, the school offers math courses through differential equations for the more advanced students. A year of technical drawing was formerly required; students learned how to draft by hand in its first semester and how to draft using a computer in the second. Now, students take a one-semester compacted version of the former drafting course, as well as a semester of introductory computer science. For the class of 2015, the one-semester computer science course was replaced with a two-semester course.
As a specialized high school, Stuyvesant offers a wide range of Advanced Placement (AP) courses. These courses focus on math, science, history, English, or foreign languages. This gives students various opportunities to earn college credit. AP computer science students can also take three additional computer programming courses after the completion of the AP course: systems level programming, computer graphics, and software development. In addition, there is a one-year computer networking class which can earn students Cisco Certified Network Associate (CCNA) certification.Stuyvesant's foreign language offerings include Mandarin Chinese, French, German, Japanese, Latin, and Spanish. In 2005, the school also started offering courses in Arabic after the school's Muslim Student Association had raised funds to support the course. Stuyvesant's biology and geo-science department offers courses in molecular biology, human physiology, medical ethics, medical and veterinary diagnosis, human disease, anthropology and sociobiology, vertebrate zoology, laboratory techniques, medical human genetics, botany, the molecular basis of cancer, nutrition science, and psychology. The chemistry and physics departments include classes in organic chemistry, physical chemistry, astronomy, engineering mechanics, and electronics.Although Stuyvesant is primarily known for its math and science focus, the school also has a comprehensive humanities program. The English Department offers students courses in British and classical literature, Shakespearean literature, science fiction, philosophy, existentialism, debate, acting, journalism, creative writing, and poetry. The Social Studies core requires two years of global history (or one year of global followed by one year of European history), one year of American history, as well as a semester each of economics and government. Humanities electives include American foreign policy; civil and criminal law, prejudice and persecution, and race, ethnicity and gender issues.In 2004, Stuyvesant entered into an agreement with City College of New York in which the college funds advanced after-school courses that are taken for college credit but taught by Stuyvesant teachers. Some of these courses include physical chemistry, linear algebra, advanced Euclidean geometry, and women's history.Prior to the 2005 revision of the SAT, Stuyvesant graduates had an average score of 1408 out of 1600 (685 in the verbal section of the test, 723 in the math section). In 2010, the average score on the SAT for Stuyvesant students was 2087 out of 2400, while the class of 2013 had an average SAT score of 2096. As of  2018, Stuyvesant students' average SAT score was 1490 of 1600 points. Stuyvesant also administers more Advanced Placement exams than any other high school in the world, as well as the highest number of students who reach the AP courses' "mastery level". As of  2018, there are 24 AP classes offered, with a little more than half of all students taking at least one AP class, and about 98% of students pass their AP tests.
Stuyvesant fields 32 varsity teams, including the swimming, golf, bowling, volleyball, soccer, basketball, gymnastics, wrestling, fencing, baseball/softball, American handball, tennis, track/cross country, cricket, football, and lacrosse teams. In addition, Stuyvesant has ultimate teams for the boys' varsity, boys' junior varsity, and girls' varsity divisions.In September 2007, the Stuyvesant football team was given a home field at Pier 40, located north of the school at Houston Street and West Street. In 2008, the baseball team was granted use of the pier after construction and delivery of an artificial turf pitching mound that met Public Schools Athletic League specifications. Stuyvesant also has its own swimming pool, but it does not contain its own running track or tennis court. Unlike most American high schools, most sports teams at Stuyvesant are individually known by different names. Only the football, Cheerleading, badminton, girls' ping pong, baseball, girls' handball, girls' bowling, and boys' lacrosse teams retain the traditional Pegleg monikers.
The student body of Stuyvesant is represented by the Stuyvesant Student Union, a student government. It comprises a group of students (elected each year for each grade) who serve the student body in two important areas: improving student life by promoting and managing extracurricular activities (clubs and publications), and by organizing out-of-school activity such as city excursions or fundraisers; and providing a voice to the student body in all discussion of school policy with the administration.
Stuyvesant allows students to join clubs, publications, and teams under a system similar to that of many colleges. As of  2015, the school had 150 student clubs.
The Spectator is Stuyvesant's official in-school newspaper, which is published biweekly and is independent from the school.  There are over 250 students who help with publication. At the beginning of the fall and spring terms, there are recruitments, but interested students may join at any time.Founded in 1915, The Spectator is one of Stuyvesant's oldest publications. It has a long-standing connection with its older namesake, Columbia University's Columbia Daily Spectator, and has been recognized by the Columbia University Graduate School of Journalism's Columbia Scholastic Press Association.
The Voice was founded in the 1973–1974 academic year as an independent publication only loosely sanctioned by school officials. It had the appearance of a magazine and gained a large readership. The Voice attracted a considerable amount of controversy and a First Amendment lawsuit, after which the administration forced it to go off-campus and to turn commercial in 1975–1976.In the beginning of the 1975–1976 academic year, The Voice decided to publish the results of a confidential random survey measuring the "sexual attitudes, preferences, knowledge and experience" of the students. The administration refused to permit The Voice to distribute the questionnaire, and the Board of Education refused to intervene, believing that "irreparable psychological damage" would be occasioned on some of the students receiving it. The editor-in-chief of The Voice, Jeff Trachtman, brought a First Amendment challenge to this decision in the United States District Court for the Southern District of New York in front of Judge Constance Baker Motley.Motley, relying on the relatively recent Supreme Court precedent Tinker v. Des Moines Independent Community School District (holding that "undifferentiated fear or apprehension of disturbance is not enough to overcome the right to freedom of expression"), ordered the Board of Education to come up with an arrangement permitting the distribution of the survey to the juniors and seniors. However, Motley's ruling was overturned on appeal to the United States Court of Appeals for the Second Circuit. Judge J. Edward Lumbard, joined by Judge Murray Gurfein and over an impassioned dissent by Judge Walter R. Mansfield, held that the distribution of the questionnaires was properly disallowed by the administration since there was basis for the belief that it might "result in significant emotional harm to a number of students throughout the Stuyvesant population." The Supreme Court denied certiorari review.
The annual theater competition known as SING! pits seniors, juniors, and "soph-frosh" (freshmen and sophomores working together) against each other in a contest to put on the best performance. SING! started in 1947 at Midwood High School in Brooklyn and has expanded to many New York City high schools since then. SING! at Stuyvesant started as a small event in 1973, and since then, has grown to a school-wide event; in 2005, nearly 1,000 students participated. The entire production is written, directed, produced, and funded by students. Their involvement ranges from being members of the production's casts, choruses, or costume and tech crews to Irish dance, Step, Bollywood, Hip-Hop, Swing, Ballet, Jazz or Latin dance groups. SING! begins in late January to February and culminates in final performances on three nights in March/April. Scoring is done on each night's performances and the winner is determined by the overall total.
Stuyvesant has produced many notable alumni, including four Nobel laureates. In 2017, Stuyvesant was ranked 71st in national rankings by U.S. News & World Report, and 21st among STEM high schools. According to a September 2002 high school ranking by Worth magazine, 3.67% of Stuyvesant students went on to attend Harvard, Princeton, and Yale universities, ranking it as the 9th top public high school in the United States and 120th among all schools, public or private. In December 2007, The Wall Street Journal studied the freshman classes at eight selective colleges and reported that Stuyvesant sent 67 students to these schools, comprising 9.9% of its 674 seniors.Stuyvesant, along with other similar schools, has regularly been excluded from Newsweek's annual list of the Top 100 Public High Schools. The May 8, 2008, issue states the reason as being, "because so many of their students score well above average on the SAT and ACT." U.S. News & World Report, however, included Stuyvesant on its list of "Best High Schools" published in December 2009, ranking 31st.  In its 2010 progress report, the New York City Department of Education assigned it an "A", the highest possible grade.Stuyvesant has contributed to the education of several Nobel laureates, winners of the Fields Medal and the Wolf Prize, and other accomplished alumni. In recent years, it has had the second highest number of National Merit Scholarship semi-finalists, behind Thomas Jefferson High School for Science and Technology in Alexandria, Virginia. From 2002 to 2010, Stuyvesant has produced 103 semi-finalists and 13 finalists on the Intel Science Talent Search, the second most of any secondary school in the United States behind the Bronx High School of Science. In 2014, Stuyvesant had 11 semifinalists for the Intel Search, the highest number of any school in the U.S.In the 2010s, exam schools including Stuyvesant have been the subject of studies questioning their effectiveness. A study by Massachusetts Institute of Technology and Duke University economists compared two sets of SHSAT test takers, whose scores differed only by a few points. One group of students failed to be accepted into Stuyvesant because they had answered a few questions incorrectly, while the other group was composed of Stuyvesant students who had the minimum number of correct test answers that were necessary for an acceptance offer. These economists found that there was no discernible difference in the two groups' average SAT and AP scores, and there were high overlaps into the colleges to which both groups were accepted.
Notable scientists among Stuyvesant alumni include mathematician Paul Cohen (1950), string theorist Brian Greene (1980), physicist Lisa Randall (1980), and genomic researcher Eric Lander (1974). Other prominent alumni include civil rights leader Robert Parris Moses, MAD Magazine editor Nick Meglin (1953), entertainers such as Thelonious Monk (1935), and actors Lucy Liu (1986), Tim Robbins (1976), and James Cagney (1918), comedian Paul Reiser (1973), sports anchor Mike Greenberg (1985), and basketball player and bookmaker Jack Molinas (1949). In business, government and politics, former United States Attorney General Eric Holder (1969) is a Stuyvesant alumnus, as are Senior Advisor to President Obama David Axelrod (1972), former adviser to President Clinton Dick Morris (1964), and founder of 5W Public Relations Ronn Torossian (1992).Pulitzer Prize-winning author Frank McCourt taught English at Stuyvesant before the publication of his memoirs Angela's Ashes, 'Tis, and Teacher Man. Teacher Man's third section, titled Coming Alive in Room 205, concerns McCourt's time at Stuyvesant, and mentions a number of students and faculty. former New York City Council member Eva Moskowitz (1982) graduated from the school, as did the creator of the BitTorrent protocol, Bram Cohen (1993). A notable Olympic medalist from the school was foil fencer Albert Axelrod. Economist Thomas Sowell was also a student of Stuyvesant High School, but dropped out at age 17 because of financial difficulties and problems in his home.Four Nobel laureates are also alumni of Stuyvesant:

Styracosaurus ( sti-RAK-ə-SOR-əs; meaning "spiked lizard" from the Ancient Greek styrax/στύραξ "spike at the butt-end of a spear-shaft" and sauros/σαῦρος "lizard") is a genus of herbivorous ceratopsian dinosaur from the Cretaceous Period (Campanian stage), about 75.5 to 75 million years ago. It had four to six long parietal spikes extending from its neck frill, a smaller jugal horn on each of its cheeks, and a single horn protruding from its nose, which may have been up to 60 centimetres (2 feet) long and 15 centimetres (6 inches) wide. The function or functions of the horns and frills have been debated for many years.
Styracosaurus was a relatively large dinosaur, reaching lengths of 5.5 metres (18 feet) and weighing nearly 3 tonnes. It stood about 1.8 meters (5.9 feet) tall. Styracosaurus possessed four short legs and a bulky body. Its tail was rather short. The skull had a beak and shearing cheek teeth arranged in continuous dental batteries, suggesting that the animal sliced up plants. Like other ceratopsians, this dinosaur may have been a herd animal, travelling in large groups, as suggested by bonebeds.
Named by Lawrence Lambe in 1913, Styracosaurus is a member of the Centrosaurinae. One species, S. albertensis, is currently assigned to Styracosaurus. Another species, S. ovatus, named in 1930 by Charles Gilmore was reassigned to a new genus, Rubeosaurus, by Andrew McDonald and Jack Horner in 2010.
Individuals of the genus Styracosaurus were approximately 5.5 metres (18 ft) long as adults and weighed around 2.7 tonnes. The skull was massive, with a large nostril, a tall straight nose horn, and a parietosquamosal frill (a neck frill) crowned with at least four large spikes. Each of the four longest frill spines was comparable in length to the nose horn, at 50 to 55 centimetres (20 to 22 inches) long. The nasal horn was estimated by Lambe at 57 centimeters (22 inches) long in the type specimen, but the tip had not been preserved. Based on other nasal horn cores from Styracosaurus and Centrosaurus, this horn may have come to a more rounded point at around half of that length.
Aside from the large nasal horn and four long frill spikes, the cranial ornamentation was variable. Some individuals had small hook-like projections and knobs at the posterior margin of the frill, similar to but smaller than those in Centrosaurus. Others had less prominent tabs. Some, like the type individual, had a third pair of long frill spikes. Others had much smaller projections, and small points are found on the side margins of some but not all specimens. Modest pyramid-shaped brow horns were present in subadults, but were replaced by pits in adults. Like most ceratopsids, Styracosaurus had large fenestrae (skull openings) in its frill. The front of the mouth had a toothless beak.
The bulky body of Styracosaurus resembled that of a rhinoceros. It had powerful shoulders which may have been useful in intraspecies combat. Styracosaurus had a relatively short tail. Each toe bore a hooflike ungual which was sheathed in horn.Various limb positions have been proposed for Styracosaurus and ceratopsids in general, including forelegs which were held underneath the body, or, alternatively, held in a sprawling position. The most recent work has put forward an intermediate crouched position as most likely.
The first fossil remains of Styracosaurus were collected in Alberta, Canada by C.M. Sternberg (from an area now known as Dinosaur Provincial Park, in a formation now called the Dinosaur Park Formation) and named by Lawrence Lambe in 1913. This quarry was revisited in 1935 by a Royal Ontario Museum crew who found the missing lower jaws and most of the skeleton. These fossils indicate that S. albertensis was around 5.5 to 5.8 meters in length and stood about 1.65 meters high at the hips. An unusual feature of this first skull is that the smallest frill spike on the left side is partially overlapped at its base by the next spike. It appears that the frill suffered a break at this point in life and was shortened by about 6 centimeters (2.4 inches). The normal shape of this area is unknown because the corresponding area of the right side of the frill was not recovered.Barnum Brown and crew, working for the American Museum of Natural History in New York, collected a nearly complete articulated skeleton with a partial skull in 1915. These fossils were also found in the Dinosaur Park Formation, near Steveville, Alberta. Brown and Erich Maren Schlaikjer compared the finds, and, though they allowed that both specimens were from the same general locality and geological formation, they considered the specimen sufficiently distinct from the holotype to warrant erecting a new species, and described the fossils as Styracosaurus parksi, named in honor of William Parks. Among the differences between the specimens cited by Brown and Schlaikjer were a cheekbone quite different from that of S. albertensis, and smaller tail vertebrae. S. parksi also had a more robust jaw, a shorter dentary, and the frill differed in shape from that of the type species. However, much of the skull consisted of plaster reconstruction, and the original 1937 paper did not illustrate the actual skull bones. It is now accepted as a specimen of S. albertensis.
In the summer of 2006, Darren Tanke of the Royal Tyrrell Museum of Palaeontology in Drumheller, Alberta relocated the long lost S. parksi site. Pieces of the skull, evidently abandoned by the 1915 crew, were found in the quarry. These were collected and it is hoped more pieces will be found, perhaps enough to warrant a redescription of the skull and test whether S. albertensis and S. parksi are the same. The Tyrrell Museum has also collected several partial Styracosaurus skulls. At least one confirmed bonebed (bonebed 42) in Dinosaur Provincial Park has also been explored (other proposed Styracosaurus bonebeds instead have fossils from a mix of animals, and nondiagnostic ceratopsian remains). Bonebed 42 is known to contain numerous pieces of skulls such as horncores, jaws and frill pieces.A third species, S. ovatus, from the Two Medicine Formation of Montana, was described by Gilmore in 1930. The fossil material is limited, with the best being a portion of the parietal bone of the frill, but one unusual feature is that the pair of spikes closest to the midline converge towards the midline, rather than away from it as in S. albertensis. There also may only have been two sets of spikes on each side of the frill, instead of three. The spikes are much shorter than in S. albertensis, with the longest only 295 millimeters (11.6 inches) long. A 2010 review of styracosaur skull remains by Ryan, Holmes, and Russell found it to be a distinct species, and in 2010 McDonald and Horner placed it in its own genus, Rubeosaurus.Several other species which were assigned to Styracosaurus have since been assigned to other genera. S. sphenocerus, described by Edward Drinker Cope in 1890 as a species of Monoclonius and based on a nasal bone with a broken Styracosaurus-like straight nose horn, was attributed to Styracosaurus in 1915. "S. makeli", mentioned informally by amateur paleontologists Stephen and Sylvia Czerkas in 1990 in a caption to an illustration, is an early name for Einiosaurus. "S. borealis" is an early informal name for S. parksi.
Styracosaurus is a member of the Centrosaurinae. Other members of the clade include Centrosaurus (from which the group takes its name), Pachyrhinosaurus, Avaceratops, Einiosaurus, Albertaceratops, Achelousaurus, Brachyceratops, and Monoclonius, although these last two are dubious. Because of the variation between species and even individual specimens of centrosaurines, there has been much debate over which genera and species are valid, particularly whether Centrosaurus and/or Monoclonius are valid genera, undiagnosable, or possibly members of the opposite sex. In 1996, Peter Dodson found enough variation between Centrosaurus, Styracosaurus, and Monoclonius to warrant separate genera, and that Styracosaurus resembled Centrosaurus more closely than either resembled Monoclonius. Dodson also believed one species of Monoclonius, M. nasicornis, may actually have been a female Styracosaurus. However, most other researchers have not accepted Monoclonius nasicornis as a female Styracosaurus, instead regarding it as a synonym of Centrosaurus apertus. While sexual dimorphism has been proposed for an earlier ceratopsian, Protoceratops, there is no firm evidence for sexual dimorphism in any ceratopsid.The cladogram depicted below represents a phylogenetic analysis by Chiba et al. (2017):
The evolutionary origins of Styracosaurus were not understood for many years because fossil evidence for early ceratopsians was sparse. The discovery of Protoceratops, in 1922, shed light on early ceratopsid relationships, but several decades passed before additional finds filled in more of the blanks. Fresh discoveries in the late 1990s and 2000s, including Zuniceratops, the earliest known ceratopsian with brow horns, and Yinlong, the first known Jurassic ceratopsian, indicate what the ancestors of Styracosaurus may have looked like. These new discoveries have been important in illuminating the origins of horned dinosaurs in general, and suggest that the group originated during the Jurassic in Asia, with the appearance of true horned ceratopsians occurring by the beginning of the late Cretaceous in North America.Goodwin and colleagues proposed in 1992 that Styracosaurus was part of the lineage leading to Einiosaurus, Achelousaurus and Pachyrhinosaurus. This was based on a series of fossil skulls from the Two Medicine Formation of Montana. The position of Styracosaurus in this lineage is now equivocal, as the remains that were thought to represent Styracosaurus have been transferred to the genus Rubeosaurus.Styracosaurus is known from a higher position in the formation (relating specifically to its own genus) than the closely related Centrosaurus, suggesting that Styracosaurus displaced Centrosaurus as the environment changed over time and/or dimension. It has been suggested that Styracosaurus albertensis is a direct descendant of Centrosaurus (C. apertus or C. nasicornis), and that it in turn evolved directly into the slightly later species Rubeosaurus ovatus. Subtle changes can be traced in the arrangement of the horns through this lineage, leading from Rubeosaurus to Einiosaurus, to Achelousaurus and Pachyrhinosaurus. However, the lineage may not be a simple, straight line, as a pachyrhinosaur-like species has been reported from the same time and place as Styracosaurus albertensis.
Styracosaurus and other horned dinosaurs are often depicted in popular culture as herd animals. A bonebed composed of Styracosaurus remains is known from the Dinosaur Park Formation of Alberta, about halfway up the formation. This bonebed is associated with different types of river deposits. The mass deaths may have been a result of otherwise non-herding animals congregating around a waterhole in a period of drought, with evidence suggesting the environment may have been seasonal and semiarid.Paleontologists Gregory Paul and Per Christiansen of the Zoological Museum of the University of Copenhagen in Denmark proposed that large ceratopsians such as Styracosaurus were able to run faster than an elephant, based on possible ceratopsian trackways which did not exhibit signs of sprawling forelimbs.
Styracosaurs were herbivorous dinosaurs; they probably fed mostly on low growth because of the position of the head. They may, however, have been able to knock down taller plants with their horns, beak, and bulk. The jaws were tipped with a deep, narrow beak, believed to have been better at grasping and plucking than biting.Ceratopsid teeth, including those of Styracosaurus, were arranged in groups called batteries. Older teeth on top were continually replaced by the teeth underneath them. Unlike hadrosaurids, which also had dental batteries, ceratopsid teeth sliced but did not grind. Some scientists have suggested that ceratopsids like Styracosaurus ate palms and cycads, while others have suggested ferns. Dodson has proposed that Late Cretaceous ceratopsians may have knocked down angiosperm trees and then sheared off leaves and twigs.
The large nasal horns and frills of Styracosaurus are among the most distinctive facial adornments of all dinosaurs. Their function has been the subject of debate since the first horned dinosaurs were discovered.
Early in the 20th century, paleontologist R. S. Lull proposed that the frills of ceratopsian dinosaurs acted as anchor points for their jaw muscles. He later noted that for Styracosaurus, the spikes would have given it a formidable appearance. In 1996, Dodson supported the idea of muscle attachments in part and created detailed diagrams of possible muscle attachments in the frills of Styracosaurus and Chasmosaurus, but did not subscribe to the idea that they completely filled in the fenestrae. C.A. Forster, however, found no evidence of large muscle attachments on the frill bones.It was long believed that ceratopsians like Styracosaurus used their frills and horns in defence against the large predatory dinosaurs of the time. Although pitting, holes, lesions, and other damage on ceratopsid skulls are often attributed to horn damage in combat, a 2006 study found no evidence for horn thrust injuries causing these forms of damage (for example, there is no evidence of infection or healing). Instead, non-pathological bone resorption, or unknown bone diseases, are suggested as causes.
However, a newer study compared incidence rates of skull lesions in Triceratops and Centrosaurus and showed that these were consistent with Triceratops using its horns in combat and the frill being adapted as a protective structure, while lower pathology rates in Centrosaurus may indicate visual rather than physical use of cranial ornamentation, or a form of combat focused on the body rather than the head; as Centrosaurus was more closely related to Styracosaurus and both genera had long nasal horns, the results for this genus would be more applicable for Styracosaurus. The researchers also concluded that the damage found on the specimens in the study was often too localized to be caused by bone disease.The large frill on Styracosaurus and related genera also may have helped to increase body area to regulate body temperature, like the ears of the modern elephant. A similar theory has been proposed regarding the plates of Stegosaurus, although this use alone would not account for the bizarre and extravagant variation seen in different members of the Ceratopsidae. This observation is highly suggestive of what is now believed to be the primary function, display.
The theory of frill use in sexual display was first proposed in 1961 by Davitashvili. This theory has gained increasing acceptance. Evidence that visual display was important, either in courtship or in other social behavior, can be seen in the fact that horned dinosaurs differ markedly in their adornments, making each species highly distinctive. Also, modern living creatures with such displays of horns and adornments use them in similar behavior. The use of the exaggerated structures in dinosaurs as species identification has been questioned, as no such function exists in modern species.
Styracosaurus is known from the Dinosaur Park Formation, and was a member of a diverse and well-documented fauna of prehistoric animals that included horned relatives such as Centrosaurus and Chasmosaurus, duckbills such as Prosaurolophus, Lambeosaurus, Gryposaurus, Corythosaurus, and Parasaurolophus, tyrannosaurids Gorgosaurus, Daspletosaurus, and armored Edmontonia and Euoplocephalus.The Dinosaur Park Formation is interpreted as a low-relief setting of rivers and floodplains that became more swampy and influenced by marine conditions over time as the Western Interior Seaway transgressed westward. The climate was warmer than present-day Alberta, without frost, but with wetter and drier seasons. Conifers were apparently the dominant canopy plants, with an understory of ferns, tree ferns, and angiosperms.

Subarachnoid hemorrhage (SAH) is bleeding into the subarachnoid space—the area between the arachnoid membrane and the pia mater surrounding the brain. Symptoms may include a severe headache of rapid onset, vomiting, decreased level of consciousness, fever, and sometimes seizures. Neck stiffness or neck pain are also relatively common. In about a quarter of people a small bleed with resolving symptoms occurs within a month of a larger bleed.SAH may occur as a result of a head injury or spontaneously, usually from a ruptured cerebral aneurysm. Risk factors for spontaneous cases included high blood pressure, smoking, family history, alcoholism, and cocaine use. Generally, the diagnosis can be determined by a CT scan of the head if done within six hours of symptom onset. Occasionally a lumbar puncture is also required. After confirmation further tests are usually performed to determine the underlying cause.Treatment is by prompt neurosurgery or radiologically guided interventions. Medications such as labetalol may be required to lower the blood pressure until repair can occur. Efforts to treat fevers are also recommended. Nimodipine, a calcium channel blocker, is frequently used to prevent vasospasm. Routine use medications to prevent further seizures is of unclear benefit. Nearly half of people with a SAH due to an underlying aneurysm die within 30 days and about a third who survive have ongoing problems. 10–15 percent die before reaching a hospital.Spontaneous SAH occurs in about one per 10,000 people per year. Females are more commonly affected than males. While it becomes more common with age, about 50% of people present under 55 years old. It is a form of stroke and comprises about 5 percent of all strokes. Surgery for aneurysms was introduced in the 1930s. Since the 1990s many aneurysms are treated by a less invasive procedure called "coiling", which is carried out through a large blood vessel.
The classic symptom of subarachnoid hemorrhage is thunderclap headache (a headache described as "like being kicked in the head", or the "worst ever", developing over seconds to minutes). This headache often pulsates towards the occiput (the back of the head). About one-third of people have no symptoms apart from the characteristic headache, and about one in ten people who seek medical care with this symptom are later diagnosed with a subarachnoid hemorrhage. Vomiting may be present, and 1 in 14 have seizures. Confusion, decreased level of consciousness or coma may be present, as may neck stiffness and other signs of meningism.Neck stiffness usually presents six hours after initial onset of SAH. Isolated dilation of a pupil and loss of the pupillary light reflex may reflect brain herniation as a result of rising intracranial pressure (pressure inside the skull). Intraocular hemorrhage (bleeding into the eyeball) may occur in response to the raised pressure: subhyaloid hemorrhage (bleeding under the hyaloid membrane, which envelops the vitreous body of the eye) and vitreous hemorrhage may be visible on fundoscopy. This is known as Terson syndrome (occurring in 3–13 percent of cases) and is more common in more severe SAH.Oculomotor nerve abnormalities (affected eye looking downward and outward and inability to lift the eyelid on the same side) or palsy (loss of movement) may indicate bleeding from the posterior communicating artery. Seizures are more common if the hemorrhage is from an aneurysm; it is otherwise difficult to predict the site and origin of the hemorrhage from the symptoms. SAH in a person known to have seizures is often diagnostic of a cerebral arteriovenous malformation.The combination of intracerebral hemorrhage and raised intracranial pressure (if present) leads to a "sympathetic surge", i.e. over-activation of the sympathetic system. This is thought to occur through two mechanisms, a direct effect on the medulla that leads to activation of the descending sympathetic nervous system and a local release of inflammatory mediators that circulate to the peripheral circulation where they activate the sympathetic system. As a consequence of the sympathetic surge there is a sudden increase in blood pressure; mediated by increased contractility of the ventricle and increased vasoconstriction leading to increased systemic vascular resistance. The consequences of this sympathetic surge can be sudden, severe, and are frequently life-threatening. The high plasma concentrations of adrenaline also may cause cardiac arrhythmias (irregularities in the heart rate and rhythm), electrocardiographic changes (in 27 percent of cases) and cardiac arrest (in 3 percent of cases) may occur rapidly after the onset of hemorrhage. A further consequence of this process is neurogenic pulmonary edema where a process of increased pressure within the pulmonary circulation causes leaking of fluid from the pulmonary capillaries into the air spaces, the alveoli, of the lung.
Subarachnoid hemorrhage may also occur in people who have had a head injury. Symptoms may include headache, decreased level of consciousness and hemiparesis (weakness of one side of the body). SAH is a frequent occurrence in traumatic brain injury, and carries a poor prognosis if it is associated with deterioration in the level of consciousness.While thunderclap headache is the characteristic symptom of subarachnoid hemorrhage, less than 10% of those with concerning symptoms have SAH on investigations. A number of other causes may need to be considered.
Most cases of SAH are due to trauma such as a blow to the head. Traumatic SAH usually occurs near the site of a skull fracture or intracerebral contusion. It often happens in the setting of other forms of traumatic brain injury. In these cases prognosis is poorer, however, it is unclear if this is a direct result of the SAH or whether the presence of subarachnoid blood is simply an indicator of a more severe head injury.In 85 percent of spontaneous cases the cause is a cerebral aneurysm—a weakness in the wall of one of the arteries in the brain that becomes enlarged. They tend to be located in the circle of Willis and its branches. While most cases are due to bleeding from small aneurysms, larger aneurysms (which are less common) are more likely to rupture. Aspirin also appears to increase the risk.In 15–20 percent of cases of spontaneous SAH, no aneurysm is detected on the first angiogram. About half of these are attributed to non-aneurysmal perimesencephalic hemorrhage, in which the blood is limited to the subarachnoid spaces around the midbrain (i.e. mesencephalon). In these, the origin of the blood is uncertain. The remainder are due to other disorders affecting the blood vessels (such as cerebral arteriovenous malformations), disorders of the blood vessels in the spinal cord, and bleeding into various tumors.Cocaine abuse and sickle cell anemia (usually in children) and, rarely, anticoagulant therapy, problems with blood clotting and pituitary apoplexy can also result in SAH. Dissection of the vertebral artery, usually caused by trauma, can lead to subarachnoid hemorrhage if the dissection involves the part of the vessel inside the skull.
Cerebral vasospasm is one of the complications caused by subarachnoid haemorrhage. It usually happens from the third day after the aneurysm event, and reaches its peak on 5th to 7th day. There are several mechanisms proposed for this complication. Blood products released from subarachnoid haemorrhage stimulates the tyrosine kinase pathway causing the release of calcium ions from intracellular storage, resulting in smooth muscle contraction of cerebral arteries. Oxyhaemoglobin in cerebrospinal fluid (CSF) causes vasoconstriction by increasing free radicals, endothelin-1, prostaglandin and reducing the level of nitric oxide and prostacyclin. Besides, the disturbances of autonomic nervous system innervating cerebral arteries is also thought to cause vasospasm.
As only 10 percent of people admitted to the emergency department with a thunderclap headache are having an SAH, other possible causes are usually considered simultaneously, such as meningitis, migraine, and cerebral venous sinus thrombosis. Intracerebral hemorrhage, in which bleeding occurs within the brain itself, is twice as common as SAH and is often misdiagnosed as the latter. It is not unusual for SAH to be initially misdiagnosed as a migraine or tension headache, which can lead to a delay in obtaining a CT scan. In a 2004 study, this occurred in 12 percent of all cases and was more likely in people who had smaller hemorrhages and no impairment in their mental status. The delay in diagnosis led to a worse outcome. In some people, the headache resolves by itself, and no other symptoms are present. This type of headache is referred to as "sentinel headache", because it is presumed to result from a small leak (a "warning leak") from an aneurysm. A sentinel headache still warrants investigations with CT scan and lumbar puncture, as further bleeding may occur in the subsequent three weeks.The initial steps for evaluating a person with a suspected subarachnoid hemorrhage are obtaining a medical history and performing a physical examination. The diagnosis cannot be made on clinical grounds alone and in general medical imaging and possibly a lumbar puncture is required to confirm or exclude bleeding.
The modality of choice is computed tomography (CT scan), without contrast, of the brain. This has a high sensitivity and will correctly identify 98.7% of cases within six hours of the onset of symptoms. Its efficacy declines thereafter, and Magnetic resonance imaging (MRI) may be more sensitive than CT after several days.
Lumbar puncture, in which cerebrospinal fluid (CSF) is removed from the subarachnoid space of the spinal canal using a hypodermic needle, shows evidence of hemorrhage in three percent of people in whom CT was found normal; lumbar puncture is therefore regarded as mandatory in people with suspected SAH if imaging is negative. At least three tubes of CSF are collected. If an elevated number of red blood cells is present equally in all bottles, this indicates a subarachnoid hemorrhage. If the number of cells decreases per bottle, it is more likely that it is due to damage to a small blood vessel during the procedure (known as a "traumatic tap"). While there is no official cutoff for red blood cells in the CSF no documented cases have occurred at less than "a few hundred cells" per high-powered field.The CSF sample is also examined for xanthochromia—the yellow appearance of centrifugated fluid. This can be determined by spectrophotometry (measuring the absorption of particular wavelengths of light) or visual examination. It is unclear which method is superior. Xanthochromia remains a reliable ways to detect SAH several days after the onset of headache. An interval of at least 12 hours between the onset of the headache and lumbar puncture is required, as it takes several hours for the hemoglobin from the red blood cells to be metabolized into bilirubin.
After a subarachnoid hemorrhage is confirmed, its origin needs to be determined. If the bleeding is likely to have originated from an aneurysm (as determined by the CT scan appearance), the choice is between cerebral angiography (injecting radiocontrast through a catheter to the brain arteries) and CT angiography (visualizing blood vessels with radiocontrast on a CT scan) to identify aneurysms. Catheter angiography also offers the possibility of coiling an aneurysm (see below).
Electrocardiographic changes are relatively common in subarachnoid hemorrhage, occurring in 40–70 percent of cases. They may include QT prolongation, Q waves, cardiac dysrhythmias, and ST elevation that mimics a heart attack.
There are several grading scales available for SAH. The Glasgow Coma Scale (GCS) is ubiquitously used for assessing consciousness. Three specialized scores are used to evaluate SAH; in each, a higher number is associated with a worse outcome. These scales have been derived by retrospectively matching characteristics of people with their outcomes.
This scale has been modified by Claassen and coworkers, reflecting the additive risk from SAH size and accompanying intraventricular hemorrhage (0 – none; 1 – minimal SAH w/o IVH; 2 – minimal SAH with IVH; 3 – thick SAH w/o IVH; 4 – thick SAH with IVH);.The World Federation of Neurosurgeons (WFNS) classification uses Glasgow coma score and focal neurological deficit to gauge severity of symptoms.
A comprehensive classification scheme has been suggested by Ogilvy and Carter to predict outcome and gauge therapy. The system consists of five grades and it assigns one point for the presence or absence of each of five factors: age greater than 50; Hunt and Hess grade 4 or 5; Fisher scale 3 or 4; aneurysm size greater than 10 mm; and posterior circulation aneurysm 25 mm or more.
Screening for aneurysms is not performed on a population level; because they are relatively rare, it would not be cost-effective. If someone has two or more first-degree relatives who have had an aneurysmal subarachnoid hemorrhage, screening may be worthwhile.Autosomal dominant polycystic kidney disease (ADPKD), a hereditary kidney condition, is known to be associated with cerebral aneurysms in 8 percent of cases, but most such aneurysms are small and therefore unlikely to rupture. As a result, screening is only recommended in families with ADPKD where one family member has had a ruptured aneurysm.An aneurysm may be detected incidentally on brain imaging; this presents a conundrum, as all treatments for cerebral aneurysms are associated with potential complications. The International Study of Unruptured Intracranial Aneurysms (ISUIA) provided prognostic data both in people having previously had a subarachnoid hemorrhage and people who had aneurysms detected by other means. Those having previously had a SAH were more likely to bleed from other aneurysms. In contrast, those having never bled and had small aneurysms (smaller than 10 mm) were very unlikely to have a SAH and were likely to sustain harm from attempts to repair these aneurysms. On the basis of the ISUIA and other studies, it is now recommended that people are considered for preventive treatment only if they have a reasonable life expectancy and have aneurysms that are highly likely to rupture. At the same time, there is only limited evidence that endovascular treatment of unruptured aneurysms is actually beneficial.
Management involves general measures to stabilize the person while also using specific investigations and treatments. These include the prevention of rebleeding by obliterating the bleeding source, prevention of a phenomenon known as vasospasm, and prevention and treatment of complications.Stabilizing the person is the first priority. Those with a depressed level of consciousness may need to be intubated and mechanically ventilated. Blood pressure, pulse, respiratory rate, and Glasgow Coma Scale are monitored frequently. Once the diagnosis is confirmed, admission to an intensive care unit may be preferable, especially since 15 percent may have further bleeding soon after admission. Nutrition is an early priority, mouth or nasogastric tube feeding being preferable over parenteral routes. In general, pain control is restricted to less-sedating agents such as codeine, as sedation may impact on the mental status and thus interfere with the ability to monitor the level of consciousness. Deep vein thrombosis is prevented with compression stockings, intermittent pneumatic compression of the calves, or both. A bladder catheter is usually inserted to monitor fluid balance. Benzodiazepines may be administered to help relieve distress. Antiemetic drugs should be given to awake persons.People with poor clinical grade on admission, acute neurologic deterioration, or progressive enlargement of ventricles on CT scan are, in general, indications for the placement of an external ventricular drain by a neurosurgeon. The external ventricular drain may be inserted at the bedside or in the operating room. In either case, strict aseptic technique must be maintained during insertion. In people with aneurysmal subarachnoid hemorrhage the EVD is used to remove cerebrospinal fluid, blood, and blood byproducts that increase intracranial pressure and may increase the risk for cerebral vasospasm.
Efforts to keep a person's systolic blood pressure below somewhere between 140 and 160 mmHg is generally recommended. Medications to achieve this may include labetalol or nicardipine.People whose CT scan shows a large hematoma, depressed level of consciousness, or focal neurologic signs may benefit from urgent surgical removal of the blood or occlusion of the bleeding site. The remainder are stabilized more extensively and undergo a transfemoral angiogram or CT angiogram later. It is hard to predict who will have a rebleed, yet it may happen at any time and carries a dismal prognosis. After the first 24 hours have passed, rebleeding risk remains around 40 percent over the subsequent four weeks, suggesting that interventions should be aimed at reducing this risk as soon as possible. Some predictors of early rebleeding are high systolic blood pressure, the presence of a hematoma in the brain or ventricles, poor Hunt-Hess grade (III-IV), aneurysms in the posterior circulation, and an aneurysm >10 mm in size.If a cerebral aneurysm is identified on angiography, two measures are available to reduce the risk of further bleeding from the same aneurysm: clipping and coiling. Clipping requires a craniotomy (opening of the skull) to locate the aneurysm, followed by the placement of clips around the neck of the aneurysm. Coiling is performed through the large blood vessels (endovascularly): a catheter is inserted into the femoral artery in the groin and advanced through the aorta to the arteries (both carotid arteries and both vertebral arteries) that supply the brain. When the aneurysm has been located, platinum coils are deployed that cause a blood clot to form in the aneurysm, obliterating it. The decision as to which treatment is undertaken is typically made by a multidisciplinary team consisting of a neurosurgeon, neuroradiologist, and often other health professionals.In general, the decision between clipping and coiling is made on the basis of the location of the aneurysm, its size and the condition of the person. Aneurysms of the middle cerebral artery and its related vessels are hard to reach with angiography and tend to be amenable to clipping. Those of the basilar artery and posterior cerebral artery are hard to reach surgically and are more accessible for endovascular management. These approaches are based on general experience, and the only randomized controlled trial directly comparing the different modalities was performed in relatively well people with small (less than 10 mm) aneurysms of the anterior cerebral artery and anterior communicating artery (together the "anterior circulation"), who constitute about 20 percent of all people with aneurysmal SAH. This trial, the International Subarachnoid Aneurysm Trial (ISAT), showed that in this group the likelihood of death or being dependent on others for activities of daily living was reduced (7.4 percent absolute risk reduction, 23.5 percent relative risk reduction) if endovascular coiling was used as opposed to surgery. The main drawback of coiling is the possibility that the aneurysm will recur; this risk is extremely small in the surgical approach. In ISAT, 8.3 percent needed further treatment in the longer term. Hence, people who have undergone coiling are typically followed up for many years afterwards with angiography or other measures to ensure recurrence of aneurysms is identified early. Other trials have also found a higher rate of recurrence necessitating further treatments.
Vasospasm, in which the blood vessels constrict and thus restrict blood flow, is a serious complication of SAH. It can cause ischemic brain injury (referred to as "delayed ischemia") and permanent brain damage due to lack of oxygen in parts of the brain. It can be fatal if severe. Delayed ischemia is characterized by new neurological symptoms, and can be confirmed by transcranial doppler or cerebral angiography. About one third of people admitted with subarachnoid hemorrhage will have delayed ischemia, and half of those have permanent damage as a result. It is possible to screen for the development of vasospasm with transcranial Doppler every 24–48 hours. A blood flow velocity of more than 120 centimeters per second is suggestive of vasospasm.The use of calcium channel blockers, thought to be able to prevent the spasm of blood vessels by preventing calcium from entering smooth muscle cells, has been proposed for prevention. The calcium channel blocker nimodipine when taken by mouth improves outcome if given between the fourth and twenty-first day after the bleeding, even if it does not reduce the amount of vasospasm detected on angiography. It is the only Food and Drug Administration (FDA) approved drug for treating cerebral vasospasm. In traumatic subarachnoid hemorrhage, nimodipine does not affect long-term outcome, and is not recommended. Other calcium channel blockers and magnesium sulfate have been studied, but are not presently recommended; neither is there any evidence that shows benefit if nimodipine is given intravenously.Some older studies have suggested that statin therapy might reduce vasospasm, but a subsequent meta-analysis including further trials did not demonstrate benefit on either vasospasm or outcomes. While corticosteroids with mineralocorticoid activity may help prevent vasospasm their use does not appear to change outcomes.
A protocol referred to as "triple H" is often used as a measure to treat vasospasm when it causes symptoms; this is the use of intravenous fluids to achieve a state of hypertension (high blood pressure), hypervolemia (excess fluid in the circulation), and hemodilution (mild dilution of the blood). Evidence for this approach is inconclusive; no randomized controlled trials have been undertaken to demonstrate its effect.If the symptoms of delayed ischemia do not improve with medical treatment, angiography may be attempted to identify the sites of vasospasms and administer vasodilator medication (drugs that relax the blood vessel wall) directly into the artery. Angioplasty (opening the constricted area with a balloon) may also be performed.
Hydrocephalus (obstruction of the flow of cerebrospinal fluid) may complicate SAH in both the short and long term. It is detected on CT scanning, on which there is enlargement of the lateral ventricles. If the level of consciousness is decreased, drainage of the excess fluid is performed by therapeutic lumbar puncture, extraventricular drain (a temporary device inserted into one of the ventricles), or occasionally a permanent shunt. Relief of hydrocephalus can lead to an enormous improvement in a person's condition. Fluctuations in blood pressure and electrolyte imbalance, as well as pneumonia and cardiac decompensation occur in about half the hospitalized persons with SAH and may worsen prognosis. Seizures occur during the hospital stay in about a third of cases.People have often been treated with preventative antiepileptic medications. This is controversial and not based on good evidence. In some studies, use of these medications was associated with a worse prognosis; although it is unclear whether this might be because the drugs themselves actually cause harm, or because they are used more often in persons with a poorer prognosis. There is a possibility of a gastric hemorrhage due to stress ulcers.
SAH is often associated with a poor outcome. The death rate (mortality) for SAH is between 40 and 50 percent, but trends for survival are improving. Of those that survive hospitalization, more than a quarter have significant restrictions in their lifestyle, and less than a fifth have no residual symptoms whatsoever. Delay in diagnosis of minor SAH (mistaking the sudden headache for migraine) contributes to poor outcome. Factors found on admission that are associated with poorer outcome include poorer neurological grade; systolic hypertension; a previous diagnosis of heart attack or SAH; liver disease; more blood and larger aneurysm on the initial CT scan; location of an aneurysm in the posterior circulation; and higher age. Factors that carry a worse prognosis during the hospital stay include occurrence of delayed ischemia resulting from vasospasm, development of intracerebral hematoma, or intraventricular hemorrhage (bleeding into the ventricles of the brain) and presence of fever on the eighth day of admission.So-called "angiogram-negative subarachnoid hemorrhage", SAH that does not show an aneurysm with four-vessel angiography, carries a better prognosis than SAH with aneurysm, but it is still associated with a risk of ischemia, rebleeding, and hydrocephalus. Perimesencephalic SAH (bleeding around the mesencephalon in the brain), however, has a very low rate of rebleeding or delayed ischemia, and the prognosis of this subtype is excellent.The prognosis of head trauma is thought to be influenced in part by the location and amount of subarachnoid bleeding. It is difficult to isolate the effects of SAH from those of other aspects of traumatic brain injury; it is unknown whether the presence of subarachnoid blood actually worsens the prognosis or whether it is merely a sign that a significant trauma has occurred. People with moderate and severe traumatic brain injury who have SAH when admitted to a hospital have as much as twice the risk of dying as those who do not. They also have a higher risk of severe disability and persistent vegetative state, and traumatic SAH has been correlated with other markers of poor outcome such as post traumatic epilepsy, hydrocephalus, and longer stays in the intensive care unit.  More than 90 percent of people with traumatic subarachnoid bleeding and a Glasgow Coma Score over 12 have a good outcome.There is also modest evidence that genetic factors influence the prognosis in SAH. For example, having two copies of ApoE4 (a variant of the gene encoding apolipoprotein E that also plays a role in Alzheimer's disease) seems to increase risk for delayed ischemia and a worse outcome. The occurrence of hyperglycemia (high blood sugars) after an episode of SAH confers a higher risk of poor outcome.
Neurocognitive symptoms, such as fatigue, mood disturbances, and other related symptoms are common sequelae. Even in those who have made good neurological recovery, anxiety, depression, posttraumatic stress disorder, and cognitive impairment are common; 46 percent of people who have had a subarachnoid hemorrhage have cognitive impairment that affects their quality of life. Over 60 percent report frequent headaches. Aneurysmal subarachnoid hemorrhage may lead to damage of the hypothalamus and the pituitary gland, two areas of the brain that play a central role in hormonal regulation and production. More than a quarter of people with a previous SAH may develop hypopituitarism (deficiencies in one or more of the hypothalamic-pituitary hormones such as growth hormone, luteinizing hormone, or follicle-stimulating hormone).
According to a review of 51 studies from 21 countries, the average incidence of subarachnoid hemorrhage is 9.1 per 100,000 annually. Studies from Japan and Finland show higher rates in those countries (22.7 and 19.7, respectively), for reasons that are not entirely understood. South and Central America, in contrast, have a rate of 4.2 per 100,000 on average.Although the group of people at risk for SAH is younger than the population usually affected by stroke, the risk still increases with age. Young people are much less likely than middle-age people (risk ratio 0.1, or 10 percent) to have a subarachnoid hemorrhage. The risk continues to rise with age and is 60 percent higher in the very elderly (over 85) than in those between 45 and 55. Risk of SAH is about 25 percent higher in women over 55 compared to men the same age, probably reflecting the hormonal changes that result from the menopause, such as a decrease in estrogen levels.Genetics may play a role in a person's disposition to SAH; risk is increased three- to fivefold in first-degree relatives of people having had a subarachnoid hemorrhage. But lifestyle factors are more important in determining overall risk. These risk factors are smoking, hypertension (high blood pressure), and excessive alcohol consumption. Having smoked in the past confers a doubled risk of SAH compared to those who have never smoked. Some protection of uncertain significance is conferred by caucasian ethnicity, hormone replacement therapy, and diabetes mellitus. There is likely an inverse relationship between total serum cholesterol and the risk of non-traumatic SAH, though confirmation of this association is hindered by a lack of studies. Approximately 4 percent of aneurysmal bleeds occur after sexual intercourse and 10 percent of people with SAH are bending over or lifting heavy objects at the onset of their symptoms.Overall, about 1 percent of all people have one or more cerebral aneurysms. Most of these are small and unlikely to rupture.
While the clinical picture of subarachnoid hemorrhage may have been recognized by Hippocrates, the existence of cerebral aneurysms and the fact that they could rupture was not established until the 18th century. The associated symptoms were described in more detail in 1886 by Edinburgh physician Dr Byrom Bramwell. In 1924, London neurologist Sir Charles P. Symonds (1890–1978) gave a complete account of all major symptoms of subarachnoid hemorrhage, and he coined the term "spontaneous subarachnoid hemorrhage". Symonds also described the use of lumbar puncture and xanthochromia in diagnosis.The first surgical intervention was performed by Norman Dott, who was a pupil of Harvey Cushing then working in Edinburgh. He introduced the wrapping of aneurysms in the 1930s, and was an early pioneer in the use of angiograms. American neurosurgeon Dr Walter Dandy, working in Baltimore, was the first to introduce clips in 1938. Microsurgery was applied to aneurysm treatment in 1972 in order to further improve outcomes. The 1980s saw the introduction of triple H therapy as a treatment for delayed ischemia due to vasospasm, and trials with nimodipine in an attempt to prevent this complication. In 1983, the Russian neurosurgeon Zubkov and colleagues reported the first use of transluminal balloon angioplasty for vasospasm after aneurysmal SAH. The Italian neurosurgeon Dr. Guido Guglielmi introduced his endovascular coil treatment in 1991.
