
The McDonnell Douglas F-4 Phantom II is a tandem two-seat, twin-engine, all-weather, long-range supersonic jet interceptor and fighter-bomber originally developed for the United States Navy by McDonnell Aircraft. It first entered service in 1960 with the U.S. Navy.  Proving highly adaptable, it was also adopted by the U.S. Marine Corps and the U.S. Air Force, and by the mid-1960s had become a major part of their air arms.The Phantom is a large fighter with a top speed of over Mach 2.2. It can carry more than 18,000 pounds (8,400 kg) of weapons on nine external hardpoints, including air-to-air missiles, air-to-ground missiles, and various bombs. The F-4, like other interceptors of its time, was designed without an internal cannon. Later models incorporated an M61 Vulcan rotary cannon. Beginning in 1959, it set 15 world records for in-flight performance, including an absolute speed record, and an absolute altitude record.The F-4 was used extensively during the Vietnam War. It served as the principal air superiority fighter for the U.S. Air Force, Navy, and Marine Corps and became important in the ground-attack and aerial reconnaissance roles late in the war.  During the Vietnam War, one U.S. Air Force pilot, two weapon systems officers (WSOs), one U.S. Navy pilot and one radar intercept officer (RIO) became aces by achieving five aerial kills against enemy fighter aircraft. The F-4 continued to form a major part of U.S. military air power throughout the 1970s and 1980s, being gradually replaced by more modern aircraft such as the F-15 Eagle and F-16 Fighting Falcon in the U.S. Air Force, the F-14 Tomcat in the U.S. Navy, and the F/A-18 Hornet in the U.S. Navy and U.S. Marine Corps.
The F-4 Phantom II remained in use by the U.S. in the reconnaissance and Wild Weasel (Suppression of Enemy Air Defenses) roles in the 1991 Gulf War, finally leaving service in 1996.  It was also the only aircraft used by both U.S. flight demonstration teams: the USAF Thunderbirds (F-4E) and the US Navy Blue Angels (F-4J). The F-4 was also operated by the armed forces of 11 other nations. Israeli Phantoms saw extensive combat in several Arab–Israeli conflicts, while Iran used its large fleet of Phantoms, acquired before the fall of the Shah, in the Iran–Iraq War. Phantom production ran from 1958 to 1981, with a total of 5,195 built, making it the most produced American supersonic military aircraft.  As of 2018, 60 years after its first flight, the F-4 remains in service with Iran, Japan, South Korea, Greece, and Turkey. The aircraft has most recently been in service against the Islamic State group in the Middle East.
In 1952, McDonnell's Chief of Aerodynamics, Dave Lewis, was appointed by CEO Jim McDonnell to be the company's preliminary design manager. With no new aircraft competitions on the horizon, internal studies concluded the Navy had the greatest need for a new and different aircraft type: an attack fighter.
In 1953, McDonnell Aircraft began work on revising its F3H Demon naval fighter, seeking expanded capabilities and better performance. The company developed several projects including a variant powered by a Wright J67 engine, and variants powered by two Wright J65 engines, or two General Electric J79 engines.  The J79-powered version promised a top speed of Mach 1.97. On 19 September 1953, McDonnell approached the United States Navy with a proposal for the "Super Demon". Uniquely, the aircraft was to be modular—it could be fitted with one- or two-seat noses for different missions, with different nose cones to accommodate radar, photo cameras, four 20 mm (.79 in) cannon, or 56 FFAR unguided rockets in addition to the nine hardpoints under the wings and the fuselage. The Navy was sufficiently interested to order a full-scale mock-up of the F3H-G/H, but felt that the upcoming Grumman XF9F-9 and Vought XF8U-1 already satisfied the need for a supersonic fighter.The McDonnell design was therefore reworked into an all-weather fighter-bomber with 11 external hardpoints for weapons and on 18 October 1954, the company received a letter of intent for two YAH-1 prototypes. On 26 May 1955, four Navy officers arrived at the McDonnell offices and, within an hour, presented the company with an entirely new set of requirements. Because the Navy already had the Douglas A-4 Skyhawk for ground attack and F-8 Crusader for dogfighting, the project now had to fulfill the need for an all-weather fleet defense interceptor. A second crewman was added to operate the powerful radar.
The XF4H-1 was designed to carry four semi-recessed AAM-N-6 Sparrow III radar-guided missiles, and to be powered by two J79-GE-8 engines. As in the McDonnell F-101 Voodoo, the engines sat low in the fuselage to maximize internal fuel capacity and ingested air through fixed geometry intakes. The thin-section wing had a leading edge sweep of 45° and was equipped with blown flaps for better low-speed handling.Wind tunnel testing had revealed lateral instability requiring the addition of 5° dihedral to the wings. To avoid redesigning the titanium central section of the aircraft, McDonnell engineers angled up only the outer portions of the wings by 12°, which averaged to the required 5° over the entire wingspan. The wings also received the distinctive "dogtooth" for improved control at high angles of attack. The all-moving tailplane was given 23° of anhedral to improve control at high angles of attack while still keeping the tailplane clear of the engine exhaust. In addition, air intakes were equipped with variable geometry ramps to regulate airflow to the engines at supersonic speeds. All-weather intercept capability was achieved thanks to the AN/APQ-50 radar. To accommodate carrier operations, the landing gear was designed to withstand landings with a sink rate of 23 ft/s (7 m/s), while the nose strut could extend by some 20 in (51 cm) to increase angle of attack at takeoff.
On 25 July 1955, the Navy ordered two XF4H-1 test aircraft and five YF4H-1 pre-production examples. The Phantom made its maiden flight on 27 May 1958 with Robert C. Little at the controls. A hydraulic problem precluded retraction of the landing gear but subsequent flights went more smoothly. Early testing resulted in redesign of the air intakes, including the distinctive addition of 12,500 holes to "bleed off" the slow-moving boundary layer air from the surface of each intake ramp. Series production aircraft also featured splitter plates to divert the boundary layer away from the engine intakes. The aircraft soon squared off against the XF8U-3 Crusader III. Due to operator workload, the Navy wanted a two-seat aircraft and on 17 December 1958 the F4H was declared a winner. Delays with the J79-GE-8 engines meant that the first production aircraft were fitted with J79-GE-2 and −2A engines, each having 16,100 lbf (71.8 kN) of afterburning thrust. In 1959, the Phantom began carrier suitability trials with the first complete launch-recovery cycle performed on 15 February 1960 from Independence.There were proposals to name the F4H "Satan" and "Mithras". In the end, the aircraft was given the less controversial name "Phantom II", the first "Phantom" being another McDonnell jet fighter, the FH-1 Phantom. The Phantom II was briefly given the designation F-110A and the name "Spectre" by the USAF, but neither name was officially used.
Early in production, the radar was upgraded to the Westinghouse AN/APQ-72, an AN/APG-50 with a larger radar antenna, necessitating the bulbous nose, and the canopy was reworked to improve visibility and make the rear cockpit less claustrophobic. During its career the Phantom underwent many changes in the form of numerous variants developed.
The USAF received Phantoms as the result of Defense Secretary Robert McNamara's push to create a unified fighter for all branches of the military. After an F-4B won the "Operation Highspeed" fly-off against the Convair F-106 Delta Dart, the USAF borrowed two Naval F-4Bs, temporarily designating them F-110A "Spectre" in January 1962, and developed requirements for their own version. Unlike the navy's focus on interception, the USAF emphasized a fighter-bomber role. With McNamara's unification of designations on 18 September 1962, the Phantom became the F-4 with the naval version designated F-4B and USAF F-4C. The first Air Force Phantom flew on 27 May 1963, exceeding Mach 2 on its maiden flight.The USN operated the F4H-1 (re-designated F-4A in 1962) with J79-GE-2 and -2A engines of 16,100 lbf (71.62 kN) thrust and later builds receiving -8 engines. A total of 45 F-4As were built and none saw combat and most ended up as test or training aircraft. The USN and USMC received the first definitive Phantom, the F-4B which was equipped with the Westinghouse APQ-72 radar (pulse only), a Texas Instruments AAA-4 Infra-red search and track pod under the nose, an AN/AJB-3 bombing system and powered by J79-GE-8,-8A and -8B engines of 10,900 lbf (48.5 kN) dry and 16,950 lbf (75.4 kN) afterburner (reheat) with the first flight on 25 March 1961. 649 F-4Bs were built with deliveries beginning in 1961 and VF-121 Pacemakers receiving the first examples at NAS Miramar.The F-4J had improved air-to-air and ground-attack capability; deliveries begun in 1966 and ended in 1972 with 522 built. It was equipped with J79-GE-10 engines with 17,844 lbf (79.374 kN) thrust, the Westinghouse AN/AWG-10 Fire Control System (making the F-4J the first fighter in the world with operational look-down/shoot-down capability), a new integrated missile control system and the AN/AJB-7 bombing system for expanded ground attack capability.The F-4N (updated F-4Bs) with smokeless engines and F-4J aerodynamic improvements started in 1972 under a U.S. Navy-initiated refurbishment program called "Project Bee Line" with 228 converted by 1978. The F-4S model resulted from the refurbishment of 265 F-4Js with J79-GE-17 smokeless engines of 17,900 lbf (79.379 kN), AWG-10B radar with digitized circuitry for improved performance and reliability, Honeywell AN/AVG-8 Visual Target Acquisition Set or VTAS (world's first operational Helmet Sighting System), classified avionics improvements, airframe reinforcement and leading edge slats for enhanced maneuvering. The USMC also operated the RF-4B with reconnaissance cameras with 46 built.Phantom II production ended in the United States in 1979 after 5,195 had been built (5,057 by McDonnell Douglas and 138 in Japan by Mitsubishi). Of these, 2,874 went to the USAF, 1,264 to the Navy and Marine Corps, and the rest to foreign customers. The last U.S.-built F-4 went to South Korea, while the last F-4 built was an F-4EJ built by Mitsubishi Heavy Industries in Japan and delivered on 20 May 1981. As of 2008, 631 Phantoms were in service worldwide, while the Phantoms were in use as a target drone (specifically QF-4Cs) operated by the U.S. military until 21 December 2016, when the Air Force officially ended use of the type.
To show off their new fighter, the Navy led a series of record-breaking flights early in Phantom development: All in all, the Phantom set 16 world records. Except for Skyburner, all records were achieved in unmodified production aircraft. Five of the speed records remained unbeaten until the F-15 Eagle appeared in 1975.
Operation Top Flight: On 6 December 1959, the second XF4H-1 performed a zoom climb to a world record 98,557 ft (30,040 m). Commander Lawrence E. Flint Jr., USN accelerated his aircraft to Mach 2.5 (2,660 km/h; 1,650 mph) at 47,000 ft (14,330 m) and climbed to 90,000 ft (27,430 m) at a 45° angle. He then shut down the engines and glided to the peak altitude. As the aircraft fell through 70,000 ft (21,300 m), Flint restarted the engines and resumed normal flight.
On 5 September 1960, an F4H-1 averaged 1,216.78 mph (1,958.16 km/h) over a 500 km (311 mi) closed-circuit course.
On 25 September 1960, an F4H-1F averaged 1,390.24 mph (2,237.37 km/h) over a 100 km (62.1 mi) closed-circuit course. FAIRecord File Number 8898.
Operation LANA: To celebrate the 50th anniversary of Naval aviation (L is the Roman numeral for 50 and ANA stood for Anniversary of Naval Aviation) on 24 May 1961, Phantoms flew across the continental United States in under three hours and included several tanker refuelings. The fastest of the aircraft averaged 869.74 mph (1,400.28 km/h) and completed the trip in 2 hours 47 minutes, earning the pilot (and future NASA Astronaut), Lieutenant Richard Gordon, USN and RIO, Lieutenant Bobbie Young, USN, the 1961 Bendix trophy.
Operation Sageburner: On 28 August 1961, a F4H-1F Phantom II averaged 1,452.777 kilometers per hour (902.714 miles per hour) over a 3 mi (4.82 km) course flying below 125 feet (38.1 m) at all times. Commander J.L. Felsman, USN was killed during the first attempt at this record on 18 May 1961 when his aircraft disintegrated in the air after pitch damper failure.
Operation Skyburner: On 22 November 1961, a modified Phantom with water injection, piloted by Lt. Col. Robert B. Robinson, set an absolute world record average speed over a 20-mile (32.2 km) long 2-way straight course of 1,606.342 mph (2,585.086 km/h).
Operation High Jump: A series of time-to-altitude records was set in early 1962: 34.523 seconds to 3,000 meters (9,840 ft), 48.787 seconds to 6,000 meters (19,700 ft), 61.629 seconds to 9,000 meters (29,500 ft), 77.156 seconds to 12,000 meters (39,400 ft), 114.548 seconds to 15,000 meters (49,200 ft), 178.5 seconds to 20,000 meters (65,600 ft), 230.44 seconds to 25,000 metres (82,000 ft), and 371.43 seconds to 30,000 metres (98,400 ft).
The F-4 Phantom is a tandem-seat fighter-bomber designed as a carrier-based interceptor to fill the U.S. Navy's fleet defense fighter role. Innovations in the F-4 included an advanced pulse-Doppler radar and extensive use of titanium in its airframe.Despite imposing dimensions and a maximum takeoff weight of over 60,000 lb (27,000 kg), the F-4 has a top speed of Mach 2.23 and an initial climb rate of over 41,000 ft/min (210 m/s). The F-4's nine external hardpoints have a capability of up to 18,650 pounds (8,480 kg) of weapons, including air-to-air and air-to-surface missiles, and unguided, guided, and thermonuclear weapons. Like other interceptors of its day, the F-4 was designed without an internal cannon.The baseline performance of a Mach 2-class fighter with long range and a bomber-sized payload would be the template for the next generation of large and light/middle-weight fighters optimized for daylight air combat.
In air combat, the Phantom's greatest advantage was its thrust, which permitted a skilled pilot to engage and disengage from the fight at will. As a massive fighter aircraft designed to fire radar-guided missiles from beyond visual range, it lacked the agility of its Soviet opponents and was subject to adverse yaw during hard maneuvering. Although thus subject to irrecoverable spins during aileron rolls, pilots reported the aircraft to be very communicative and easy to fly on the edge of its performance envelope. In 1972, the F-4E model was upgraded with leading edge slats on the wing, greatly improving high angle of attack maneuverability at the expense of top speed.
The J79 engines produced noticeable amounts of black smoke (at mid-throttle/cruise settings), a severe disadvantage in that the enemy could spot the aircraft. This was solved on the F-4S fitted with the −10A engine variant which used a smokeless combustor.The F-4's biggest weakness, as it was initially designed, was its lack of an internal cannon. For a brief period, doctrine held that turning combat would be impossible at supersonic speeds and little effort was made to teach pilots air combat maneuvering. In reality, engagements quickly became subsonic, as pilots would slow down in an effort to get behind their adversaries. Furthermore, the relatively new heat-seeking and radar-guided missiles at the time were frequently reported as unreliable and pilots had to use multiple shots (also known as ripple-firing), just to hit one enemy fighter. To compound the problem, rules of engagement in Vietnam precluded long-range missile attacks in most instances, as visual identification was normally required. Many pilots found themselves on the tail of an enemy aircraft but too close to fire short-range Falcons or Sidewinders. Although by 1965 USAF F-4Cs began carrying SUU-16 external gunpods containing a 20 mm (.79 in) M61A1 Vulcan Gatling cannon, USAF cockpits were not equipped with lead-computing gunsights until the introduction of the SUU-23, virtually assuring a miss in a maneuvering fight. Some Marine Corps aircraft carried two pods for strafing. In addition to the loss of performance due to drag, combat showed the externally mounted cannon to be inaccurate unless frequently boresighted, yet far more cost-effective than missiles. The lack of a cannon was finally addressed by adding an internally mounted 20 mm (.79 in) M61A1 Vulcan on the F-4E.
Note: Original amounts were in 1965 U.S. dollars. The figures in these tables have been adjusted for inflation to the current year.
In USAF service, the F-4 was initially designated the F-110 Spectre prior to the introduction of the 1962 United States Tri-Service aircraft designation system. The USAF quickly embraced the design and became the largest Phantom user. The first USAF Phantoms in Vietnam were F-4Cs from the 555th Tactical Fighter Squadron "Triple Nickel", which arrived in December 1964.Unlike the U.S. Navy and U.S. Marine Corps, which flew the Phantom with a Naval Aviator (pilot) in the front seat and a Naval Flight Officer as a radar intercept officer (RIO) in the back seat, the USAF initially flew its Phantoms with a rated Air Force Pilot in front and back seats. While the rear pilot (GIB, or "guy in back") could fly and ostensibly land the aircraft, he had fewer flight instruments and a very restricted forward view. The Air Force later assigned a rated Air Force Navigator qualified as a weapon/targeting systems officer (later designated as weapon systems officer or WSO) in the rear seat instead of another pilot.On 10 July 1965, F-4Cs of the 45th Tactical Fighter Squadron, 15th TFW, on temporary assignment in Ubon, Thailand, scored the USAF's first victories against North Vietnamese MiG-17s using AIM-9 Sidewinder air-to-air missiles. On 26 April 1966, an F-4C from the 480th Tactical Fighter Squadron scored the first aerial victory by a U.S. aircrew over a North Vietnamese MiG-21 "Fishbed". On 24 July 1965, another Phantom from the 45th Tactical Fighter Squadron became the first American aircraft to be downed by an enemy SAM, and on 5 October 1966 an 8th Tactical Fighter Wing F-4C became the first U.S. jet lost to an air-to-air missile, fired by a MiG-21.
Early aircraft suffered from leaks in wing fuel tanks that required re-sealing after each flight and 85 aircraft were found to have cracks in outer wing ribs and stringers. There were also problems with aileron control cylinders, electrical connectors, and engine compartment fires. Reconnaissance RF-4Cs made their debut in Vietnam on 30 October 1965, flying the hazardous post-strike reconnaissance missions. The USAF Thunderbirds used the F-4E from the 1969 season until 1974.
Although the F-4C was essentially identical to the Navy/Marine Corps F-4B in flight performance and carried the AIM-9 Sidewinder missiles, USAF-tailored F-4Ds initially arrived in June 1967 equipped with AIM-4 Falcons. However, the Falcon, like its predecessors, was designed to shoot down heavy bombers flying straight and level. Its reliability proved no better than others and its complex firing sequence and limited seeker-head cooling time made it virtually useless in combat against agile fighters. The F-4Ds reverted to using Sidewinders under the "Rivet Haste" program in early 1968, and by 1972 the AIM-7E-2 "Dogfight Sparrow" had become the preferred missile for USAF pilots. Like other Vietnam War Phantoms, the F-4Ds were urgently fitted with radar warning receivers to detect the Soviet-built S-75 Dvina SAMs.From the initial deployment of the F-4C to Southeast Asia, USAF Phantoms performed both air superiority and ground attack roles, supporting not only ground troops in South Vietnam but also conducting bombing sorties in Laos and North Vietnam. As the F-105 force underwent severe attrition between 1965 and 1968, the bombing role of the F-4 proportionately increased until after November 1970 (when the last F-105D was withdrawn from combat) it became the primary USAF tactical ordnance delivery system. In October 1972 the first squadron of EF-4C Wild Weasel aircraft deployed to Thailand on temporary duty. The "E" prefix was later dropped and the aircraft was simply known as the F-4C Wild Weasel.
Sixteen squadrons of Phantoms were permanently deployed between 1965 and 1973, and 17 others deployed on temporary combat assignments. Peak numbers of combat F-4s occurred in 1972, when 353 were based in Thailand. A total of 445 Air Force Phantom fighter-bombers were lost, 370 in combat and 193 of those over North Vietnam (33 to MiGs, 30 to SAMs, and 307 to AAA).The RF-4C was operated by four squadrons, and of the 83 losses, 72 were in combat including 38 over North Vietnam (seven to SAMs and 65 to AAA). By war's end, the U.S. Air Force had lost a total of 528 F-4 and RF-4C Phantoms. When combined with U.S. Navy and Marine Corps losses of 233 Phantoms, 761 F-4/RF-4 Phantoms were lost in the Vietnam War.On 28 August 1972, Captain Steve Ritchie became the first USAF ace of the war. On 9 September 1972, WSO Capt Charles B. DeBellevue became the highest-scoring American ace of the war with six victories. and WSO Capt Jeffrey Feinstein became the last USAF ace of the war on 13 October 1972. Upon return to the United States, DeBellevue and Feinstein were assigned to undergraduate pilot training (Feinstein was given a vision waiver) and requalified as USAF pilots in the F-4. USAF F-4C/D/E crews claimed 107½ MiG kills in Southeast Asia (50 by Sparrow, 31 by Sidewinder, five by Falcon, 15.5 by gun, and six by other means).On 31 January 1972, the 170th Tactical Fighter Squadron/183d Tactical Fighter Group of the Illinois Air National Guard became the first Air National Guard unit to transition to Phantoms from Republic F-84F Thunderstreaks which were found to have corrosion problems.  Phantoms would eventually equip numerous tactical fighter and tactical reconnaissance units in the USAF active, National Guard, and reserve.
On 2 June 1972, a Phantom flying at supersonic speed shot down a MiG-19 over Thud Ridge in Vietnam for the first supersonic gun kill. At a recorded speed of Mach 1.2, Major Phil Handley's shoot down was the first and only recorded gun kill while flying at supersonic speeds.
On 15 August 1990, 24 F-4G Wild Weasel Vs and six RF-4Cs were deployed to Shaikh Isa AB, Bahrain, for Operation Desert Storm. The F-4G was the only aircraft in the USAF inventory equipped for the Suppression of Enemy Air Defenses (SEAD) role, and was needed to protect coalition aircraft from Iraq's extensive air defense system. The RF-4C was the only aircraft equipped with the ultra-long-range KS-127 LOROP (long-range oblique photography) camera, and was used for a variety of reconnaissance missions. In spite of flying almost daily missions, only one RF-4C was lost in a fatal accident before the start of hostilities. One F-4G was lost when enemy fire damaged the fuel tanks and the aircraft ran out of fuel near a friendly airbase. The last USAF Phantoms, F-4G Wild Weasel Vs from 561st Fighter Squadron, were retired on 26 March 1996. The last operational flight of the F-4G Wild Weasel was from the 190th Fighter Squadron, Idaho Air National Guard, in April 1996. The last operational USAF/ANG F-4 to land was flown by Maj Mike Webb and Maj Gary Leeder of the Idaho ANG.
Like the Navy, the Air Force has operated QF-4 target drones, serving with the 82d Aerial Targets Squadron at Tyndall Air Force Base, Florida, and Holloman Air Force Base, New Mexico. It was expected that the F-4 would remain in the target role with the 82d ATRS until at least 2015, when they would be replaced by early versions of the F-16 Fighting Falcon converted to a QF-16 configuration. Several QF-4s also retain capability as manned aircraft and are maintained in historical color schemes, being displayed as part of Air Combat Command's Heritage Flight at air shows, base open houses, and other events while serving as non-expendable target aircraft during the week. On 19 November 2013, BAE Systems delivered the last QF-4 aerial target to the Air Force. The example had been in storage for over 20 years before being converted. Over 16 years, BAE had converted 314 F-4 and RF-4 Phantom IIs into QF-4s and QRF-4s, with each aircraft taking six months to adapt. As of December 2013, QF-4 and QRF-4 aircraft had flown over 16,000 manned and 600 unmanned training sorties, with 250 unmanned aircraft being shot down in firing exercises. The remaining QF-4s and QRF-4s held their training role until the first of 126 QF-16s were delivered by Boeing. The final flight of an Air Force QF-4 from Tyndall AFB took place on 27 May 2015 to Holloman AFB. After Tyndall AFB ceased operations, the 53d Weapons Evaluation Group at Holloman became the fleet of 22 QF-4s' last remaining operator.  The base continued using them to fly manned test and unmanned live fire test support and Foreign Military Sales testing, with the final unmanned flight taking place in August 2016. The type was officially retired from US military service with a four–ship flight at Holloman during an event on 21 December 2016. The remaining QF-4s were to be demilitarized after 1 January 2017.
On 30 December 1960, the VF-121 "Pacemakers" at NAS Miramar became the first Phantom operator with its F4H-1Fs (F-4As). The VF-74 "Be-devilers" at NAS Oceana became the first deployable Phantom squadron when it received its F4H-1s (F-4Bs) on 8 July 1961. The squadron completed carrier qualifications in October 1961 and Phantom's first full carrier deployment between August 1962 and March 1963 aboard Forrestal. The second deployable U.S. Atlantic Fleet squadron to receive F-4Bs was the VF-102 "Diamondbacks", who promptly took their new aircraft on the shakedown cruise of Enterprise. The first deployable U.S. Pacific Fleet squadron to receive the F-4B was the VF-114 "Aardvarks", which participated in the September 1962 cruise aboard USS Kitty Hawk.By the time of the Tonkin Gulf incident, 13 of 31 deployable navy squadrons were armed with the type. F-4Bs from Constellation made the first Phantom combat sortie of the Vietnam War on 5 August 1964, flying bomber escort in Operation Pierce Arrow. The first Phantom air-to-air victory of the war took place on 9 April 1965 when an F-4B from VF-96 "Fighting Falcons" piloted by Lieutenant (junior grade) Terence M. Murphy and his RIO, Ensign Ronald Fegan, shot down a Chinese MiG-17 "Fresco". The Phantom was then shot down, probably by an AIM-7 Sparrow from one of its wingmen. There continues to be controversy over whether the Phantom was shot down by MiG guns or, as enemy reports later indicated, an AIM-7 Sparrow III from one of Murphy's and Fegan's wingmen. On 17 June 1965, an F-4B from VF-21 "Freelancers" piloted by Commander Louis Page and Lieutenant John C. Smith shot down the first North Vietnamese MiG of the war.On 10 May 1972, Lieutenant Randy "Duke" Cunningham and Lieutenant (junior grade) William P. Driscoll flying an F-4J, call sign "Showtime 100", shot down three MiG-17s to become the first American flying aces of the war. Their fifth victory was believed at the time to be over a mysterious North Vietnamese ace, Colonel Nguyen Toon, now considered mythical. On the return flight, the Phantom was damaged by an enemy surface-to-air missile. To avoid being captured, Cunningham and Driscoll flew their burning aircraft using only the rudder and afterburner (the damage to the aircraft rendered conventional control nearly impossible), until they could eject over water.
During the war, U.S. Navy F-4 Phantom squadrons participated in 84 combat tours with F-4Bs, F-4Js, and F-4Ns. The Navy claimed 40 air-to-air victories at a cost of 73 Phantoms lost in combat (seven to enemy aircraft, 13 to SAMs, and 53 to AAA). An additional 54 Phantoms were lost in mishaps.In 1984, the F-4Ns had been retired, and by 1987 the last F-4Ss were retired in the U.S. Navy deployable squadrons. On 25 March 1986, an F-4S belonging to the VF-151 "Vigilantes," became the last active duty U.S. Navy Phantom to launch from an aircraft carrier, in this case, Midway. On 18 October 1986, an F-4S from the VF-202 "Superheats", a Naval Reserve fighter squadron, made the last-ever Phantom carrier landing while operating aboard America. In 1987, the last of the Naval Reserve-operated F-4S aircraft were replaced by F-14As. The last Phantoms in service with the Navy were QF-4 target drones operated by the Naval Air Warfare Center at NAS Point Mugu, California. These airframes were subsequently retired in 2004.
The Marine Corps received its first F-4Bs in June 1962, with the "Black Knights" of VMFA-314 at Marine Corps Air Station El Toro, California becoming the first operational squadron. Marine Phantoms from VMFA-531 "Gray Ghosts" were assigned to Da Nang airbase on South Vietnam's northeast coast on 10 May 1965 and were initially assigned to provide air defense for the USMC. They soon began close air support missions (CAS) and VMFA-314 'Black Knights', VMFA-232 'Red Devils, VMFA-323 'Death Rattlers', and VMFA-542 'Bengals' soon arrived at the primitive airfield. Marine F-4 pilots claimed three enemy MiGs (two while on exchange duty with the USAF) at the cost of 75 aircraft lost in combat, mostly to ground fire, and four in accidents. VMCJ-1 Golden Hawks (now VMAQ-1 and VMAQ-4 which has the old RM tailcode) flew the first RF-4B photo recon mission on 3 November 1966 from Da Nang and remained there until 1970 with no RF-4B losses and one damaged by AAA. VMCJ-2 and VMCJ-3 (now VMAQ-3) provided aircraft for VMCJ-1 in Da Nang and VMFP-3 was formed in 1975 at MCAS El Toro, CA consolidating all USMC RF-4Bs in one unit that became known as "The Eyes of the Corps." VMFP-3 disestablished in August 1990 after the Advanced Tactical Airborne Reconnaissance System was introduced for the F/A-18D Hornet. The F-4 continued to equip fighter-attack squadrons in both Marine Corps active and reserve units throughout the 1960s, 1970s and 1980s and into the early 1990s. In the early 1980s, these squadrons began to transition to the F/A-18 Hornet, starting with the same squadron that introduced the F-4 to the Marine Corps, VMFA-314 at MCAS El Toro, California. On 18 January 1992, the last Marine Corps Phantom, an F-4S in the Marine Corps Reserve, was retired by the "Cowboys" of VMFA-112, after which the squadron was re-equipped with F/A-18 Hornets.
The USAF and the US Navy had high expectations of the F-4 Phantom, assuming that the massive firepower, the best available on-board radar, the highest speed and acceleration properties, coupled with new tactics, would provide Phantoms with an advantage over the MiGs. But in confrontations with the lighter MiG-21, F-4s began to suffer losses. Over the course of the air war in Vietnam, between 3 April 1965 and 8 January 1973, each side would ultimately claim favorable kill ratios.During the war, U.S. Navy F-4 Phantom scored 40 air-to-air victories at a loss of seven Phantoms to enemy aircraft. U.S Marine F-4 pilots claimed three enemy MiGs at the cost of one aircraft in air-combat. USAF F-4 Phantom crews claimed 107½ MiG kills (including 33½ MiG-17s, eight MiG-19s and 66 MiG-21s) at a cost of 33 Phantoms in air-combat F-4 pilots claimed a total of 150½ MiG kills at a cost of 42 Phantoms in air-combat. 
However, the VPAF claimed 103 F-4 Phantoms were shot down by MiG-21s at a cost of 54 MiG-21s were shot down by F-4 Phantoms. During the war, VPAF lost only 131 MiGs in air combat (63 MiG-17s, eight MiG-19s and 60 MiG-21s), one half by F-4s. From 1966 to November 1968, in 46 air battles conducted over North Vietnam between F-4s and MiG-21s, VPAF claimed 27 F-4s were shot down by MiG-21s at a cost of 20 MiG-21s In 1970, one F-4 Phantom were shot down by MiG-21. In 1972, total of 201 air battles took place between American and Vietnamese planes, VPAF lost 54 MiGs (including 36 MiG-21s and one MiG-21US) and they claimed 90 U.S aircraft were shot down, including 74 F-4 Phantoms and two spy RF-4C (MiG-21 shot down 67 enemy aircraft, MiG-17 shot down 11 and MiG-19 shot down 12 enemy aircraft)
The culmination of the struggle in the air in early 1972 was 10 May when VPAF aircraft completed 64 sorties, resulting in 15 air battles. The VPAF claimed seven F-4s were shot down, while U.S. confirmed five F-4s were lost. Those, in turn, managed to destroy two MiG-21s, three MiG-17s and one MiG-19. On 11 May, two MiG-21s, which played the role of "bait", brought the four F-4s to two MiG-21s circling at low altitude. The MiGs quickly stormed the Phantoms and 3 missiles shot down two F-4s. On 18 May, Vietnamese aircraft made 26 sorties in eight air engagements, which cost 4 F-4 Phantoms; Vietnamese fighters on that day did not suffer losses. On 13 June, a MiG-21 unit intercepted a group of F-4s, the second pair of MiGs made a missile attack and was hit by two F-4s and did not suffer losses.
The Phantom has served with the air forces of many countries, including Australia, Egypt, Germany, United Kingdom, Greece, Iran, Israel, Japan, Spain, South Korea and Turkey.
The Royal Australian Air Force (RAAF) leased 24 USAF F-4Es from 1970 to 1973 while waiting for their order for the General Dynamics F-111C to be delivered. They were so well-liked that the RAAF considered retaining the aircraft after the F-111Cs were delivered. They were operated from RAAF Amberley by No. 1 Squadron and No. 6 Squadron.
In 1979, the Egyptian Air Force purchased 35 former USAF F-4Es along with a number of Sparrow, Sidewinder, and Maverick missiles from the U.S. for $594 million as part of the "Peace Pharaoh" program. An additional seven surplus USAF aircraft were purchased in 1988. Three attrition replacements had been received by the end of the 1990s.
The German Air Force (Luftwaffe) initially ordered the reconnaissance RF-4E in 1969, receiving a total of 88 aircraft from January 1971. In 1982, the initially unarmed RF-4Es were given a secondary ground attack capability; these aircraft were retired in 1994.
In 1973, under the "Peace Rhine" program, the Luftwaffe purchased the F-4F (a lightened and simplified version of the F-4E) which was upgraded in the mid-1980s. 24 German F-4F Phantom IIs were operated by the 49th Tactical Fighter Wing of the USAF at Holloman AFB to train Luftwaffe crews until December 2004. In 1975, Germany also received 10 F-4Es for training in the U.S. In the late 1990s, these were withdrawn from service after being replaced by F-4Fs. Germany also initiated the Improved Combat Efficiency (ICE) program in 1983. The 110 ICE-upgraded F-4Fs entered service in 1992, and were expected to remain in service until 2012. All the remaining Luftwaffe Phantoms were based at Wittmund with Jagdgeschwader 71 (fighter wing 71) in Northern Germany and WTD61 at Manching.  Phantoms were deployed to NATO states under the Baltic Air Policing starting in 2005, 2008, 2009, 2011 and 2012.  The German Air Force retired its last F-4Fs on 29 June 2013. German F-4Fs flew 279,000 hours from entering service on 31 August 1973 until retirement.
In 1971, the Hellenic Air Force ordered brand new F-4E Phantoms, with deliveries starting in 1974. In the early 1990s, the Hellenic AF acquired surplus RF-4Es and F-4Es from the Luftwaffe and U.S. ANG.Following the success of the German ICE program, on 11 August 1997, a contract was signed between DASA of Germany and Hellenic Aerospace Industry for the upgrade of 39 aircraft to the very similar "Peace Icarus 2000" standard. The Hellenic AF operated 34 upgraded F-4E-PI2000 (338 and 339 Squadrons) and 12 RF-4E aircraft (348 Squadron) as of September 2013.
On 5 May 2017, the Hellenic Air Force officially retired the RF-4E Phantom II during a public ceremony.
In the 1960s and 1970s when the U.S. and Iran were on friendly terms, the U.S. sold 225 F-4D, F-4E, and RF-4E Phantoms to Iran. The Imperial Iranian Air Force saw at least one engagement, resulting in a loss, after an RF-4C was rammed by a Soviet MiG-21 during Project Dark Gene, an ELINT operation during the Cold War.
The Islamic Republic of Iran Air Force Phantoms saw heavy action in the Iran–Iraq War in the 1980s and are kept operational by overhaul and servicing from Iran's aerospace industry. Notable operations of Iranian F-4s during the war included Operation Scorch Sword, an attack by two F-4s against the Iraqi Osirak nuclear reactor site near Baghdad on 30 September 1980, and the attack on H3, a 4 April 1981 strike by eight Iranian F-4s against the H-3 complex of air bases in the far west of Iraq, which resulted in many Iraqi aircraft being destroyed or damaged for no Iranian losses.On 5 June 1984, two Saudi Arabian fighter pilots shot down two Iranian F-4 fighters. The Royal Saudi Air Force pilots were flying American-built F-15s and fired air-to-air missiles to bring down the Iranian planes. The Saudi fighter pilots had KC-135 aerial tanker planes and Boeing E-3 Sentry AWACS surveillance planes assist in the encounter. The aerial fight occurred in Saudi airspace over the Persian Gulf near the Saudi island Al Arabiyah, about 60 miles northeast of Jubail.Iranian F-4s were in use as of late 2014; the aircraft reportedly conducted air strikes on ISIS targets in the eastern Iraqi province of Diyala.
The Israeli Air Force was the largest foreign operator of the Phantom, flying both newly built and ex-USAF aircraft, as well as several one-off special reconnaissance variants. The first F-4Es, nicknamed "Kurnass" (Sledgehammer), and RF-4Es, nicknamed "Orev" (Raven), were delivered in 1969 under the "Peace Echo I" program. Additional Phantoms arrived during the 1970s under "Peace Echo II" through "Peace Echo V" and "Nickel Grass" programs. Israeli Phantoms saw extensive combat during Arab–Israeli conflicts, first seeing action during the War of Attrition. In the 1980s, Israel began the "Kurnass 2000" modernization program which significantly updated avionics. The last Israeli F-4s were retired in 2004.
From 1968, the Japan Air Self-Defense Force (JASDF) purchased a total of 140 F-4EJ Phantoms without aerial refueling, AGM-12 Bullpup missile system, nuclear control system or ground attack capabilities. Mitsubishi built 138 under license in Japan and 14 unarmed reconnaissance RF-4Es were imported.  One of the aircraft (17-8440) was the very last of the 5,195 F-4 Phantoms to be produced. It was manufactured by Mitsubishi Heavy Industries on 21 May 1981. "The Final Phantom" served with 306th Tactical Fighter Squadron and later transferred to the 301st Tactical Fighter Squadron.
Of these, 96 F-4EJs were modified to the F-4EJ Kai (改, modified) standard. 15 F-4EJs were converted to reconnaissance aircraft designated RF-4EJ, with similar upgrades as the F-4EJ Kai. Japan had a fleet of 90 F-4s in service in 2007. After studying several replacement fighters the F-35 Lightning II was chosen in 2011. Delays with the F-35 program have meant that some F-4s have remained in service. As of 2017 all three of the JASDF's remaining Phantom squadrons are based at Hyakuri Air Base in Ibaraki prefecture north of Tokyo. Among the remaining three squadrons, the 302nd Squadron is scheduled to be reorganized into the first JASDF F-35 Squadron at Misawa Air Base in the beginning of 2019. The other two squadrons are anticipated to retire their F-4s in 2020. Some F-4s are also operated by the Air Development and Test Wing in Gifu Prefecture.
The Republic of Korea Air Force purchased its first batch of secondhand USAF F-4D Phantoms in 1968 under the "Peace Spectator" program. The F-4Ds continued to be delivered until 1988. The "Peace Pheasant II" program also provided new-built and former USAF F-4Es.
The Spanish Air Force acquired its first batch of ex-USAF F-4C Phantoms in 1971 under the "Peace Alfa" program. Designated C.12, the aircraft were retired in 1989. At the same time, the air arm received a number of ex-USAF RF-4Cs, designated CR.12. In 1995–1996, these aircraft received extensive avionics upgrades. Spain retired its RF-4s in 2002.
The Turkish Air Force (TAF) received 40 F-4Es in 1974, with a further 32 F-4Es and 8 RF-4Es in 1977–78 under the "Peace Diamond III" program, followed by 40 ex-USAF aircraft in "Peace Diamond IV" in 1987, and a further 40 ex-U.S. Air National Guard Aircraft in 1991. A further 32 RF-4Es were transferred to Turkey after being retired by the Luftwaffe between 1992 and 1994. In 1995, Israel Aerospace Industries (IAI) implemented an upgrade similar to Kurnass 2000 on 54 Turkish F-4Es which were dubbed the F-4E 2020 Terminator. Turkish F-4s, and more modern F-16s have been used to strike Kurdish PKK bases in ongoing military operations in Northern Iraq. On 22 June 2012, a Turkish RF-4E was shot down by Syrian air defenses while flying a reconnaissance flight near the Turkish-Syrian border. Turkey has stated the reconnaissance aircraft was in international airspace when it was shot down, while Syrian authorities stated it was inside Syrian airspace. Turkish F-4s remained in use as of 2015.On 24 February 2015, two RF-4Es crashed in the Malatya region in the southeast of Turkey, under yet unknown circumstances, killing both crew of two each. On 5 March 2015, an F-4E-2020 crashed in central Anatolia killing both crew. After the recent accidents, the TAF withdrew RF-4Es from active service. Turkey was reported to have used F-4 jets to attack PKK separatists and the ISIS capital on 19 September 2015. The Turkish Air Force has reportedly used the F-4E 2020s against the more recent Third Phase of the PKK conflict on heavy bombardment missions into Iraq on 15 November 2015, 12 January 2016, and 12 March 2016.
The United Kingdom bought versions based on the U.S. Navy's F-4J for use with the Royal Air Force and the Royal Navy's Fleet Air Arm. The UK was the only country outside the United States to operate the Phantom at sea, launching them from HMS Ark Royal. The main differences were the use of the British Rolls-Royce Spey engines and of British-made avionics. The RN and RAF versions were given the designation F-4K and F-4M respectively, and entered service with the British military aircraft designations Phantom FG.1 (fighter/ground attack) and Phantom FGR.2 (fighter/ground attack/reconnaissance). Initially, the FGR.2 was used in the ground attack and reconnaissance role, primarily with RAF Germany, while 43 Squadron was formed in the air defence role using the FG.1s that had been intended for the Fleet Air Arm for use aboard HMS Eagle. The superiority of the Phantom over the English Electric Lightning in terms of both range and weapon load, combined with the successful introduction of the SEPECAT Jaguar, meant that, during the mid-1970s, most of the ground attack Phantoms in Germany were redeployed to the UK to replace air defence Lightning squadrons. A second RAF squadron, 111 Squadron, was formed on the FG.1 in 1979 after the disbandment of 892 NAS.
In 1982, during the Falklands War, three Phantom FGR2s of No. 29 Squadron were on active Quick Reaction Alert duty on Ascension Island to protect the base from air attack. After the Falklands War, 15 upgraded ex-USN F-4Js, known as the F-4J(UK) entered RAF service to compensate for one interceptor squadron redeployed to the Falklands.Around 15 RAF squadrons received various marks of Phantom, many of them based in Germany. The first to be equipped was No. 228 Operational Conversion Unit at RAF Coningsby in August 1968. One noteworthy operator was No. 43 Squadron where Phantom FG1s remained the squadron equipment for 20 years, arriving in September 1969 and departing in July 1989. During this period the squadron was based at Leuchars.The interceptor Phantoms were replaced by the Panavia Tornado F3 from the late 1980s onwards, and the last British Phantoms were retired in October 1992 when No. 74 Squadron was disbanded.
Sandia National Laboratories used an F-4 mounted on a "rocket sled" in a crash test to see the results of an aircraft hitting a reinforced concrete structure, such as a nuclear power plant.
One aircraft, an F-4D (civilian registration N749CF), is operated by the Massachusetts-based non-profit organization Collings Foundation as a "living history" exhibit. Funds to maintain and operate the aircraft, which is based in Houston, Texas, are raised through donations/sponsorships from public and commercial parties.NASA used the F-4 to photograph and film Titan II missiles after launch from Cape Canaveral during the 1960s. Retired U.S. Air Force Colonel Jack Petry described how he put his F-4 into a Mach 1.2 dive synchronized to the launch countdown, then "'walked the (rocket's) contrail' up to the intercept point, tweaking closing speed and updating mission control while camera pods mounted under each wing shot film at 900 frames per second." Petry's Phantom stayed with the Titan for 90 seconds, then broke away as the missile continued into space.NASA's Dryden Flight Research Center acquired an F-4A on 3 December 1965. It made 55 flights in support of short programs, chase on X-15 missions and lifting body flights. The F-4 also supported a biomedical monitoring program involving 1,000 flights by NASA Flight Research Center aerospace research pilots and students of the USAF Aerospace Research Pilot School flying high-performance aircraft. The pilots were instrumented to record accurate and reliable data of electrocardiogram, respiration rate and normal acceleration. In 1967, the Phantom supported a brief military-inspired program to determine whether an airplane's sonic boom could be directed and whether it could be used as a weapon of sorts, or at least an annoyance. NASA also flew an F-4C in a spanwise blowing study from 1983 to 1985, after which it was returned.
Variants for the U.S. Navy and the U.S. Marine Corps. F-4B was upgraded to F-4N, and F-4J was upgraded to F-4S.
Variants for the U.S. Air Force. F-4E introduced an internal M61 Vulcan cannon. The F-4D and E were the most numerously built, widely exported, and also extensively used under the Semi Automatic Ground Environment (SAGE) U.S. air defense system.
A dedicated SEAD variant for the U.S. Air Force with updated radar and avionics, converted from F-4E. The designation F-4G was applied earlier to an entirely different U.S. Navy Phantom.
Variants for the Royal Navy and Royal Air Force, respectively, re-engined with Rolls-Royce Spey turbofans.
Retired aircraft converted into remote-controlled target drones used for weapons and defensive systems research by USAF and USN / USMC.
The Phantom gathered a number of nicknames during its career. Some of these names included "Snoopy", "Rhino", "Double Ugly", "Old Smokey", the "Flying Anvil", "Flying Footlocker", "Flying Brick", "Lead Sled", the "Big Iron Sled" and the "St. Louis Slugger". In recognition of its record of downing large numbers of Soviet-built MiGs, it was called the "World's Leading Distributor of MiG Parts". As a reflection of excellent performance in spite of its bulk, the F-4 was dubbed "the triumph of thrust over aerodynamics." German Luftwaffe crews called their F-4s the Eisenschwein ("Iron Pig"), Fliegender Ziegelstein ("Flying Brick") and Luftverteidigungsdiesel ("Air Defense Diesel").Imitating the spelling of the aircraft's name, McDonnell issued a series of patches. Pilots became "Phantom Phlyers", backseaters became "Phantom Pherrets", fans of the F-4 "Phantom Phanatics", and call it the "Phabulous Phantom". Ground crewmen who worked on the aircraft are known as "Phantom Phixers".
The aircraft's emblem is a whimsical cartoon ghost called "The Spook", which was created by McDonnell Douglas technical artist, Anthony "Tony" Wong, for shoulder patches. The name "Spook" was coined by the crews of either the 12th Tactical Fighter Wing or the 4453rd Combat Crew Training Wing at MacDill AFB. The figure is ubiquitous, appearing on many items associated with the F-4. The Spook has followed the Phantom around the world adopting local fashions; for example, the British adaptation of the U.S. "Phantom Man" is a Spook that sometimes wears a bowler hat and smokes a pipe.
65-0749 – F-4D airworthy with the Collings Foundation in Stow, Massachusetts. It is operated as a "living history" exhibit.
145310 – F4H-1 under restoration to airworthy with F4 Phantom II Corporation in Santa Fe, New Mexico. It was previously located at the Wings and Rotors Air Museum in Murrieta, California.
A RF-4C variant is on display at the Royal Museum of the Armed Forces and of Military History in Brussels, Belgium.
A F-4C, made famous by Pardo's Push, is on static display at the Fairmount, Indiana American Legion.
On 6 June 1971, Hughes Airwest Flight 706, a McDonnell Douglas DC-9-31 collided in mid-air with a United States Marine Corps F-4B Phantom above the San Gabriel Mountains, while en route from Los Angeles International Airport to Salt Lake City. All 49 on board the DC-9 were killed, while the pilot of the F-4B was unable to eject and died when the aircraft crashed shortly afterwards. The F-4B's Radar Intercept Officer successfully ejected from the plane and parachuted to safety, being the sole survivor of the incident.
On 9 August 1974, a Royal Air Force Phantom FGR2 was involved in a fatal collision with a civilian PA-25-235 Pawnee crop-sprayer over Norfolk, England.
On 21 March 1987, Captain Dean Paul Martin (son of entertainer Dean Martin), a pilot in the 163d Tactical Fighter Group of the California Air National Guard, crashed his F-4C into San Gorgonio Mountain, California shortly after departure from March AFB. Both Martin and his weapon systems officer (WSO) Captain Ramon Ortiz were killed.
Data from The Great Book of Fighters Quest for Performance, Encyclopedia of USAF Aircraft, and McDonnell F-4 Phantom: Spirit in the SkiesGeneral characteristics
Fuel capacity: 1,994 US gal (1,660 imp gal; 7,550 l) internal, 3,335 US gal (2,777 imp gal; 12,620 l) with 2x 370 US gal (310 imp gal; 1,400 l) external tanks on the outer wing hardpoints and either a 600 or 610 US gal (500 or 510 imp gal; 2,300 or 2,300 l) tank for the centre-line station.
Powerplant: 2 × General Electric J79-GE-17A after-burning turbojet engines, 11,905 lbf (52.96 kN) thrust each dry, 17,845 lbf (79.38 kN) with afterburnerPerformance
Up to 18,650 lb (8,480 kg) of weapons on nine external hardpoints, including general purpose bombs, cluster bombs, TV- and laser-guided bombs, rocket pods, air-to-ground missiles, anti-ship missiles, gun pods, and nuclear weapons. Reconnaissance, targeting, electronic countermeasures and baggage pods, and external fuel tanks may also be carried.
4× AIM-9 Sidewinders on wing pylons, Israeli F-4 Kurnass 2000 carried Python-3, Japanese F-4EJ Kai carry AAM-3
4× AIM-7 Sparrow in fuselage recesses, upgraded Hellenic F-4E and German F-4F ICE carry AIM-120 AMRAAM, UK Phantoms carried Skyflash missiles

The Royal Australian Air Force (RAAF) has operated McDonnell Douglas F/A-18 Hornet fighter aircraft since 1984. The Australian Government purchased 75 "A" and "B" variants of the F/A-18 in 1981 to replace the RAAF's Dassault Mirage III fighters. The Hornets entered service with the RAAF between 1984 and 1990, and 69 remain in operation as of 2019. Of the other six Hornets, four were destroyed in flying accidents during the late 1980s and early 1990s and two were transferred to Canada in February 2019.
RAAF Hornets were first sent on a combat deployment as part of the Australian contribution to the 2003 invasion of Iraq. During the invasion, 14 Hornets flew patrols over Iraq, as well as close air support sorties to assist coalition ground forces. RAAF F/A-18s also provided security for the American air base at Diego Garcia between late 2001 and early 2002, and have protected a number of high-profile events in Australia. Between 2015 and 2017 a detachment of Hornets was deployed to the Middle East and struck ISIL targets as part of Operation Okra.
Since 1999 the RAAF has put its Hornets through a series of upgrades to improve their effectiveness. However, the aircraft are becoming increasingly difficult to operate and are at risk of being outclassed by the fighters and air-defence systems operated by other countries. As a result, the RAAF will begin to retire its F/A-18s in the late 2010s, and the last aircraft will leave service in the early 2020s. Under current Australian Government planning they will be replaced by 72 Lockheed Martin F-35 Lightning II fighters. The Australian Government has offered the Hornets for sale once they are no longer needed by the RAAF, and finalised a deal to sell 25 of the aircraft to Canada in early 2019.
The RAAF began the initial stages of scoping a replacement for its Dassault Mirage III fighters in 1968. The service issued an Air Staff Requirement for new fighter aircraft in December 1971, which received a larger than expected number of proposals from manufacturers. At this time the RAAF expected to start phasing out the Mirage IIIs in 1980. In 1973, a team of RAAF personnel inspected the McDonnell Douglas F-15 Eagle, Northrop YF-17, Saab 37 Viggen and Dassault Mirage F1 programs, but recommended that any decisions about a suitable replacement be delayed so that several new fighters that were expected to soon become available could also be considered. In August 1974 the Australian Government decided to defer the fighter replacement project and extend the Mirage IIIs' operational life into the 1980s. One of the four Mirage III-equipped squadrons was also disbanded at this time.
Work on the Mirage replacement program resumed in 1975, and the Tactical Fighter Project Office was established in 1976 to manage the process of selecting the RAAF's next fighter. A request for proposals was issued in November that year and attracted eleven responses. By March 1977 the office had chosen to focus on the F-15 Eagle, General Dynamics F-16 Fighting Falcon, Dassault Mirage 2000  and Panavia Tornado, as well as the McDonnell Douglas F-18A and F-18L; the F-18A was a carrier-based fighter developed from the YF-17 for the United States Navy, and the F-18L was a land-based variant of this design. The Grumman F-14 Tomcat was also considered by the project office, but was regarded as unsuitable and never placed on the official shortlist. In November 1978 the F-15 and Tornado were removed from the list of aircraft being considered. The Tornado was excluded as it was principally a strike aircraft and had limited air-to-air capability. While the F-15 was an impressive aircraft that met or exceeded almost all of the RAAF's requirements, it was believed that the air force did not need a fighter with such advanced capabilities and that introducing it into service could destabilise Australia's region.Further evaluation of the remaining aircraft took place during 1979. Wing Commander (and later Air Vice-Marshal) Bob Richardson test-flew a Mirage 2000 in April 1979, and reported that while the aircraft had excellent aerodynamic characteristics, its avionics, radar, fuel system, cockpit and weapons capability were inferior to those of US designs. Richardson also test-flew a YF-17 that was being used as a demonstrator for the F-18L in mid-1979, and was impressed by its capabilities. No F-18Ls had been ordered at this time, however, and the RAAF did not want to take on the risk of being the lead customer for the design. At about the same time, the RAAF rejected an offer of F-14 Tomcats that had been originally ordered by the Iranian Government but not delivered as a result of the revolution in that country. While the Tomcats were made available at a greatly reduced price, the air force judged that these aircraft were too large and complex for its requirements.
With the Mirage 2000 and F-18L rejected, the RAAF was faced with a choice between the F-16 and F-18A. Richardson and several other RAAF pilots tested United States Air Force (USAF) F-16Bs in 1979 and 1980, and reported that the aircraft had excellent performance but could be difficult to control at times. The evaluation team was also concerned about the reliability of the F-16's engine and regarded the aircraft as technologically immature. It was also noted that the aircraft's radar was inferior to that of the F-18A, and that F-16s could not fire the beyond-visual-range (BVR) air-to-air missiles and long-range anti-shipping missiles that the F-18A was capable of operating. In contrast, the evaluation team was impressed by the F-18A, and regarded it as being a more robust and survivable aircraft as it had been designed to operate from aircraft carriers; these features were important for operations from bare bases in northern Australia. Richardson and three other RAAF pilots test-flew F-18As, and reported that the aircraft handled well, but had some deficiencies with its flight control system and engines; these were not seen as major flaws by the evaluation team, however. The F-18A's twin engines were considered to be its main advantage over the single-engined F-16, as research conducted by the evaluation team found that the attrition rate for single-engined fighters was twice that for aircraft with two engines. Overall, however, the RAAF judged that both the F-16 and F-18A were too immature for a decision to be made in 1980 as had been originally planned, and recommended to the Government that this be deferred by a year.The Government accepted the RAAF's recommendation, and delayed its decision on a Mirage III replacement until late 1981. This gave General Dynamics an opportunity to offer the improved F-16C to the RAAF. The capability of these aircraft was closer to that of the F-18 as they were equipped with BVR missiles. Richardson and another RAAF pilot test-flew F-16Cs in May 1981. The F-18 design was also improved during 1981, and was redesignated the F/A-18. When RAAF test pilots flew these aircraft during 1981, they found that the deficiencies they had detected in 1980 were now addressed. Overall, the RAAF concluded that while both aircraft met its requirements and the F-16 was less expensive, the F/A-18 was the superior design as it was more technologically mature, easier to maintain during operational deployments, and likely to have a much lower attrition rate. The Government accepted this advice, and announced on 20 October 1981 that 75 F/A-18s would be ordered. As part of this announcement, Minister for Defence Jim Killen acknowledged that the F-16 would have been seven percent cheaper to purchase, but stated that the F/A-18's lower running costs and expected attrition rate greatly reduced the difference between the lifetime cost of the two designs.Instead of directly ordering the aircraft from McDonnell Douglas, the Australian Government purchased its F/A-18s through the US Government's Foreign Military Sales (FMS) program. Ordering the aircraft via the US Government allowed the RAAF to take advantage of the superior purchasing power of the US military, and reduced the service's project management requirements. This led to a complicated arrangement whereby the aircraft were ordered by the US Government, delivered to the US Navy, and then transferred to the RAAF once initial flight testing had taken place. The process functioned smoothly, however, and was cost effective.
The RAAF's order of 75 Hornets comprised 57 single-seat "A" variant fighters and 18 two-seat "B" variant operational training aircraft. It was planned that each of the three fighter squadrons and the single operational conversion unit that were to operate the F/A-18 would be allocated 16 aircraft, of which 12 were expected to be operational at any time while the other four were undergoing maintenance. The remaining eleven Hornets were labelled the "half-life attrition buy" and would replace the aircraft that were expected to have been lost by 2000; as it happened, this greatly exceeded the RAAF's actual losses. Deliveries were planned to start in late 1984 and be completed in 1990. The total cost of the F/A-18 program, including the aircraft, spare parts, other equipment and modifications to the RAAF's fighter bases, was calculated as A$2.427 billion in August 1981, but was rapidly revised upwards due to the depreciation of the Australian dollar at this time.The Australian Hornets were very similar to the standard US Navy variants, but incorporated a number of minor modifications. These included the addition of an Instrument Landing System/Very High Frequency Omnidirectional Range (ILS/VOR) system, a high-frequency radio, a different ejection seat harness and the deletion of all equipment used only to launch the aircraft from catapults. In addition, two of the Australian aircraft were fitted with flight-test instrumentation so that they could be used as part of trials.
The Government sought to use the Mirage III replacement program as a means to increase the capabilities of Australia's manufacturing industry. Accordingly, it was decided to build the aircraft in Australia, though it was recognised that this would lead to higher costs than if the fighters were purchased directly from the United States. While the first two RAAF Hornets were built in the United States, the remainder were assembled at the Government Aircraft Factories plant at Avalon Airport in Victoria, and their engines were produced by the Commonwealth Aircraft Corporation at Fishermans Bend in Melbourne. Another twelve Australian companies were involved in other stages of the project. These firms were sub-contracted to McDonnell Douglas and the other major US companies that produced components for the F/A-18, and had to comply with the requirements of the FMS program. The Australian Government hoped that Singapore and New Zealand would purchase Australian-built Hornets, but this did not eventuate. The Canadian Government expressed interest in purchasing 25 Australian-built F/A-18As in 1988 in order to increase its force of these aircraft after they had ceased to be manufactured in the United States, but this did not lead to any sales.
The Australian Hornets began to roll off the production lines in 1984. The first two aircraft (serial numbers A21-101 and A21-102) were entirely built at McDonnell Douglas' factory in St. Louis, and were handed over to the RAAF on 29 October 1984. These aircraft remained in the United States until May 1985 for training and trials purposes. The next two Australian Hornets (A21-103 and A21-104) were also built at St. Louis, but were then disassembled and flown to Avalon in June 1984 on board a USAF Lockheed C-5 Galaxy. The aircraft were then reassembled, and A21-103 was rolled out at a ceremony attended by Prime Minister Bob Hawke and the Chief of the Air Staff, Air Marshal David Evans, on 16 November. However, the aircraft's initial test flight was delayed until 26 February 1985 by a demarcation dispute over which category of pilot was permitted to fly the aircraft.In order to meet production targets, GAF was required to complete 1.5 Hornets per month. Production fell behind schedule during the first half of 1987, however, as a result of inefficiencies at the company's factory and industrial relations problems. GAF was able to accelerate production later in the year, though some components that were planned to be manufactured in Australia were purchased from companies in the United States instead. The final cost of the Hornet project was A$4.668 billion; after adjusting for the depreciation of the Australian dollar this was $186 million less than the initial estimate.The RAAF began to accept Hornets into service in 1985. A21-103 was formally delivered on 4 May of that year. Two weeks later, A21-101 and 102 were flown from Naval Air Station Lemoore in California to RAAF Base Williamtown in New South Wales between 16 and 17 May 1985. This ferry flight was conducted as a non-stop journey, and USAF McDonnell Douglas KC-10 Extender tankers refuelled each of the Hornets 15 times as they crossed the Pacific. As of 2005 this remained the longest single flight to have been undertaken by F/A-18s. Despite the delays to production in 1987, the final Australian Hornet (A21-57) was delivered on schedule at a ceremony held in Canberra on 16 May 1990. The F/A-18As were allocated serial numbers A21-1 through to A21-57 and the F/A-18Bs were allocated A21-101 to A21-118.A major capital works program was also undertaken to prepare RAAF bases for the Hornets. Over $150 million was spent upgrading the runways, hangars and maintenance facilities at RAAF Base Williamtown, which has been the main F/A-18 base throughout the aircraft's service. The pre-existing airfield at RAAF Base Tindal in the Northern Territory was also developed into a major air base between 1985 and 1988 at a cost of $215 million so that it could accommodate No. 75 Squadron. Until this time the squadron had been stationed at RAAF Base Darwin which, due to its location on Australia's north coast, was vulnerable to damage from cyclones and difficult to defend during wartime.Owing to concerns over the airworthiness of the RAAF's General Dynamics F-111 bombers and delays to the Lockheed Martin F-35 Lightning II program, the Australian Government ordered 24 F/A-18F Super Hornets in 2006. This design is significantly different from the original (or "classic") Hornet, however. The RAAF's first Super Hornets entered service in 2010 and deliveries were completed the next year. In 2013 the Australian Government ordered 12 Boeing EA-18G Growler electronic warfare variants of the Super Hornet, and all were delivered to the RAAF between 2015 and 2017.
Maintenance of the RAAF's Hornets is carried out by both air force personnel and civilian contractors. Until the early 1990s, all routine servicing and a significant proportion of intensive "deeper maintenance" was undertaken by the air force. However, the share of intensive maintenance tasks outsourced to the private sector was increased during the 1990s under the RAAF-wide Commercial Support Program. Under current arrangements, the four Hornet-equipped units undertake all routine servicing and some of the more complex deeper maintenance tasks. The remainder of the deeper maintenance work, as well as all major refurbishments and upgrade projects, are carried out by commercial firms. BAE Systems has been the lead contractor for Hornet deeper maintenance since 2003, and Boeing Australia has also provided maintenance services for the aircraft since it won a contract to do so in 2010. In August 2017 Boeing's contract was extended until the planned retirement of the Hornets in 2021, with the company also gaining responsibility for integrating weapons onto the type. This change was made to free up RAAF personnel for activities associated with introducing the F-35 into service.The RAAF's Hornet fleet received few modifications until the late 1990s. During this period, the AN/AAS-38 "Nite Hawk" targeting pod was the only new system fitted to the aircraft. However, several Asian countries introduced Mikoyan MiG-29 fighters into service during the 1990s, raising concerns that the RAAF's aircraft would be outclassed. The air force considered replacing the Hornet with the Eurofighter Typhoon or Boeing F/A-18E/F Super Hornet, but concluded that both aircraft were technologically immature. As a result, it was decided to upgrade the Hornets.
The Hornet Upgrade Program (HUG) began in 1999, and has had three main phases. In Phase 1, which ran from mid-2000 through 2002, the Hornets' computer systems, navigation system and radio were replaced. The aircraft were also fitted to operate the ASRAAM air-to-air missile; these weapons replaced the AIM-9 Sidewinder. HUG Phase 2 comprised four sub-elements and sought to improve the Hornets' combat performance. During Phase 2.1 the APG-65 radar was replaced with the improved AN/APG-73, and the aircraft were fitted with a secure voice encryption communications system as well as various updates to their computer systems. In HUG Phase 2.2, the most important element of the program, the Hornets were fitted with a Joint Helmet Mounted Cueing System, equipment needed to share data through the Link 16 network, a new countermeasures dispensing system and several upgrades to their cockpit displays. All of the Hornets were upgraded to this standard between January 2005 and December 2006. In Phase 2.3, an improved Electronic Counter Measures system was fitted to the Hornets; the AN/ALR-2002 was originally selected, but proved unsuccessful. It was replaced by the ALR-67 Radar Warning Receiver in late 2006. As of early 2012, 14 Hornets had been fitted with the system and the remainder were scheduled to receive it by the end of the year. During HUG Phase 2.4 the Hornets were modified to be able to use the AN/AAQ-28(v) "LITENING" targeting pod and 37 of these systems were purchased; this phase was completed in 2007.The third stage of the Hornet Upgrade Program sought to rectify airframe damage. HUG Phase 3.1 involved minor structural work to all aircraft as they passed through other phases of the program. The centre fuselages of the ten Hornets assessed as suffering the greatest amount of structural damage were replaced in HUG Phase 3.2. It was originally intended that all the RAAF's Hornets would receive new centre fuselages, but the scope of this phase of the program was reduced after it was found that the number of man-hours needed to upgrade each aircraft was much greater than originally estimated. The ten aircraft were upgraded at an L-3 Communications facility in Canada, and all were returned to service by June 2010.The long-running HUG process complicated the RAAF's management of its fleet of F/A-18s. At any one time, the capabilities of individual aircraft differed considerably depending on their upgrades. Accordingly, the long-standing arrangement where aircraft were almost permanently assigned to each squadron was replaced by a system where they were pooled. Attempts to allocate Hornets with similar levels of modifications from the common pool to each squadron were not successful.
The RAAF's Hornets have been fitted with several different types of air-to-air weapons. The aircraft are equipped with an internal M61A1 cannon for use against air and ground targets; 578 rounds can be carried for this weapon. During the initial years of the Hornet's service, the aircraft were equipped with AIM-9M Sidewinder short range air-to-air missiles and AIM-7M Sparrow medium-range air-to-air missiles. The Sparrows were replaced by the AIM-120 AMRAAM in 2002, and in 2004 the Sidewinders were replaced by ASRAAMs. The older missiles are occasionally used in training exercises, however.A variety of unguided and guided weapons can also be used against ground targets. The Hornets carry Mark 82, Mark 83 and Mark 84 bombs, as well as GBU-10, GBU-12 and GBU-16 Paveway II laser-guided bombs. In addition, the aircraft have operated bombs fitted with JDAM guidance kits since 2008. The long-ranged JDAM-ER variant of these bombs were ordered in 2011 and will begin to enter service in 2015. During exercises the Hornets carry BDU-33 and BDU-57 LGTR training bombs. Since November 2011, the RAAF's Hornets have also been equipped with AGM-158 JASSM cruise missiles. The F/A-18s main weapon in the maritime strike role is the Harpoon anti-ship missile; the RAAF initially operated the Block IC variant of this missile, but purchased Block II variants in 2003. In addition to these weapons, the Hornets can also be fitted with 330-US-gallon (1,200 L) drop tanks to extend their range.
Four RAAF units converted to the Hornet between 1985 and 1988. The first 14 Hornets were allocated to No. 2 Operational Conversion Unit (2OCU) at RAAF Base Williamtown, and were used to train the pilots and instructors needed to convert the RAAF's three fighter squadrons to the aircraft. 2OCU's first Hornet operational conversion course began on 19 August 1985. In addition to the unit's training activities, 2OCU aircraft travelled widely around Australia and South East Asia during 1985 and 1986 to showcase the new aircraft. No. 3 Squadron was the first fighter unit to convert from the Mirage III, and became operational with the Hornet in August 1986. It was followed by No. 77 Squadron in June 1987 and No. 75 Squadron in May 1988. No. 81 Wing, whose headquarters is located at Williamtown, has commanded these four units since they converted to the F/A-18. As of 2012, 2OCU, No. 3 and No. 77 Squadrons are stationed at Williamtown and No. 75 Squadron is located at Tindal. In addition, two Hornets are allocated to the Aircraft Research and Development Unit at RAAF Base Edinburgh in South Australia.
The RAAF's Mirage III pilots generally found the process of converting to the Hornet to be straightforward. While the F/A-18 was considered to be easier to fly, its more sophisticated avionics and weapons systems required improved cockpit workload management skills. The Hornets have also proven to be mechanically reliable and easy to maintain, though shortages of spare parts reduced availability rates during the early years of their service with the RAAF. The updates installed as part of the HUG process have further simplified maintenance procedures. In recent years, however, the aging aircraft have required much more servicing than was the case in the past.To extend the Hornet's range, four of the RAAF's six Boeing 707 transport aircraft were converted to tankers in the early 1990s; the first Boeing 707 tanker entered service in 1990. The tankers were operated by No. 33 Squadron and supported the Hornet units until the 707s were retired in 2008. These aircraft were replaced with KC-30A tanker-transports in 2011.The RAAF has at times suffered from shortfalls of Hornet-qualified pilots. The service began to experience shortages of F/A-18 and F-111 fast-jet pilots in the mid-1980s due to competition from commercial airlines and relatively low recruitment rates. By June 1999 the three operational Hornet-equipped squadrons had only 40 pilots, which was less than the number of aircraft allocated to these units. The RAAF claimed that the squadrons were able to meet their readiness targets, however. To overcome this shortfall, the RAAF gave its fast jet units a higher priority for aircrew, implemented measures to reduce separation rates, and recruited pilots from other countries. These reforms coincided with reduced demand for civil pilots following the 11 September attacks, and by late 2003 the RAAF's fast-jet units were at near full strength. A 2010 article in the magazine Australian Aviation stated that No. 3 Squadron typically had "about 18 pilots on strength" at any point in time. At this time the total strength of the squadron, including air and ground crew, was around 300 personnel.
As the Hornets are multi-role fighters, their pilots practice a wide range of tasks during peacetime training. Each year the three Hornet squadrons rotate between four-month training "blocks" focused on air-to-air combat, air-to-ground tactics and Australian Defence Force support tasks. The units undertake the air-to-air and air-to-ground "blocks" before assuming responsibility for Australian Defence Force support (which involves operating with the Australian Army and Royal Australian Navy). No. 81 Wing's headquarters oversees this training program and monitors adherence to common standards and procedures. Training sorties may include such tasks as defending air bases, infrastructure and shipping from enemy aircraft, attacking naval and ground targets, and practicing in-flight refueling. More unusual tasks such as dropping naval mines have also been practiced at times. Major exercises often involve other RAAF units and aircraft, as well as units from the Army and Navy and contingents from other countries.As part of their regular training activities, F/A-18 Hornets operate in different parts of Australia and the Asia-Pacific region. Regular deployments are made to Singapore and RMAF Butterworth in Malaysia as part of Integrated Air Defence System exercises. In addition, RAAF F/A-18s have participated in exercises in the Philippines, Thailand and the United States. These deployments have seen Australian fighter squadrons range as far afield as Eielson Air Force Base in Alaska, where they took part in Red Flag – Alaska exercises in 2008 and 2011.Four of the RAAF's Hornets were destroyed in flying accidents during the late 1980s and early 1990s. A21-104 was the first aircraft to be lost when it crashed at Great Palm Island in Queensland on 18 November 1987; its pilot was killed. The next loss occurred on 2 August 1990 when two No. 75 Squadron Hornets (A21-29 and A21-42) collided. A21-42 crashed, killing the unit's commanding officer; the other aircraft was damaged but managed to return to base. On 5 June 1990 A21-41 crashed 100 kilometres (62 mi) north-east of Weipa, Queensland, killing its pilot. A21-106 was the fourth aircraft to be lost when it crashed inland from Shoalwater Bay in Queensland on 19 May 1992 – its pilot and a passenger from the Defence Science and Technology Organisation died. As of September 2017, all of the remaining 71 F/A-18s were still in service. Aviation writer Nigel Pittaway has noted that the type has "enjoyed an exemplary safety record during its RAAF service", especially when compared to the loss of 41 of the RAAF's 116 Mirages due to accidents.
In late 1990 consideration was given to deploying a squadron of F/A-18s to the Middle East as part of an expanded Australian contribution to the Gulf War. The Department of Defence opposed dispatching the aircraft on the grounds that doing so would greatly strain the fighter force in Australia, and this option was not adopted by the government. As a result, the Hornets' only role in the war was to support the training of the Royal Australian Navy warships which were sent to the Gulf by conducting mock attacks on the vessels as they sailed from Sydney to Perth.During late 1999, No. 75 Squadron was placed on alert to provide close air support and air defence for the international forces deployed to East Timor as part of INTERFET. While Indonesian forces posed a potential threat to this force, no fighting eventuated and the Hornets were not required.
The first operational deployment of RAAF Hornets took place in 2001. Following the 11 September terrorist attacks, the Australian Government agreed to deploy F/A-18s to protect the major USAF air base on the Indian Ocean island of Diego Garcia, which was being used to mount operations in Afghanistan. Four No. 77 Squadron Hornets and 70 personnel departed for the island on 9 November. No. 3 Squadron pilots and ground crew relieved the No. 77 Squadron personnel in early February 2002. RAAF Hornets were not assigned to the War in Afghanistan as at the time they were less capable than other available coalition aircraft. While the F/A-18s were occasionally scrambled in response to reports of aircraft near the base, no threat developed. The detachment returned to Australia on 21 May 2002.No. 75 Squadron formed part of the Australian contribution to the 2003 invasion of Iraq. The squadron began initial planning for this deployment in December 2002, and intensive training was undertaken from January 2003. To improve the unit's readiness, air and ground crew as well as aircraft were also posted to No. 75 Squadron from other units. The Australian Government announced on 1 February that it would begin deploying RAAF aircraft, including a squadron of F/A-18s, to the Middle East. No. 75 Squadron departed from Tindal on 13 February, and arrived at Al Udeid Air Base in Qatar on the 16th of the month. The 14 F/A-18A Hornets selected for this deployment had received the HUG 2.1 package of upgrades and recently completed major servicing. These upgrades allowed the F/A-18s to operate alongside other coalition aircraft. In addition to No. 75 Squadron, several experienced Hornet pilots were also posted to the USAF Combined Air and Space Operations Center in the Middle East to provide advice on how to make the best use of the squadron.The Australian Hornets saw combat in several roles during the Iraq War. Following the outbreak of war on 20 March, No. 75 Squadron was initially used to escort high-value Coalition aircraft, such as tankers and airborne early warning and control aircraft. As it rapidly became clear that the Iraqi Air Force posed no threat, from 21 March No. 75 Squadron also began to also conduct air interdiction sorties against Iraqi forces. These sorties were initially flown in support of the United States Army's V Corps, but the squadron was rarely assigned any targets to attack. As a result, the Australian commanders in the Middle East had No. 75 Squadron reassigned to support the United States Marine Corps' I Marine Expeditionary Force. At this time the squadron also began flying close air support sorties. During the first two weeks of the war the squadron typically flew 12 sorties per day. To avoid pilot fatigue, additional aircrew were posted to the Middle East from Australia. The number of sorties dropped to between six and ten per day from 5 April onwards as the American forces closed on Baghdad and few targets remained in southern Iraq. On 12 April, No. 75 Squadron supported elements of the Special Air Service Regiment and 4th Battalion, Royal Australian Regiment, which occupied Al Asad Airbase. During the last weeks of the war the squadron continued to fly sorties across western, central and southern Iraq to support British and American forces. In several of the squadron's operations in the final week of the war, the Hornets made low altitude and high speed passes over Iraqi positions to encourage their defenders to surrender. No. 75 Squadron conducted its final combat sorties on 27 April. During the war the squadron flew 350 combat missions (including 670 individual sorties) and dropped 122 laser-guided bombs. No. 75 Squadron did not suffer any casualties, and all 14 Hornets returned to Tindal on 14 May 2003.
RAAF Hornets have also provided air defence for several high-profile events in Australia since the 11 September attacks. In 2002, Hornets patrolled over the Commonwealth Heads of Government Meeting (CHOGM) at Coolum Beach, Queensland; this was the first time RAAF aircraft had flown air defence sorties over Australia since World War II. On 22 and 23 October that year a detachment of Hornets patrolled over Canberra during US President George W. Bush's visit to the city. A detachment of aircraft from No. 77 Squadron was deployed to RAAF Base East Sale in March 2006 to protect the Commonwealth Games, which were being held in Melbourne. In September 2007, Hornets patrolled over Sydney during the APEC leaders meeting there. Eight Hornets were also deployed from Williamstown to RAAF Base Pearce in October 2011 to protect the CHOGM meeting in nearby Perth. On 16 and 17 November that year, Hornets operated over Canberra and Darwin while President Barack Obama was present.In March 2015 six F/A-18As from No. 75 Squadron were deployed to the Middle East as part of Operation Okra, replacing a detachment of Super Hornets. No. 81 Wing's involvement in Operation Okra concluded in May 2017, with No. 1 Squadron resuming responsibility for this task. By this time all of the wing's three squadrons had completed at least one rotation to the Middle East: No. 3 Squadron was deployed once, and the other two squadrons conducted two deployments. The squadrons used a common 'pool' of aircraft during these deployments, with either six or seven Hornets being stationed in the Middle East at any time. The aircraft were typically deployed for eight months before rotating back to Australia when becoming due for major servicing. Members of the three Hornet-equipped squadrons served five or six month rotations, and ground crew from 2OCU and No. 81 Wing's workshops were also deployed to fill specialist roles. The Hornets attacked ISIL personnel and facilities in both Iraq and Syria, including in support of Iraqi forces engaged in the Battle of Mosul. Overall, No. 81 Wing conducted 1,973 sorties over Iraq and Syria during which 1,961 munitions were released. Despite the age of the aircraft and the harsh environmental conditions in the Middle East, the detachment sustained a very high serviceability rate.
While the Hornet Upgrade Program has been successful, the RAAF's Hornets are approaching retirement. It is expected that the aircraft will be increasingly expensive to operate as they age, and improvements to the fighter aircraft and air defences operated by other countries will reduce the Hornets' combat effectiveness. The Australian Government is currently planning to replace the RAAF's F/A-18 Hornets with Lockheed Martin F-35A Lightning II fighters from 2018 onwards. The Defence Materiel Organisation's Project AIR 6000 Phase 2A/B specifies that 72 F-35A fighters will be acquired to equip three squadrons and an operational training unit. No. 3 Squadron will be the first Hornet unit to be reequipped, and ceased operating the type in December 2017. It began to transition to the F-35 in early 2018. The squadron's Hornets and most of its personnel were transferred to No. 77 Squadron, which was expanded from two to three flights as part of this change. All of the F/A-18A and Bs are scheduled to be retired by 2023. The RAAF's Hornet sustainability planning has been designed to allow the type to be retained in service for longer if the F-35 program experiences further delays.In the meantime, the RAAF has implemented several measures to keep the Hornets in service. These include a structural refurbishment program, increased monitoring of fatigue-related issues as well as repainting the aircraft and frequently washing them to reduce the risks posed by corrosion. In 2015 the Defence Science and Technology Group conducted new statistical analysis of the Hornets' structural condition which found that the airframes were less fatigued than previously believed, and so able to remain in service for a longer period than planned if necessary. This finding was accepted by the Directorate General Technical Airworthiness – Australian Defence Force. As of September 2017, none of the RAAF Hornets were subject to flying restrictions due to airframe fatigue. However, the cost of maintaining the ageing aircraft in service has been increasing. A 2017 article by Canadian defence analyst Christopher Cowan and Australian Strategic Policy Institute analyst Dr. Andrew Davies stated that the RAAF "has done an excellent job managing its Hornet fleet", with each aircraft having a unique plan to minimise airframe fatigue. At this time each of the Hornets had, on average, been flown for 4,200 hours, as compared to the nominal fatigue life of 6,000 hours for the type.
The Australian Government is planning to sell the Hornets and associated spare parts after the type is retired from RAAF service. In August 2017 the Canadian Government initiated discussions to purchase a number of Australian F/A-18s to augment the Royal Canadian Air Force's fleet of similar McDonnell Douglas CF-18 Hornets in the event that a planned purchase of Super Hornets was cancelled as the result of a trade dispute. A Canadian delegation also visited Australia that month to inspect RAAF Hornets. The Canadian Government lodged a formal expression of interest to purchase Australian F/A-18s on 29 September 2017.On 13 December 2017, Australian Minister for Defence Marise Payne confirmed the sale of 18 F/A-18 Hornets and associated spare parts to Canada. The Canadian Government announced at the same time that it had cancelled its plans to acquire Super Hornets. The Australian aircraft are being acquired to enable the RCAF to continue to meet its international commitments until a new fighter type is ordered and enters service. In June 2018 the Canadian Government requested a further seven Australian Hornets. These additional aircraft will be used as a source of spare parts. The sale of the 25 Hornets was finalised in early 2019, with the purchase price being C$90 million. Of these aircraft, 18 will be issued to operational units and the remainder used for trials purposes and as a source of spare parts. After they arrive in Canada, the aircraft will be fitted with different ejection seats and software so that they are identical to CF-18s.Deliveries of ex-RAAF Hornets to Canada began in February 2019. Two of the aircraft were flown to CFB Cold Lake by Australian pilots in mid-February, and handed over after they had formed part of the RAAF contingent at a Red Flag exercise in the United States. At this time, deliveries of the other 23 Hornets were scheduled to be completed in 2021. However, this schedule is dependent on progress with introducing the F-35 into Australian service.

The McDonnell XF-85 Goblin is an American prototype fighter aircraft conceived during World War II by McDonnell Aircraft. It was intended to deploy from the bomb bay of the giant Convair B-36 bomber as a parasite fighter. The XF-85's intended role was to defend bombers from hostile interceptor aircraft, a need demonstrated during World War II. McDonnell built two prototypes before the Air Force (USAAF) terminated the program.
The XF-85 was a response to a USAAF requirement for a fighter to be carried within the Northrop XB-35 and B-36, then under development. This was to address the limited range of existing interceptor aircraft compared to the greater range of new bomber designs. The XF-85 was a diminutive jet aircraft featuring a distinctive egg-shaped fuselage and a forked-tail stabilizer design. The prototypes were built and underwent testing and evaluation in 1948. Flight tests showed promise in the design, but the aircraft's performance was inferior to the jet fighters it would have faced in combat, and there were difficulties in docking. The XF-85 was swiftly canceled, and the prototypes were thereafter relegated to museum exhibits. The 1947 successor to the USAAF, the United States Air Force (USAF), continued to examine the concept of parasite aircraft under Project MX-106 "Tip Tow", Project FICON and Project "Tom-Tom" following the cancellation.
During World War II, American bombers such as the Boeing B-17 Flying Fortress, Consolidated B-24 Liberator and Boeing B-29 Superfortress were protected by long-range escort fighters such as the Republic P-47 Thunderbolt and North American P-51 Mustang. These fighters could not match the range of the Northrop B-35 or Convair B-36, the next generation of bombers developed by the United States Army Air Forces (USAAF). The development cost for longer-ranged fighters was high, while aerial refueling was still considered risky and technologically difficult. Pilot fatigue had also been a problem during long fighter escort missions in Europe and the Pacific, giving further impetus to innovative approaches.The USAAF considered a number of different options including the use of remotely piloted vehicles before choosing parasite fighters as the most viable B-36 defense. The concept of a parasite fighter had its origins in 1918, when the Royal Air Force examined the viability of Sopwith Camel parasite fighters operating from R23 airships. In the 1930s, the U.S. Navy had a short-lived operational parasite fighter, the Curtiss F9C Sparrowhawk, aboard the airships Akron and Macon. Starting in 1931, aircraft designer Vladimir Vakhmistrov conducted experiments in the Soviet Union as part of the Zveno project during which up to five fighters of various types were carried by Polikarpov TB-2 and Tupolev TB-3 bombers. In August 1941, these combinations flew the only combat missions ever undertaken by parasite fighters – TB-3s carrying Polikarpov I-16SPB dive bombers attacked the Cernavodă bridge and Constantsa docks, in Romania. After that attack, the squadron, based in the Crimea, carried out a tactical attack on a bridge over the river Dnieper at Zaporozhye, which had been captured by advancing German troops. Later in World War II, the Luftwaffe experimented with the Messerschmitt Me 328 as a parasite fighter, but problems with its pulsejet engines could not be overcome. Other late-war rocket-powered parasite fighter projects such as the Arado E.381 and Sombold So 344 were unrealized "paper projects".On 3 December 1942, the USAAF sent out a Request for Proposals (RfP) for a diminutive piston-engined fighter. By January 1944, the Air Technical Service Command refined the RfP and in January 1945, the specifications were further revised in MX-472 to specify a jet-powered aircraft. Although a number of aerospace companies studied the feasibility of such aircraft, McDonnell was the only company to submit a proposal to the original 1942 request and later revised requirements. The company's Model 27 proposal was completely reworked to meet the new specifications.
The initial concept for the Model 27 was for the fighter to be carried half-exposed under the B-29, B-35 or B-36. The USAAF rejected this proposal, citing increased drag, and hence reduced range for the composite bomber-fighter configuration. On 19 March 1945, McDonnell's design team led by Herman D. Barkey, submitted a revised proposal, the extensively redesigned Model 27D. The smaller aircraft had an egg-shaped fuselage, three fork-shaped vertical stabilizers, horizontal stabilizers with a significant dihedral, and 37° swept-back folding wings to allow it to fit in the confines of a bomb bay. The diminutive aircraft measured 14 ft 10 in (4.52 m) long; the folding wings spanned 21 ft (6.4 m). Only a limited fuel supply of 112 US gal (93 imp gal; 420 l) was deemed necessary for the specified 30-minute combat endurance. A hook was installed along the aircraft's center of gravity; in flight, it retracted to lie flat in the upper part of the nose. The aircraft had an empty weight just short of 4,000 pounds (1.8 t). To save weight, the fighter had no landing gear. During the testing program, a fixed steel skid under the fuselage and spring-steel "runners" at the underside of the wingtips were installed in case of an emergency landing. Despite the cramped quarters, a pilot was provided with a cordite ejection seat, bail-out oxygen bottle and high-speed ribbon parachute. Four .50 in (12.7 mm) machine guns in the nose made up the aircraft's armament.In service, the parasite fighter would be launched and retrieved by a trapeze. With the trapeze fully extended, the engine would be airstarted and the release from the mother ship was accomplished by the pilot pulling the nose back to disengage from the hook. In recovery, the aircraft would approach the mother ship from underneath and link up with the trapeze using the retractable hook in the aircraft's nose. The anticipated production shift would see a mixed B-36 fleet with both "fighter carriers" and bombers employed on missions. There were plans that, from the 24th B-36 onward, provisions would be made to accommodate one XF-85, with a maximum of four per bomber envisioned. Up to 10 percent of the B-36s on order were to be converted to fighter carriers with three or four F-85s instead of a bomb load.
On 9 October 1945, the USAAF signed a letter of intent covering the engineering development for two prototypes (US serial numbers 46-523/4), although the contract was not finalized until February 1947. After the successful conclusion of two reviews of a wooden mock-up in 1946 and 1947 by USAAF engineering staff, McDonnell constructed two prototypes in late 1947. The Model 27D was re-designated XP-85, but by June 1948, it was changed to XF-85 and given the name "Goblin". There were plans to acquire 30 production P-85s, but the USAAF took the cautious approach – if test results from the two prototypes were positive, production orders for more than 100 Goblins would be finalized later.
During wind tunnel testing at Moffett Field, California, the first prototype XF-85 was accidentally dropped from a crane at a height of 40 ft (12 m), causing substantial damage to the forward fuselage, air intake and lower fuselage. The second prototype had to be substituted for the remainder of the wind tunnel tests and the initial flight tests.As a production series B-36 was unavailable, all XF-85 flight tests were carried out using a converted EB-29B Superfortress mother ship that had a modified, "cutaway" bomb bay complete with trapeze, front airflow deflector and an array of camera equipment and instrumentation. Since the EB-29B, named Monstro, was smaller than the B-36, the XF-85 would be flight tested, half-exposed. To load the XF-85 into the host plane, a special "loading pit" was dug into the tarmac at South Base, Muroc Field, where all the flight tests originated. On 23 July 1948, the XF-85 flew the first of five captive flights, designed to test whether the EB-29B and its parasite fighter could fly "mated". The XF-85 was carried in a stowed position, but was sometimes tethered and extended into the airstream with the engine off, for the pilot to gain some feel for the aircraft in flight.
McDonnell test pilot Edwin Schoch was assigned to the project, riding in the XF-85 while it was stowed aboard the EB-29B, before attempting a "free" flight on 23 August 1948. After Schoch was released from the bomber at a height of 20,000 ft (6,000 m), he completed a 10-minute proving flight at speeds between 180 and 250 mph (290–400 km/h), testing controls and maneuverability. When he attempted a hook-up, it became obvious the Goblin was extremely sensitive to the bomber's turbulence, as well as being affected by the air cushion created by the two aircraft operating in close proximity. Constant but gentle adjustments of throttle and trim were necessary to overcome the cushioning effect. After three attempts to hook onto the trapeze, Schoch miscalculated his approach and struck the trapeze so violently that the canopy was smashed and ripped free and his helmet and mask were torn off. He saved the prototype by making a belly landing on the reinforced skid at the dry lake bed at Muroc. All flight testing was suspended for seven weeks while the XF-85 was repaired and modified. Schoch used the down period to undertake a series of problem-free dummy dockings with a Lockheed P-80 Shooting Star fighter.After boosting the trim power by 50 percent, adjusting the aerodynamics, and other modifications, two further mated test flights were carried out before Schoch was able to make a successful release and hookup on 14 October 1948. During the fifth free flight on 22 October 1948, Schoch again found it difficult to hook the Goblin to the bomber's trapeze, aborting four attempts before hitting the trapeze bar and breaking the hook on the XF-85's nose. Again, a forced landing was successfully carried out at Muroc.With the first prototype's repairs completed, it also joined the flight test program, completing captive flights. While in flight, the Goblin was stable, easy to fly, and recoverable from spins, although initial estimates of a 648 mph (1,043 km/h) top speed proved optimistic. The first test flights revealed that turbulence during approach to the B-29 was significant, leading to the addition of upper and lower fins at the extreme rear fuselage, as well as two wingtip fins to compensate for the increased directional instability in docking. All the initial flights had the hook secured in a fixed position, but when the hook was stowed and later raised, the resulting buffeting added to the difficulty in attempting a hookup. To address the problem, small aerodynamic fairings were added to the hook well that reduced the buffeting when the hook was extended and retracted. When testing resumed, on the 18 March 1948 test flight, Schoch continued to have difficulty in hooking up, striking and damaging the trapeze's nose-stabilizing section, before resorting to another emergency belly landing. After repairs to the trapeze, Schoch flew the first prototype on 8 April 1949, completing a 30-minute free flight test, but after three attempts, abandoned his efforts and resorted to another belly landing at Muroc.Aware of the problems revealed in flight tests, McDonnell reviewed the program and proposed a new development based on a more conventional design promising a Mach 0.9 capability, using alternatively a 35° swept wing and delta wing. McDonnell also considered adding a telescoping extension to the docking trapeze that would extend the device below the turbulent air under the mother ship. Before any further work on the trapeze, other modifications to the XF-85, or continued design studies on its follow-up could be carried out, the USAF canceled the XF-85 program on 24 October 1949.Two main reasons contributed to the cancellation. The XF-85's deficiencies revealed in flight testing included a lackluster performance in relation to contemporary jet fighters, and the high demands on pilot skill experienced during docking revealed a critical shortcoming that was never fully corrected. The development of practical aerial refueling for conventional fighters used as bomber escort was also a factor in the cancellation. The two Goblins flew seven times, with a total flight time of 2 hours and 19 minutes with only three of the free flights ending in a successful hookup. Schoch was the only pilot who ever flew the aircraft.
Despite cancellation of the XF-85, the USAF continued to examine the concept of parasite aircraft as defensive fighters through a series of projects. These included Project MX-106 "Tip Tow", Project FICON, and Project "Tom-Tom"—which involved fighter aircraft attached to bomber aircraft by their wingtips. Project FICON ("fighter conveyor") emerged as an effective Convair GRB-36D and Republic RF-84K Thunderflash combined bomber-reconnaissance-fighter, although the role was changed to that of strategic reconnaissance. Project FICON drew heavily on data from the abortive XF-85 project and closely followed McDonnell's recommendations in designing a more refined trapeze. A total of 10 converted B-36s and 25 reconnaissance fighters saw limited service with the Strategic Air Command in 1955–1956, before they were supplemented by more effective aircraft and satellite systems.
After the program's termination, the two XF-85 prototypes were stored, before being surplussed and relegated to museum display in 1950.
46-0523 - National Museum of the United States Air Force at Wright-Patterson Air Force Base near Dayton, Ohio. Following the cancellation of the program, the aircraft was transferred to the museum on 23 August 1950 and was one of the first experimental aircraft to be displayed at the new Air Force Museum. For several decades, the aircraft was displayed alongside the museum's Convair B-36. In 2000, the aircraft was moved to the museum's Experimental Aircraft Hangar. Museum staff and visitors objected to this move, believing the aircraft should be displayed alongside the B-36 to properly represent its original design intentions.
46-0524 - Strategic Air and Space Museum in Ashland, Nebraska. It was originally transferred to the Norton Air Force Base (near San Bernardino, California) in 1950, still in a damaged state after its last emergency landing. When the base museum was closed and its collection dispersed, the second XF-85 prototype languished in an unrestored condition as part of the Tallmantz private collection in California, until being acquired by Offutt AFB. It is now refurbished and displayed on its ground-handling trestle, nestled under the wing of a B-36J bomber (serial number 52-2217).
Data from Experimental & Prototype U.S. Air Force Jet Fighters, Boeing, National Museum of the United States Air Force.General characteristics

The McKinley Birthplace Memorial dollar was a commemorative coin struck in gold by the United States Bureau of the Mint in 1916 and 1917, depicting the 25th President of the United States, William McKinley.  The coin's obverse was designed by Mint Chief Engraver Charles E. Barber, and the reverse by his assistant, George T. Morgan.  As McKinley had appeared on a version of the 1903-dated Louisiana Purchase Exposition dollar, the 1916 release made him the first person to appear on two issues of U.S. coins.
The coins were to be sold at a premium to finance the National McKinley Birthplace Memorial at Niles, Ohio, and were vended by the group constructing it.  The issue was originally proposed as a silver dollar; this was changed when it was realized it would not be appropriate to honor a president who had supported the gold standard with such a piece. The coins were poorly promoted, and did not sell well. Despite an authorized mintage of 100,000, only about 20,000 were sold, many of these at a reduced price to Texas coin dealer B. Max Mehl. Another 10,000 pieces were returned to the Mint for melting.
William McKinley was born in Niles, Ohio, in 1843. He left college to work as a teacher, and enlisted in the Union Army when the American Civil War broke out in 1861. He served throughout the war, ending it as a brevet major. Afterwards, he attended law school and was admitted to the bar.  He settled in Canton, Ohio, and after practicing there, was elected to Congress in 1876.  In 1890, he was defeated for re-election, but was elected governor the following year, serving two two-year terms.With the aid of his close adviser Mark Hanna, he secured the Republican nomination for president in 1896, amid a deep economic depression. He defeated his Democratic rival, William Jennings Bryan, after a front porch campaign in which he advocated "sound money", that is, the gold standard unless modified by international agreement. This contrasted to "free silver", pushed by Bryan in his campaign.McKinley was president during the Spanish–American War of 1898, in which the U.S. victory was quick and decisive. As part of the peace settlement, Spain turned over to the United States its overseas colonies of Puerto Rico, Guam, and the Philippines. With the nation prosperous, McKinley defeated Bryan again in the 1900 presidential election. President McKinley was assassinated by Leon Czolgosz in September 1901, and was succeeded by Vice President Theodore Roosevelt.In the years after McKinley's death, several memorials were built to him, including a large structure housing his remains at Canton. Another memorial was built at his birthplace in Niles under the auspices of the National McKinley Birthplace Memorial Association (the Association).  Designed by the firm of McKim, Mead, and White, the Greek Classic memorial was built of Georgia marble and was dedicated in 1917.  Housing a museum, library, and auditorium, as well as a statue of McKinley and busts of his associates, it remains open to the public, free of charge.
The McKinley Birthplace Memorial dollar was proposed as a fundraiser for the construction of the site in Niles. In February 1915, the Association's head, Joseph G. Butler, Jr., met with Ohio Congressman William A. Ashbrook, chairman of the House Committee on Coinage, Weights, and Measures, to propose a silver dollar in honor of McKinley.  Ashbrook was willing, and the two men saw Treasury Secretary William G. McAdoo and Acting Director of the Mint Frederic Dewey, who envisioned no difficulty.  Accordingly, Ashbrook introduced a bill in the final days of the 63rd Congress, which took no action on it.  When the 64th Congress opened in December 1915, Ashbrook reintroduced his bill, H.R. 2.A hearing was held before Ashbrook's committee on January 13.  Originally, the bill called for mintage of 100,000 silver dollars in commemoration of McKinley, but at the hearing, Butler requested that they be gold instead, stating, "if you will recall the fact, McKinley was elected in 1896 mainly on the question of the gold standard." The gold dollar had not been struck as a circulating coin since 1889.  Asked a question from New York Congressman James W. Husted as to whether a gold dollar would be too small to be a souvenir, Butler responded, "No; I do not think so. I think, on the other hand, a silver dollar might be too large. I think we can dispose of gold dollars very much easier. Mr. Husted, and you know gold dollars are rather scarce just now." Ashbrook agreed, and stated:
my understanding is that these dollars will be sold at not less than $2 each which would make a profit of at least $100,000. I think there will be no trouble about disposing of them at that price. I understand they will be on sale in this memorial, and visitors who go to see it very largely will not leave the building without buying one, and will be willing to pay at least $2. I might say in that connection that any gold dollar coined by the United States is worth at least $2 at this time. They all command a premium, and there is no reason why this dollar would not sell for at least $2 and likely more.
Ohio Senator Warren G. Harding was present at the House committee meeting and spoke in favor of the bill, arguing that "this assistance on the part of the Federal Government will cost nothing more than the making of the dies ".  On being told that the dies, per the legislation, would be at the Association's expense, Harding replied, "I did not notice that. Then, it essentially costs the Government nothing whatever to render this mark of tribute and assistance." The committee reported the bill favorably on January 18, amending the bill to allow for the 100,000 gold dollars, to be purchased by the Association at par and sold at a profit to help build the memorial. The report indicated that the committee members "believe it is a deserved testimonial to the worth and service of a great man who lost his life while serving our Nation as its Chief Executive". The bill passed the House on February 7, 1916, and the Senate on February 15. It was enacted when President Woodrow Wilson signed it on February 23, 1916.The act provided that no more than 100,000 pieces be struck, with the necessary gold bullion to be acquired in the open market. The Association could purchase the coins at face value. The act required that the pieces be struck at the Philadelphia Mint, one of only two pieces of authorizing legislation in the classic commemorative coin series (through 1954) that specified the place of striking (the Panama-Pacific issue of 1915 had to be struck in San Francisco).  The act also required that the dies be destroyed after the coining was done, something numismatists Anthony Swiatek and Walter Breen questioned as duplicating provisions in the Coinage Act of 1873.The designs were prepared in-house at the Philadelphia Mint by Chief Engraver Charles E. Barber, who designed the obverse and his assistant, George T. Morgan, who prepared the reverse. They did not seek outside artists to submit proposals. Numismatic author Q. David Bowers suggested that this was because Secretary McAdoo had sought non-employees to propose designs for the five Panama-Pacific coins along with those sketches prepared by Barber and his assistants, and the Mint's engravers had prepared only two of the five, and that because the artist assigned one, Evelyn Longman, had fallen ill. When the McKinley designs were submitted to the Commission of Fine Arts on March 31, 1916, it recommended changes, though Don Taxay, who so stated, does not say what they were. In any event, no alteration was made, and the designs were approved.
The obverse of the dollar features an unadorned bust of McKinley, facing left, with the name of the country above and "McKinley Dollar" (in all capitals) below.  McKinley, who had appeared on one version of the Louisiana Purchase Exposition dollar (issued beginning in 1903), thus became the first person to appear on two different issues of American coinage.  The earlier pieces had also been designed by Barber, and the later coins, according to Bowers, "present an image so different that the uninformed observer would not know that the same man was being depicted". Bowers suggested that Barber might have been trying to create "a distinctively new version". Taxay agreed, opining that Barber's "chief concern seems to have been in making the portrait of McKinley as different as possible from that on the Louisiana Purchase coins".The reverse, designed by Morgan, is intended to be a facing view of the McKinley birthplace memorial in Niles, but according to Swiatek and Breen, "the most charitable view must characterize it as inaccurate and incompetently done". Above the building is "McKinley Birthplace/Niles Ohio", and beneath it the date and "Memorial".Art historian Cornelius Vermeule, in his volume on U.S. coins, disliked the McKinley pieces. "When Barber and Morgan collaborated ... the results were almost always oppressive. The McKinley Memorial dollars of 1916 and 1917 bear witness to these stylistic judgments, the unclothed bust on the obverse looking tastelessly Roman and the classical, colonnaded Memorial Building placed across a reverse further constricted by too much, too large lettering."
The Philadelphia Mint struck 20,000 gold dollars in August and October 1916, plus 26 extra reserved for inspection and testing at the 1917 meeting of the United States Assay Commission. In February 1917, 10,000 more (plus 14 assay coins) were minted, again at Philadelphia. The Association sold these to the public at $3 each, the same price at which the Louisiana Purchase pieces, which sold poorly, had been vended. The McKinley Memorial pieces were ill-publicized, and few were sold at full price. Texas coin dealer B. Max Mehl purchased 10,000 pieces at an unknown price, selling them for years afterwards at $2.50 each.The Washington Post reported on July 30, 1916 that the gold dollars had been released and were being "gobbled up as souvenirs". Nevertheless, according to Mehl in his 1937 volume on commemoratives, "the Committee in charge apparently realized that the number of collectors in the country could not and would not absorb an issue of 100,000 coins at $3.00 each" and some 10,000 coins "were disposed of at a greatly reduced price to the 'Texas Dealer' [that is, himself] who in turn distributed them extensively among collectors of the country at a reduced price". A total of 10,023 were returned by the Committee to the Mint for melting. It is uncertain how many of each year were melted, as the Mint did not keep records of this.  Mehl estimated that the Committee sold 15,000 of the 1916 and 5,000 of the 1917 (including the sale to him), meaning that about 5,000 of each were melted. Bowers deemed these figures "probably correct", given Mehl's personal dealings with the Committee. Bowers calculated that 8,000 of the 1916 were sold by the Committee to collectors and the public, plus 7,000 to Mehl. He opined that 2,000 of the 1917 were sold by the Committee at full price, plus 3,000 to Mehl. Swiatek, in his 2012 book on commemoratives, estimated that between a third and half of the melted pieces were dated 1917.According to R. S. Yeoman's 2015 edition of A Guide Book of United States Coins, the 1916 is catalogued for $500 in slightly-worn AU-50 (almost uncirculated) to $1,850 in near-pristine MS-66. The 1917 is listed at $550 in AU-50 and $3,250 in MS-66. A 1916 in MS-68 condition was sold by Heritage Auctions in 2009 for $16,100.
Bowers, Q. David (1992). Commemorative Coins of the United States: A Complete Encyclopedia. Wolfeboro, NH: Bowers and Merena Galleries, Inc. ISBN 9780943161358.
Flynn, Kevin (2008). The Authoritative Reference on Commemorative Coins 1892–1954. Roswell, GA: Kyle Vick. OCLC 711779330.
Harpine, William D. (2005). From the Front Porch to the Front Page: McKinley and Bryan in the 1896 Presidential Campaign. Presidential Rhetoric. 13. College Station, Texas: Texas A&M University Press. ISBN 978-1-58544-559-2.
House Committee on Coinage; Weights and Measures (January 13, 1916). Coinage of McKinley Souvenir Dollar. United States Government Printing Office. (subscription required)
McElroy, Richard L. (1996). William McKinley and Our America (softcover ed.). Canton, Ohio: Stark County Historical Society. ISBN 978-0-9634712-1-5.
Mehl, B. Max (1937). The Commemorative Coinage of the United States. Fort Worth, TX: B. Max Mehl. OCLC 2872685.
"The National McKinley Memorial Niles, Ohio" (PDF). Niles, OH: The National McKinley Memorial Association. Retrieved August 13, 2014.
Slabaugh, Arlie R. (1975). United States Commemorative Coinage (second ed.). Racine, WI: Whitman Publishing. ISBN 978-0-307-09377-6.
Swiatek, Anthony (2012). Encyclopedia of the Commemorative Coins of the United States. Chicago: KWS Publishers. ISBN 978-0-9817736-7-4.
Swiatek, Anthony; Breen, Walter (1981). The Encyclopedia of United States Silver & Gold Commemorative Coins, 1892 to 1954. New York: Arco Publishing. ISBN 978-0-668-04765-4.
Taxay, Don (1967). An Illustrated History of U.S. Commemorative Coinage. New York: Arco Publishing. ISBN 978-0-668-01536-3.
Vermeule, Cornelius (1971). Numismatic Art in America. Cambridge, MA: The Belknap Press of Harvard University Press. ISBN 978-0-674-62840-3.
Yeoman, R.S. (2014). A Guide Book of United States Coins (68th ed.). Atlanta, GA: Whitman Publishing. ISBN 978-0-7948-4215-4.
Me and Juliet is a musical comedy by Richard Rodgers (music) and Oscar Hammerstein II (lyrics and book) and their sixth stage collaboration. The work tells a story of romance backstage at a long-running musical: assistant stage manager Larry woos chorus girl Jeanie behind the back of her electrician boyfriend, Bob. Me and Juliet premiered in 1953 and was not considered a success, although it ran for much of a year on Broadway and returned a small profit to its backers.
Rodgers had long wanted to write a musical comedy about the cast and crew backstage at a theatre. After Rodgers and Hammerstein had another hit with The King and I in 1951, Rodgers proposed the backstage project to his partner. Hammerstein was unenthusiastic, thinking the subject matter trivial, but agreed to do the project. The play required complex machinery, designed by Jo Mielziner, so that the audience could view action not only on the stage of the theatre where the show-within-the-show (also named Me and Juliet) takes place, but in the wings and on the light bridge (high above the stage, from which the lighting technicians train spotlights) as well.
When Me and Juliet began tryout performances in Cleveland, the duo realized that the show had problems with the plot and staging. Extensive revisions during the remaining Cleveland and Boston tryouts failed to cure the difficulties with the plot, which the critics considered weak and uninteresting. The show was met with less than favorable reviews, though Mielziner's staging won praise from audience and critics. The show closed once it had exhausted its advance sales. Bill Hayes, the show's star, states in his autobiography Like Sands Through the Hourglass (2005): "We played nearly five hundred performances, all to full houses. Production costs were paid off and substantial profits went into the R&H till. So, though not in the same category as the storied five that were made into films - Oklahoma!, Carousel, South Pacific, The King and I and The Sound of Music - our show must be considered a success". With the exception of a short run in Chicago, there was no national tour, and the show has seldom been seen. A small-scale production was presented by London's Finborough Theatre in 2010.
The origins of Me and Juliet can be traced to the early days of the relationship between Rodgers and Hammerstein. The musical Oklahoma! opened in 1943; it was Rodgers and Hammerstein's first work together and a massive hit. Soon after Oklahoma! opened, Rodgers began considering the idea of a musical set backstage at a theatre staging a musical. The production could explore different areas of the backstage world. Rodgers also saw it as the opportunity to write a pure musical comedy, without the serious themes which had marked their early works—such as the attacks on racism in South Pacific, and the cultural tolerance in The King and I.Hammerstein was initially unenthusiastic, thinking the subject matter trivial, but Rodgers pressed the matter. It was Hammerstein's turn to give in to his partner; Rodgers had agreed to the project that became the 1947 musical Allegro, their initial failure, under pressure from Hammerstein, who had long dreamed of doing a serious musical about an ordinary man. According to Stephen Sondheim, a protégé of Hammerstein, "Oscar was able to keep the partnership together by taking Dick's suggestion [for a backstage musical], which he did not want to take." As the two discussed the backstage idea, Hammerstein became more enthusiastic, suggesting that the show start with the stage entirely bare, as if the audience had come in not at performance time but at another time during the day. Such effects are today well-known following the success of other "backstagers" such as A Chorus Line; in the early 1950s they were unrealized and novel.The two discussed the matter at a meeting in early 1952 in Palm Beach, Florida, where Rodgers was vacationing as he worked on melodic sketches for the television documentary Victory at Sea. Rodgers suggested dispensing with the overture, reserving that for the overture of the show-within-the-show. Following another meeting in mid-1952, they called in long-time Rodgers and Hammerstein stage designer Jo Mielziner and hired him to design the sets. Mielziner confirmed that a scene could be played part onstage and part in the backstage world, but that this would be expensive. In August 1952, Hammerstein began a sketch of the plot; by early October he had a near-complete first draft. As the show was to be musical comedy, the pair hired one of the top musical comedy directors, George Abbott, who accepted the position without reading the script. He regretted the haste of this decision as soon as he read the script, finding it sentimental and melodramatic. He confided his concerns to the pair; in response, Hammerstein told him to make whatever changes in the script he thought best. With Hammerstein's permission, Abbott made major changes to the plot.Hammerstein had only briefly described the show-within-the-show. Fearing the show would be uninteresting, Abbott hoped that some highlights would be furnished when the show-within-the-show, as yet only briefly described by Hammerstein, was fleshed out. According to author and composer Ethan Mordden in his book about the duo's works, Hammerstein thought the show-within-the-show was to be:
 something bizarre, to stand out and amaze us, the better to set off the plain life of the actor ... We shall imagine some rather advanced musical of the near future, something beyond even Allegro, with archetypical characters—a simple hero and his lovable Juliet, the rapacious Don Juan and his volatile Carmen. Then the audience will always know where it is. Contrast is the key. The show-within must look and sound, at every moment, as far from real life as possible.
Hammerstein included an incident he had seen when he was a neophyte assistant stage manager: a chorus boy came up to a chorus girl and asked to use some of her mascara—to disguise a hole in the boy's black socks. Hammerstein stated, "we were religious in keeping away from the trite things—the kindly old stage door man named Pop, the pretty little understudy who replaces the star on opening night. We steered clear, too, of the backstage story of a company putting on a new show, with all the anxieties of the actors and producers ... It seemed right to focus on a show which is already running because we wanted to tell a story about a community, the backstage community, and this community becomes settled and established after a show opens."In addition to Abbott, the duo recruited other professionals experienced in musical comedy. Choreographer Robert Alton had worked in such hits as Panama Hattie and in movie musicals. Don Walker was hired to do the orchestrations; his would be simpler than those of Robert Russell Bennett, who usually performed that function in the pair's musicals but who was not available. Irene Sharaff was engaged to design the more than 300 costumes which would be needed. The show was originally named Hercules and Juliet, but they soon changed it to Me and Juliet. The Majestic Theatre, which Rodgers and Hammerstein desired to have for Me and Juliet, was currently occupied by their South Pacific, four years into its run. Arrangements were made to shift South Pacific to the Broadway Theatre though, due to schedule conflicts, this meant moving that show to Boston for five weeks.
For theatrical terminology, see Stage (theatre).The entire action of the show takes place in and close to a Broadway theatre in which the long-running musical Me and Juliet  (the "show-within-the-show") is playing. The setting is the early 1950s.
A half-hour remains before the show is to begin. Electrician Sidney and chorus girl Jeanie are irritated at Sidney's fellow electrician, Bob, for not being there. Sidney needs Bob's help; Jeanie, Bob's girlfriend, is annoyed at being stood up. Sidney warns Jeanie that Bob may not be the right man for her; these are doubts she has too (Musical numbers: "A Very Special Day"/"That's the Way it Happens").
Jeanie leaves, and Bob appears. Bob tells Sidney he likes dating Jeanie, but does not plan to marry her. When Sidney jokes that Jeanie can do better than Bob, the larger man momentarily chokes him. Jeanie sees this, adding to her doubts about Bob. Larry, the assistant stage manager, is also attracted to Jeanie (reprise of "That's the Way it Happens").
Stage manager Mac sees to the final preparations, and the overture to the internal show is played by the orchestra, led by Dario, the conductor ("Overture to Me and Juliet"). The internal show's curtain rises ("Marriage Type Love"): the main male character, "Me" (performed by Charlie, a singer), tells the audience about the girl he wants to marry, Juliet (Lily, a singer). He also tells the audience of the girl he is determined not to marry, Carmen, who scares him. "Me" feels Carmen (the lead female dancing role) is better suited to his boss, Don Juan (the lead male dancer). As the internal show continues, Bob and Sidney are on the light bridge. Bob identifies with Don Juan for his reluctance to marry ("Keep It Gay").
Another day at Me and Juliet, and the dancers are practicing under Mac's supervision (conclusion of "Keep It Gay"). At Larry's urging, Jeanie decides to audition for the position of second understudy for the role of Juliet. On learning this, Mac takes Larry aside and warns him never to get involved with a cast member of a show while in charge of it. No sooner has Mac said this than his girlfriend Betty (currently in the show across the street) auditions for the role of Carmen. The producer gives her the role. As Larry looks on with amusement, Mac accepts this professionally, then stamps off in disgust.
Jeanie practices for her own audition ("No Other Love"), and Larry tells her that the audience will accept her if she's "a real kid" like Juliet, but reject her if she's a "phony" ("The Big Black Giant"). Larry desires a romance with Jeannie, but fears the larger and stronger Bob.
Several months pass, during which Jeanie gets the job as second understudy. Larry and Jeanie are meeting secretly and keeping their budding romance from Bob. The rest of the cast is aware of their dates—one dancer spotted them in a chili restaurant on Eighth Avenue.
Mac, true to his principles, has dumped Betty, but the two are still attracted to each other. Betty enjoys acting ("It's Me"). As she performs in the internal show, Bob and Sidney are on the light bridge again.
Bob has been fooled by Jeanie's lies about why they are not going out, and is enlightened when Sidney lets slip that Larry and Jeanie are seeing each other. Bob demands proof, and Sidney tells Bob to watch what happens in the wings during the upcoming Act 1 finale to Me and Juliet. Bob sees Larry and Jeanie kiss after she comes offstage with a tray of flowers, an action caught by Bob's spotlight. Mac enters, grasps the situation, sends Larry away, then puts the tray back in Jeanie's hands and pushes her onstage. She is pursued by Bob's spotlight, which relentlessly follows her around the stage as more and more of the dancers become aware something has gone badly wrong. Bob drops a sandbag from the light bridge; it knocks the tray Jeanie is holding to the ground. Mac orders the curtain lowered in front of a stage in panic.
In the downstairs lounge, a few minutes before the Act 2 curtain for Me and Juliet rises, the ushers comment on the remarkable conclusion to Act 1—although the audience has noticed nothing unusual ("Intermission Talk"). As Act 2 of the internal show starts, an enraged Bob is searching the theatre for Jeannie and Larry. Unable to find them, he takes up position at a bar across the street where he can watch the theatre doors ("It Feels Good"). The perspective shifts to the onstage action in Me and Juliet, where Don Juan and Carmen are on a date ("We Deserve Each Other"), before moving to the manager's office where Larry and Jeanie are hiding out ("I'm Your Girl"). Mac has only just begun his lecture to them when Bob enters through the window, having heard familiar voices. In the ensuing fight, Bob knocks out Mac, but when the electrician grabs for Jeannie, Larry strongly defends her. The fight ends when Bob accidentally hits his head on a radiator and is knocked out as well.
Ruby, the company manager, sends Larry and Jeannie down to the stage to continue the play. After Bob and Mac recover, Ruby informs Bob that Larry and Jeanie had secretly married earlier that day, and the surprised electrician leaves. Mac, fearful of more mayhem, goes in search of him. As Mac exits, the phone rings, and Ruby takes the call. It is the producer, calling for Mac to transfer him to another show, thereby setting him free to resume his romance with Betty.
Onstage, Me and Juliet is concluding. After the internal show finishes ("Finale to Me and Juliet"), Larry, who will be the new stage manager, insists on rehearsing a scene from the show. Seeing Bob enter with a scowl, Larry orders him and Sidney to be present the next morning to re-angle the lights. Taken aback, and rather sheepishly, Bob says "I didn't know you were married" before quietly leaving, after stating, "I'll be here, I guess." Jeanie is congratulated by her showmates, but Larry, all business, waves them to their places to rehearse the scene. As Lily has had to leave, Jeanie stands in for her as Juliet, while Larry sings the part of Me in the scene, as the curtain falls ("Finale of Our Play").
The cast consisted mostly of unknowns, though Isabel Bigley, who had just originated Sister Sarah Brown in Guys and Dolls, was given the leading role of chorus girl Jeanie. For Larry, the assistant stage manager who falls in love with Jeanie, they cast Bill Hayes, a well-known stage and television actor. William Tabbert, the original Lt. Joe Cable in South Pacific was considered for the part of Larry, but lost out because he was thought to be too tall to be afraid of Mark Dawson, hired as the towering bully Bob.
Chorus auditions began March 10, 1953 at Broadway's Majestic Theatre; Rodgers, Hammerstein, and Abbott listened to more than 1,000 people. Rehearsals opened at the Majestic for principals and the Alvin Theatre for dancers. According to Saul Pett, a freelance reporter who was allowed to observe the rehearsals, "everyone seems relaxed except Hammerstein." The lyricist's son James served as second assistant stage manager. James Hammerstein remembered having a difficult relationship with Rodgers; the composer suggested James do his work from front of house, rather than from backstage. "I think he thought it was his show and his bailiwick. Why should a Hammerstein be back there?" James Hammerstein found the lead female dancer attractive, and asked her out. Just before the date, Rodgers fired her, telling James Hammerstein to break the news.Pett recorded the technical problems which had to be solved to accomplish the complex staging:
 A number of key scenes required the audience to both see the play-within-the-play and at the same time observe the realism of the stage manager's operations in the wings. To achieve this result and to make both elements simultaneous, the major part of the production had to hang on specially-constructed overhead steel tracks. Synchronized electric motors slowly moved the stage pictures off into the wings far enough to expose the stage manager's desk and actors and stagehands offstage awaiting their cues.
During the rehearsals, the duo took out two production numbers, "Wake Up, Little Theatre" and "Dance", concerned that the show was running long. The actress playing Juliet in the internal play proved to be a fine singer but a poor actress; she was replaced by Helena Scott. Abbott had few negative comments after the final New York run-through, and the company entrained for Cleveland, site of the first tryouts, in high spirits, sleeping little on the train ride. RCA Records put up the $350,000 cost of the production in exchange for a fifty-percent interest and rights to the original cast recording.The tryouts in Cleveland were at the Hanna Theatre. The dress rehearsal the night before the initial performance revealed a number of problems with the show; during the first act alone, Hammerstein dictated eight pages of notes. The sandbag which falls from the light bridge near the end of Act 1 dropped off-cue, nearly striking Isabel Bigley, who played Jeanie. Pett remembered that the rehearsal was stopped often, as Rodgers sought to work out each problem as it arose, and the rehearsal, which began at 8 p.m. lasted until 2 a.m.The Cleveland premiere on the evening of April 20, 1953 saw a distinguished crowd turn out. When the stage backdrop failed to come down on time, Hammerstein was heard to mutter, "Damn and damn and damn! This is a new way: they saved it for the performance!" Nevertheless, the crowd gave the show a rousing welcome. The Cleveland critics thought well of the show, but were concerned about the weak story. After the Cleveland reactions and problems, according to Rodgers biographer Meryle Secrest, "what had seemed to be a show needing minor adjustments became a musical in serious trouble". Bigley remembered that she had just come from a hit and "there just wasn't the same energy". Hammerstein had intended to omit the overture, with the audience to watch, after the curtain rose, a blank stage on which the play-within-the-play performers and crew drifted in and began their preparations. Expecting the orchestra to begin the evening, the audience talked throughout the initial scenes before being quieted by the internal show's overture; in response, the duo abandoned Hammerstein's concept and opened with an overture.In contrast to the levity on their first train ride, the company was downcast and exhausted en route from Cleveland to Boston for the final tryouts. The show opened previews in Boston on May 6. A majority of the Boston critics liked the show, and expressed confidence that Rodgers and Hammerstein could fix the problems with the plot. The pair took out one song, "Meat and Potatoes", which was felt to be too raunchy. After watching it performed by Joan McCracken, who played Betty (Carmen in the play-within-the-play), the pair decided it had too many double entendres and cut it. It was replaced by "We Deserve Each Other", which the pair had written in a Cleveland hotel room. Another cut song, "You Never Had It So Good", included lyrics which satirized the duo's own earlier efforts. Its lyrics, "I'll sew, I'll bake / I'll try to make your evenings all enchanted. / My honeycake, / I'm yours to take, but don't take me for granted", alluded to two songs from South Pacific, "Some Enchanted Evening" and "Honey Bun". Audiences continued to greet the show warmly.During the Boston previews, the duo heard the audience praise the sets, a reaction which usually augered ill for the show itself. Hammerstein wrote to The King and I director John van Druten:
 Me and Juliet looks like a great big hit. It is a change of pace for us and in some quarters we may be criticized because it is not as high-falutin' as our most recent efforts. It is in fact an out and out musical comedy. If this be treason, make the most of it.
The musical opened on Broadway on May 28, 1953 at the Majestic Theatre. Large advance sales guaranteed a considerable run; by the start of November, it had paid back its advance, and closed after 358 performances, paying a small profit to RCA. Thomas Hischak, in his The Rodgers and Hammerstein Encyclopedia, suggests that business fell off after the advance sales were exhausted "because audiences had come to expect more from a Rodgers and Hammerstein musical". According to Frederick W. Nolan in his book about the duo's works, "despite a $500,000 advance sale, despite a ten-month run (which, for anyone except Rodgers and Hammerstein, would have represented a major success), and despite an eventual profit in excess of $100,000, Me and Juliet has to be classed as a failure".The backstage drama portrayed in the musical was matched by actual difficulties among the cast. McCracken, who played Betty, was the wife of choreographer Bob Fosse and became pregnant during the run. Bill Hayes later wrote that she lost her baby through miscarriage about the same time she lost her husband to Gwen Verdon. The baby was in fact aborted, because the pregnancy would have endangered McCracken's health as a result of her diabetes. Hayes noted that in the fifteen months he played Larry, he did not recall ever having a conversation with Isabel Bigley, who was supposedly his love interest and wife: "I doubt that the audience ever believed we were deeply in love." The show received no Tony Award nominations. During the run, Hammerstein followed his usual practice of visiting the theatre now and again to ensure that the performers were not taking liberties with his book. Upon his return, Hammerstein's secretary asked him how the show was going. The lyricist thought for a second, then said "I hate that show." According to Bill Hayes in his autobiography Like Sands Through the Hourglass published in 2005 he states We played nearly five hundred performances, however, all to full houses. Production costs were paid off and substantial profits went into the R&H till. So, though not in the same category as the storied five that were made into films - Oklahoma!, Carousel, South Pacific, The King and I and The Sound of Music - our show must be considered a success.No national tour was attempted, but the show did have a six-week run at the Shubert Theatre in Chicago in spring 1954. Among those who played in the chorus during the New York run was future star Shirley MacLaine; Shirley Jones was a chorus girl in the Chicago performances. Subsequent productions include one by Kansas City's Starlight Theatre in 1955. Equity Library Theatre produced it in New York in 1970; it returned to that city, though not to Broadway, in 2002 with the York Theatre. A London production was presented by the Finborough Theatre in 2010 in a fifty-seat theatre; the production was billed as the show's European premiere.
One source of Rodgers's excitement for the concept that became Me and Juliet was his view that a contemporary musical gave him the opportunity for a contemporary score. At the time Rodgers wrote the score, a Latin dance craze had swept the United States, and its influence found its way into the music for Me and Juliet. Rodgers put an onstage jazz trio in the production and encouraged the members to improvise. Among the trio was jazz artist Barbara Carroll as Chris, rehearsal pianist. "Intermission Talk", the chatter among audience members early in the second act, is given a bouncy melody and sly references to a number of shows then on Broadway—including the duo's own The King and I. According to author and composer Ethan Mordden, Rodgers's score "found [Rodgers & Hammerstein] going for impish, nimble, the sound of the Hit Parade as reimagined by [them]".Rodgers borrowed the music for "No Other Love (1953 song)," a tango, from his award-winning score for Victory at Sea. RCA, which had those rights, arranged for Perry Como to record the song, and it was rushed onto the market to coincide with the show's Broadway opening. The record became a number-one hit for Como on the Disc Jockey chart, though #2 as a best seller, remaining on the charts for 22 weeks.Hischak described the original cast album as "surprisingly lively and mostly enjoyable for a musical that was considered so dull on stage." He pointed to "Intermission Talk" as a number which probably works better in a recording than on stage and states that "there is no mistaking the hypnotic power of 'No Other Love'". The original cast recording was released on compact disc in 1993.
Critics' views were neutral to unfavorable. The New York Times critic Brooks Atkinson praised the acting and choreography, but stated, "This is their Valentine to show business, expressed in the form of a show-within-a-show; and it has just about everything except an intelligible story." Herald-Tribune critic Walter Kerr noted that "Rodgers and Hammerstein have come perilously close to writing a show-without-a-show." George Jean Nathan of the Journal American stated that "Hammerstein's book has the effect of hanging idly around waiting for an idea to come to him." Robert Coleman of the Daily Mirror noted, "Having set new high standards for musicals throughout the world, Rodgers and Hammerstein dipped into the lower drawer of their desk for Me and Juliet. It proved a big disappointment for this dyed-in-the-wool R. & H. fan." John Chapman of the Daily News commented, "It is at its most interesting when Jo Mielziner's sets are in motion". According to Steven Suskin in his compilation of Broadway opening night reviews, the seven major New York critics allotted the production no raves, one favorable review, one mixed, four unfavorable, and one pan.One well-received number was "Keep It Gay", a song which in rehearsal had been assigned to several different performers before ending with Bob. The song was liked in part due to the novelty of its setting: it begins with Bob singing from the light bridge high above the stage; following a blackout the internal play performers take it up on the stage below, and following another blackout, the performers are seen in their workout clothes, at a rehearsal some weeks later. Hammerstein gave credit for the scene to Mielziner, and suggested that it demonstrated one way in which the book had affected the music.Abbott stated that there were two reasons for what he considered to be the show's failure. The first was Rodgers and Hammerstein's overconfidence; they thought of themselves as Broadway's "Golden Boys" who could do no wrong. The other was the play-within-the-play, which had not been thoroughly thought out by anyone. According to Abbott, Hammerstein remained "positively Sphinx-like" on the subject. At a loss to understand the characters of the play-within-the play, Alton came up with nothing more than routine song-and-dance numbers. During the run, the duo approached choreographer Jerome Robbins and asked him if he could fix the dances. Robbins said that he could, but he would not, as "it would kill Bob Alton". According to Hammerstein biographer Hugh Fordin, "[the] intended contrast between onstage and backstage life was never achieved because the onstage show was so tepid and confusing.""That's the Way it Happens" was included in the 1996 stage version of Rodgers and Hammerstein's 1945 movie musical, State Fair. According to David Lewis in his history of the Broadway musical, "The Rodgers and Hammerstein office has, it would appear, given up on [later R&H musical] Pipe Dream and [Me and] Juliet ever finding an audience ... so these songs are up for grabs."Composer and author Ethan Mordden, in his book on the duo's works, wrote of the conceptual difficulties which Rodgers and Hammerstein had with the musical:
 [Me and Juliet] was the first of their plays without a powerful sense of destiny, of characters consequentially interconnected. In Oklahoma!, Carousel, South Pacific and The King and I especially, the principals—whether noble or weak, just or impetuous—change each other's lives. Me and Juliet's characters appear to be thrown together by chance and—except for the lovers—will part company unaffected by each other as soon as the show closes. This left Hammerstein with nothing to seek out in his people, and Rodgers with nothing to illustrate.
Hayes, Bill; Hayes, Susan Seaforth (2005). Like sands through the hourglass. New York: New American Library. ISBN 0-451-21660-1.Fordin, Hugh (1995) [1977]. Getting to Know Him: A Biography of Oscar Hammerstein II (reprint, illustrated ed.). Jefferson, N.C.: Da Capo Press. ISBN 978-0-306-80668-1.
Hischak, Thomas S. (2007). The Rodgers and Hammerstein Encyclopedia (illustrated ed.). Westport, Conn.: Greenwood Publishing Group. ISBN 978-0-313-34140-3.
Hyland, William G. (1998). Richard Rodgers (illustrated ed.). New Haven, Conn.: Yale University Press. ISBN 978-0-300-07115-3.
Lewis, David H. (2002). Broadway Musicals: a Hundred Year History. Ned Miller (Composer) "Sunday" (illustrated ed.). Jefferson, N.C.: McFarland & Company, Inc. ISBN 978-0-7864-1269-3.
Macfarlane, Malcolm; Crossland, Ken (2009). Perry Como: A Biography and Career Record (illustrated ed.). Jefferson, N.C.: McFarland. ISBN 978-0-7864-3701-6.
Mariska, Bradley (2004). "Who Expects a Miracle to Happen Every Day?": Rediscovering Me and Juliet and Pipe Dream, The Forgotten Musicals of Rodgers and Hammerstein (PDF) (thesis ed.). College Park, MD: University of Maryland.
Mordden, Ethan (1992). Rodgers & Hammerstein (illustrated ed.). New York: Harry N. Abrams. ISBN 978-0-8109-1567-1.
Nolan, Frederick (2002) [1979]. The Sound of Their Music: The Story of Rodgers and Hammerstein (reprint ed.). Cambridge, Mass.: Applause Theatre and Cinema Books. ISBN 978-1-55783-473-7.
Rodgers, Richard; Hammerstein, Oscar, II (1953). Six Plays by Rodgers and Hammerstein. New York: Random House (Modern Library).
Sagolla, Lisa Jo (2003). The Girl Who Fell Down: A Biography of Joan McCracken. Boston, Mass.: Northeastern University Press. ISBN 1-55553-573-9.
Secrest, Meryle (2001). Somewhere for Me: A Biography of Richard Rodgers (illustrated ed.). Cambridge, Mass.: Applause Theatre and Cinema Books. ISBN 978-1-55783-581-9.
Suskin, Steven (1990). Opening Night on Broadway. New York: Schirmer Books. ISBN 978-0-02-872625-0.
Willis, John (1998). Theatre World 1995–1996. New York: Hal Leonard Corporation. ISBN 978-1-55783-323-5.Articles and webpages
Atkinson, Brooks (May 29, 1953). "First Night at the Theatre; ' Me and Juliet' Is a Valentine to the Theatre by Rodgers and Hammerstein". The New York Times. p. 17. Retrieved February 6, 2011. (subscription required)
Gussow, Mel (May 15, 1970). "Equity Library revives Me and Juliet" (PDF). The New York Times. p. 42. Retrieved February 6, 2011. (subscription required)
Metcalf, Steve (February 7, 1993). "Torrent of CD releases showcases Broadway classics and obscurities". Hartford Courant. p. G1. Retrieved January 30, 2011. (subscription required)
Peck, Seymour (May 24, 1953). "About Me and Juliet". The New York Times. p. X1. Retrieved February 6, 2011. (subscription required)
Wolf, Matt (October 19, 2010). "Theatre Review: In the city of second chances". The New York Times. Retrieved February 6, 2011. (subscription required)

A mechanical filter is a signal processing filter usually used in place of an electronic filter at radio frequencies.  Its purpose is the same as that of a normal electronic filter: to pass a range of signal frequencies, but to block others.  The filter acts on mechanical vibrations which are the analogue of the electrical signal.  At the input and output of the filter, transducers convert the electrical signal into, and then back from, these mechanical vibrations.
The components of a mechanical filter are all directly analogous to the various elements found in electrical circuits.  The mechanical elements obey mathematical functions which are identical to their corresponding electrical elements.  This makes it possible to apply electrical network analysis and filter design methods to mechanical filters.  Electrical theory has developed a large library of mathematical forms that produce useful filter frequency responses and the mechanical filter designer is able to make direct use of these.  It is only necessary to set the mechanical components to appropriate values to produce a filter with an identical response to the electrical counterpart.
Steel alloys and iron–nickel alloys are common materials for mechanical filter components; nickel is sometimes used for the input and output couplings.  Resonators in the filter made from these materials need to be machined to precisely adjust their resonance frequency before final assembly.
While the meaning of mechanical filter in this article is one that is used in an electromechanical role, it is possible to use a mechanical design to filter mechanical vibrations or sound waves (which are also essentially mechanical) directly. For example, filtering of audio frequency response in the design of loudspeaker cabinets can be achieved with mechanical components.  In the electrical application, in addition to mechanical components which correspond to their electrical counterparts, transducers are needed to convert between the mechanical and electrical domains.  A representative selection of the wide variety of component forms and topologies for mechanical filters are presented in this article.
The theory of mechanical filters was first applied to improving the mechanical parts of phonographs in the 1920s.  By the 1950s mechanical filters were being manufactured as self-contained components for applications in radio transmitters and high-end receivers.  The high "quality factor", Q, that mechanical resonators can attain, far higher than that of an all-electrical LC circuit, made possible the construction of mechanical filters with excellent selectivity. Good selectivity, being important in radio receivers, made such filters highly attractive.  Contemporary researchers are working on microelectromechanical filters, the mechanical devices corresponding to electronic integrated circuits.
The elements of a passive linear electrical network consist of inductors, capacitors and resistors which have the properties of inductance, elastance (inverse capacitance) and resistance, respectively. The mechanical counterparts of these properties are, respectively, mass, stiffness and damping. In most electronic filter designs, only inductor and capacitor elements are used in the body of the filter (although the filter may be terminated with resistors at the input and output). Resistances are not present in a theoretical filter composed of ideal components and only arise in practical designs as unwanted parasitic elements. Likewise, a mechanical filter would ideally consist only of components with the properties of mass and stiffness, but in reality some damping is present as well.The mechanical counterparts of voltage and electric current in this type of analysis are, respectively, force (F) and velocity (v) and represent the signal waveforms. From this, a mechanical impedance can be defined in terms of the imaginary angular frequency, jω, which entirely follows the electrical analogy.
The scheme presented in the table is known as the impedance analogy. Circuit diagrams produced using this analogy match the electrical impedance of the mechanical system seen by the electrical circuit, making it intuitive from an electrical engineering standpoint. There is also the mobility analogy, in which force corresponds to current and velocity corresponds to voltage. This has equally valid results but requires using the reciprocals of the electrical counterparts listed above. Hence, M → C, S → 1/L, D → G where G is electrical conductance, the inverse of resistance. Equivalent circuits produced by this scheme are similar, but are the dual impedance forms whereby series elements become parallel, capacitors become inductors, and so on. Circuit diagrams using the mobility analogy more closely match the mechanical arrangement of the circuit, making it more intuitive from a mechanical engineering standpoint. In addition to their application to electromechanical systems, these analogies are widely used to aid analysis in acoustics.Any mechanical component will unavoidably possess both mass and stiffness. This translates in electrical terms to an LC circuit, that is, a circuit consisting of an inductor and a capacitor, hence mechanical components are resonators and are often used as such. It is still possible to represent inductors and capacitors as individual lumped elements in a mechanical implementation by minimising (but never quite eliminating) the unwanted property. Capacitors may be made of thin, long rods, that is, the mass is minimised and the compliance is maximised. Inductors, on the other hand, may be made of short, wide pieces which maximise the mass in comparison to the compliance of the piece.Mechanical parts act as a transmission line for mechanical vibrations. If the wavelength is short in comparison to the part then a lumped element model as described above is no longer adequate and a distributed element model must be used instead.  The mechanical distributed elements are entirely analogous to electrical distributed elements and the mechanical filter designer can use the methods of electrical distributed element filter design.
Mechanical filter design was developed by applying the discoveries made in electrical filter theory to mechanics. However, a very early example (1870s) of acoustic filtering was the "harmonic telegraph", which arose precisely because electrical resonance was poorly understood but mechanical resonance (in particular, acoustic resonance) was very familiar to engineers. This situation was not to last for long; electrical resonance had been known to science for some time before this, and it was not long before engineers started to produce all-electric designs for filters. In its time, though, the harmonic telegraph was of some importance. The idea was to combine several telegraph signals on one telegraph line by what would now be called frequency division multiplexing thus saving enormously on line installation costs. The key of each operator activated a vibrating electromechanical reed which converted this vibration into an electrical signal. Filtering at the receiving operator was achieved by a similar reed tuned to precisely the same frequency, which would only vibrate and produce a sound from transmissions by the operator with the identical tuning.Versions of the harmonic telegraph were developed by Elisha Gray, Alexander Graham Bell, Ernest Mercadier and others. Its ability to act as a sound transducer to and from the electrical domain was to inspire the invention of the telephone.
Once the basics of electrical network analysis began to be established, it was not long before the ideas of complex impedance and filter design theories were carried over into mechanics by analogy. Kennelly, who was also responsible for introducing complex impedance, and Webster were the first to extend the concept of impedance into mechanical systems in 1920. Mechanical admittance and the associated mobility analogy came much later and are due to Firestone in 1932.It was not enough to just develop a mechanical analogy. This could be applied to problems that were entirely in the mechanical domain, but for mechanical filters with an electrical application it is necessary to include the transducer in the analogy as well. Poincaré in 1907 was the first to describe a transducer as a pair of linear algebraic equations relating electrical variables (voltage and current) to mechanical variables (force and velocity). These equations can be expressed as a matrix relationship in much the same way as the z-parameters of a two-port network in electrical theory, to which this is entirely analogous:
    {\displaystyle {\begin{bmatrix}V\\F\end{bmatrix}}={\begin{bmatrix}z_{11}&z_{12}\\z_{21}&z_{22}\end{bmatrix}}{\begin{bmatrix}I\\v\end{bmatrix}}}
  where V and I represent the voltage and current respectively on the electrical side of the transducer.
Wegel, in 1921, was the first to express these equations in terms of mechanical impedance as well as electrical impedance. The element 
   is the open circuit mechanical impedance, that is, the impedance presented by the mechanical side of the transducer when no current is entering the electrical side. The element 
  , conversely, is the clamped electrical impedance, that is, the impedance presented to the electrical side when the mechanical side is clamped and prevented from moving (velocity is zero). The remaining two elements, 
  , describe the transducer forward and reverse transfer functions respectively. Once these ideas were in place, engineers were able to extend electrical theory into the mechanical domain and analyse an electromechanical system as a unified whole.
An early application of these new theoretical tools was in phonographic sound reproduction. A recurring problem with early phonograph designs was that mechanical resonances in the pickup and sound transmission mechanism caused excessively large peaks and troughs in the frequency response, resulting in poor sound quality. In 1923, Harrison of the Western Electric Company filed a patent for a phonograph in which the mechanical design was entirely represented as an electrical circuit. The horn of the phonograph is represented as a transmission line, and is a resistive load for the rest of the circuit, while all the mechanical and acoustic parts—from the pickup needle through to the horn—are translated into lumped components according to the impedance analogy. The circuit arrived at is a ladder topology of series resonant circuits coupled by shunt capacitors. This can be viewed as a bandpass filter circuit. Harrison designed the component values of this filter to have a specific passband corresponding to the desired audio passband (in this case 100 Hz to 6 kHz) and a flat response. Translating these electrical element values back into mechanical quantities provided specifications for the mechanical components in terms of mass and stiffness, which in turn could be translated into physical dimensions for their manufacture. The resulting phonograph has a flat frequency response in its passband and is free of the resonances previously experienced. Shortly after this, Harrison filed another patent using the same methodology on telephone transmit and receive transducers.
Harrison used Campbell's image filter theory, which was the most advanced filter theory available at the time. In this theory, filter design is viewed essentially as an impedance matching problem. More advanced filter theory was brought to bear on this problem by Norton in 1929 at Bell Labs. Norton followed the same general approach though he later described to Darlington the filter he designed as being "maximally flat". Norton's mechanical design predates the paper by Butterworth who is usually credited as the first to describe the electronic maximally flat filter. The equations Norton gives for his filter correspond to a singly terminated Butterworth filter, that is, one driven by an ideal voltage source with no impedance, whereas the form more usually given in texts is for the doubly terminated filter with resistors at both ends, making it hard to recognise the design for what it is. Another unusual feature of Norton's filter design arises from the series capacitor, which represents the stiffness of the diaphragm. This is the only series capacitor in Norton's representation, and without it, the filter could be analysed as a low-pass prototype. Norton moves the capacitor out of the body of the filter to the input at the expense of introducing a transformer into the equivalent circuit (Norton's figure 4). Norton has used here the "turning round the L" impedance transform to achieve this.The definitive description of the subject from this period is Maxfield and Harrison's 1926 paper. There, they describe not only how mechanical bandpass filters can be applied to sound reproduction systems, but also apply the same principles to recording systems and describe a much improved disc cutting head.
The first volume production of mechanical filters was undertaken by Collins Radio Company starting in the 1950s. These were originally designed for telephone frequency-division multiplex applications where there is commercial advantage in using high quality filters. Precision and steepness of the transition band leads to a reduced width of guard band, which in turn leads to the ability to squeeze more telephone channels into the same cable.  This same feature is useful in radio transmitters for much the same reason.  Mechanical filters quickly also found popularity in VHF/UHF radio intermediate frequency (IF) stages of the high end radio sets (military, marine, amateur radio and the like) manufactured by Collins.  They were favoured in the radio application because they could achieve much higher Q-factors than the equivalent LC filter.  High Q allows filters to be designed which have high selectivity, important for distinguishing adjacent radio channels in receivers.  They also had an advantage in stability over both LC filters and monolithic crystal filters. The most popular design for radio applications was torsional resonators because radio IF typically lies in the 100 to 500 kHz band.
Both magnetostrictive and piezoelectric transducers are used in mechanical filters. Piezoelectric transducers are favoured in recent designs since the piezoelectric material can also be used as one of the resonators of the filter, thus reducing the number of components and thereby saving space. They also avoid the susceptibility to extraneous magnetic fields of the  magnetostrictive type of transducer.
A magnetostrictive material is one which changes shape when a magnetic field is applied. In reverse, it produces a magnetic field when distorted. The magnetostrictive transducer requires a coil of conducting wire around the magnetostrictive material. The coil either induces a magnetic field in the transducer and sets it in motion or else picks up an induced current from the motion of the transducer at the filter output. It is also usually necessary to have a small magnet to bias the magnetostrictive material into its operating range. It is possible to dispense with the magnets if the biasing is taken care of on the electronic side by providing a d.c. current superimposed on the signal, but this approach would detract from the generality of the filter design.The usual magnetostrictive materials used for the transducer are either ferrite or compressed powdered iron. Mechanical filter designs often have the resonators coupled with steel or nickel-iron wires, but on some designs, especially older ones, nickel wire may be used for the input and output rods. This is because it is possible to wind the transducer coil directly on to a nickel coupling wire since nickel is slightly magnetostrictive. However, it is not strongly so and coupling to the electrical circuit is weak. This scheme also has the disadvantage of eddy currents, a problem that is avoided if ferrites are used instead of nickel.The coil of the transducer adds some inductance on the electrical side of the filter. It is common practice to add a capacitor in parallel with the coil so that an additional resonator is formed which can be incorporated into the filter design. While this will not improve performance to the extent that an additional mechanical resonator would, there is some benefit and the coil has to be there in any case.
A piezoelectric material is one which changes shape when an electric field is applied. In reverse, it produces an electric field when it is distorted. A piezoelectric transducer, in essence, is made simply by plating electrodes on to the piezoelectric material. Early piezoelectric materials used in transducers such as barium titanate had poor temperature stability.  This precluded the transducer from functioning as one of the resonators; it had to be a separate component.  This problem was solved with the introduction of lead zirconate titanate (abbreviated PZT) which is stable enough to be used as a resonator. Another common piezoelectric material is quartz, which has also been used in mechanical filters.  However, ceramic materials such as PZT are preferred for their greater electromechanical coupling coefficient.One type of piezoelectric transducer is the Langevin type, named after a transducer used by Paul Langevin in early sonar research. This is good for longitudinal modes of vibration. It can also be used on resonators with other modes of vibration if the motion can be mechanically converted into a longitudinal motion. The transducer consists of a layer of piezoelectric material sandwiched transversally into a coupling rod or resonator.Another kind of piezoelectric transducer has the piezoelectric material sandwiched in longitudinally, usually into the resonator itself. This kind is good for torsional vibration modes and is called a torsional transducer.
It is possible to achieve an extremely high Q with mechanical resonators. Mechanical resonators typically have a Q of 10,000 or so, and 25,000 can be achieved in torsional resonators using a particular nickel-iron alloy. This is an unreasonably high figure to achieve with LC circuits, whose Q is limited by the resistance of the inductor coils.Early designs in the 1940s and 1950s started by using steel as a resonator material. This has given way to nickel-iron alloys, primarily to maximise the Q since this is often the primary appeal of mechanical filters rather than price. Some of the metals that have been used for mechanical filter resonators and their Q are shown in the table.Piezoelectric crystals are also sometimes used in mechanical filter designs. This is especially true for resonators that are also acting as transducers for inputs and outputs.One advantage that mechanical filters have over LC electrical filters is that they can be made very stable. The resonance frequency can be made so stable that it varies only 1.5 parts per billion (ppb) from the specified value over the operating temperature range (−25 to 85 °C), and its average drift with time can be as low as 4 ppb per day. This stability with temperature is another reason for using nickel-iron as the resonator material. Variations with temperature in the resonance frequency (and other features of the frequency function) are directly related to variations in the Young's modulus, which is a measure of stiffness of the material.  Materials are therefore sought that have a small temperature coefficient of Young's modulus.  In general, Young's modulus has a negative temperature coefficient (materials become less stiff with increasing temperature) but additions of small amounts of certain other elements in the alloy can produce a material with a temperature coefficient that changes sign from negative through zero to positive with temperature. Such a material will have a zero coefficient of temperature with resonance frequency around a particular temperature. It is possible to adjust the point of zero temperature coefficient to a desired position by heat treatment of the alloy.
It is usually possible for a mechanical part to vibrate in a number of different modes, however the design will be based on a particular vibrational mode and the designer will take steps to try to restrict the resonance to this mode. As well as the straightforward longitudinal mode some others which are used include flexural mode, torsional mode, radial mode and drumhead mode.Modes are numbered according to the number of half-wavelengths in the vibration.  Some modes exhibit vibrations in more than one direction (such as drumhead mode which has two) and consequently the mode number consists of more than one number.  When the vibration is in one of the higher modes, there will be multiple nodes on the resonator where there is no motion. For some types of resonator, this can provide a convenient place to make a mechanical attachment for structural support. Wires attached at nodes will have no effect on the vibration of the resonator or the overall filter response. In figure 5, some possible anchor points are shown as wires attached at the nodes. The modes shown are (5a) the second longitudinal mode fixed at one end, (5b) the first torsional mode, (5c) the second torsional mode, (5d) the second flexural mode, (5e) first radial expansion mode and (5f) first radially symmetric drumhead mode.
There are a great many combinations of resonators and transducers that can be used to construct a mechanical filter. A selection of some of these is shown in the diagrams. Figure 6 shows a filter using disc flexural resonators and magnetostrictive transducers. The transducer drives the centre of the first resonator, causing it to vibrate. The edges of the disc move in antiphase to the centre when the driving signal is at, or close to, resonance, and the signal is transmitted through the connecting rods to the next resonator.  When the driving signal is not close to resonance, there is little movement at the edges, and the filter rejects (does not pass) the signal. Figure 7 shows a similar idea involving longitudinal resonators connected together in a chain by connecting rods. In this diagram, the filter is driven by piezoelectric transducers. It could equally well have used magnetostrictive transducers. Figure 8 shows a filter using torsional resonators. In this diagram, the input has a torsional piezoelectric transducer and the output has a magnetostrictive transducer. This would be quite unusual in a real design, as both input and output usually have the same type of transducer. The magnetostrictive transducer is only shown here to demonstrate how longitudinal vibrations may be converted to torsional vibrations and vice versa. Figure 9 shows a filter using drumhead mode resonators. The edges of the discs are fixed to the casing of the filter (not shown in the diagram) so the vibration of the disc is in the same modes as the membrane of a drum. Collins calls this type of filter a disc wire filter.The various types of resonator are all particularly suited to different frequency bands. Overall, mechanical filters with lumped elements of all kinds can cover frequencies from about 5 to 700 kHz although mechanical filters down as low as a few kilohertz (kHz) are rare. The lower part of this range, below 100 kHz, is best covered with bar flexural resonators. The upper part is better done with torsional resonators. Drumhead disc resonators are in the middle, covering the range from around 100 to 300 kHz.The frequency response behaviour of all mechanical filters can be expressed as an equivalent electrical circuit using the impedance analogy described above. An example of this is shown in figure 8b which is the equivalent circuit of the mechanical filter of figure 8a. Elements on the electrical side, such as the inductance of the magnetostrictive transducer, are omitted but would be taken into account in a complete design. The series resonant circuits on the circuit diagram represent the torsional resonators, and the shunt capacitors represent the coupling wires. The component values of the electrical equivalent circuit can be adjusted, more or less at will, by modifying the dimensions of the mechanical components. In this way, all the theoretical tools of electrical analysis and filter design can be brought to bear on the mechanical design. Any filter realisable in electrical theory can, in principle, also be realised as a mechanical filter. In particular, the popular finite element approximations to an ideal filter response of the Butterworth and Chebyshev filters can both readily be realised. As with the electrical counterpart, the more elements that are used, the closer the approximation approaches the ideal, however, for practical reasons the number of resonators does not normally exceed eight.
Frequencies of the order of megahertz (MHz) are above the usual range for mechanical filters. The components start to become very small, or alternatively the components are large compared to the signal wavelength. The lumped element model described above starts to break down and the components must be considered as distributed elements. The frequency at which the transition from lumped to distributed models takes place is much lower for mechanical filters than it is for their electrical counterparts. This is because mechanical vibrations travel at the speed of sound for the material the component is composed of. For solid components, this is many times (x15 for nickel-iron) the speed of sound in air (343 m/s) but still considerably less than the speed of electromagnetic waves (approx. 3x108 m/s in vacuum). Consequently, mechanical wavelengths are much shorter than electrical wavelengths for the same frequency.  Advantage can be taken of these effects by deliberately designing components to be distributed elements, and the components and methods used in electrical distributed element filters can be brought to bear. The equivalents of stubs and impedance transformers are both achievable. Designs which use a mixture of lumped and distributed elements are referred to as semi-lumped.An example of such a design is shown in figure 10a. The resonators are disc flexural resonators similar to those shown in figure 6, except that these are energised from an edge, leading to vibration in the fundamental flexural mode with a node in the centre, whereas the figure 6 design is energised in the centre leading to vibration in the second flexural mode at resonance. The resonators are mechanically attached to the housing by pivots at right angles to the coupling wires. The pivots are to ensure free turning of the resonator and minimise losses. The resonators are treated as lumped elements; however, the coupling wires are made exactly one half-wavelength (λ/2) long and are equivalent to a λ/2 open circuit stub in the electrical equivalent circuit. For a narrow-band filter, a stub of this sort has the approximate equivalent circuit of a parallel shunt tuned circuit as shown in figure 10b. Consequently, the connecting wires are being used in this design to add additional resonators into the circuit and will have a better response than one with just the lumped resonators and short couplings.  For even higher frequencies, microelectromechanical methods can be used as described below.
Bridging wires are rods that couple together resonators that are not adjacent. They can be used to produce poles of attenuation in the stopband. This has the benefit of increasing the stopband rejection. When the pole is placed near the passband edge, it also has the benefit of increasing roll-off and narrowing the transition band. The typical effects of some of these on filter frequency response are shown in figure 11. Bridging across a single resonator (figure 11b) can produce a pole of attenuation in the high stopband. Bridging across two resonators (figure 11c) can produce a pole of attenuation in both the high and the low stopband. Using multiple bridges (figure 11d) will result in multiple poles of attenuation. In this way, the attenuation of the stopbands can be deepened over a broad frequency range.
The method of coupling between non-adjacent resonators is not limited to mechanical filters.  It can be applied to other filter formats and the general term for this class is cross-coupled filter. For instance, channels can be cut between cavity resonators, mutual inductance can be used with discrete component filters, and feedback paths can be used with active analogue or digital filters.  Nor was the method first discovered in the field of mechanical filters; the earliest description is in a 1948 patent for filters using microwave cavity resonators.  However, mechanical filter designers were the first (1960s) to develop practical filters of this kind and the method became a particular feature of mechanical filters.
A new technology emerging in mechanical filtering is microelectromechanical systems (MEMS).  MEMS are very small micromachines with component sizes measured in micrometres (μm), but not as small as nanomachines.  These filters can be designed to operate at much higher frequencies than can be achieved with traditional mechanical filters.  These systems are mostly fabricated from silicon (Si), silicon nitride (Si3N4), or polymers. A common component used for radio frequency filtering (and MEMS applications generally), is the cantilever resonator. Cantilevers are simple mechanical components to manufacture by much the same methods used by the semiconductor industry; masking, photolithography and etching, with a final undercutting etch to separate the cantilever from the substrate. The technology has great promise since cantilevers can be produced in large numbers on a single substrate—much as large numbers of transistors are currently contained on a single silicon chip.The resonator shown in figure 12 is around 120 μm in length. Experimental complete filters with an operating frequency of 30 GHz have been produced using cantilever varactors as the resonator elements. The size of this filter is around 4×3.5 mm. Cantilever resonators are typically applied at frequencies below 200 MHz, but other structures, such as micro-machined cavities, can be used in the microwave bands. Extremely high Q resonators can be made with this technology; flexural mode resonators with a Q in excess of 80,000 at 8 MHz are reported.
The precision applications in which mechanical filters are used require that the resonators are accurately adjusted to the specified resonance frequency.  This is known as trimming and usually involves a mechanical machining process.  In most filter designs, this can be difficult to do once the resonators have been assembled into the complete filter so the resonators are trimmed before assembly.  Trimming is done in at least two stages; coarse and fine, with each stage bringing the resonance frequency closer to the specified value.  Most trimming methods involve removing material from the resonator which will increase the resonance frequency.  The target frequency for a coarse trimming stage consequently needs to be set below the final frequency since the tolerances of the process could otherwise result in a frequency higher than the following fine trimming stage could adjust for.The coarsest method of trimming is grinding of the main resonating surface of the resonator; this process has an accuracy of around ±800 ppm.  Better control can be achieved by grinding the edge of the resonator instead of the main surface.  This has a less dramatic effect and consequently better accuracy.  Processes that can be used for fine trimming, in order of increasing accuracy, are sandblasting, drilling, and laser ablation.  Laser trimming is capable of achieving an accuracy of ±40 ppm.Trimming by hand, rather than machine, was used on some early production components but would now normally only be encountered during product development.  Methods available include sanding and filing.  It is also possible to add material to the resonator by hand, thus reducing the resonance frequency.  One such method is to add solder, but this is not suitable for production use since the solder will tend to reduce the high Q of the resonator.In the case of MEMS filters, it is not possible to trim the resonators outside of the filter because of the integrated nature of the device construction.  However, trimming is still a requirement in many MEMS applications.  Laser ablation can be used for this but material deposition methods are available as well as material removal.  These methods include laser or ion-beam induced deposition.
Blanchard, J. "The history of electrical resonance", Bell System Technical Journal, vol.23, pp. 415–433, 1944.
Bureau of Naval Personnel, Basic Electronics: Rate Training Manual, New York: Courier Dover Publications, 1973 ISBN 0-486-21076-6.
Darlington, S. "A history of network synthesis and filter theory for circuits composed of resistors, inductors, and capacitors", IEEE Transactions: Circuits and Systems, vol 31, pp. 3–13, 1984 doi:10.1109/TCS.1984.1085415.
Gatti, Paolo L.; Ferrari, Vittorio, Applied Structural and Mechanical Vibrations: Theory, Methods, and Measuring Instrumentation, London: Taylor & Francis, 1999 ISBN 0-419-22710-5.
George, R. W. "Mechanically resonant filter devices", U.S. Patent 2,762,985, filed 20th Sep 1952, issued 11th Sep 1956.
Harrison, Henry C., "Acoustic device", U.S. Patent 1,730,425, filed 11 October 1927 (and in Germany 21 October 1923), issued 8 October 1929.
Harrison, H. C. "Electromagnetic system", U.S. Patent 1,773,082, filed 6 December 1923, issued 12 August 1930.
Hunt, Frederick V. Electroacoustics: the Analysis of Transduction, and its Historical Background, Cambridge: Harvard University Press, 1954 OCLC 2042530.
Johnson, R.A. "Electrical circuit models of disk-wire mechanical filters", IEEE Transactions: Sonics and Ultrasonics, vol.15, issue 1, January 1968, pp. 41–50, ISSN 0018-9537.
Kasai, Yoshihiko; Hayashi, Tsunenori, "Automatic frequency adjusting method for mechanical resonators", U.S. Patent 4,395,849, filed 22 October 1980, issued 2 August 1983.
Levy, R. Cohn, S.B., "A History of microwave filter research, design, and development", IEEE Transactions: Microwave Theory and Techniques, pp. 1055–1067, vol.32, issue 9, 1984.
Lin, Liwei; Howe, Roger T.; Pisano, Albert P. "Microelectromechanical filters for signal processing", Journal of Microelectromechanical Systems,vol.7, No.3, 1998, p. 286
Mason, Warren P. "Electromechanical wave filter", U.S. Patent 2,981,905, filed 20 August 1958, issued 25 April 1961.
Matthaei, George L.; Young, Leo; Jones, E. M. T. Microwave Filters, Impedance-Matching Networks, and Coupling Structures, New York: McGraw-Hill 1964 OCLC 282667.
Norton, Edward L. "Sound reproducer", U.S. Patent 1,792,655, filed 31 May 1929, issued 17 February 1931.
Pierce, Allan D. Acoustics: an Introduction to its Physical Principles and Applications, New York: Acoustical Society of America 1989 ISBN 0-88318-612-8.
Rosen, Carol Zwick; Hiremath, Basavaraj V.; Newnham, Robert Everest (eds) Piezoelectricity, New York: American Institute of Physics, 1992 ISBN 0-88318-647-0.
de los Santos, Héctor J. RF MEMS Circuit Design for Wireless Communications, Boston: Artech House, 2002 ISBN 1-58053-329-9.
Talbot-Smith, Michael Audio Engineer's Reference Book, Oxford: Focal Press, 2001 ISBN 0-240-51685-0.
Taylor, John T.; Huang, Qiuting CRC Handbook of Electrical Filters, Boca Raton: CRC Press, 1997 ISBN 0-8493-8951-8.
Johnson, R. A.; Börner, M.; Konno, M., "Mechanical Filters-A Review of Progress", IEEE Transactions on Sonics and Ultrasonics, vol. 18, iss. 3, pp. 155-170, July 1971.
Medieval cuisine includes foods, eating habits, and cooking methods of various European cultures during the Middle Ages, which lasted from the fifth to the fifteenth century. During this period, diets and cooking changed less than they did in the early modern period that followed, when those changes helped lay the foundations for modern European cuisine.
Cereals remained the most important staple during the early Middle Ages as rice was introduced late, and the potato was only introduced in 1536, with a much later date for widespread consumption. Barley, oat and rye were eaten by the poor. Wheat was for the governing classes. These were consumed as bread, porridge, gruel and pasta by all of society's members. Fava beans and vegetables were important supplements to the cereal-based diet of the lower orders. (Phaseolus beans, today the "common bean", were of New World origin and were introduced after the Columbian exchange in the 16th century.)
Meat was more expensive and therefore more prestigious. Game, a form of meat acquired from hunting, was common only on the nobility's tables. The most prevalent butcher's meats were pork, chicken and other domestic fowl; beef, which required greater investment in land, was less common. Cod and herring were mainstays among the northern populations; dried, smoked or salted, they made their way far inland, but a wide variety of other saltwater and freshwater fish was also eaten.Slow transportation and food preservation techniques (based on drying, salting, smoking and pickling) made long-distance trade of many foods very expensive. Because of this, the nobility's food was more prone to foreign influence than the cuisine of the poor; it was dependent on exotic spices and expensive imports. As each level of society imitated the one above it, innovations from international trade and foreign wars from the 12th century onward gradually disseminated through the upper middle class of medieval cities. Aside from economic unavailability of luxuries such as spices, decrees outlawed consumption of certain foods among certain social classes and sumptuary laws limited conspicuous consumption among the nouveaux riches. Social norms also dictated that the food of the working class be less refined, since it was believed there was a natural resemblance between one's labour and one's food; manual labour required coarser, cheaper food.
A type of refined cooking developed in the late Middle Ages that set the standard among the nobility all over Europe. Common seasonings in the highly spiced sweet-sour repertory typical of upper-class medieval food included verjuice, wine and vinegar in combination with spices such as black pepper, saffron and ginger. These, along with the widespread use of sugar or honey, gave many dishes a sweet-sour flavor. Almonds were very popular as a thickener in soups, stews, and sauces, particularly as almond milk.
The cuisines of the cultures of the Mediterranean Basin since antiquity had been based on cereals, particularly various types of wheat. Porridge, gruel and later, bread, became the basic food staple that made up the majority of calorie intake for most of the population. From the 8th to the 11th centuries, the proportion of various cereals in the diet rose from about a third to three quarters. Dependence on wheat remained significant throughout the medieval era, and spread northward with the rise of Christianity. In colder climates, however, it was usually unaffordable for the majority population, and was associated with the higher classes. The centrality of bread in religious rituals such as the Eucharist meant that it enjoyed an especially high prestige among foodstuffs. Only (olive) oil and wine had a comparable value, but both remained quite exclusive outside the warmer grape- and olive-growing regions. The symbolic role of bread as both sustenance and substance is illustrated in a sermon given by Saint Augustine:
This bread retells your history … You were brought to the threshing floor of the Lord and were threshed … While awaiting catechism, you were like grain kept in the granary … At the baptismal font you were kneaded into a single dough. In the oven of the Holy Ghost you were baked into God's true bread.
The Roman Catholic, Eastern Orthodox Churches and their calendars had great influence on eating habits; consumption of meat was forbidden for a full third of the year for most Christians. All animal products, including eggs and dairy products (but not fish), were generally prohibited during Lent and fast. Additionally, it was customary for all citizens to fast prior to taking the Eucharist. These fasts were occasionally for a full day and required total abstinence.
Both the Eastern and the Western churches ordained that feast should alternate with fast. In most of Europe, Fridays were fast days, and fasting was observed on various other days and periods, including Lent and Advent. Meat, and animal products such as milk, cheese, butter and eggs, were not allowed, only fish. The fast was intended to mortify the body and invigorate the soul, and also to remind the faster of Christ's sacrifice for humanity. The intention was not to portray certain foods as unclean, but rather to teach a spiritual lesson in self-restraint through abstention. During particularly severe fast days, the number of daily meals was also reduced to one. Even if most people respected these restrictions and usually made penance when they violated them, there were also numerous ways of circumventing them, a conflict of ideals and practice summarized by writer Bridget Ann Henisch:
It is the nature of man to build the most complicated cage of rules and regulations in which to trap himself, and then, with equal ingenuity and zest, to bend his brain to the problem of wriggling triumphantly out again. Lent was a challenge; the game was to ferret out the loopholes.
While animal products were to be avoided during times of penance, pragmatic compromises often prevailed. The definition of "fish" was often extended to marine and semi-aquatic animals such as whales, barnacle geese, puffins and even beavers. The choice of ingredients may have been limited, but that did not mean that meals were smaller. Neither were there any restrictions against (moderate) drinking or eating sweets. Banquets held on fish days could be splendid, and were popular occasions for serving illusion food that imitated meat, cheese and eggs in various ingenious ways; fish could be moulded to look like venison and fake eggs could be made by stuffing empty egg shells with fish roe and almond milk and cooking them in coals. While Byzantine church officials took a hard-line approach, and discouraged any culinary refinement for the clergy, their Western counterparts were far more lenient. There was also no lack of grumbling about the rigours of fasting among the laity. During Lent, kings and schoolboys, commoners and nobility, all complained about being deprived of meat for the long, hard weeks of solemn contemplation of their sins. At Lent, owners of livestock were even warned to keep an eye out for hungry dogs frustrated by a "hard siege by Lent and fish bones".The trend from the 13th century onward was toward a more legalistic interpretation of fasting. Nobles were careful not to eat meat on fast days, but still dined in style; fish replaced meat, often as imitation hams and bacon; almond milk replaced animal milk as an expensive non-dairy alternative; faux eggs made from almond milk were cooked in blown-out eggshells, flavoured and coloured with exclusive spices. In some cases the lavishness of noble tables was outdone by Benedictine monasteries, which served as many as sixteen courses during certain feast days. Exceptions from fasting were frequently made for very broadly defined groups. Thomas Aquinas (c. 1225–1274) believed dispensation should be provided for children, the old, pilgrims, workers and beggars, but not the poor as long as they had some sort of shelter. There are many accounts of members of monastic orders who flouted fasting restrictions through clever interpretations of the Bible. Since the sick were exempt from fasting, there often evolved the notion that fasting restrictions only applied to the main dining area, and many Benedictine friars would simply eat their fast day meals in what was called the misericord (at those times) rather than the refectory. Newly assigned Catholic monastery officials sought to amend the problem of fast evasion not merely with moral condemnations, but by making sure that well-prepared non-meat dishes were available on fast days.
Medieval society was highly stratified. In a time when famine was commonplace and social hierarchies were often brutally enforced, food was an important marker of social status in a way that has no equivalent today in most developed countries. According to the ideological norm, society consisted of the three estates of the realm: commoners, that is, the working classes—by far the largest group; the clergy, and the nobility. The relationship between the classes was strictly hierarchical, with the nobility and clergy claiming worldly and spiritual overlordship over commoners. Within the nobility and clergy there were also a number of ranks ranging from kings and popes to dukes, bishops and their subordinates, such as priests. One was expected to remain in one's social class and to respect the authority of the ruling classes. Political power was displayed not just by rule, but also by displaying wealth. Nobles dined on fresh game seasoned with exotic spices, and displayed refined table manners; rough laborers could make do with coarse barley bread, salt pork and beans and were not expected to display etiquette. Even dietary recommendations were different: the diet of the upper classes was considered to be as much a requirement of their refined physical constitution as a sign of economic reality. The digestive system of a lord was held to be more discriminating than that of his rustic subordinates and demanded finer foods.In the late Middle Ages, the increasing wealth of middle class merchants and traders meant that commoners began emulating the aristocracy, and threatened to break down some of the symbolic barriers between the nobility and the lower classes. The response came in two forms: didactic literature warning of the dangers of adapting a diet inappropriate for one's class, and sumptuary laws that put a cap on the lavishness of commoners' banquets.
Medical science of the Middle Ages had a considerable influence on what was considered healthy and nutritious among the upper classes. One's lifestyle—including diet, exercise, appropriate social behavior, and approved medical remedies—was the way to good health, and all types of food were assigned certain properties that affected a person's health. All foodstuffs were also classified on scales ranging from hot to cold and moist to dry, according to the four bodily humours theory proposed by Galen that dominated Western medical science from late Antiquity until the 17th century.
Medieval scholars considered human digestion to be a process similar to cooking. The processing of food in the stomach was seen as a continuation of the preparation initiated by the cook. In order for the food to be properly "cooked" and for the nutrients to be properly absorbed, it was important that the stomach be filled in an appropriate manner. Easily digestible foods would be consumed first, followed by gradually heavier dishes. If this regimen were not respected it was believed that heavy foods would sink to the bottom of the stomach, thus blocking the digestion duct, so that food would digest very slowly and cause putrefaction of the body and draw bad humours into the stomach. It was also of vital importance that food of differing properties not be mixed.Before a meal, the stomach would preferably be "opened" with an apéritif (from Latin aperire, "to open") that was preferably of a hot and dry nature: confections made from sugar- or honey-coated spices like ginger, caraway and seeds of anise, fennel or cumin, wine and sweetened fortified milk drinks. As the stomach had been opened, it should then be "closed" at the end of the meal with the help of a digestive, most commonly a dragée, which during the Middle Ages consisted of lumps of spiced sugar, or hypocras, a wine flavoured with fragrant spices, along with aged cheese. A meal would ideally begin with easily digestible fruit, such as apples. It would then be followed by vegetables such as lettuce, cabbage, purslane, herbs, moist fruits, light meats, such as chicken or goat kid, with potages and broths. After that came the "heavy" meats, such as pork and beef, as well as vegetables and nuts, including pears and chestnuts, both considered difficult to digest. It was popular, and recommended by medical expertise, to finish the meal with aged cheese and various digestives.The most ideal food was that which most closely matched the humour of human beings, i.e. moderately warm and moist. Food should preferably also be finely chopped, ground, pounded and strained to achieve a true mixture of all the ingredients. White wine was believed to be cooler than red and the same distinction was applied to red and white vinegar. Milk was moderately warm and moist, but the milk of different animals was often believed to differ. Egg yolks were considered to be warm and moist while the whites were cold and moist. Skilled cooks were expected to conform to the regimen of humoral medicine. Even if this limited the combinations of food they could prepare, there was still ample room for artistic variation by the chef.
The caloric content and structure of medieval diet varied over time, from region to region, and between classes. However, for most people, the diet tended to be high-carbohydrate, with most of the budget spent on, and the majority of calories provided by, cereals and alcohol (such as beer). Even though meat was highly valued by all, lower classes often could not afford it, nor were they allowed by the church to consume it every day. In England in the 13th century, meat contributed a negligible portion of calories to a typical harvest worker's diet; however, its share increased after the Black Death and, by the 15th century, it provided about 20% of the total. Even among the lay nobility of medieval England, grain provided 65–70% of calories in the early 14th century, though a generous provision of meat and fish was included, and their consumption of meat increased in the aftermath of the Black Death as well. In one early 15th-century English aristocratic household for which detailed records are available (that of the Earl of Warwick), gentle members of the household received a staggering 3.8 pounds (1.7 kg) of assorted meats in a typical meat meal in the autumn and 2.4 pounds (1.1 kg) in the winter, in addition to 0.9 pounds (0.41 kg) of bread and 1⁄4 imperial gallon (1.1 L; 0.30 US gal) of beer or possibly wine (and there would have been two meat meals per day, five days a week, except during Lent). In the household of Henry Stafford in 1469, gentle members received 2.1 pounds (0.95 kg) of meat per meal, and all others received 1.04 pounds (0.47 kg), and everyone was given 0.4 pounds (0.18 kg) of bread and 1⁄4 imperial gallon (1.1 L; 0.30 US gal) of alcohol. On top of these quantities, some members of these households (usually, a minority) ate breakfast, which would not include any meat, but would probably include another 1⁄4 imperial gallon (1.1 L; 0.30 US gal) of beer; and uncertain quantities of bread and ale could have been consumed in between meals. The diet of the lord of the household differed somewhat from this structure, including less red meat, more high-quality wild game, fresh fish, fruit, and wine.In monasteries, the basic structure of the diet was laid down by the Rule of Saint Benedict in the 7th century and tightened by Pope Benedict XII in 1336, but (as mentioned above) monks were adept at "working around" these rules. Wine was restricted to about 10 imperial fluid ounces (280 mL; 9.6 US fl oz) per day, but there was no corresponding limit on beer, and, at Westminster Abbey, each monk was given an allowance of 1 imperial gallon (4.5 L; 1.2 US gal) of beer per day. Meat of "four-footed animals" was prohibited altogether, year-round, for everyone but the very weak and the sick. This was circumvented in part by declaring that offal, and various processed foods such as bacon, were not meat. Secondly, Benedictine monasteries contained a room called the misericord, where the Rule of Saint Benedict did not apply, and where a large number of monks ate. Each monk would be regularly sent either to the misericord or to the refectory. When Pope Benedict XII ruled that at least half of all monks should be required to eat in the refectory on any given day, monks responded by excluding the sick and those invited to the abbot's table from the reckoning. Overall, a monk at Westminster Abbey in the late 15th century would have been allowed 2.25 pounds (1.02 kg) of bread per day; 5 eggs per day, except on Fridays and in Lent; 2 pounds (0.91 kg) of meat per day, 4 days/week (excluding Wednesday, Friday, and Saturday), except in Advent and Lent; and 2 pounds (0.91 kg) of fish per day, 3 days/week and every day during Advent and Lent. This caloric structure partly reflected the high-class status of late Medieval monasteries in England, and partly that of Westminster Abbey, which was one of the richest monasteries in the country; diets of monks in other monasteries may have been more modest.
The overall caloric intake is subject to some debate. One typical estimate is that an adult peasant male needed 2,900 calories (12,000 kJ) per day, and an adult female needed 2,150 calories (9,000 kJ). Both lower and higher estimates have been proposed. Those engaged in particularly heavy physical labor, as well as sailors and soldiers, may have consumed 3,500 calories (15,000 kJ) or more per day. Intakes of aristocrats may have reached 4,000 to 5,000 calories (17,000 to 21,000 kJ) per day. Monks consumed 6,000 calories (25,000 kJ) per day on "normal" days, and 4,500 calories (19,000 kJ) per day when fasting. As a consequence of these excesses, obesity was common among upper classes. Monks especially frequently suffered from obesity-related (in some cases) conditions such as arthritis.
The regional specialties that are a feature of early modern and contemporary cuisine were not in evidence in the sparser documentation that survives. Instead, medieval cuisine can be differentiated by the cereals and the oils that shaped dietary norms and crossed ethnic and, later, national boundaries. Geographical variation in eating was primarily the result of differences in climate, political administration, and local customs that varied across the continent. Though sweeping generalizations should be avoided, more or less distinct areas where certain foodstuffs dominated can be discerned. In the British Isles, northern France, the Low Countries, the northern German-speaking areas, Scandinavia and the Baltic, the climate was generally too harsh for the cultivation of grapes and olives. In the south, wine was the common drink for both rich and poor alike (though the commoner usually had to settle for cheap second pressing wine) while beer was the commoner's drink in the north and wine an expensive import. Citrus fruits (though not the kinds most common today) and pomegranates were common around the Mediterranean. Dried figs and dates were available in the north, but were used rather sparingly in cooking.Olive oil was a ubiquitous ingredient in Mediterranean cultures, but remained an expensive import in the north where oils of poppy, walnut, hazel and filbert were the most affordable alternatives. Butter and lard, especially after the terrible mortality during the Black Death made them less scarce, were used in considerable quantities in the northern and northwestern regions, especially in the Low Countries. Almost universal in middle and upper class cooking all over Europe was the almond, which was in the ubiquitous and highly versatile almond milk, which was used as a substitute in dishes that otherwise required eggs or milk, though the bitter variety of almonds came along much later.
In Europe there were typically two meals a day: dinner at mid-day and a lighter supper in the evening. The two-meal system remained consistent throughout the late Middle Ages. Smaller intermediate meals were common, but became a matter of social status, as those who did not have to perform manual labor could go without them. Moralists frowned on breaking the overnight fast too early, and members of the church and cultivated gentry avoided it. For practical reasons, breakfast was still eaten by working men, and was tolerated for young children, women, the elderly and the sick. Because the church preached against gluttony and other weaknesses of the flesh, men tended to be ashamed of the weak practicality of breakfast. Lavish dinner banquets and late-night reresopers (from Occitan rèire-sopar, "late supper") with considerable amounts of alcoholic beverage were considered immoral. The latter were especially associated with gambling, crude language, drunkenness, and lewd behavior. Minor meals and snacks were common (although also disliked by the church), and working men commonly received an allowance from their employers in order to buy nuncheons, small morsels to be eaten during breaks.
As with almost every part of life at the time, a medieval meal was generally a communal affair. The entire household, including servants, would ideally dine together. To sneak off to enjoy private company was considered a haughty and inefficient egotism in a world where people depended very much on each other. In the 13th century, English bishop Robert Grosseteste advised the Countess of Lincoln: "forbid dinners and suppers out of hall, in secret and in private rooms, for from this arises waste and no honour to the lord and lady." He also recommended watching that the servants not make off with leftovers to make merry at rere-suppers, rather than giving it as alms. Towards the end of the Middle Ages, the wealthy increasingly sought to escape this regime of stern collectivism. When possible, rich hosts retired with their consorts to private chambers where the meal could be enjoyed in greater exclusivity and privacy. Being invited to a lord's chambers was a great privilege and could be used as a way to reward friends and allies and to awe subordinates. It allowed lords to distance themselves further from the household and to enjoy more luxurious treats while serving inferior food to the rest of the household that still dined in the great hall. At major occasions and banquets, however, the host and hostess generally dined in the great hall with the other diners. Although there are descriptions of dining etiquette on special occasions, less is known about the details of day-to-day meals of the elite or about the table manners of the common people and the destitute. However, it can be assumed there were no such extravagant luxuries as multiple courses, luxurious spices or hand-washing in scented water in everyday meals.
Things were different for the wealthy. Before the meal and between courses, shallow basins and linen towels were offered to guests so they could wash their hands, as cleanliness was emphasized. Social codes made it difficult for women to uphold the ideal of immaculate neatness and delicacy while enjoying a meal, so the wife of the host often dined in private with her entourage or ate very little at such feasts. She could then join dinner only after the potentially messy business of eating was done. Overall, fine dining was a predominantly male affair, and it was uncommon for anyone but the most honored of guests to bring his wife or her ladies-in-waiting. The hierarchical nature of society was reinforced by etiquette where the lower ranked were expected to help the higher, the younger to assist the elder, and men to spare women the risk of sullying dress and reputation by having to handle food in an unwomanly fashion. Shared drinking cups were common even at lavish banquets for all but those who sat at the high table, as was the standard etiquette of breaking bread and carving meat for one's fellow diners.Food was mostly served on plates or in stew pots, and diners would take their share from the dishes and place it on trenchers of stale bread, wood or pewter with the help of spoons or bare hands. In lower-class households it was common to eat food straight off the table. Knives were used at the table, but most people were expected to bring their own, and only highly favored guests would be given a personal knife. A knife was usually shared with at least one other dinner guest, unless one was of very high rank or well-acquainted with the host. Forks for eating were not in widespread usage in Europe until the early modern period, and early on were limited to Italy. Even there it was not until the 14th century that the fork became common among Italians of all social classes. The change in attitudes can be illustrated by the reactions to the table manners of the Byzantine princess Theodora Doukaina in the late 11th century. She was the wife of Domenico Selvo, the Doge of Venice, and caused considerable dismay among upstanding Venetians. The foreign consort's insistence on having her food cut up by her eunuch servants and then eating the pieces with a golden fork shocked and upset the diners so much that there was a claim that Peter Damian, Cardinal Bishop of Ostia, later interpreted her refined foreign manners as pride and referred to her as "...the Venetian Doge's wife, whose body, after her excessive delicacy, entirely rotted away." However, this is ambiguous since Peter Damian died in 1072 or 1073, and their marriage (Theodora and Domenico) took place in 1075.
All types of cooking involved the direct use of fire. Kitchen stoves did not appear until the 18th century, and cooks had to know how to cook directly over an open fire. Ovens were used, but they were expensive to construct and only existed in fairly large households and bakeries. It was common for a community to have shared ownership of an oven to ensure that the bread baking essential to everyone was made communal rather than private. There were also portable ovens designed to be filled with food and then buried in hot coals, and even larger ones on wheels that were used to sell pies in the streets of medieval towns. But for most people, almost all cooking was done in simple stewpots, since this was the most efficient use of firewood and did not waste precious cooking juices, making potages and stews the most common dishes. Overall, most evidence suggests that medieval dishes had a fairly high fat content, or at least when fat could be afforded. This was considered less of a problem in a time of back-breaking toil, famine, and a greater acceptance—even desirability—of plumpness; only the poor or sick, and devout ascetics, were thin.Fruit was readily combined with meat, fish and eggs. The recipe for Tart de brymlent, a fish pie from the recipe collection Forme of Cury, includes a mix of figs, raisins, apples and pears with fish (salmon, codling or haddock) and pitted damson plums under the top crust. It was considered important to make sure that the dish agreed with contemporary standards of medicine and dietetics. This meant that food had to be "tempered" according to its nature by an appropriate combination of preparation and mixing certain ingredients, condiments and spices; fish was seen as being cold and moist, and best cooked in a way that heated and dried it, such as frying or oven baking, and seasoned with hot and dry spices; beef was dry and hot and should therefore be boiled; pork was hot and moist and should therefore always be roasted. In some recipe collections, alternative ingredients were assigned with more consideration to the humoral nature than what a modern cook would consider to be similarity in taste. In a recipe for quince pie, cabbage is said to work equally well, and in another turnips could be replaced by pears.The completely edible shortcrust pie did not appear in recipes until the 15th century. Before that the pastry was primarily used as a cooking container in a technique known as huff paste. Extant recipe collections show that gastronomy in the Late Middle Ages developed significantly. New techniques, like the shortcrust pie and the clarification of jelly with egg whites began to appear in recipes in the late 14th century and recipes began to include detailed instructions instead of being mere memory aids to an already skilled cook.
In most households, cooking was done on an open hearth in the middle of the main living area, to make efficient use of the heat. This was the most common arrangement, even in wealthy households, for most of the Middle Ages, where the kitchen was combined with the dining hall. Towards the Late Middle Ages a separate kitchen area began to evolve. The first step was to move the fireplaces towards the walls of the main hall, and later to build a separate building or wing that contained a dedicated kitchen area, often separated from the main building by a covered arcade. This way, the smoke, odors and bustle of the kitchen could be kept out of sight of guests, and the fire risk lessened. Few medieval kitchens survive as they were "notoriously ephemeral structures".Many basic variations of cooking utensils available today, such as frying pans, pots, kettles, and waffle irons, already existed, although they were often too expensive for poorer households. Other tools more specific to cooking over an open fire were spits of various sizes, and material for skewering anything from delicate quails to whole oxen. There were also cranes with adjustable hooks so that pots and cauldrons could easily be swung away from the fire to keep them from burning or boiling over. Utensils were often held directly over the fire or placed into embers on tripods. To assist the cook there were also assorted knives, stirring spoons, ladles and graters. In wealthy households one of the most common tools was the mortar and sieve cloth, since many medieval recipes called for food to be finely chopped, mashed, strained and seasoned either before or after cooking. This was based on a belief among physicians that the finer the consistency of food, the more effectively the body would absorb the nourishment. It also gave skilled cooks the opportunity to elaborately shape the results. Fine-textured food was also associated with wealth; for example, finely milled flour was expensive, while the bread of commoners was typically brown and coarse. A typical procedure was farcing (from the Latin farcio, "to cram"), to skin and dress an animal, grind up the meat and mix it with spices and other ingredients and then return it into its own skin, or mold it into the shape of a completely different animal.
The kitchen staff of huge noble or royal courts occasionally numbered in the hundreds: pantlers, bakers, waferers, sauciers, larderers, butchers, carvers, page boys, milkmaids, butlers and numerous scullions. While an average peasant household often made do with firewood collected from the surrounding woodlands, the major kitchens of households had to cope with the logistics of daily providing at least two meals for several hundred people. Guidelines on how to prepare for a two-day banquet can be found in the cookbook Du fait de cuisine ("On cookery") written in 1420 in part to compete with the court of Burgundy by Maistre Chiquart, master chef of Amadeus VIII, Duke of Savoy. Chiquart recommends that the chief cook should have at hand at least 1,000 cartloads of "good, dry firewood" and a large barnful of coal.
Food preservation methods were basically the same as had been used since antiquity, and did not change much until the invention of canning in the early 19th century. The most common and simplest method was to expose foodstuffs to heat or wind to remove moisture, thereby prolonging the durability if not the flavor of almost any type of food from cereals to meats; the drying of food worked by drastically reducing the activity of various water-dependent microorganisms that cause decay. In warm climates this was mostly achieved by leaving food out in the sun, and in the cooler northern climates by exposure to strong winds (especially common for the preparation of stockfish), or in warm ovens, cellars, attics, and at times even in living quarters. Subjecting food to a number of chemical processes such as smoking, salting, brining, conserving or fermenting also made it keep longer. Most of these methods had the advantage of shorter preparation times and of introducing new flavors. Smoking or salting meat of livestock butchered in autumn was a common household strategy to avoid having to feed more animals than necessary during the lean winter months. Butter tended to be heavily salted (5–10%) in order not to spoil. Vegetables, eggs or fish were also often pickled in tightly packed jars, containing brine and acidic liquids (lemon juice, verjuice or vinegar). Another method was to seal the food by cooking it in sugar or honey or fat, in which it was then stored. Microbial modification was also encouraged, however, by a number of methods; grains, fruit and grapes were turned into alcoholic drinks thus killing any pathogens, and milk was fermented and curdled into a multitude of cheeses or buttermilk.
The majority of the European population before industrialization lived in rural communities or isolated farms and households. The norm was self-sufficiency with only a small percentage of production being exported or sold in markets. Large towns were exceptions and required their surrounding hinterlands to support them with food and fuel. The dense urban population could support a wide variety of food establishments that catered to various social groups. Many of the poor city dwellers had to live in cramped conditions without access to a kitchen or even a hearth, and many did not own the equipment for basic cooking. Food from vendors was in such cases the only option. Cookshops could either sell ready-made hot food, an early form of fast food, or offer cooking services while the customers supplied some or all of the ingredients. Travellers, such as pilgrims en route to a holy site, made use of professional cooks to avoid having to carry their provisions with them. For the more affluent, there were many types of specialist that could supply various foods and condiments: cheesemongers, pie bakers, saucers, waferers, etc. Well-off citizens who had the means to cook at home could on special occasions hire professionals when their own kitchen or staff could not handle the burden of throwing a major banquet.Urban cookshops that catered to workers or the destitute were regarded as unsavory and disreputable places by the well-to-do and professional cooks tended to have a bad reputation. Geoffrey Chaucer's Hodge of Ware, the London cook from the Canterbury Tales, is described as a sleazy purveyor of unpalatable food. French cardinal Jacques de Vitry's sermons from the early 13th century describe sellers of cooked meat as an outright health hazard. While the necessity of the cook's services was occasionally recognized and appreciated, they were often disparaged since they catered to the baser of bodily human needs rather than spiritual betterment. The stereotypical cook in art and literature was male, hot-tempered, prone to drunkenness, and often depicted guarding his stewpot from being pilfered by both humans and animals. In the early 15th century, the English monk John Lydgate articulated the beliefs of many of his contemporaries by proclaiming that "Hoot ffir [fire] and smoke makith many an angry cook."
The period between c. 500 and 1300 saw a major change in diet that affected most of Europe. More intense agriculture on an ever-increasing acreage resulted in a shift from animal products, like meat and dairy, to various grains and vegetables as the staple of the majority population. Before the 14th century bread was not as common among the lower classes, especially in the north where wheat was more difficult to grow. A bread-based diet became gradually more common during the 15th century and replaced warm intermediate meals that were porridge- or gruel-based. Leavened bread was more common in wheat-growing regions in the south, while unleavened flatbread of barley, rye or oats remained more common in northern and highland regions, and unleavened flatbread was also common as provisions for troops.The most common grains were rye, barley, buckwheat, millet and oats. Rice remained a fairly expensive import for most of the Middle Ages and was grown in northern Italy only towards the end of the period. Wheat was common all over Europe and was considered to be the most nutritious of all grains, but was more prestigious and thus more expensive. The finely sifted white flour that modern Europeans are most familiar with was reserved for the bread of the upper classes. As one descended the social ladder, bread became coarser, darker, and its bran content increased. In times of grain shortages or outright famine, grains could be supplemented with cheaper and less desirable substitutes like chestnuts, dried legumes, acorns, ferns, and a wide variety of more or less nutritious vegetable matter.One of the most common constituents of a medieval meal, either as part of a banquet or as a small snack, were sops, pieces of bread with which a liquid like wine, soup, broth, or sauce could be soaked up and eaten. Another common sight at the medieval dinner table was the frumenty, a thick wheat porridge often boiled in a meat broth and seasoned with spices. Porridges were also made of every type of grain and could be served as desserts or dishes for the sick, if boiled in milk (or almond milk) and sweetened with sugar. Pies filled with meats, eggs, vegetables, or fruit were common throughout Europe, as were turnovers, fritters, doughnuts, and many similar pastries. By the Late Middle Ages biscuits (cookies in the U.S.) and especially wafers, eaten for dessert, had become high-prestige foods and came in many varieties. Grain, either as bread crumbs or flour, was also the most common thickener of soups and stews, alone or in combination with almond milk.
The importance of bread as a daily staple meant that bakers played a crucial role in any medieval community. Bread consumption was high in most of Western Europe by the 14th century. Estimates of bread consumption from different regions are fairly similar: around 1 to 1.5 kilograms (2.2 to 3.3 lb) of bread per person per day. Among the first town guilds to be organized were the bakers', and laws and regulations were passed to keep bread prices stable. The English Assize of Bread and Ale of 1266 listed extensive tables where the size, weight, and price of a loaf of bread were regulated in relation to grain prices. The baker's profit margin stipulated in the tables was later increased through successful lobbying from the London Baker's Company by adding the cost of everything from firewood and salt to the baker's wife, house, and dog. Since bread was such a central part of the medieval diet, swindling by those who were trusted with supplying the precious commodity to the community was considered a serious offense. Bakers who were caught tampering with weights or adulterating dough with less expensive ingredients could receive severe penalties. This gave rise to the "baker's dozen": a baker would give 13 for the price of 12, to be certain of not being known as a cheat.
While grains were the primary constituent of most meals, vegetables such as cabbage, chard, onions, garlic and carrots were common foodstuffs. Many of these were eaten daily by peasants and workers and were less prestigious than meat. The cookbooks, which appeared in the late Middle Ages and were intended mostly for those who could afford such luxuries, contained only a small number of recipes using vegetables as the main ingredient. The lack of recipes for many basic vegetable dishes, such as potages, has been interpreted not to mean that they were absent from the meals of the nobility, but rather that they were considered so basic that they did not require recording. Carrots were available in many variants during the Middle Ages: among them a tastier reddish-purple variety and a less prestigious green-yellow type. Various legumes, like chickpeas, fava beans and field peas were also common and important sources of protein, especially among the lower classes. With the exception of peas, legumes were often viewed with some suspicion by the dietitians advising the upper class, partly because of their tendency to cause flatulence but also because they were associated with the coarse food of peasants. The importance of vegetables to the common people is illustrated by accounts from 16th-century Germany stating that many peasants ate sauerkraut from three to four times a day.Fruit was popular and could be served fresh, dried, or preserved, and was a common ingredient in many cooked dishes. Since sugar and honey were both expensive, it was common to include many types of fruit in dishes that called for sweeteners of some sort. The fruits of choice in the south were lemons, citrons, bitter oranges (the sweet type was not introduced until several hundred years later), pomegranates, quinces, and grapes. Farther north, apples, pears, plums, and wild strawberries were more common. Figs and dates were eaten all over Europe, but remained rather expensive imports in the north.Common and often basic ingredients in many modern European cuisines like potatoes, kidney beans, cacao, vanilla, tomatoes, chili peppers and maize were not available to Europeans until after 1492, after European contact with the Americas, and even then it often took considerable time, sometimes several centuries, for the new foodstuffs to be accepted by society at large.
Milk was an important source of animal protein for those who could not afford meat. It would mostly come from cows, but milk from goats and sheep was also common. Plain fresh milk was not consumed by adults except the poor or sick, and was usually reserved for the very young or elderly. Poor adults would sometimes drink buttermilk or whey or milk that was soured or watered down. Fresh milk was overall less common than other dairy products because of the lack of technology to keep it from spoiling. On occasion it was used in upper-class kitchens in stews, but it was difficult to keep fresh in bulk and almond milk was generally used in its stead.Cheese was far more important as a foodstuff, especially for common people, and it has been suggested that it was, during many periods, the chief supplier of animal protein among the lower classes. Many varieties of cheese eaten today, like Dutch Edam, Northern French Brie and Italian Parmesan, were available and well known in late medieval times. There were also whey cheeses, like ricotta, made from by-products of the production of harder cheeses. Cheese was used in cooking for pies and soups, the latter being common fare in German-speaking areas. Butter, another important dairy product, was in popular use in the regions of Northern Europe that specialized in cattle production in the latter half of the Middle Ages, the Low Countries and Southern Scandinavia. While most other regions used oil or lard as cooking fats, butter was the dominant cooking medium in these areas. Its production also allowed for a lucrative butter export from the 12th century onward.
While all forms of wild game were popular among those who could obtain it, most meat came from domestic animals. Domestic working animals that were no longer able to work were slaughtered but not particularly appetizing and therefore were less valued as meat. Beef was not as common as today because raising cattle was labor-intensive, requiring pastures and feed, and oxen and cows were much more valuable as draught animals and for producing milk. Mutton and lamb were fairly common, especially in areas with a sizeable wool industry, as was veal. Far more common was pork, as domestic pigs required less attention and cheaper feed. Domestic pigs often ran freely even in towns and could be fed on just about any organic waste, and suckling pig was a sought-after delicacy. Just about every part of the pig was eaten, including ears, snout, tail, tongue, and womb. Intestines, bladder and stomach could be used as casings for sausage or even illusion food such as giant eggs. Among the meats that today are rare or even considered inappropriate for human consumption are the hedgehog and porcupine, occasionally mentioned in late medieval recipe collections. Rabbits remained a rare and highly prized commodity. In England, they were deliberately introduced by the 13th century and their colonies were carefully protected. Further south, domesticated rabbits were commonly raised and bred both for their meat and fur. They were of particular value for monasteries, because newborn rabbits were allegedly declared fish (or, at least, not-meat) by the church and therefore they could be eaten during Lent.A wide range of birds were eaten, including swans, peafowl, quail, partridge, storks, cranes, larks, linnets and other songbirds that could be trapped in nets, and just about any other wild bird that could be hunted. Swans and peafowl were domesticated to some extent, but were only eaten by the social elite, and more praised for their fine appearance as stunning entertainment dishes, entremets, than for their meat. As today, geese and ducks had been domesticated but were not as popular as the chicken, the fowl equivalent of the pig. Curiously enough the barnacle goose was believed to reproduce not by laying eggs like other birds, but by growing in barnacles, and was hence considered acceptable food for fast and Lent. But at the Fourth Council of the Lateran (1215), Pope Innocent III explicitly prohibited the eating of barnacle geese during Lent, arguing that they lived and fed like ducks and so were of the same nature as other birds.Meats were more expensive than plant foods. Though rich in protein, the calorie-to-weight ratio of meat was less than that of plant food. Meat could be up to four times as expensive as bread. Fish was up to 16 times as costly, and was expensive even for coastal populations. This meant that fasts could mean an especially meager diet for those who could not afford alternatives to meat and animal products like milk and eggs. It was only after the Black Death had eradicated up to half of the European population that meat became more common even for poorer people. The drastic reduction in many populated areas resulted in a labor shortage, meaning that wages dramatically increased. It also left vast areas of farmland untended, making them available for pasture and putting more meat on the market.
Although less prestigious than other animal meats, and often seen as merely an alternative to meat on fast days, seafood was the mainstay of many coastal populations. "Fish" to the medieval person was also a general name for anything not considered a proper land-living animal, including marine mammals such as whales and porpoises. Also included were the beaver, due to its scaly tail and considerable time spent in water, and barnacle geese, due to the belief that they developed underwater in the form of barnacles. Such foods were also considered appropriate for fast days, though rather contrived classification of barnacle geese as fish was not universally accepted. The Holy Roman Emperor Frederick II examined barnacles and noted no evidence of any bird-like embryo in them, and the secretary of Leo of Rozmital wrote a very skeptical account of his reaction to being served barnacle goose at a fish-day dinner in 1456.Especially important was the fishing and trade in herring and cod in the Atlantic and the Baltic Sea. The herring was of unprecedented significance to the economy of much of Northern Europe, and it was one of the most common commodities traded by the Hanseatic League, a powerful north German alliance of trading guilds. Kippers made from herring caught in the North Sea could be found in markets as far away as Constantinople. While large quantities of fish were eaten fresh, a large proportion was salted, dried, and, to a lesser extent, smoked. Stockfish, cod that was split down the middle, fixed to a pole and dried, was very common, though preparation could be time-consuming, and meant beating the dried fish with a mallet before soaking it in water. A wide range of mollusks including oysters, mussels and scallops were eaten by coastal and river-dwelling populations, and freshwater crayfish were seen as a desirable alternative to meat during fish days. Compared to meat, fish was much more expensive for inland populations, especially in Central Europe, and therefore not an option for most. Freshwater fish such as pike, carp, bream, perch, lamprey and trout were common.
While in modern times, water is often drunk with a meal, in the Middle Ages, however, concerns over purity, medical recommendations and its low prestige value made it less favored, and alcoholic beverages were preferred. They were seen as more nutritious and beneficial to digestion than water, with the invaluable bonus of being less prone to putrefaction due to the alcohol content. Wine was consumed on a daily basis in most of France and all over the Western Mediterranean wherever grapes were cultivated. Further north it remained the preferred drink of the bourgeoisie and the nobility who could afford it, and far less common among peasants and workers. The drink of commoners in the northern parts of the continent was primarily beer or ale.Juices, as well as wines, of a multitude of fruits and berries had been known at least since Roman antiquity and were still consumed in the Middle Ages: pomegranate, mulberry and blackberry wines, perry, and cider which was especially popular in the north where both apples and pears were plentiful. Medieval drinks that have survived to this day include prunellé from wild plums (modern-day slivovitz), mulberry gin and blackberry wine. Many variants of mead have been found in medieval recipes, with or without alcoholic content. However, the honey-based drink became less common as a table beverage towards the end of the period and was eventually relegated to medicinal use. Mead has often been presented as the common drink of the Slavs. This is partially true since mead bore great symbolic value at important occasions. When agreeing on treaties and other important affairs of state, mead was often presented as a ceremonial gift. It was also common at weddings and baptismal parties, though in limited quantity due to its high price. In medieval Poland, mead had a status equivalent to that of imported luxuries, such as spices and wines. Kumis, the fermented milk of mares or camels, was known in Europe, but as with mead was mostly something prescribed by physicians.Plain milk was not consumed by adults except the poor or sick, being reserved for the very young or elderly, and then usually as buttermilk or whey. Fresh milk was overall less common than other dairy products because of the lack of technology to keep it from spoiling. Tea and coffee, both made from plants found in the Old World, were popular in East Asia and the Muslim world during the Middle Ages. However, neither of these non-alcoholic social drinks were consumed in Europe before the late 16th and early 17th centuries.
Wine was commonly drunk and was also regarded as the most prestigious and healthy choice. According to Galen's dietetics it was considered hot and dry but these qualities were moderated when wine was watered down. Unlike water or beer, which were considered cold and moist, consumption of wine in moderation (especially red wine) was, among other things, believed to aid digestion, generate good blood and brighten the mood. The quality of wine differed considerably according to vintage, the type of grape and more importantly, the number of grape pressings. The first pressing was made into the finest and most expensive wines which were reserved for the upper classes. The second and third pressings were subsequently of lower quality and alcohol content. Common folk usually had to settle for a cheap white or rosé from a second or even third pressing, meaning that it could be consumed in quite generous amounts without leading to heavy intoxication. For the poorest (or the most pious), watered-down vinegar (similar to Ancient Roman posca) would often be the only available choice.The aging of high quality red wine required specialized knowledge as well as expensive storage and equipment, and resulted in an even more expensive end product. Judging from the advice given in many medieval documents on how to salvage wine that bore signs of going bad, preservation must have been a widespread problem. Even if vinegar was a common ingredient, there was only so much of it that could be used. In the 14th-century cookbook Le Viandier there are several methods for salvaging spoiling wine; making sure that the wine barrels are always topped up or adding a mixture of dried and boiled white grape seeds with the ash of dried and burnt lees of white wine were both effective bactericides, even if the chemical processes were not understood at the time. Spiced or mulled wine was not only popular among the affluent, but was also considered especially healthy by physicians. Wine was believed to act as a kind of vaporizer and conduit of other foodstuffs to every part of the body, and the addition of fragrant and exotic spices would make it even more wholesome. Spiced wines were usually made by mixing an ordinary (red) wine with an assortment of spices such as ginger, cardamom, pepper, grains of paradise, nutmeg, cloves and sugar. These would be contained in small bags which were either steeped in wine or had liquid poured over them to produce hypocras and claré. By the 14th century, bagged spice mixes could be bought ready-made from spice merchants.
While wine was the most common table beverage in much of Europe, this was not the case in the northern regions where grapes were not cultivated. Those who could afford it drank imported wine, but even for nobility in these areas it was common to drink beer or ale, particularly towards the end of the Middle Ages. In England, the Low Countries, northern Germany, Poland and Scandinavia, beer was consumed on a daily basis by people of all social classes and age groups. By the mid-15th century, barley, a cereal known to be somewhat poorly suited for breadmaking but excellent for brewing, accounted for 27% of all cereal acreage in England. However, the heavy influence from Arab and Mediterranean culture on medical science (particularly due to the Reconquista and the influx of Arabic texts) meant that beer was often heavily disfavoured. For most medieval Europeans, it was a humble brew compared with common southern drinks and cooking ingredients, such as wine, lemons and olive oil. Even comparatively exotic products like camel's milk and gazelle meat generally received more positive attention in medical texts. Beer was just an acceptable alternative and was assigned various negative qualities. In 1256, the Sienese physician Aldobrandino described beer in the following way:
But from whichever it is made, whether from oats, barley or wheat, it harms the head and the stomach, it causes bad breath and ruins the teeth, it fills the stomach with bad fumes, and as a result anyone who drinks it along with wine becomes drunk quickly; but it does have the property of facilitating urination and makes one's flesh white and smooth.
The intoxicating effect of beer was believed to last longer than that of wine, but it was also admitted that it did not create the "false thirst" associated with wine. Though less prominent than in the north, beer was consumed in northern France and the Italian mainland. Perhaps as a consequence of the Norman conquest and the travelling of nobles between France and England, one French variant described in the 14th-century cookbook Le Menagier de Paris was called godale (most likely a direct borrowing from the English "good ale") and was made from barley and spelt, but without hops. In England there were also the variants poset ale, made from hot milk and cold ale, and brakot or braggot, a spiced ale prepared much like hypocras.That hops could be used for flavoring beer had been known at least since Carolingian times, but was adopted gradually due to difficulties in establishing the appropriate proportions. Before the widespread use of hops, gruit, a mix of various herbs, had been used. Gruit had the same preserving properties as hops, though less reliable depending on what herbs were in it, and the end result was much more variable. Another flavoring method was to increase the alcohol content, but this was more expensive and lent the beer the undesired characteristic of being a quick and heavy intoxicant. Hops may have been widely used in England in the tenth century; they were grown in Austria by 1208 and in Finland by 1249, and possibly much earlier.Before hops became popular as an ingredient, it was difficult to preserve this beverage for any time, and so, it was mostly consumed fresh. It was unfiltered, and therefore cloudy, and likely had a lower alcohol content than the typical modern equivalent. Quantities of beer consumed by medieval residents of Europe, as recorded in contemporary literature, far exceed intakes in the modern world. For example, sailors in 16th-century England and Denmark received a ration of 1 imperial gallon (4.5 L; 1.2 US gal) of beer per day. Polish peasants consumed up to 3 litres (0.66 imp gal; 0.79 US gal) of beer per day.In the Early Middle Ages beer was primarily brewed in monasteries, and on a smaller scale in individual households. By the High Middle Ages breweries in the fledgling medieval towns of northern Germany began to take over production. Though most of the breweries were small family businesses that employed at most eight to ten people, regular production allowed for investment in better equipment and increased experimentation with new recipes and brewing techniques. These operations later spread to the Netherlands in the 14th century, then to Flanders and Brabant, and reached England by the 15th century. Hopped beer became very popular in the last decades of the Late Middle Ages. In England and the Low Countries, the per capita annual consumption was around 275 to 300 litres (60 to 66 imp gal; 73 to 79 US gal), and it was consumed with practically every meal: low alcohol-content beers for breakfast, and stronger ones later in the day. When perfected as an ingredient, hops could make beer keep for six months or more, and facilitated extensive exports. In Late Medieval England, the word beer came to mean a hopped beverage, whereas ale had to be unhopped. In turn, ale or beer was classified into "strong" and "small", the latter less intoxicating, regarded as a drink of temperate people, and suitable for consumption by children. As late as 1693, John Locke stated that the only drink he considered suitable for children of all ages was small beer, while criticizing the apparently common practice among Englishmen of the time to give their children wine and strong alcohol.By modern standards, the brewing process was relatively inefficient, but capable of producing quite strong alcohol when that was desired. One recent attempt to recreate medieval English "strong ale" using recipes and techniques of the era (albeit with the use of modern yeast strains) yielded a strongly alcoholic brew with original gravity of 1.091 (corresponding to a potential alcohol content over 9%) and "pleasant, apple-like taste".
The ancient Greeks and Romans knew of the technique of distillation, but it was not practiced on a major scale in Europe until some time around the 12th century, when Arabic innovations in the field combined with water-cooled glass alembics were introduced. Distillation was believed by medieval scholars to produce the essence of the liquid being purified, and the term aqua vitae ("water of life") was used as a generic term for all kinds of distillates. The early use of various distillates, alcoholic or not, was varied, but it was primarily culinary or medicinal; grape syrup mixed with sugar and spices was prescribed for a variety of ailments, and rose water was used as a perfume and cooking ingredient and for hand washing. Alcoholic distillates were also occasionally used to create dazzling, fire-breathing entremets (a type of entertainment dish after a course) by soaking a piece of cotton in spirits. It would then be placed in the mouth of the stuffed, cooked and occasionally redressed animals, and lit just before presenting the creation.Aqua vitae in its alcoholic forms was highly praised by medieval physicians. In 1309 Arnaldus of Villanova wrote that "[i]t prolongs good health, dissipates superfluous humours, reanimates the heart and maintains youth." In the Late Middle Ages, the production of moonshine started to pick up, especially in the German-speaking regions. By the 13th century, Hausbrand (literally "home-burnt" from gebrannter wein, brandwein; "burnt [distilled] wine") was commonplace, marking the origin of brandy. Towards the end of the Late Middle Ages, the consumption of spirits became so ingrained even among the general population that restrictions on sales and production began to appear in the late 15th century. In 1496 the city of Nuremberg issued restrictions on the selling of aquavit on Sundays and official holidays.
Spices were among the most luxurious products available in the Middle Ages, the most common being black pepper, cinnamon (and the cheaper alternative cassia), cumin, nutmeg, ginger and cloves. They all had to be imported from plantations in Asia and Africa, which made them extremely expensive, and gave them social cachet such that pepper for example was hoarded, traded and conspicuously donated in the manner of gold bullion. It has been estimated that around 1,000 tons of pepper and 1,000 tons of the other common spices were imported into Western Europe each year during the late Middle Ages. The value of these goods was the equivalent of a yearly supply of grain for 1.5 million people. While pepper was the most common spice, the most exclusive (though not the most obscure in its origin) was saffron, used as much for its vivid yellow-red color as for its flavor, for according to the humours, yellow signified hot and dry, valued qualities; turmeric provided a yellow substitute, and touches of gilding at banquets supplied both the medieval love of ostentatious show and Galenic dietary lore: at the sumptuous banquet that Cardinal Riario offered the daughter of the King of Naples in June 1473, the bread was gilded. Among the spices that have now fallen into obscurity are grains of paradise, a relative of cardamom which almost entirely replaced pepper in late medieval north French cooking, long pepper, mace, spikenard, galangal and cubeb. Sugar, unlike today, was considered to be a type of spice due to its high cost and humoral qualities. Few dishes employed just one type of spice or herb, but rather a combination of several different ones. Even when a dish was dominated by a single flavor it was usually combined with another to produce a compound taste, for example parsley and cloves or pepper and ginger.Common herbs such as sage, mustard, and parsley were grown and used in cooking all over Europe, as were caraway, mint, dill and fennel. Many of these plants grew throughout all of Europe or were cultivated in gardens, and were a cheaper alternative to exotic spices. Mustard was particularly popular with meat products and was described by Hildegard of Bingen (1098–1179) as poor man's food. While locally grown herbs were less prestigious than spices, they were still used in upper-class food, but were then usually less prominent or included merely as coloring. Anise was used to flavor fish and chicken dishes, and its seeds were served as sugar-coated comfits.
Surviving medieval recipes frequently call for flavoring with a number of sour, tart liquids. Wine, verjuice (the juice of unripe grapes or fruits) vinegar and the juices of various fruits, especially those with tart flavors, were almost universal and a hallmark of late medieval cooking. In combination with sweeteners and spices, it produced a distinctive "pungeant, fruity" flavor. Equally common, and used to complement the tanginess of these ingredients, were (sweet) almonds. They were used in a variety of ways: whole, shelled or unshelled, slivered, ground and, most importantly, processed into almond milk. This last type of non-dairy milk product is probably the single most common ingredient in late medieval cooking and blended the aroma of spices and sour liquids with a mild taste and creamy texture.Salt was ubiquitous and indispensable in medieval cooking. Salting and drying was the most common form of food preservation and meant that fish and meat in particular were often heavily salted. Many medieval recipes specifically warn against oversalting and there were recommendations for soaking certain products in water to get rid of excess salt. Salt was present during more elaborate or expensive meals. The richer the host, and the more prestigious the guest, the more elaborate would be the container in which it was served and the higher the quality and price of the salt. Wealthy guests were seated "above the salt", while others sat "below the salt", where salt cellars were made of pewter, precious metals or other fine materials, often intricately decorated. The rank of a diner also decided how finely ground and white the salt was. Salt for cooking, preservation or for use by common people was coarser; sea salt, or "bay salt", in particular, had more impurities, and was described in colors ranging from black to green. Expensive salt, on the other hand, looked like the standard commercial salt common today.
The term "dessert" comes from the Old French desservir, "to clear a table", literally "to un-serve", and originated during the Middle Ages. It would typically consist of dragées and mulled wine accompanied by aged cheese, and by the Late Middle Ages could also include fresh fruit covered in sugar, honey or syrup and boiled-down fruit pastes. Sugar, from its first appearance in Europe, was viewed as much as a drug as a sweetener; its long-lived medieval reputation as an exotic luxury encouraged its appearance in elite contexts accompanying meats and other dishes that to modern taste are more naturally savoury. There was a wide variety of fritters, crêpes with sugar, sweet custards and darioles, almond milk and eggs in a pastry shell that could also include fruit and sometimes even bone marrow or fish. German-speaking areas had a particular fondness for krapfen: fried pastries and dough with various sweet and savory fillings. Marzipan in many forms was well known in Italy and southern France by the 1340s and is assumed to be of Arab origin. Anglo-Norman cookbooks are full of recipes for sweet and savory custards, potages, sauces and tarts with strawberries, cherries, apples and plums. The English chefs also had a penchant for using flower petals such as roses, violets, and elder flowers. An early form of quiche can be found in Forme of Cury, a 14th-century recipe collection, as a Torte de Bry with a cheese and egg yolk filling. Le Ménagier de Paris ("Parisian Household Book") written in 1393 includes a quiche recipe made with three kinds of cheese, eggs, beet greens, spinach, fennel fronds, and parsley.
In northern France, a wide assortment of waffles and wafers was eaten with cheese and hypocras or a sweet malmsey as issue de table ("departure from the table"). The ever-present candied ginger, coriander, aniseed and other spices were referred to as épices de chambre ("parlor spices") and were taken as digestibles at the end of a meal to "close" the stomach. Like their Muslim counterparts in Spain, the Arab conquerors of Sicily introduced a wide variety of new sweets and desserts that eventually found their way to the rest of Europe. Just like Montpellier, Sicily was once famous for its comfits, nougat candy (torrone, or turrón in Spanish) and almond clusters (confetti). From the south, the Arabs also brought the art of ice cream making that produced sorbet and several examples of sweet cakes and pastries; cassata alla Siciliana (from Arabic qas'ah, the term for the terracotta bowl with which it was shaped), made from marzipan, sponge cake and sweetened ricotta and cannoli alla Siciliana, originally cappelli di turchi ("Turkish hats"), fried, chilled pastry tubes with a sweet cheese filling.
Research into medieval foodways was, until around 1980, a much neglected field of study. Misconceptions and outright errors were common among historians, and are still present in as a part of the popular view of the Middle Ages as a backward, primitive and barbaric era. Medieval cookery was described as revolting due to the often unfamiliar combination of flavors, the perceived lack of vegetables and a liberal use of spices. The heavy use of spices has been popular as an argument to support the claim that spices were employed to disguise the flavor of spoiled meat, a conclusion without support in historical fact and contemporary sources. Fresh meat could be procured throughout the year by those who could afford it. The preservation techniques available at the time, although crude by today's standards, were perfectly adequate. The astronomical cost and high prestige of spices, and thereby the reputation of the host, would have been effectively undone if wasted on cheap and poorly handled foods.The common method of grinding and mashing ingredients into pastes and the many potages and sauces has been used as an argument that most adults within the medieval nobility lost their teeth at an early age, and hence were forced to eat nothing but porridge, soup and ground-up meat. The image of nobles gumming their way through multi-course meals of nothing but mush has lived side by side with the contradictory apparition of the "mob of uncouth louts (disguised as noble lords) who, when not actually hurling huge joints of greasy meat at one another across the banquet hall, are engaged in tearing at them with a perfectly healthy complement of incisors, canines, bicuspids and molars".The numerous descriptions of banquets from the later Middle Ages concentrated on the pageantry of the event rather than the minutiae of the food, which was not the same for most banqueters as those choice mets served at the high table. Banquet dishes were apart from mainstream of cuisine, and have been described as "the outcome of grand banquets serving political ambition rather than gastronomy; today as yesterday" by historian Maguelonne Toussant-Samat.
Cookbooks, or more specifically, recipe collections, compiled in the Middle Ages are among the most important historical sources for medieval cuisine. The first cookbooks began to appear towards the end of the 13th century. The Liber de Coquina, perhaps originating near Naples, and the Tractatus de modo preparandi have found a modern editor in Marianne Mulon, and a cookbook from Assisi found at Châlons-sur-Marne has been edited by Maguelonne Toussaint-Samat. Though it is assumed that they describe real dishes, food scholars do not believe they were used as cookbooks might be today, as a step-by-step guide through the cooking procedure that could be kept at hand while preparing a dish. Few in a kitchen, at those times, would have been able to read, and working texts have a low survival rate.The recipes were often brief and did not give precise quantities. Cooking times and temperatures were seldom specified since accurate portable clocks were not available and since all cooking was done with fire. At best, cooking times could be specified as the time it took to say a certain number of prayers or how long it took to walk around a certain field. Professional cooks were taught their trade through apprenticeship and practical training, working their way up in the highly defined kitchen hierarchy. A medieval cook employed in a large household would most likely have been able to plan and produce a meal without the help of recipes or written instruction. Due to the generally good condition of surviving manuscripts it has been proposed by food historian Terence Scully that they were records of household practices intended for the wealthy and literate master of a household, such as Le Ménagier de Paris from the late 14th century. Over 70 collections of medieval recipes survive today, written in several major European languages.The repertory of housekeeping instructions laid down by manuscripts like the Ménagier de Paris also include many details of overseeing correct preparations in the kitchen. Towards the onset of the early modern period, in 1474, the Vatican librarian Bartolomeo Platina wrote De honesta voluptate et valetudine ("On honourable pleasure and health") and the physician Iodocus Willich edited Apicius in Zurich in 1563.
High-status exotic spices and rarities like ginger, pepper, cloves, sesame, citron leaves and "onions of Escalon" all appear in an eighth-century list of spices that the Carolingian cook should have at hand. It was written by Vinidarius, whose excerpts of Apicius survive in an eighth-century uncial manuscript. Vinidarius' own dates may not be much earlier.
Adamson, Melitta Weiss (editor), Food in the Middle Ages: A Book of Essays. Garland, New York. 1995. ISBN 0-85976-145-2
Adamson, Melitta Weiss (editor), Regional Cuisines of Medieval Europe: A Book of Essays. Routledge, New York. 2002. ISBN 0-415-92994-6
Adamson, Melitta Weiss, Food in Medieval Times. Greenwood Press, Westport, CT. 2004. ISBN 0-313-32147-7
Bynum, Caroline, Holy Feast and Holy Fast: The Religious Significance of Food to Medieval Women. University of California Press, Berkeley. 1987. ISBN 0-520-05722-8
Carlin, Martha & Rosenthal, Joel T. (editors), Food and Eating in Medieval Europe. The Hambledon Press, London. 1998. ISBN 1-85285-148-1
Carnevale Schianca, Enrico, La cucina medievale. Lessico, storia, preparazioni. Olschki, Firenze. 2011. ISBN 978-88-222-6073-4
Creighton, Oliver; Christie, Neil (2015), "The Archaeology of Wallingford Castle: a summary of the current state of knowledge",  in Keats-Rohan, K. S. B.; Christie, Neil; Roffe, David, Wallingford: The Castle and the Town in Context, BAR British Series, Oxford: Archaeopress, ISBN 978-1-4073-1418-1
Dembinska, Maria, Food and Drink in Medieval Poland: Rediscovering a Cuisine of the Past. translated by Magdalena Thomas, revised and adapted by William Woys Weaver. University of Pennsylvania Press, Philadelphia. 1999. ISBN 0-8122-3224-0
Dyer, Christopher, Everyday life in medieval England, Continuum International Publishing Group, 2000
Eßlinger, Hans Michael (editor), Handbook of Brewing: Processes, Technology, Markets. Wiley-VCH, Weinheim. 2009. ISBN 978-3-527-31674-8
Fenton, Alexander & Kisbán, Eszter (editors), Food in Change: Eating Habits from the Middle Ages to the Present Day. John Donald Publishers, Edinburgh. 1986. ISBN 0-85976-145-2
Freedman, Paul Out of the East: Spices and the Medieval Imagination. Yale University Press, New Haven. 2008. ISBN 978-0-300-11199-6
Hanson, Davd J. Preventing alcohol abuse: alcohol, culture, and control. Greenwood Publishing Group, Westport. 1995. ISBN 0-275-94926-5
Harvey, Barbara F., Living and dying in England, 1100–1540: the monastic experience, Oxford University Press, 1993
Henisch, Bridget Ann, Fast and Feast: Food in Medieval Society. The Pennsylvania State Press, University Park. 1976. ISBN 0-271-01230-7
Hunt, Edwin S. & Murray, James H., A history of business in Medieval Europe, 1200–1550. Cambridge University Press, Cambridge. 1999. ISBN 0-521-49923-2
Glick, Thomas, Livesey, Steven J. & Wallis, Faith (editors), Medieval Science, Technology, and Medicine: an Encyclopedia. Routledge, New York. 2005. ISBN 0-415-96930-1
(in French) Mulon, "Deux traités d'art culinaire médié", Bulletin philologique et historique. Comité des travaux historiques et scientifiques, Paris. 1958.
Scully, Terence, The Art of Cookery in the Middle Ages. The Boydell Press, Woodbridge. 1995. ISBN 0-85115-611-8
Toussant-Samat, Maguelonne, The History of Food. 2nd edition (translation: Anthea Bell) Wiley-Blackwell, Chichester. 2009. ISBN 978-1-4051-8119-8
Unger, Richard W., Beer in the Middle Ages and the Renaissance. University of Pennsylvania Press, Philadelphia. 2007. ISBN 978-0-8122-1999-9
Rambourg, Patrick, Histoire de la cuisine et de la gastronomie françaises, Paris, Ed. Perrin (coll. tempus n° 359), 2010, 381 pages. ISBN 978-2-262-03318-7
