
Maiden Castle is an Iron Age hill fort 1.6 miles (2.6 km) south west of Dorchester, in the English county of Dorset. Hill forts were fortified hill-top settlements constructed across Britain during the Iron Age.
The earliest archaeological evidence of human activity on the site consists of a Neolithic causewayed enclosure and bank barrow. In about 1800 BC, during the Bronze Age, the site was used for growing crops before being abandoned. Maiden Castle itself was built in about 600 BC; the early phase was a simple and unremarkable site, similar to many other hill forts in Britain and covering 6.4 hectares (16 acres). Around 450 BC it was greatly expanded and the enclosed area nearly tripled in size to 19 ha (47 acres), making it the largest hill fort in Britain and, by some definitions, the largest in Europe. At the same time, Maiden Castle's defences were made more complex with the addition of further ramparts and ditches. Around 100 BC, habitation at the hill fort went into decline and became concentrated at the eastern end of the site. It was occupied until at least the Roman period, by which time it was in the territory of the Durotriges, a Celtic tribe.
After the Roman conquest of Britain in the 1st century AD, Maiden Castle appears to have been abandoned, although the Romans may have had a military presence on the site. In the late 4th century AD, a temple and ancillary buildings were constructed. In the 6th century AD the hill top was entirely abandoned and was used only for agriculture during the medieval period.
Maiden Castle has provided inspiration for composer John Ireland and authors Thomas Hardy and John Cowper Powys. The study of hill forts was popularised in the 19th century by archaeologist Augustus Pitt Rivers. In the 1930s, archaeologist Mortimer Wheeler and Tessa Verney Wheeler undertook the first archaeological excavations at Maiden Castle, raising its profile among the public. Further excavations were carried out under Niall Sharples, which added to an understanding of the site and repaired damage caused in part by the large number of visitors. Today the site is protected as a Scheduled Ancient Monument and is maintained by English Heritage.
Before the hill fort was built, a Neolithic causewayed enclosure was constructed on the site. Dating from around 4000 BC, it was an oval area enclosed by two ditches, It is called a causewayed enclosure because the way the ditches were dug meant that there would originally have been gaps. These gaps, and the bank being only 17 centimetres (6.7 in) high, indicate the site would not have been defensive. Instead the ditches may have been symbolic, separating the interior of the enclosure and its activities from the outside. Archaeologist Niall Sharples, who was involved in excavating the hill fort in the 1980s, has identified the hilltop views of the surrounding landscape as a likely factor for the enclosure's position. Situated on the side of the hill, it would have been visible from several miles away, and when first cut the ditches would have exposed the underlying white chalk and stood out against the green hillside. The interior of the enclosure has been disturbed by later habitation and farming. The site does not appear to have been inhabited, although a grave containing the remains of two children, aged 6–7, has been discovered. The enclosure is the earliest evidence of human activity on the site.The purpose of Neolithic causewayed enclosures is unclear, and they probably had a variety of functions. In addition to the burials, which indicate the site at Maiden Castle was important for rituals related to death, pottery from the coast and areas to the east and west was found here, indicating that the site was a meeting place that attracted people over long distances. Radiocarbon dating indicates that the enclosure was abandoned around 3,400 BC. Arrowheads discovered in the ditches may indicate that activity at the enclosure met a violent end.Within a period of about 50 years, a bank barrow was built over the enclosure. It was a 546-metre (1,791 ft) long mound of earth with a ditch on either side; the parallel ditches were 19.5 m (64 ft) apart. Many barrows lie over graves and are monuments to the deceased, but as the barrow at Maiden Castle did not cover any burials, scholars have suggested that it was a boundary marker. This would explain the limited human activity on the hilltop for the 500 years after the bank barrow's construction. Around 1,800 BC, during the early Bronze Age, the hill was cleared and used to grow crops, but the soil was quickly exhausted and the site abandoned. This period of abandonment lasted until the Iron Age, when the hill fort was built. The bank barrow survived into the Iron Age as a low mound, and throughout this period construction over it was avoided.
Hill forts developed in the Late Bronze Age and Early Iron Age, roughly the start of the first millennium BC. The reason for their emergence in Britain, and their purpose, has been a subject of debate. It has been argued that they could have been defensive sites constructed in response to invasion from continental Europe, built by invaders, or a military reaction to social tensions caused by an increasing population and resulting pressure on agriculture. 
Since the 1960s, the dominant view has been that the increasing use of iron led to social changes in Britain. Deposits of iron ore were located in different places to the tin and copper ore necessary to make bronze. As a result, trading patterns shifted, and the old elites lost their economic and social status. Power passed into the hands of a new group of people.Archaeologist Barry Cunliffe believes that population increase still played a role and has stated that 
"[the forts] provided defensive possibilities for the community at those times when the stress [of an increasing population] burst out into open warfare. But I wouldn't see them as having been built because there was a state of war. They would be functional as defensive strongholds when there were tensions and undoubtedly some of them were attacked and destroyed, but this was not the only, or even the most significant, factor in their construction".
There are around 31 hill forts in Dorset; archaeologist Sharples, who undertook excavations at Maiden Castle, proposed that hill forts were used to control agricultural land to support a large community. Those in Dorset were situated near expanses of fertile land. Monumental defences such as the ditch at Maiden Castle indicate that the land was disputed and communities fought each other for control. This is supported by Cunliffe, who argues that the elaborate earthworks such as those around the entrances to Maiden Castle and Danebury were used to defend the weakest part of the hill fort. They increased the time the attackers took to reach the gateway, which would have left them vulnerable to defenders armed with slings. Hoards of carefully selected sling stones have been found at both sites.Constructed on a territorial boundary in about 600 BC, the first hill fort at Maiden Castle was a 6.4-hectare (16-acre) area surrounded by a single ditch. The hill it sits on is part of a ridge on the north side of the South Winterborne valley, which feeds the River Frome. At the eastern end of the ridge and rising 132 m (433 ft) above sea level, the site of the first hill fort was not the highest point along the ridge. The highest point is the neighbouring Hog Hill, which is only 1 m (3.3 ft) higher. The hill projects about 40 m (131 ft) above the surrounding countryside, which is about 90 m (295 ft) above sea level. The defences were 8.4 m (28 ft) high and consisted of the V-shaped ditch and a rampart. The rampart would probably have been timber-faced around just the entrances. Elaborate timber facing would have been used to impress visitors. The site could be accessed by an entrance in the northwest and a double entrance in the east. The double entrance is unique in hill forts in the British Isles. The reason for a double entrance is unclear; however, archaeologist Niall Sharples has suggested that it was a form of segregation. It is likely that several farming communities lived in the hill fort and wanted different entrances.The defences of the first hill fort were rebuilt on at least one occasion; the ditch was deepened by 1.5 to 7 m (4.9 to 23.0 ft). The spoil from re-digging the ditch was deposited on the back of the rampart. At the same time, the defences around the eastern entrances were made more complex. A bank and ditch were built outside the two entrances, and a bank was erected between them. The bank had a wall faced with limestone, which was brought from more than 3 km (2 mi) away. Sharples believes this would have created an impressive entrance and was a demonstration of the settlement's high status. The Early Iron Age archaeology has been largely destroyed due to later activity on the site. However, nearby Poundbury and Chalbury date to the same period, so through comparison it is possible to infer the Early Iron Age activity at Maiden Castle. From parallels at these sites, Sharples deduces that it was probably densely occupied, with separate areas for habitation and storage. Not much is known about the material culture and economy of the Early Iron Age, and the paucity of finds from this period at Maiden Castle makes it difficult to draw conclusions about activity on the site.
In the Early Iron Age, Maiden Castle was generally unexceptional; it was one of over 100 hill forts of similar size built around the same time in the area that is now Berkshire, Dorset, Hampshire, and Wiltshire. In the Middle Iron Age, Maiden Castle was expanded and in the process it became the largest hill fort in Britain and one of the largest in Europe. According to archaeologist Niall Sharples it is, by some definitions, the largest in western Europe. In about 450 BC, Maiden Castle was expanded from 6.4 to 19 ha (16 to 47 acres). The area was initially enclosed by a single bank and ditch, with the bank standing 2.7 m (8.9 ft) high although the ditch was shallow. The hill fort's expansion was not unique; it was one of a series of "developed hill forts" in southern England. As some hill forts were expanded, many of the smaller hill forts that had proliferated in the Early Iron Age fell out of use, as was the case in Dorset. The developed hill forts in Dorset were spaced widely apart. This, and the abandonment of the smaller hill forts in the area when the developed hill forts were built, indicates that these developed hill forts were important. The developed hill forts of Berkshire, Dorset, Hampshire, and Wiltshire were equally spaced apart, with roughly equal access to resources such as water.The emergence of developed hill forts has been attributed to Iron Age society becoming more complex. The emergence of one dominant hill fort in an area indicates that the inhabitants of a particular hill fort became more important than their contemporaries, possibly through warfare. However, a general dearth of evidence for destruction and an increase of artefacts associated with crafts and industry suggest that the reason for change was economic. Hill forts may have become important as centres of trade. This is supported by the possibility that the multiple rings of ditches often employed at developed hill forts (the technical term for which is "multivallate") were likely to be not just defensive; so many ditches and ramparts, such as those at Maiden Castle, were excessive for defence alone so were likely used as statements of power and authority. Developed hill forts were generally densely occupied; this is best demonstrated at Danebury, where 57% of the site has been excavated. While developed hill forts were of a higher status than their smaller predecessors, they were not all equal. Cunliffe states that the Maiden Castle's monumental defences probably indicate that it was of higher status than other developed hill forts.
Maiden Castle expanded westwards, and the ditch was extended to enclose the neighbouring Hog Hill. The peaks of the two hills encompassed by the new, larger hill fort were separated by a dry valley. A shaft dug into the valley was possibly used as a water source. Almost immediately after the single ditch enclosure was expanded to 19 ha (47 acres), work began on making the defences more elaborate. The existing rampart was heightened to 3.5 m (11 ft), and more ramparts and ditches were added. On the south of the fort, four ramparts and three ditches were added, but because of the steepness of the northern slope of the hill, the fourth rampart did not extend all the way round, and only three ramparts were built on the northern side. At the same time, the eastern entrance was again made more complex through the addition of further earthworks, lengthening the approach to the site.The four-post structures common in hill forts throughout England are also found in Maiden Castle. Their purpose on this site is uncertain however, since at 2 m (6.6 ft) square they have been considered by archaeologists to be too small for dwellings; as a result, it has been concluded that these structures were probably granaries. The presence of granaries suggests that the fort was used to control the area's food supply. Little evidence has been discovered for houses in Maiden Castle during the site's reconstruction in the 5th century BC; this is probably because the site has not been fully excavated and a quarry used to provide material for the rampart may have obliterated the evidence. It appears that houses were not built near the ramparts until after the defences were complete. Maiden Castle was occupied throughout the Iron Age and its inhabitants lived in roundhouses. The later houses appear to be organised in rows, and to be roughly similar in size, a reorganisation which indicates the increasing power of the elites over Iron Age society.Bronze objects such as pins, jewellery, and rivets have been found on the site, dating from the Middle Iron Age. As there was no local source of tin and copper ore, this demonstrates long distance trade, probably with the southwest. Although bronze was not produced at Maiden Castle, there is evidence of it being reworked. Good quality iron ore could be found in the surrounding area, but the hill fort does not appear to have been a centre for iron production in this period; this is not unusual as very few hill forts in Berkshire, Dorset, Hampshire, and Wiltshire exhibit traces of iron production. Early in the Iron Age, most of the pottery found at Maiden Castle was produced locally – within about 15 km (9.3 mi) – however later on sources further afield became more important, and by the Late Iron Age 95% of the pottery came from the area around Poole Harbour, more than 35 mi (56 km) away. This long-range trade has been taken as evidence for increasing relationships with groups of people over large areas and the emergence of tribal identities. Although Sharples states that developed hill forts such as Maiden Castle are not towns and cannot be considered truly urban because they are so closely related to agriculture and storage, Cunliffe and fellow-archaeologists Mark Corney and Andrew Payne describe developed hill forts as "town-like settlements", a form of proto-urbanism.
Across Britain, many hill forts fell out of use in the 100 years around the turn of the millennium. It has been suggested that this, and the contemporary change in material culture of the Britons (such as the introduction of coinage and cemeteries and an increase in craft industries), was caused by increased interaction with the Roman Empire. The developing industries may have resulted in a shift away from the hill fort elites, whose power was based on agriculture. Such change is not as obvious in Dorset as it is in the rest of Britain, but there is a trend for abandonment of hill forts in the area and a proliferation of small undefended farmsteads, indicating a migration of the population.Around 100 BC, Maiden Castle's organised street pattern was replaced by more random habitation. At the same time, the western half of the site was abandoned and occupation was concentrated in the east of the fort. Also during the Late Iron Age, some of the earthworks around the eastern gateway were filled in and settlement expanded beyond the entrance, and into the areas between the banks. Excavations by archaeologist Mortimer Wheeler in this area revealed several houses, storage pits, an area used for iron working, and a cemetery. On the industrial site, more than 62 kg (137 lb) of iron slag was discovered in an area of 30 m2 (320 sq ft), and it is believed the site produced around 200 kg (440 lb) of iron. The amount of ore required could not have been supplied by local sources, so most likely originated from areas of specialist iron production such as the Weald, south west England, and Wales. Maiden Castle is one of the most important iron production sites from the Late Iron Age in southern Britain.There is little evidence for burial in the Iron Age until late on in the period, and it is believed that the prevalent method of disposing of a body was by excarnation. Wheeler's excavations on the cemetery in the eastern gateway revealed 52 burials, but only part of the cemetery was investigated, so the total number of burials is likely to be at least double this figure. One area of the cemetery featured burials of 14 people who had died in violent circumstances, including one body with a Roman catapult bolt in its back. Wheeler used the "war cemetery", as he described it, as evidence of a Roman attack on Maiden Castle.
In AD 43, the Roman conquest of Britain began. Vespasian's subsequent campaign to conquer the tribes of the Atrebates, Dumnonii, and Durotriges in the southwest of Britain took place in AD 43–47. Based on the discovery of a group of bodies in the Late Iron Age formal cemetery that had met a violent death, archaeologist Mortimer Wheeler created a vivid story of the fall of Maiden Castle to Roman forces. He believed a legion wreaked destruction on the site, butchering men, women, and children, before setting fire to the site and slighting its defences. However, there is little archaeological evidence to support this version of events, or even that the hill fort was attacked by the Romans. Although there is a layer of charcoal, it is associated with the iron works, and the main evidence for slighting of defences comes from the collapse of an entranceway to the fort. Although 14 bodies in the cemetery exhibited signs of a violent death, there is no evidence that they died at Maiden Castle.The eastern part of the hill fort remained in use for at least the first few decades of the Roman occupation, although the duration and nature of habitation is uncertain. Many 1st-century Roman artefacts have been discovered near the east entrance and in the centre of the hill fort. It has been suggested that Maiden Castle was occupied as a Roman military outpost or fort and the settlement discontinued, as there is no known fort in the area and it was not uncommon for hill forts in the southwest to have been occupied by Roman forces. This was a characteristic of Vespasian's campaign in the region; there was military occupation at Cadbury Castle in Somerset, Hembury in Devon, and Hodd Hill in Dorset.Maiden Castle had been abandoned by the end of the 1st century, a time when Durnovaria (Dorchester) rose to prominence as the civitas, or regional capital, of the Durotriges, a Celtic tribe whose territory was in southwest England. However, in July 2015 archaeologists from Bournemouth University discovered the remains of the Iron Age settlement of Duropolis and believe that the abandonment of the fort may be connected with the new site. According to the ancient geographer Ptolemy, writing in the 2nd century AD, Dunium was the main settlement of the Durotriges. Although Dunium has long been thought to refer to Maiden Castle, Hod Hill and Hengistbury have been identified as two other possible sites for Dunium. Dunium may have derived from British duno- which meant "a fort". Sometime after 367, a Romano-Celtic temple was built at Maiden Castle in the eastern half of the hill fort. The date was deduced from a hoard of coins discovered beneath a mosaic floor in the temple. A central room, measuring 6 m (20 ft) square, was surrounded by a 3 m (9.8 ft) passageway, similar to many Romano-Celtic temples found in the south of England. Nearby were two other buildings: a rectangular building 7.9 m × 5.5 m (26 ft × 18 ft) with two rooms that may have been a house for a priest, and a circular building that may have been a shrine. At the same time as the temple was built, the fort's eastern gateway was refurbished; there was possibly another shrine inside the gateway.
The 4th-century temple gradually fell into disuse and Maiden Castle was used predominantly as pasture. There is evidence for activity on the site in the form of a few post-Roman or Anglo Saxon burials, some possibly Christian, but the hill fort was not reused as a settlement. In the 16th and 17th centuries, a barn was built over the "war cemetery". The only other significant activity on the hill top after the Romans was a short period of cultivation in the 17th century, as demonstrated by traces of ridge and furrow caused by ploughing.The modern name for the hill fort is first recorded in 1607 as Mayden Castell; it is not unique to the site and occurs in several other places in Britain and is widely taken to mean a "fortification that looks impregnable" or one that has never been taken in battle. Alternatively, the name may derive from the Brittonic mai-dun, meaning a "great hill". A more recent explanation has been advanced by Richard Coates suggesting that the name is only of medieval origin, and was applied simultaneously to the considerable number of identically named locations around the country.
Over the following centuries, the site was abandoned completely and became open pasture, although it was of interest to antiquarians. Thomas Hardy, who built his house within sight of it, described the castle in a short story, "Ancient Earthworks and What Two Enthusiastic Scientists Found Therein" (1885) about a local antiquarian who spent much time investigating the site. In 1921, composer John Ireland wrote Mai-Dun, a symphonic rhapsody, about the hill fort in Dorset. John Cowper Powys wrote a novel titled Maiden Castle in 1936, which was set in Dorset.
The first widespread investigation of hill forts was carried out in the second half of the 19th century under the direction of Augustus Pitt-Rivers, but it was not until the 1930s that Maiden Castle was methodically investigated, the first large-scale excavation of the interior of a hill fort. Between 1934 and 1937, Mortimer Wheeler and Tessa Verney Wheeler excavated both the interior and the defences, work that was funded almost entirely by donations from the public. Wheeler's use of the media to disseminate information about the site resulted in Maiden Castle becoming well known. It was one of about 80 hill forts to have been excavated by 1940, in a period known as "hill fort mania" during the 1920s and 1930s.Between 1985 and 1986 further excavations under Niall Sharples were prompted by the hill fort's deteriorating condition, partly caused by the large number of visitors to the site. Under the auspices of English Heritage, repair work and archaeological investigations were undertaken concurrently. Techniques such as radiocarbon dating were available to Sharples that were unavailable to Wheeler, allowing the site to be dated. The structure was made a Scheduled Ancient Monument in 1981, giving Maiden Castle protection against unauthorised change; it is now maintained by English Heritage. With parking facilities and information boards for visitors, Maiden Castle is open to the public all year round. Today, the site is in the civil parish of Winterborne Monkton at grid reference SY66938848.
in 1921, the English composer John Ireland (1879–1962) wrote the tone poem Mai-Dun, A Symphonic Rhapsody about the place, adopting Hardy's name for it. In 1931, Ireland arranged his piece for piano four hands.
Sharples, Nial (1991), Maiden Castle: Excavations and field survey 1985-6, English Heritage, ISBN 978-1-84802-167-9

In astronomy, the main sequence is a continuous and distinctive band of stars that appears on plots of stellar color versus brightness. These color-magnitude plots are known as Hertzsprung–Russell diagrams after their co-developers, Ejnar Hertzsprung and Henry Norris Russell. Stars on this band are known as main-sequence stars or dwarf stars. These are the most numerous true stars in the universe, and include the Earth's Sun.
After condensation and ignition of a star, it generates thermal energy in its dense core region through nuclear fusion of hydrogen into helium. During this stage of the star's lifetime, it is located on the main sequence at a position determined primarily by its mass, but also based upon its chemical composition and age. The cores of main-sequence stars are in hydrostatic equilibrium, where outward thermal pressure from the hot core is balanced by the inward pressure of gravitational collapse from the overlying layers. The strong dependence of the rate of energy generation on temperature and pressure helps to sustain this balance. Energy generated at the core makes its way to the surface and is radiated away at the photosphere. The energy is carried by either radiation or convection, with the latter occurring in regions with steeper temperature gradients, higher opacity or both.
The main sequence is sometimes divided into upper and lower parts, based on the dominant process that a star uses to generate energy. Stars below about 1.5 times the mass of the Sun (1.5 M☉) primarily fuse hydrogen atoms together in a series of stages to form helium, a sequence called the proton–proton chain. Above this mass, in the upper main sequence, the nuclear fusion process mainly uses atoms of carbon, nitrogen and oxygen as intermediaries in the CNO cycle that produces helium from hydrogen atoms. Main-sequence stars with more than two solar masses undergo convection in their core regions, which acts to stir up the newly created helium and maintain the proportion of fuel needed for fusion to occur. Below this mass, stars have cores that are entirely radiative with convective zones near the surface. With decreasing stellar mass, the proportion of the star forming a convective envelope steadily increases. Main-sequence stars below 0.4 M☉ undergo convection throughout their mass. When core convection does not occur, a helium-rich core develops surrounded by an outer layer of hydrogen.
In general, the more massive a star is, the shorter its lifespan on the main sequence. After the hydrogen fuel at the core has been consumed, the star evolves away from the main sequence on the HR diagram, into a supergiant, red giant, or directly to a white dwarf.
In the early part of the 20th century, information about the types and distances of stars became more readily available. The spectra of stars were shown to have distinctive features, which allowed them to be categorized. Annie Jump Cannon and Edward C. Pickering at Harvard College Observatory developed a method of categorization that became known as the Harvard Classification Scheme, published in the Harvard Annals in 1901.In Potsdam in 1906, the Danish astronomer Ejnar Hertzsprung noticed that the reddest stars—classified as K and M in the Harvard scheme—could be divided into two distinct groups. These stars are either much brighter than the Sun, or much fainter. To distinguish these groups, he called them "giant" and "dwarf" stars. The following year he began studying star clusters; large groupings of stars that are co-located at approximately the same distance. He published the first plots of color versus luminosity for these stars. These plots showed a prominent and continuous sequence of stars, which he named the Main Sequence.At Princeton University, Henry Norris Russell was following a similar course of research. He was studying the relationship between the spectral classification of stars and their actual brightness as corrected for distance—their absolute magnitude. For this purpose he used a set of stars that had reliable parallaxes and many of which had been categorized at Harvard. When he plotted the spectral types of these stars against their absolute magnitude, he found that dwarf stars followed a distinct relationship. This allowed the real brightness of a dwarf star to be predicted with reasonable accuracy.Of the red stars observed by Hertzsprung, the dwarf stars also followed the spectra-luminosity relationship discovered by Russell. However, the giant stars are much brighter than dwarfs and so do not follow the same relationship. Russell proposed that the "giant stars must have low density or great surface-brightness, and the reverse is true of dwarf stars". The same curve also showed that there were very few faint white stars.In 1933, Bengt Strömgren introduced the term Hertzsprung–Russell diagram to denote a luminosity-spectral class diagram. This name reflected the parallel development of this technique by both Hertzsprung and Russell earlier in the century.As evolutionary models of stars were developed during the 1930s, it was shown that, for stars of a uniform chemical composition, a relationship exists between a star's mass and its luminosity and radius. That is, for a given mass and composition, there is a unique solution for determining the star's radius and luminosity. This became known as the Vogt-Russell theorem; named after Heinrich Vogt and Henry Norris Russell. By this theorem, when a star's chemical composition and its position on the main sequence is known, so too is the star's mass and radius. (However, it was subsequently discovered that the theorem breaks down somewhat for stars of non-uniform composition.)A refined scheme for stellar classification was published in 1943 by William Wilson Morgan and Philip Childs Keenan. The MK classification assigned each star a spectral type—based on the Harvard classification—and a luminosity class.  The Harvard classification had been developed by assigning a different letter to each star based on the strength of the hydrogen spectral line, before the relationship between spectra and temperature was known.  When ordered by temperature and when duplicate classes were removed,  the spectral types of stars followed, in order of decreasing temperature with colors ranging from blue to red, the sequence O, B, A, F, G, K and M. (A popular mnemonic for memorizing this sequence of stellar classes is "Oh Be A Fine Girl/Guy, Kiss Me".) The luminosity class ranged from I to V, in order of decreasing luminosity. Stars of luminosity class V belonged to the main sequence.In April 2018, astronomers reported the detection of the most distant "ordinary" (i.e., main sequence) star, named Icarus (formally, MACS J1149 Lensed Star 1), at 9 billion light-years away from Earth.
When a protostar is formed from the collapse of a giant molecular cloud of gas and dust in the local interstellar medium, the initial composition is homogeneous throughout, consisting of about 70% hydrogen, 28% helium and trace amounts of other elements, by mass. The initial mass of the star depends on the local conditions within the cloud. (The mass distribution of newly formed stars is described empirically by the initial mass function.) During the initial collapse, this pre-main-sequence star generates energy through gravitational contraction. Upon reaching a suitable density, energy generation is begun at the core using an exothermic nuclear fusion process that converts hydrogen into helium.
When nuclear fusion of hydrogen becomes the dominant energy production process and the excess energy gained from gravitational contraction has been lost, the star lies along a curve on the Hertzsprung–Russell diagram (or HR diagram) called the standard main sequence. Astronomers will sometimes refer to this stage as "zero age main sequence", or ZAMS. The ZAMS curve can be calculated using computer models of stellar properties at the point when stars begin hydrogen fusion. From this point, the brightness and surface temperature of stars typically increase with age.A star remains near its initial position on the main sequence until a significant amount of hydrogen in the core has been consumed, then begins to evolve into a more luminous star. (On the HR diagram, the evolving star moves up and to the right of the main sequence.) Thus the main sequence represents the primary hydrogen-burning stage of a star's lifetime.
The majority of stars on a typical HR diagram lie along the main-sequence curve. This line is pronounced because both the spectral type and the luminosity depend only on a star's mass, at least to zeroth-order approximation, as long as it is fusing hydrogen at its core—and that is what almost all stars spend most of their "active" lives doing.The temperature of a star determines its spectral type via its effect on the physical properties of plasma in its photosphere. A star's energy emission as a function of wavelength is influenced by both its temperature and composition. A key indicator of this energy distribution is given by the color index, B − V, which measures the star's magnitude in blue (B) and green-yellow (V) light by means of filters. This difference in magnitude provides a measure of a star's temperature.
Main-sequence stars are called dwarf stars, but this terminology is partly historical and can be somewhat confusing. For the cooler stars, dwarfs such as red dwarfs, orange dwarfs, and yellow dwarfs are indeed much smaller and dimmer than other stars of those colors. However, for hotter blue and white stars, the size and brightness difference between so-called "dwarf" stars that are on the main sequence and the so-called "giant" stars that are not becomes smaller; for the hottest stars it is not directly observable. For those stars the terms "dwarf" and "giant" refer to differences in spectral lines which indicate if a star is on the main sequence or off it.  Nevertheless, very hot main-sequence stars are still sometimes called dwarfs, even though they have roughly the same size and brightness as the "giant" stars of that temperature.The common use of "dwarf" to mean main sequence is confusing in another way, because there are dwarf stars which are not main-sequence stars. For example, a white dwarf is the dead core of a star that is left after the star has shed its outer layers, that is much smaller than a main-sequence star, roughly the size of Earth. These represent the final evolutionary stage of many main-sequence stars.
By treating the star as an idealized energy radiator known as a black body, the luminosity L and radius R can be related to the effective temperature Teff by the Stefan–Boltzmann law:
  where σ is the Stefan–Boltzmann constant. As the position of a star on the HR diagram shows its approximate luminosity, this relation can be used to estimate its radius.The mass, radius and luminosity of a star are closely interlinked, and their respective values can be approximated by three relations. First is the Stefan–Boltzmann law, which relates the luminosity L, the radius R and the surface temperature Teff. Second is the mass–luminosity relation, which relates the luminosity L and the mass M. Finally, the relationship between M and R is close to linear. The ratio of M to R increases by a factor of only three over 2.5 orders of magnitude of M. This relation is roughly proportional to the star's inner temperature TI, and its extremely slow increase reflects the fact that the rate of energy generation in the core strongly depends on this temperature, whereas it has to fit the mass–luminosity relation. Thus, a too high or too low temperature will result in stellar instability.
A better approximation is to take ε = L/M, the energy generation rate per unit mass, as ε is proportional to TI15, where TI is the core temperature. This is suitable for stars at least as massive as the Sun, exhibiting the CNO cycle, and gives the better fit R ∝ M0.78.
The table below shows typical values for stars along the main sequence. The values of luminosity (L), radius (R) and mass (M) are relative to the Sun—a dwarf star with a spectral classification of G2 V. The actual values for a star may vary by as much as 20–30% from the values listed below.
All main-sequence stars have a core region where energy is generated by nuclear fusion. The temperature and density of this core are at the levels necessary to sustain the energy production that will support the remainder of the star. A reduction of energy production would cause the overlaying mass to compress the core, resulting in an increase in the fusion rate because of higher temperature and pressure. Likewise an increase in energy production would cause the star to expand, lowering the pressure at the core. Thus the star forms a self-regulating system in hydrostatic equilibrium that is stable over the course of its main sequence lifetime.Main-sequence stars employ two types of hydrogen fusion processes, and the rate of energy generation from each type depends on the temperature in the core region. Astronomers divide the main sequence into upper and lower parts, based on which of the two is the dominant fusion process. In the lower main sequence, energy is primarily generated as the result of the proton-proton chain, which directly fuses hydrogen together in a series of stages to produce helium. Stars in the upper main sequence have sufficiently high core temperatures to efficiently use the CNO cycle. (See the chart.) This process uses atoms of carbon, nitrogen and oxygen as intermediaries in the process of fusing hydrogen into helium.
At a stellar core temperature of 18 million Kelvin, the PP process and CNO cycle are equally efficient, and each type generates half of the star's net luminosity. As this is the core temperature of a star with about 1.5 M☉, the upper main sequence consists of stars above this mass. Thus, roughly speaking, stars of spectral class F or cooler belong to the lower main sequence, while A-type stars or hotter are upper main-sequence stars. The transition in primary energy production from one form to the other spans a range difference of less than a single solar mass. In the Sun, a one solar-mass star, only 1.5% of the energy is generated by the CNO cycle. By contrast, stars with 1.8 M☉ or above generate almost their entire energy output through the CNO cycle.The observed upper limit for a main-sequence star is 120–200 M☉. The theoretical explanation for this limit is that stars above this mass can not radiate energy fast enough to remain stable, so any additional mass will be ejected in a series of pulsations until the star reaches a stable limit. The lower limit for sustained proton–proton nuclear fusion is about 0.08 M☉ or 80 times the mass of Jupiter. Below this threshold are sub-stellar objects that can not sustain hydrogen fusion, known as brown dwarfs.
Because there is a temperature difference between the core and the surface, or photosphere, energy is transported outward. The two modes for transporting this energy are radiation and convection. A radiation zone, where energy is transported by radiation, is stable against convection and there is very little mixing of the plasma. By contrast, in a convection zone the energy is transported by bulk movement of plasma, with hotter material rising and cooler material descending. Convection is a more efficient mode for carrying energy than radiation, but it will only occur under conditions that create a steep temperature gradient.In massive stars (above 10 M☉) the rate of energy generation by the CNO cycle is very sensitive to temperature, so the fusion is highly concentrated at the core. Consequently, there is a high temperature gradient in the core region, which results in a convection zone for more efficient energy transport. This mixing of material around the core removes the helium ash from the hydrogen-burning region, allowing more of the hydrogen in the star to be consumed during the main-sequence lifetime. The outer regions of a massive star transport energy by radiation, with little or no convection.Intermediate-mass stars such as Sirius may transport energy primarily by radiation, with a small core convection region. Medium-sized, low-mass stars like the Sun have a core region that is stable against convection, with a convection zone near the surface that mixes the outer layers. This results in a steady buildup of a helium-rich core, surrounded by a hydrogen-rich outer region. By contrast, cool, very low-mass stars (below 0.4 M☉) are convective throughout. Thus the helium produced at the core is distributed across the star, producing a relatively uniform atmosphere and a proportionately longer main sequence lifespan.
As non-fusing helium ash accumulates in the core of a main-sequence star, the reduction in the abundance of hydrogen per unit mass results in a gradual lowering of the fusion rate within that mass. Since it is the outflow of fusion-supplied energy that supports the higher layers of the star, the core is compressed, producing higher temperatures and pressures. Both factors increase the rate of fusion thus moving the equilibrium towards a smaller, denser, hotter core producing more energy whose increased outflow pushes the higher layers further out. Thus there is a steady increase in the luminosity and radius of the star over time. For example, the luminosity of the early Sun was only about 70% of its current value. As a star ages this luminosity increase changes its position on the HR diagram. This effect results in a broadening of the main sequence band because stars are observed at random stages in their lifetime. That is, the main sequence band develops a thickness on the HR diagram; it is not simply a narrow line.Other factors that broaden the main sequence band on the HR diagram include uncertainty in the distance to stars and the presence of unresolved binary stars that can alter the observed stellar parameters. However, even perfect observation would show a fuzzy main sequence because mass is not the only parameter that affects a star's color and luminosity. Variations in chemical composition caused by the initial abundances, the star's evolutionary status, interaction with a close companion, rapid rotation, or a magnetic field can all slightly change a main-sequence star's HR diagram position, to name just a few factors. As an example, there are metal-poor stars (with a very low abundance of elements with higher atomic numbers than helium) that lie just below the main sequence and are known as subdwarfs. These stars are fusing hydrogen in their cores and so they mark the lower edge of main sequence fuzziness caused by variance in chemical composition.A nearly vertical region of the HR diagram, known as the instability strip, is occupied by pulsating variable stars known as Cepheid variables. These stars vary in magnitude at regular intervals, giving them a pulsating appearance. The strip intersects the upper part of the main sequence in the region of class A and F stars, which are between one and two solar masses. Pulsating stars in this part of the instability strip that intersects the upper part of the main sequence are called Delta Scuti variables. Main-sequence stars in this region experience only small changes in magnitude and so this variation is difficult to detect. Other classes of unstable main-sequence stars, like Beta Cephei variables, are unrelated to this instability strip.
The total amount of energy that a star can generate through nuclear fusion of hydrogen is limited by the amount of hydrogen fuel that can be consumed at the core. For a star in equilibrium, the energy generated at the core must be at least equal to the energy radiated at the surface. Since the luminosity gives the amount of energy radiated per unit time, the total life span can be estimated, to first approximation, as the total energy produced divided by the star's luminosity.For a star with at least 0.5 M☉, when the hydrogen supply in its core is exhausted and it expands to become a red giant, it can start to fuse helium atoms to form carbon. The energy output of the helium fusion process per unit mass is only about a tenth the energy output of the hydrogen process, and the luminosity of the star increases. This results in a much shorter length of time in this stage compared to the main sequence lifetime. (For example, the Sun is predicted to spend 130 million years burning helium, compared to about 12 billion years burning hydrogen.) Thus, about 90% of the observed stars above 0.5 M☉ will be on the main sequence. On average, main-sequence stars are known to follow an empirical mass-luminosity relationship. The luminosity (L) of the star is roughly proportional to the total mass (M) as the following power law:
  This relationship applies to main-sequence stars in the range 0.1–50 M☉.The amount of fuel available for nuclear fusion is proportional to the mass of the star. Thus, the lifetime of a star on the main sequence can be estimated by comparing it to solar evolutionary models. The Sun has been a main-sequence star for about 4.5 billion years and it will become a red giant in 6.5 billion years, for a total main sequence lifetime of roughly 1010 years. Hence:
    {\displaystyle {\begin{smallmatrix}\tau _{\rm {MS}}\ \approx \ 10^{10}{\text{years}}\cdot \left[{\frac {M}{M_{\bigodot }}}\right]\cdot \left[{\frac {L_{\bigodot }}{L}}\right]\ =\ 10^{10}{\text{years}}\cdot \left[{\frac {M}{M_{\bigodot }}}\right]^{-2.5}\end{smallmatrix}}}
Although more massive stars have more fuel to burn and might intuitively be expected to last longer, they also radiate a proportionately greater amount with increased mass. This is required by the stellar equation of state; for a massive star to maintain equilibrium, the outward pressure of radiated energy generated in the core not only must but will rise to match the titanic inward gravitational pressure of its  envelope. Thus, the most massive stars may remain on the main sequence for only a few million years, while stars with less than a tenth of a solar mass may last for over a trillion years.The exact mass-luminosity relationship depends on how efficiently energy can be transported from the core to the surface. A higher opacity has an insulating effect that retains more energy at the core, so the star does not need to produce as much energy to remain in hydrostatic equilibrium. By contrast, a lower opacity means energy escapes more rapidly and the star must burn more fuel to remain in equilibrium. Note, however, that a sufficiently high opacity can result in energy transport via convection, which changes the conditions needed to remain in equilibrium.In high-mass main-sequence stars, the opacity is dominated by electron scattering, which is nearly constant with increasing temperature. Thus the luminosity only increases as the cube of the star's mass. For stars below 10 M☉, the opacity becomes dependent on temperature, resulting in the luminosity varying approximately as the fourth power of the star's mass. For very low-mass stars, molecules in the atmosphere also contribute to the opacity. Below about 0.5 M☉, the luminosity of the star varies as the mass to the power of 2.3, producing a flattening of the slope on a graph of mass versus luminosity. Even these refinements are only an approximation, however, and the mass-luminosity relation can vary depending on a star's composition.
When a main-sequence star has consumed the hydrogen at its core, the loss of energy generation causes its gravitational collapse to resume and the star evolves off the main sequence.  The path which the star follows across the HR diagram is called an evolutionary track.Stars with less than 0.23 M☉, are predicted to directly become white dwarfs when energy generation by nuclear fusion of hydrogen at their core comes to a halt although no stars are old enough for this to have occurred.
In stars more massive than 0.23 M☉, the hydrogen surrounding the helium core reaches sufficient temperature and pressure to undergo fusion, forming a hydrogen-burning shell and causing the outer layers of the star to expand and cool.  The stage as these stars move away from the main sequence is known as the subgiant branch; it is relatively brief and appears as a gap in the evolutionary track since few stars are observed at that point.
When the helium core of low-mass stars becomes degenerate, or the outer layers of intermediate-mass stars cool sufficiently to become opaque, their hydrogen shells increase in temperature and the stars start to become more luminous.  This is known as the red giant branch; it is a relatively long-lived stage and it appears prominently in H-R diagrams.  These stars will eventually end their lives as white dwarfs.The most massive stars do not become red giants, instead their cores quickly become hot enough to fuse helium and eventually heavier elements and they are known as supergiants.  They follow approximately horizontal evolutionary tracks from the main sequence across the top of the H-R diagram.  Supergiants are relatively rare and do not show prominently on most H-R diagrams.  Their cores will eventually collapse, usually leading to a supernova and leaving behind either a neutron star or black hole.When a cluster of stars is formed at about the same time, the main sequence lifespan of these stars will depend on their individual masses.  The most massive stars will leave the main sequence first, followed in sequence by stars of ever lower masses.  The position where stars in the cluster are leaving the main sequence is known as the turnoff point.  By knowing the main sequence lifespan of stars at this point, it becomes possible to estimate the age of the cluster.
Bahcall, John N.; Pinsonneault, M.H.; Basu, Sarbani (2001). "Solar Models: Current Epoch and Time Dependences, Neutrinos, and Helioseismological Properties". The Astrophysical Journal. 555 (2).
Barnes, C. A.; Clayton, D. D.; Schramm, D. N., eds. (1982). Essays in Nuclear Astrophysics. Cambridge: Cambridge University Press.
Carroll, Bradley W. & Ostlie, Dale A. (2007). An Introduction to Modern Astrophysics. San Francisco: Person Education Addison-Wesley. ISBN 978-0-8053-0402-2.
Chabrier, Gilles; Baraffe, Isabelle (2000). "Theory of Low-Mass Stars and Substellar Objects". Annual Review of Astronomy and Astrophysics. 38: 337.
Clayton, Donald D. (1983). Principles of Stellar Evolution and Nucleosynthesis. Chicago: University of Chicago.
Cox, J. P.; Giuli, R. T. (1968). Principles of Stellar Structure. New York City: Gordon and Breach.
Fowler, William A.; Caughlan, Georgeanne R.; Zimmerman, Barbara A. (1967). "Thermonuclear Reaction Rates, I". Annual Review of Astronomy and Astrophysics. 5: 525.
Fowler, William A.; Caughlan, Georgeanne R.; Zimmerman, Barbara A. (1975). "Thermonuclear Reaction Rates, II". Annual Review of Astronomy and Astrophysics. 13: 69.
Hansen, Carl J.; Kawaler, Steven D.; Trimble, Virginia (2004). Stellar Interiors: Physical Principles, Structure, and Evolution, Second Edition. New York: Springer-Verlag.
Harris, Michael J.; Fowler, William A.; Caughlan, Georgeanne R.; Zimmerman, Barbara A. (1983). "Thermonuclear Reaction Rates, III". Annual Review of Astronomy and Astrophysics. 21: 165.
Iben, Icko, Jr (1967). "Stellar Evolution Within and Off the Main Sequence". Annual Review of Astronomy and Astrophysics. 5: 571.
Iglesias, Carlos A.; Rogers, Forrest J. (1996). "Updated Opal Opacities". The Astrophysical Journal. 464: 943.
Kippenhahn, Rudolf; Weigert, Alfred (1990). Stellar Structure and Evolution. Berlin: Springer-Verlag.
Liebert, James; Probst, Ronald G. (1987). "Very Low Mass Stars". Annual Review of Astronomy and Astrophysics. 25: 437.
Novotny, Eva (1973). Introduction to Stellar Atmospheres and Interior. New York City: Oxford University Press.
Prialnik, Dina (2000). An Introduction to the Theory of Stellar Structure and Evolution. Cambridge: Cambridge University Press.

The Maine Centennial half dollar is a commemorative coin struck in 1920 by the United States Bureau of the Mint.  It was sculpted by Anthony de Francisci, following sketches by an unknown artist from the U.S. state of Maine.
Officials in Maine wanted a commemorative half dollar to circulate as an advertisement for the centennial of the state's admission to the Union, and of the planned celebrations. A bill to allow such a coin passed Congress without opposition, but then the state's centennial commission decided to sell the coins at $1, double the face value.  The Commission of Fine Arts disliked the proposed design, and urged changes, but Maine officials insisted, and de Francisci converted the sketches to plaster models, from which coinage dies could be made. 
Fifty thousand pieces, half the authorized mintage, were struck for release to the public. They were issued too late to be sold at the centennial celebrations in Portland, but eventually the coins were all sold, though relatively few went to coin collectors.  They list for hundreds to thousands of dollars, depending on condition.
Governor Carl Milliken and the council of Maine wanted a half dollar issued to commemorate the centennial of the state's 1820 admission to the Union.  Initially, the idea was to have a circulating commemorative that could advertise the centennial celebrations in Maine. Later, after federal authorizing legislation for the coin was approved by Congress, the centennial commission decided to sell the coins for $1 each, rather than letting them pass from hand to hand in circulation.That legislation for a Maine Centennial half dollar had been introduced in the House of Representatives by the state's John A. Peters on February 11, 1920, with the bill designated as H.R. 12460. It was referred to the Committee on Coinage, Weights and Measures, of which Indiana Congressman Albert Vestal was the chairman. When the committee met, on February 23, 1920, Congressman Peters told members of the history of the state and citizens' desire to celebrate the centennial, including with a commemorative coin. He stated that he had spoken with the Director of the Mint, Raymond T. Baker, who had told Peters that he and  Treasury Secretary David F. Houston planned to endorse the bill, the text of which had been borrowed from the bill authorizing the 1918 Illinois Centennial half dollar. Ohio's William A. Ashbrook recalled that he had been a member of the committee that had approved the Illinois bill; he had favored it and now favored the Maine bill. Minnesota's Oscar E. Keller asked Peters to confirm there would be no expense to the federal government, which Peters did. Clay Stone Briggs of Texas wanted to know if the Maine bill's provisions were identical to those of the Illinois act, and Peters confirmed it. On March 20, Vestal filed a report on behalf of his committee, recommending that the House pass the bill, and reproducing a letter from Houston stating that the Treasury had no objection.Three coinage bills—Maine Centennial, Alabama Centennial, and Pilgrim Tercentenary—were considered in that order by the House of Representatives on April 21, 1920. After Peters addressed the House in favor of the Maine bill, Connecticut's John Q. Tilson inquired if the proposed coin would replace the existing design (the Walking Liberty half dollar) for the rest of the year; Peters explained that it would not, and that only 100,000 coins would bear the commemorative design.  John Franklin Miller of Washington state asked who would bear the expenses of the coinage dies, and Peters responded that the state of Maine would. Virginia's Andrew Jackson Montague asked if the Treasury Department had endorsed the bill, and Peters informed him that both Houston and Baker had. Vestal asked that the bill be passed, but Ohio's Warren Gard had questions about what would happen to the coins once they entered circulation; Peters stated that they would, once issued, be treated as ordinary half dollars. In response to questions by Gard, Peters explained that although Maine would pay for the dies, they would become federal government property. Peters added that though there would be no statewide celebration in Maine for the centennial, there would be local observances. Gard had no further questions about the Maine bill (he would also quiz the sponsors of the Alabama and Pilgrim bills), and on Vestal's motion it passed without recorded dissent.The following day, April 22, 1920, the House reported its passage of the Maine bill to the Senate. The bill was referred to the Senate Committee on Banking and Currency; on April 28, Connecticut's George P. McLean reported it back with a recommendation that it pass. On May 3, McLean asked that the three coin bills (Maine, Alabama and Pilgrim) be considered by the Senate immediately, fearing that though they were on the Senate's agenda for that day, they might not be reached, and believing urgent action was required. Utah Senator Reed Smoot objected: Smoot's attempt to bring up an anti-dumping trade bill out of turn had just been objected to by Charles S. Thomas of Colorado. Smoot, however, stated if the coin bills had not been reached by about 2:00 pm, there would probably not be any objection. When McLean tried again to advance the coin bills, Kansas' Charles Curtis asked if there was any urgency. McLean replied that as the three coin bills were to mark ongoing anniversaries, there was a need to have them authorized and get the production process started. All three bills passed the Senate without opposition and the Maine bill was enacted with the signature of President Woodrow Wilson on May 10, 1920.
On May 14, 1920, four days after Wilson signed the bill, Director of the Mint Baker sent sketches of the proposed design to the chairman of the Commission of Fine Arts, Charles Moore, for an opinion as to its merits. The design had been prepared by the officials in charge of the centennial commemoration, and had been given to Baker by Peters. Moore forwarded the sketches to the sculptor-member of the Commission, James Earle Fraser.  Having received no reply, Moore on May 26 sent a telegram to Fraser telling him that the Maine authorities wanted the coins by June 28.  Fraser immediately replied by telegram, that he disliked the design as it was "ordinary", and that it was an error to approve sketches; a plaster model should be made by a sculptor. Moore expanded on this in a letter to Houston the following day, "our new silver coinage has reached a high degree of perfection because it was designed by competent men. We should not return to the low standards which have formerly prevailed."Moore in his letter urged a change of design, stating that the sketch, if translated into a coin, "would bring humiliation to the people of Maine". However, Maine officials refused and insisted on the submitted sketches. After discussions among Peters, Moore, and various officials, an agreement was reached whereby the sketches would be converted into plaster models, and Fraser engaged his onetime student, Anthony de Francisci, to do the work. The younger sculptor completed the work by early July, and the models were approved by the Commission on July 9. The Engraving Department at the Philadelphia Mint created the coin dies utilizing de Francisci's models. Either Chief Engraver George T. Morgan, or his assistant, future chief engraver John R. Sinnock, changed the moose and pine tree on the coin from being in relief (as in de Francisci's models), to be sunken into the coin. This was probably in an attempt to improve the striking quality of the coins, and if so, had limited success, as the full detail would not appear on many coins.
The obverse of the Maine Centennial half dollar depicts the arms of Maine, based on the state's seal. At its center is a shield with a pine tree, sunken incuse into the coin, and below the tree a moose, lying down. The shield is flanked by two male figures, one bearing a scythe and representing Agriculture; the other, supporting an anchor, represents Commerce. Above the shield is the legend Dirigo, Latin for "I direct", and above that a five-pointed star. Below the shield is a scroll with the state's name. Near the rim are UNITED STATES OF AMERICA and HALF DOLLAR. The reverse contains a wreath of pine needles and cones (Maine is known as the Pine Tree State) around MAINE CENTENNIAL 1820–1920 as well as the various mottoes required by law to be present on the coinage.Numismatist Don Taxay, in his history of commemorative coins, speculated that "De  [sic] Francisci did not altogether favor them". According to Taxay, the two human figures on the obverse "were too small to retain their beauty after reduction [from the plaster models to coin size] and seem trivial. The reverse, with its wreath of pine cones, is eminently uninspired." Arlie Slabaugh, in his volume on commemoratives, noted that the half dollar "does not resemble the work by the same sculptor for the Peace dollar the following year [1921]."Art historian Cornelius Vermeule deprecated the Maine half dollar, but did not blame de Francisci, as the piece "was modeled by the sculptor according to required specifications and is therefore not considered typical of his art, or indeed of any art." Vermeule stated, "it looks just like a prize medal for a county fair or school athletic day." Nonetheless, feeling that de Francisci could have insisted on a more artistic design, Vermeule found "the Maine Centennial was not his shining moment".
Celebrations for the state's centennial were held in Maine's largest city, Portland, on July 4, 1920. Peters had hoped to have the half dollars available for distribution then, but because of the design controversy, they were not. He wrote to Assistant Director of the Mint Mary M. O'Reilly on July 14, expressing his frustration at the delay and stating that though the Portland festivities had passed, the state could still get some benefit from the coins if they received them during 1920. Otherwise, "we might as well wait for the next Centennial [in 2020] which I judge would be more convenient and in accordance with the speed at which we are going". He concluded by asking that the Mint let him know of the next obstacle ahead of time. Governor Milliken also wrote, on July 20, reminding Mint officials that the coin was authorized by a special act of Congress, and asking when the first consignment would be ready.In the late summer of 1920, a total of 50,028 Maine Centennial half dollars were produced at the Philadelphia Mint, including 28 pieces reserved for inspection and testing at the 1921 meeting of the annual Assay Commission. No special care was taken in the minting; they were ejected into bins and many display bag marks. They were sent to Maine and placed on sale through the Office of the State Treasurer at a price of $1. Thirty thousand sold immediately and they remained on sale through the treasurer's office until all fifty thousand were vended, though this did not happen until at least 1929. Bowers speculated that had the full 100,000 authorized coins been struck, most of the additional quantity would have been returned to the Mint and melted for lack of buyers. Many pieces were spent in the years after 1920 and entered circulation.Relatively few were sold to the coin collecting community, and the majority of surviving specimens display the effects of careless handling. The 2015 deluxe edition of Richard S. Yeoman's A Guide Book of United States Coins lists the coin at $140 to $685, depending on condition—an exceptional piece sold for $7,050 in 2014.
Bowers, Q. David (1992). Commemorative Coins of the United States: A Complete Encyclopedia. Wolfeboro, NH: Bowers and Merena Galleries, Inc. ISBN 978-0-943161-35-8.
Flynn, Kevin (2008). The Authoritative Reference on Commemorative Coins 1892–1954. Roswell, GA: Kyle Vick. OCLC 711779330.
Slabaugh, Arlie R. (1975). United States Commemorative Coinage (second ed.). Racine, WI: Whitman Publishing. ISBN 978-0-307-09377-6.
Swiatek, Anthony (2012). Encyclopedia of the Commemorative Coins of the United States. Chicago: KWS Publishers. ISBN 978-0-9817736-7-4.
Swiatek, Anthony & Breen, Walter (1981). The Encyclopedia of United States Silver & Gold Commemorative Coins, 1892 to 1954. New York: Arco Publishing. ISBN 978-0-668-04765-4.
Taxay, Don (1967). An Illustrated History of U.S. Commemorative Coinage. New York: Arco Publishing. ISBN 978-0-668-01536-3.
United States House of Representatives Committee on Coinage, Weights and Measures (March 26, 1920). Authorizing Coinage of Memorial 50-Cent Piece for the State of Alabama. United States Government Printing Office. (Subscription required (help)).
Vermeule, Cornelius (1971). Numismatic Art in America. Cambridge, MA: The Belknap Press of Harvard University Press. ISBN 978-0-674-62840-3.
Yeoman, R.S. (2015). A Guide Book of United States Coins (1st Deluxe ed.). Atlanta, GA: Whitman Publishing. ISBN 978-0-7948-4307-6.

Major depressive disorder (MDD), also known simply as depression, is a mental disorder characterized by at least two weeks of low mood that is present across most situations. It is often accompanied by low self-esteem, loss of interest in normally enjoyable activities, low energy, and pain without a clear cause. People may also occasionally have false beliefs or see or hear things that others cannot. Some people have periods of depression separated by years in which they are normal, while others nearly always have symptoms present. Major depressive disorder can negatively affect a person's personal life, work life, or education, as well as sleeping, eating habits, and general health. Between 2–8% of adults with major depression die by suicide, and about 50% of people who die by suicide had depression or another mood disorder.The cause is believed to be a combination of genetic, environmental, and psychological factors. Risk factors include a family history of the condition, major life changes, certain medications, chronic health problems, and substance abuse. About 40% of the risk appears to be related to genetics. The diagnosis of major depressive disorder is based on the person's reported experiences and a mental status examination. There is no laboratory test for major depression. Testing, however, may be done to rule out physical conditions that can cause similar symptoms. Major depression is more severe and lasts longer than sadness, which is a normal part of life. The United States Preventive Services Task Force (USPSTF) recommends screening for depression among those over the age 12, while a prior Cochrane review found that the routine use of screening questionnaires has little effect on detection or treatment.Typically, people are treated with counseling and antidepressant medication. Medication appears to be effective, but the effect may only be significant in the most severely depressed. It is unclear whether medications affect the risk of suicide. Types of counseling used include cognitive behavioral therapy (CBT) and interpersonal therapy. If other measures are not effective, electroconvulsive therapy (ECT) may be considered. Hospitalization may be necessary in cases with a risk of harm to self and may occasionally occur against a person's wishes.Major depressive disorder affected approximately 216 million people (3% of the world's population) in 2015. The percentage of people who are affected at one point in their life varies from 7% in Japan to 21% in France. Lifetime rates are higher in the developed world (15%) compared to the developing world (11%). It causes the second-most years lived with disability, after lower back pain. The most common time of onset is in a person's 20s and 30s. Females are affected about twice as often as males. The American Psychiatric Association added "major depressive disorder" to the Diagnostic and Statistical Manual of Mental Disorders (DSM-III) in 1980. It was a split of the previous depressive neurosis in the DSM-II, which also encompassed the conditions now known as dysthymia and adjustment disorder with depressed mood. Those currently or previously affected may be stigmatized.
Major depression significantly affects a person's family and personal relationships, work or school life, sleeping and eating habits, and general health. Its impact on functioning and well-being has been compared to that of other chronic medical conditions, such as diabetes.A person having a major depressive episode usually exhibits a very low mood, which pervades all aspects of life, and an inability to experience pleasure in activities that were formerly enjoyed. Depressed people may be preoccupied with, or ruminate over, thoughts and feelings of worthlessness, inappropriate guilt or regret, helplessness, hopelessness, and self-hatred. In severe cases, depressed people may have symptoms of psychosis. These symptoms include delusions or, less commonly, hallucinations, usually unpleasant. Other symptoms of depression include poor concentration and memory (especially in those with melancholic or psychotic features), withdrawal from social situations and activities, reduced sex drive, irritability, and thoughts of death or suicide. Insomnia is common among the depressed. In the typical pattern, a person wakes very early and cannot get back to sleep. Hypersomnia, or oversleeping, can also happen. Some antidepressants may also cause insomnia due to their stimulating effect.A depressed person may report multiple physical symptoms such as fatigue, headaches, or digestive problems; physical complaints are the most common presenting problem in developing countries, according to the World Health Organization's criteria for depression. Appetite often decreases, with resulting weight loss, although increased appetite and weight gain occasionally occur. Family and friends may notice that the person's behavior is either agitated or lethargic. Older depressed people may have cognitive symptoms of recent onset, such as forgetfulness, and a more noticeable slowing of movements. Depression often coexists with physical disorders common among the elderly, such as stroke, other cardiovascular diseases, Parkinson's disease, and chronic obstructive pulmonary disease.Depressed children may often display an irritable mood rather than a depressed one, and show varying symptoms depending on age and situation. Most lose interest in school and show a decline in academic performance. They may be described as clingy, demanding, dependent, or insecure. Diagnosis may be delayed or missed when symptoms are interpreted as "normal moodiness."
Major depression frequently co-occurs with other psychiatric problems. The 1990–92 National Comorbidity Survey (US) reports that half of those with major depression also have lifetime anxiety and its associated disorders such as generalized anxiety disorder. Anxiety symptoms can have a major impact on the course of a depressive illness, with delayed recovery, increased risk of relapse, greater disability and increased suicide attempts. There are increased rates of alcohol and drug abuse and particularly dependence, and around a third of individuals diagnosed with ADHD develop comorbid depression. Post-traumatic stress disorder and depression often co-occur. Depression may also coexist with attention deficit hyperactivity disorder (ADHD), complicating the diagnosis and treatment of both. Depression is also frequently comorbid with alcohol abuse and personality disorders. Depression can also be exacerbated during particular months (usually winter) for those with seasonal affective disorder.
Depression and pain often co-occur. One or more pain symptoms are present in 65% of depressed patients, and anywhere from 5 to 85% of patients with pain will be suffering from depression, depending on the setting; there is a lower prevalence in general practice, and higher in specialty clinics. The diagnosis of depression is often delayed or missed, and the outcome can worsen if the depression is noticed but completely misunderstood.Depression is also associated with a 1.5- to 2-fold increased risk of cardiovascular disease, independent of other known risk factors, and is itself linked directly or indirectly to risk factors such as smoking and obesity. People with major depression are less likely to follow medical recommendations for treating and preventing cardiovascular disorders, which further increases their risk of medical complications. In addition, cardiologists may not recognize underlying depression that complicates a cardiovascular problem under their care.
The cause of major depressive disorder is unknown. The biopsychosocial model proposes that biological, psychological, and social factors all play a role in causing depression. The diathesis–stress model specifies that depression results when a preexisting vulnerability, or diathesis, is activated by stressful life events. The preexisting vulnerability can be either genetic, implying an interaction between nature and nurture, or schematic, resulting from views of the world learned in childhood.Childhood abuse, either physical, sexual or psychological, are all risk factors for depression, among other psychiatric issues that co-occur such as anxiety and drug abuse. Childhood trauma also correlates with severity of depression, lack of response to treatment and length of illness. However, some are more susceptible to developing mental illness such as depression after trauma, and various genes have been suggested to control susceptibility.
Family and twin studies find that nearly 40% of individual differences in risk for major depressive disorder can be explained by genetic factors. Like most psychiatric disorders, major depressive disorder is likely to be influenced by many individual genetic changes. In 2018, a genome-wide association study discovered 44 variants in the genome linked to risk for major depression. This was followed by a 2019 study that found 102 variants in the genome linked to depression. These results have enabled scientists to calculate polygenic risk scores, which combine the estimated effects across the genome into a single score in order to estimate a person’s genetic liability for depression. Although these scores explain a small proportion (less than 2%) of individual differences in the risk for major depressive disorder.
The 5-HTTLPR, or serotonin transporter promoter gene's short allele has been associated with increased risk of depression. However, since the 1990s, results have been inconsistent, with three recent reviews finding an effect and two finding none. Other genes that have been linked to a gene-environment interaction include CRHR1, FKBP5 and BDNF, the first two of which are related to the stress reaction of the HPA axis, and the latter of which is involved in neurogenesis. There is no conclusive effects of candidate gene on depression, either alone or in combination with life stress. Research focusing on specific candidate genes has been criticized for its tendency to generate false positive findings. There are also other efforts to examine interactions between life stress and polygenic risk for depression.
Depression may also come secondary to a chronic or terminal medical condition, such as HIV/AIDS or asthma, and may be labeled "secondary depression." It is unknown whether the underlying diseases induce depression through effect on quality of life, of through shared etiologies (such as degeneration of the basal ganglia in Parkinson's disease or immune dysregulation in asthma). Depression may also be iatrogenic (the result of healthcare), such as drug-induced depression. Therapies associated with depression include interferons, beta-blockers, isotretinoin, contraceptives, cardiac agents, anticonvulsants, antimigraine drugs, antipsychotics, and hormonal agents such as gonadotropin-releasing hormone agonist. Drug abuse in early age is also associated with increased risk of developing depression later in life. Depression that occurs as a result of pregnancy is called postpartum depression, and is thought to be the result of hormonal changes associated with pregnancy. Seasonal affective disorder, a type of depression associated with seasonal changes in sunlight, is thought to be the result of decreased sunlight.
The pathophysiology of depression is not yet understood, but the current theories center around monoaminergic systems, the circadian rhythm, immunological dysfunction, HPA axis dysfunction and structural or functional abnormalities of emotional circuits.
The monoamine theory, derived from the efficacy of monoaminergic drugs in treating depression, was the dominant theory until recently. The theory postulates that insufficient activity of monoamine neurotransmitters is the primary cause of depression. Evidence for the monoamine theory comes from multiple areas. Firstly, acute depletion of tryptophan, a necessary precursor of serotonin, a monoamine, can cause depression in those in remission or relatives of depressed patients; this suggests that decreased serotonergic neurotransmission is important in depression. Secondly, the correlation between depression risk and polymorphisms in the 5-HTTLPR gene, which codes for serotonin receptors, suggests a link. Third, decreased size of the locus coeruleus, decreased activity of tyrosine hydroxylase, increased density of alpha-2 adrenergic receptor, and evidence from rat models suggest decreased adrenergic neurotransmission in depression. Furthermore, decreased levels of homovanillic acid, altered response to dextroamphetamine, responses of depressive symptoms to dopamine receptor agonists, decreased dopamine receptor D1 binding in the striatum, and polymorphism of dopamine receptor genes implicate dopamine, another monoamine, in depression. Lastly, increased activity of monoamine oxidase, which degrades monoamines, has been associated with depression. However, this theory is inconsistent with the fact that serotonin depletion does not cause depression in healthy persons, the fact that antidepressants instantly increase levels of monoamines but take weeks to work, and the existence of atypical antidepressants which can be effective despite not targeting this pathway. One proposed explanation for the therapeutic lag, and further support for the deficiency of monoamines, is a desensitization of self-inhibition in raphe nuclei by the increased serotonin mediated by antidepressants. However, disinhibition of the dorsal raphe has been proposed to occur as a result of decreased serotonergic activity in tryptophan depletion, resulting in a depressed state mediated by increased serotonin. Further countering the monoamine hypothesis is the fact that rats with lesions of the dorsal raphe are not more depressive than controls, the finding of increased jugular 5-HIAA in depressed patients that normalized with SSRI treatment, and the preference for carbohydrates in depressed patients. Already limited, the monoamine hypothesis has been further oversimplified when presented to the general public.Immune system abnormalities have been observed, including increased levels of cytokines involved in generating sickness behavior (which shares overlap with depression). The effectiveness of nonsteroidal anti-inflammatory drugs (NSAIDs) and cytokine inhibitors in treating depression, and normalization of cytokine levels after successful treatment further suggest immune system abnormalities in depression.HPA axis abnormalities have been suggested in depression given the association of CRHR1 with depression and the increased frequency of dexamethasone test non-suppression in depressed patients. However, this abnormality is not adequate as a diagnosis tool, because its sensitivity is only 44%. These stress-related abnormalities have been hypothesized to be the cause of hippocampal volume reductions seen in depressed patients. Furthermore, a meta-analysis yielded decreased dexamethasone suppression, and increased response to psychological stressors. Further abnormal results have been obscured with the cortisol awakening response, with increased response being associated with depression.Theories unifying neuroimaging findings have been proposed. The first model proposed is the "Limbic Cortical Model", which involves hyperactivity of the ventral paralimbic regions and hypoactivity of frontal regulatory regions in emotional processing. Another model, the "Corito-Striatal model", suggests that abnormalities of the prefrontal cortex in regulating striatal and subcortical structures results in depression. Another model proposes hyperactivity of salience structures in identifying negative stimuli, and hypoactivity of cortical regulatory structures resulting in a negative emotional bias and depression, consistent with emotional bias studies.
A diagnostic assessment may be conducted by a suitably trained general practitioner, or by a psychiatrist or psychologist, who records the person's current circumstances, biographical history, current symptoms, and family history. The broad clinical aim is to formulate the relevant biological, psychological, and social factors that may be impacting on the individual's mood. The assessor may also discuss the person's current ways of regulating mood (healthy or otherwise) such as alcohol and drug use. The assessment also includes a mental state examination, which is an assessment of the person's current mood and thought content, in particular the presence of themes of hopelessness or pessimism, self-harm or suicide, and an absence of positive thoughts or plans. Specialist mental health services are rare in rural areas, and thus diagnosis and management is left largely to primary-care clinicians. This issue is even more marked in developing countries. The mental health examination may include the use of a rating scale such as the Hamilton Rating Scale for Depression, the Beck Depression Inventory or the Suicide Behaviors Questionnaire-Revised. The score on a rating scale alone is insufficient to diagnose depression to the satisfaction of the DSM or ICD, but it provides an indication of the severity of symptoms for a time period, so a person who scores above a given cut-off point can be more thoroughly evaluated for a depressive disorder diagnosis. Several rating scales are used for this purpose.Primary-care physicians and other non-psychiatrist physicians have more difficulty with underrecognition and undertreatment of depression compared to psychiatric physicians, in part because of the physical symptoms that often accompany depression, in addition to many potential patient, provider, and system barriers. A review found that non-psychiatrist physicians miss about two-thirds of cases, though this has improved somewhat in more recent studies.Before diagnosing a major depressive disorder, in general a doctor performs a medical examination and selected investigations to rule out other causes of symptoms. These include blood tests measuring TSH and thyroxine to exclude hypothyroidism; basic electrolytes and serum calcium to rule out a metabolic disturbance; and a full blood count including ESR to rule out a systemic infection or chronic disease. Adverse affective reactions to medications or alcohol misuse are often ruled out, as well. Testosterone levels may be evaluated to diagnose hypogonadism, a cause of depression in men. Vitamin D levels might be evaluated, as low levels of vitamin D have been associated with greater risk for depression.Subjective cognitive complaints appear in older depressed people, but they can also be indicative of the onset of a dementing disorder, such as Alzheimer's disease. Cognitive testing and brain imaging can help distinguish depression from dementia. A CT scan can exclude brain pathology in those with psychotic, rapid-onset or otherwise unusual symptoms. In general, investigations are not repeated for a subsequent episode unless there is a medical indication.
No biological tests confirm major depression. Biomarkers of depression have been sought to provide an objective method of diagnosis. There are several potential biomarkers, including brain-derived neurotrophic factor and various functional MRI (fMRI) techniques. One study developed a decision tree model of interpreting a series of fMRI scans taken during various activities. In their subjects, the authors of that study were able to achieve a sensitivity of 80% and a specificity of 87%, corresponding to a negative predictive value of 98% and a positive predictive value of 32% (positive and negative likelihood ratios were 6.15, 0.23, respectively). However, much more research is needed before these tests can be used clinically.
The most widely used criteria for diagnosing depressive conditions are found in the American Psychiatric Association's Diagnostic and Statistical Manual of Mental Disorders and the World Health Organization's International Statistical Classification of Diseases and Related Health Problems which uses the name depressive episode for a single episode and recurrent depressive disorder for repeated episodes. The latter system is typically used in European countries, while the former is used in the US and many other non-European nations, and the authors of both have worked towards conforming one with the other.Both DSM-5 and ICD-10 mark out typical (main) depressive symptoms. ICD-10 defines three typical depressive symptoms (depressed mood, anhedonia, and reduced energy), two of which should be present to determine the depressive disorder diagnosis. According to DSM-5, there are two main depressive symptoms- a depressed mood and loss of interest/pleasure in activities (anhedonia). These symptoms, as well as five out of the nine more specific symptoms listed, must frequently occur for more than two weeks (to the extent in which it impairs functioning) for the diagnosis.Major depressive disorder is classified as a mood disorder in DSM-5. The diagnosis hinges on the presence of single or recurrent major depressive episodes. Further qualifiers are used to classify both the episode itself and the course of the disorder. The category Unspecified Depressive Disorder is diagnosed if the depressive episode's manifestation does not meet the criteria for a major depressive episode. The ICD-10 system does not use the term major depressive disorder but lists very similar criteria for the diagnosis of a depressive episode (mild, moderate or severe); the term recurrent may be added if there have been multiple episodes without mania.
A major depressive episode is characterized by the presence of a severely depressed mood that persists for at least two weeks. Episodes may be isolated or recurrent and are categorized as mild (few symptoms in excess of minimum criteria), moderate, or severe (marked impact on social or occupational functioning). An episode with psychotic features—commonly referred to as psychotic depression—is automatically rated as severe. If the patient has had an episode of mania or markedly elevated mood, a diagnosis of bipolar disorder is made instead. Depression without mania is sometimes referred to as unipolar because the mood remains at one emotional state or "pole".DSM-IV-TR excludes cases where the symptoms are a result of bereavement, although it is possible for normal bereavement to evolve into a depressive episode if the mood persists and the characteristic features of a major depressive episode develop. The criteria were criticized because they do not take into account any other aspects of the personal and social context in which depression can occur. In addition, some studies have found little empirical support for the DSM-IV cut-off criteria, indicating they are a diagnostic convention imposed on a continuum of depressive symptoms of varying severity and duration. Bereavement is no longer an exclusion criterion in DSM-5, and it is now up to the clinician to distinguish between normal reactions to a loss and MDD. Excluded are a range of related diagnoses, including dysthymia, which involves a chronic but milder mood disturbance; recurrent brief depression, consisting of briefer depressive episodes; minor depressive disorder, whereby only some symptoms of major depression are present; and adjustment disorder with depressed mood, which denotes low mood resulting from a psychological response to an identifiable event or stressor. Three new depressive disorders were added to the DSM-5: disruptive mood dysregulation disorder, classified by significant childhood irritability and tantrums, premenstrual dysphoric disorder (PMDD), causing periods of anxiety, depression, or irritability in the week or two before a woman's menstruation, and persistent depressive disorder.
The DSM-5 recognizes six further subtypes of MDD, called specifiers, in addition to noting the length, severity and presence of psychotic features:
"Melancholic depression" is characterized by a loss of pleasure in most or all activities, a failure of reactivity to pleasurable stimuli, a quality of depressed mood more pronounced than that of grief or loss, a worsening of symptoms in the morning hours, early-morning waking, psychomotor retardation, excessive weight loss (not to be confused with anorexia nervosa), or excessive guilt.
"Atypical depression" is characterized by mood reactivity (paradoxical anhedonia) and positivity, significant weight gain or increased appetite (comfort eating), excessive sleep or sleepiness (hypersomnia), a sensation of heaviness in limbs known as leaden paralysis, and significant social impairment as a consequence of hypersensitivity to perceived interpersonal rejection.
"Catatonic depression" is a rare and severe form of major depression involving disturbances of motor behavior and other symptoms. Here, the person is mute and almost stuporous, and either remains immobile or exhibits purposeless or even bizarre movements. Catatonic symptoms also occur in schizophrenia or in manic episodes, or may be caused by neuroleptic malignant syndrome.
"Depression with anxious distress" was added into the DSM-V as a means to emphasize the common co-occurrence between depression or mania and anxiety, as well as the risk of suicide of depressed individuals with anxiety. Specifying in such a way can also help with the prognosis of those diagnosed with a depressive or bipolar disorder.
"Depression with peri-partum onset" refers to the intense, sustained and sometimes disabling depression experienced by women after giving birth or while a woman is pregnant. DSM-IV-TR used the classification "postpartum depression," but this was changed in order to not exclude cases of depressed woman during pregnancy. Depression with peripartum onset has an incidence rate of 10–15% among new mothers. The DSM-V mandates that, in order to qualify as depression with peripartum onset, onset occur during pregnancy or within one month of delivery. It has been said that postpartum depression can last as long as three months.
"Seasonal affective disorder" (SAD) is a form of depression in which depressive episodes come on in the autumn or winter, and resolve in spring. The diagnosis is made if at least two episodes have occurred in colder months with none at other times, over a two-year period or longer.
In 2016, the United States Preventive Services Task Force (USPSTF) recommended screening in the adult populations with evidence that it increases the detection of people with depression and with proper treatment improves outcomes. They recommend screening in those between the age of 12 to 18 as well.A Cochrane review from 2005 found screening programs do not significantly improve detection rates, treatment, or outcome.
To confirm major depressive disorder as the most likely diagnosis, other potential diagnoses must be considered, including dysthymia, adjustment disorder with depressed mood, or bipolar disorder. Dysthymia is a chronic, milder mood disturbance in which a person reports a low mood almost daily over a span of at least two years. The symptoms are not as severe as those for major depression, although people with dysthymia are vulnerable to secondary episodes of major depression (sometimes referred to as double depression). Adjustment disorder with depressed mood is a mood disturbance appearing as a psychological response to an identifiable event or stressor, in which the resulting emotional or behavioral symptoms are significant but do not meet the criteria for a major depressive episode. Bipolar disorder, also known as manic–depressive disorder, is a condition in which depressive phases alternate with periods of mania or hypomania. Although depression is currently categorized as a separate disorder, there is ongoing debate because individuals diagnosed with major depression often experience some hypomanic symptoms, indicating a mood disorder continuum. Further differential diagnoses involve chronic fatigue syndrome.Other disorders need to be ruled out before diagnosing major depressive disorder. They include depressions due to physical illness, medications, and substance abuse. Depression due to physical illness is diagnosed as a mood disorder due to a general medical condition. This condition is determined based on history, laboratory findings, or physical examination. When the depression is caused by a medication, drug of abuse, or exposure to a toxin, it is then diagnosed as a specific mood disorder (previously called substance-induced mood disorder in the DSM-IV-TR).
Preventative efforts may result in decreases in rates of the condition of between 22 and 38%. Eating large amounts of fish may also reduce the risk.Behavioral interventions, such as interpersonal therapy and cognitive-behavioral therapy, are effective at preventing new onset depression. Because such interventions appear to be most effective when delivered to individuals or small groups, it has been suggested that they may be able to reach their large target audience most efficiently through the Internet.However, an earlier meta-analysis found preventive programs with a competence-enhancing component to be superior to behavior-oriented programs overall, and found behavioral programs to be particularly unhelpful for older people, for whom social support programs were uniquely beneficial. In addition, the programs that best prevented depression comprised more than eight sessions, each lasting between 60 and 90 minutes, were provided by a combination of lay and professional workers, had a high-quality research design, reported attrition rates, and had a well-defined intervention.The Netherlands mental health care system provides preventive interventions, such as the "Coping with Depression" course (CWD) for people with sub-threshold depression. The course is claimed to be the most successful of psychoeducational interventions for the treatment and prevention of depression (both for its adaptability to various populations and its results), with a risk reduction of 38% in major depression and an efficacy as a treatment comparing favorably to other psychotherapies.
The three most common treatments for depression are psychotherapy, medication, and electroconvulsive therapy. Psychotherapy is the treatment of choice (over medication) for people under 18. The UK National Institute for Health and Care Excellence (NICE) 2004 guidelines indicate that antidepressants should not be used for the initial treatment of mild depression, because the risk-benefit ratio is poor. The guidelines recommend that antidepressants treatment in combination with psychosocial interventions should be considered for:
As a first line treatment for moderate or severe depression.The guidelines further note that antidepressant treatment should be continued for at least six months to reduce the risk of relapse, and that SSRIs are better tolerated than tricyclic antidepressants.American Psychiatric Association treatment guidelines recommend that initial treatment should be individually tailored based on factors including severity of symptoms, co-existing disorders, prior treatment experience, and patient preference. Options may include pharmacotherapy, psychotherapy, exercise, electroconvulsive therapy (ECT), transcranial magnetic stimulation (TMS) or light therapy. Antidepressant medication is recommended as an initial treatment choice in people with mild, moderate, or severe major depression, and should be given to all patients with severe depression unless ECT is planned. There is evidence that collaborative care by a team of health care practitioners produces better results than routine single-practitioner care.Treatment options are much more limited in developing countries, where access to mental health staff, medication, and psychotherapy is often difficult. Development of mental health services is minimal in many countries; depression is viewed as a phenomenon of the developed world despite evidence to the contrary, and not as an inherently life-threatening condition. A 2014 Cochrane review found insufficient evidence to determine the effectiveness of psychological versus medical therapy in children.
Physical exercise is recommended for management of mild depression, and has a moderate effect on symptoms. Exercise has also been found to be effective for (unipolar) major depression. It is equivalent to the use of medications or psychological therapies in most people. In older people it does appear to decrease depression. Exercise may be recommended to people who are willing, motivated, and physically healthy enough to participate in an exercise program as treatment.There is a small amount of evidence that skipping a night's sleep may improve depressive symptoms, with the effects usually showing up within a day. This effect is usually temporary. Besides sleepiness, this method can cause a side effect of mania or hypomania.In observational studies, smoking cessation has benefits in depression as large as or larger than those of medications.Besides exercise, sleep and diet may play a role in depression, and interventions in these areas may be an effective add-on to conventional methods.
Psychotherapy can be delivered to individuals, groups, or families by mental health professionals. A 2015 review found that cognitive behavioral therapy appears to be similar to antidepressant medication in terms of effect. A 2012 review found psychotherapy to be better than no treatment but not other treatments. With more complex and chronic forms of depression, a combination of medication and psychotherapy may be used. A 2014 Cochrane review found that work-directed interventions combined with clinical interventions helped to reduce sick days taken by people with depression. There is moderate-quality evidence that psychological therapies are a useful addition to standard antidepressant treatment of treatment-resistant depression in the short term.Psychotherapy has been shown to be effective in older people. Successful psychotherapy appears to reduce the recurrence of depression even after it has been terminated or replaced by occasional booster sessions.
Cognitive behavioral therapy (CBT) currently has the most research evidence for the treatment of depression in children and adolescents, and CBT and interpersonal psychotherapy (IPT) are preferred therapies for adolescent depression. In people under 18, according to the National Institute for Health and Clinical Excellence, medication should be offered only in conjunction with a psychological therapy, such as CBT, interpersonal therapy, or family therapy. Cognitive behavioral therapy has also been shown to reduce the number of sick days taken by people with depression, when used in conjunction with primary care.The most-studied form of psychotherapy for depression is CBT, which teaches clients to challenge self-defeating, but enduring ways of thinking (cognitions) and change counter-productive behaviors. Research beginning in the mid-1990s suggested that CBT could perform as well as or better than antidepressants in patients with moderate to severe depression. CBT may be effective in depressed adolescents, although its effects on severe episodes are not definitively known. Several variables predict success for cognitive behavioral therapy in adolescents: higher levels of rational thoughts, less hopelessness, fewer negative thoughts, and fewer cognitive distortions. CBT is particularly beneficial in preventing relapse.Cognitive behavioral therapy and occupational programs (including modification of work activities and assistance) have been shown to be effective in reducing sick days taken by workers with depression.
Several variants of cognitive behavior therapy have been used in those with depression, the most notable being rational emotive behavior therapy, and mindfulness-based cognitive therapy. Mindfulness-based stress reduction programs may reduce depression symptoms. Mindfulness programs also appear to be a promising intervention in youth.
Psychoanalysis is a school of thought, founded by Sigmund Freud, which emphasizes the resolution of unconscious mental conflicts. Psychoanalytic techniques are used by some practitioners to treat clients presenting with major depression. A more widely practiced therapy, called psychodynamic psychotherapy, is in the tradition of psychoanalysis but less intensive, meeting once or twice a week. It also tends to focus more on the person's immediate problems, and has an additional social and interpersonal focus. In a meta-analysis of three controlled trials of Short Psychodynamic Supportive Psychotherapy, this modification was found to be as effective as medication for mild to moderate depression.
Conflicting results have arisen from studies that look at the effectiveness of antidepressants in people with acute, mild to moderate depression. Stronger evidence supports the usefulness of antidepressants in the treatment of depression that is chronic (dysthymia) or severe.
While small benefits were found, researchers Irving Kirsch and Thomas Moore state they may be due to issues with the trials rather than a true effect of the medication. In a later publication, Kirsch concluded that the overall effect of new-generation antidepressant medication is below recommended criteria for clinical significance. Similar results were obtained in a meta-analysis by Fornier.A review commissioned by the National Institute for Health and Care Excellence (UK) concluded that there is strong evidence that selective serotonin reuptake inhibitors (SSRIs), such as escitalopram, paroxetine, and sertraline, have greater efficacy than placebo on achieving a 50% reduction in depression scores in moderate and severe major depression, and that there is some evidence for a similar effect in mild depression. Similarly, a Cochrane systematic review of clinical trials of the generic tricyclic antidepressant amitriptyline concluded that there is strong evidence that its efficacy is superior to placebo.In 2014 the U.S. Food and Drug Administration published a systematic review of all antidepressant maintenance trials submitted to the agency between 1985 and 2012. The authors concluded that maintenance treatment reduced the risk of relapse by 52% compared to placebo, and that this effect was primarily due to recurrent depression in the placebo group rather than a drug withdrawal effect.To find the most effective antidepressant medication with minimal side-effects, the dosages can be adjusted, and if necessary, combinations of different classes of antidepressants can be tried. Response rates to the first antidepressant administered range from 50–75%, and it can take at least six to eight weeks from the start of medication to remission. Antidepressant medication treatment is usually continued for 16 to 20 weeks after remission, to minimize the chance of recurrence, and even up to one year of continuation is recommended. People with chronic depression may need to take medication indefinitely to avoid relapse.SSRIs are the primary medications prescribed, owing to their relatively mild side-effects, and because they are less toxic in overdose than other antidepressants. People who do not respond to one SSRI can be switched to another antidepressant, and this results in improvement in almost 50% of cases. Another option is to switch to the atypical antidepressant bupropion. Venlafaxine, an antidepressant with a different mechanism of action, may be modestly more effective than SSRIs. However, venlafaxine is not recommended in the UK as a first-line treatment because of evidence suggesting its risks may outweigh benefits, and it is specifically discouraged in children and adolescents.For children, some research has supported the use of the SSRI antidepressant fluoxetine. The benefit however appears to be slight in children, while other antidepressants have not been shown to be effective. Medications are not recommended in children with mild disease. There is also insufficient evidence to determine effectiveness in those with depression complicated by dementia. Any antidepressant can cause low blood sodium levels; nevertheless, it has been reported more often with SSRIs. It is not uncommon for SSRIs to cause or worsen insomnia; the sedating atypical antidepressant mirtazapine can be used in such cases.Irreversible monoamine oxidase inhibitors, an older class of antidepressants, have been plagued by potentially life-threatening dietary and drug interactions. They are still used only rarely, although newer and better-tolerated agents of this class have been developed. The safety profile is different with reversible monoamine oxidase inhibitors, such as moclobemide, where the risk of serious dietary interactions is negligible and dietary restrictions are less strict.For children, adolescents, and probably young adults between 18 and 24 years old, there is a higher risk of both suicidal ideations and suicidal behavior in those treated with SSRIs. For adults, it is unclear whether SSRIs affect the risk of suicidality. One review found no connection; another an increased risk; and a third no risk in those 25–65 years old and a decreased risk in those more than 65. A black box warning was introduced in the United States in 2007 on SSRIs and other antidepressant medications due to the increased risk of suicide in patients younger than 24 years old. Similar precautionary notice revisions were implemented by the Japanese Ministry of Health.
There is some evidence that omega-3 fatty acids fish oil supplements containing high levels of eicosapentaenoic acid (EPA) to docosahexaenoic acid (DHA) are effective in the treatment of, but not the prevention of major depression. However, a Cochrane review determined there was insufficient high quality evidence to suggest omega-3 fatty acids were effective in depression. There is limited evidence that vitamin D supplementation is of value in alleviating the symptoms of depression in individuals who are vitamin D-deficient. There is some preliminary evidence that COX-2 inhibitors, such as celecoxib, have a beneficial effect on major depression. Lithium appears effective at lowering the risk of suicide in those with bipolar disorder and unipolar depression to nearly the same levels as the general population. There is a narrow range of effective and safe dosages of lithium thus close monitoring may be needed. Low-dose thyroid hormone may be added to existing antidepressants to treat persistent depression symptoms in people who have tried multiple courses of medication. Limited evidence suggests stimulants, such as amphetamine and modafinil, may be effective in the short term, or as adjuvant therapy. Also, it is suggested that folate supplements may have a role in depression management.
Electroconvulsive therapy (ECT) is a standard psychiatric treatment in which seizures are electrically induced in patients to provide relief from psychiatric illnesses. ECT is used with informed consent as a last line of intervention for major depressive disorder.A round of ECT is effective for about 50% of people with treatment-resistant major depressive disorder, whether it is unipolar or bipolar. Follow-up treatment is still poorly studied, but about half of people who respond relapse within twelve months.Aside from effects in the brain, the general physical risks of ECT are similar to those of brief general anesthesia. Immediately following treatment, the most common adverse effects are confusion and memory loss. ECT is considered one of the least harmful treatment options available for severely depressed pregnant women.A usual course of ECT involves multiple administrations, typically given two or three times per week, until the patient is no longer suffering symptoms. ECT is administered under anesthesia with a muscle relaxant. Electroconvulsive therapy can differ in its application in three ways: electrode placement, frequency of treatments, and the electrical waveform of the stimulus. These three forms of application have significant differences in both adverse side effects and symptom remission. After treatment, drug therapy is usually continued, and some patients receive maintenance ECT.ECT appears to work in the short term via an anticonvulsant effect mostly in the frontal lobes, and longer term via neurotrophic effects primarily in the medial temporal lobe.
Transcranial magnetic stimulation (TMS) or deep transcranial magnetic stimulation is a noninvasive method used to stimulate small regions of the brain. TMS was approved by the FDA for treatment-resistant major depressive disorder (trMDD) in 2008 and as of 2014 evidence supports that it is probably effective. The American Psychiatric Association the Canadian Network for Mood and Anxiety Disorders, and the Royal Australia and New Zealand College of Psychiatrists have endorsed TMS for trMDD.
Bright light therapy reduces depression symptom severity, with benefit for both seasonal affective disorder and for nonseasonal depression, and an effect similar to those for conventional antidepressants. For nonseasonal depression, adding light therapy to the standard antidepressant treatment was not effective. For nonseasonal depression, where light was used mostly in combination with antidepressants or wake therapy, a moderate effect was found, with response better than control treatment in high-quality studies, in studies that applied morning light treatment, and with people who respond to total or partial sleep deprivation. Both analyses noted poor quality, short duration, and small size of most of the reviewed studies. There is insufficient evidence for Reiki and dance movement therapy in depression.
Major depressive episodes often resolve over time whether or not they are treated. Outpatients on a waiting list show a 10–15% reduction in symptoms within a few months, with approximately 20% no longer meeting the full criteria for a depressive disorder. The median duration of an episode has been estimated to be 23 weeks, with the highest rate of recovery in the first three months.Studies have shown that 80% of those suffering from their first major depressive episode will suffer from at least one more during their life, with a lifetime average of 4 episodes. Other general population studies indicate that around half those who have an episode recover (whether treated or not) and remain well, while the other half will have at least one more, and around 15% of those experience chronic recurrence. Studies recruiting from selective inpatient sources suggest lower recovery and higher chronicity, while studies of mostly outpatients show that nearly all recover, with a median episode duration of 11 months. Around 90% of those with severe or psychotic depression, most of whom also meet criteria for other mental disorders, experience recurrence.A high proportion of people who experience full symptomatic remission still have at least one not fully resolved symptom after treatment. Recurrence or chronicity is more likely if symptoms have not fully resolved with treatment. Current guidelines recommend continuing antidepressants for four to six months after remission to prevent relapse. Evidence from many randomized controlled trials indicate continuing antidepressant medications after recovery can reduce the chance of relapse by 70% (41% on placebo vs. 18% on antidepressant). The preventive effect probably lasts for at least the first 36 months of use.People experiencing repeated episodes of depression require ongoing treatment in order to prevent more severe, long-term depression. In some cases, people must take medications for the rest of their lives.Cases when outcome is poor are associated with inappropriate treatment, severe initial symptoms including psychosis, early age of onset, previous episodes, incomplete recovery after one year of treatment, pre-existing severe mental or medical disorder, and family dysfunction.Depressed individuals have a shorter life expectancy than those without depression, in part because depressed patients are at risk of dying of suicide. However, they also have a higher rate of dying from other causes, being more susceptible to medical conditions such as heart disease. Up to 60% of people who die of suicide have a mood disorder such as major depression, and the risk is especially high if a person has a marked sense of hopelessness or has both depression and borderline personality disorder. The lifetime risk of suicide associated with a diagnosis of major depression in the US is estimated at 3.4%, which averages two highly disparate figures of almost 7% for men and 1% for women (although suicide attempts are more frequent in women). The estimate is substantially lower than a previously accepted figure of 15%, which had been derived from older studies of hospitalized patients.Depression is often associated with unemployment and poverty. Major depression is currently the leading cause of disease burden in North America and other high-income countries, and the fourth-leading cause worldwide. In the year 2030, it is predicted to be the second-leading cause of disease burden worldwide after HIV, according to the WHO. Delay or failure in seeking treatment after relapse and the failure of health professionals to provide treatment are two barriers to reducing disability.
Major depressive disorder affects approximately 216 million people in 2015 (3% of the global population). The percentage of people who are affected at one point in their life varies from 7% in Japan to 21% in France. In most countries the number of people who have depression during their lives falls within an 8–18% range. In North America, the probability of having a major depressive episode within a year-long period is 3–5% for males and 8–10% for females. Major depression is about twice as common in women as in men, although it is unclear why this is so, and whether factors unaccounted for are contributing to this. The relative increase in occurrence is related to pubertal development rather than chronological age, reaches adult ratios between the ages of 15 and 18, and appears associated with psychosocial more than hormonal factors. Depression is a major cause of disability worldwide.People are most likely to develop their first depressive episode between the ages of 30 and 40, and there is a second, smaller peak of incidence between ages 50 and 60. The risk of major depression is increased with neurological conditions such as stroke, Parkinson's disease, or multiple sclerosis, and during the first year after childbirth. It is also more common after cardiovascular illnesses, and is related more to those with a poor cardiac disease outcome than to a better one. Studies conflict on the prevalence of depression in the elderly, but most data suggest there is a reduction in this age group. Depressive disorders are more common in urban populations than in rural ones and the prevalence is increased in groups with poorer socioeconomic factors, e.g., homelessness.
The Ancient Greek physician Hippocrates described a syndrome of melancholia as a distinct disease with particular mental and physical symptoms; he characterized all "fears and despondencies, if they last a long time" as being symptomatic of the ailment. It was a similar but far broader concept than today's depression; prominence was given to a clustering of the symptoms of sadness, dejection, and despondency, and often fear, anger, delusions and obsessions were included.The term depression itself was derived from the Latin verb deprimere, "to press down". From the 14th century, "to depress" meant to subjugate or to bring down in spirits. It was used in 1665 in English author Richard Baker's Chronicle to refer to someone having "a great depression of spirit", and by English author Samuel Johnson in a similar sense in 1753. The term also came into use in physiology and economics. An early usage referring to a psychiatric symptom was by French psychiatrist Louis Delasiauve in 1856, and by the 1860s it was appearing in medical dictionaries to refer to a physiological and metaphorical lowering of emotional function. Since Aristotle, melancholia had been associated with men of learning and intellectual brilliance, a hazard of contemplation and creativity. The newer concept abandoned these associations and through the 19th century, became more associated with women.
Although melancholia remained the dominant diagnostic term, depression gained increasing currency in medical treatises and was a synonym by the end of the century; German psychiatrist Emil Kraepelin may have been the first to use it as the overarching term, referring to different kinds of melancholia as depressive states.Sigmund Freud likened the state of melancholia to mourning in his 1917 paper Mourning and Melancholia. He theorized that objective loss, such as the loss of a valued relationship through death or a romantic break-up, results in subjective loss as well; the depressed individual has identified with the object of affection through an unconscious, narcissistic process called the libidinal cathexis of the ego. Such loss results in severe melancholic symptoms more profound than mourning; not only is the outside world viewed negatively but the ego itself is compromised. The patient's decline of self-perception is revealed in his belief of his own blame, inferiority, and unworthiness. He also emphasized early life experiences as a predisposing factor. Adolf Meyer put forward a mixed social and biological framework emphasizing reactions in the context of an individual's life, and argued that the term depression should be used instead of melancholia. The first version of the DSM (DSM-I, 1952) contained depressive reaction and the DSM-II (1968) depressive neurosis, defined as an excessive reaction to internal conflict or an identifiable event, and also included a depressive type of manic-depressive psychosis within Major affective disorders.In the mid-20th century, researchers theorized that depression was caused by a chemical imbalance in neurotransmitters in the brain, a theory based on observations made in the 1950s of the effects of reserpine and isoniazid in altering monoamine neurotransmitter levels and affecting depressive symptoms. The chemical imbalance theory has never been proven.The term "unipolar" (along with the related term "bipolar") was coined by the neurologist and psychiatrist Karl Kleist, and subsequently used by his disciples Edda Neele and Karl Leonhard.The term Major depressive disorder was introduced by a group of US clinicians in the mid-1970s as part of proposals for diagnostic criteria based on patterns of symptoms (called the "Research Diagnostic Criteria", building on earlier Feighner Criteria), and was incorporated into the DSM-III in 1980. To maintain consistency the ICD-10 used the same criteria, with only minor alterations, but using the DSM diagnostic threshold to mark a mild depressive episode, adding higher threshold categories for moderate and severe episodes. The ancient idea of melancholia still survives in the notion of a melancholic subtype.
The new definitions of depression were widely accepted, albeit with some conflicting findings and views. There have been some continued empirically based arguments for a return to the diagnosis of melancholia. There has been some criticism of the expansion of coverage of the diagnosis, related to the development and promotion of antidepressants and the biological model since the late 1950s.
The term "depression" is used in a number of different ways. It is often used to mean this syndrome but may refer to other mood disorders or simply to a low mood. People's conceptualizations of depression vary widely, both within and among cultures. "Because of the lack of scientific certainty," one commentator has observed, "the debate over depression turns on questions of language. What we call it—'disease,' 'disorder,' 'state of mind'—affects how we view, diagnose, and treat it." There are cultural differences in the extent to which serious depression is considered an illness requiring personal professional treatment, or is an indicator of something else, such as the need to address social or moral problems, the result of biological imbalances, or a reflection of individual differences in the understanding of distress that may reinforce feelings of powerlessness, and emotional struggle.The diagnosis is less common in some countries, such as China. It has been argued that the Chinese traditionally deny or somatize emotional depression (although since the early 1980s, the Chinese denial of depression may have modified). Alternatively, it may be that Western cultures reframe and elevate some expressions of human distress to disorder status. Australian professor Gordon Parker and others have argued that the Western concept of depression "medicalizes" sadness or misery. Similarly, Hungarian-American psychiatrist Thomas Szasz and others argue that depression is a metaphorical illness that is inappropriately regarded as an actual disease. There has also been concern that the DSM, as well as the field of descriptive psychiatry that employs it, tends to reify abstract phenomena such as depression, which may in fact be social constructs. American archetypal psychologist James Hillman writes that depression can be healthy for the soul, insofar as "it brings refuge, limitation, focus, gravity, weight, and humble powerlessness." Hillman argues that therapeutic attempts to eliminate depression echo the Christian theme of resurrection, but have the unfortunate effect of demonizing a soulful state of being.
Historical figures were often reluctant to discuss or seek treatment for depression due to social stigma about the condition, or due to ignorance of diagnosis or treatments. Nevertheless, analysis or interpretation of letters, journals, artwork, writings, or statements of family and friends of some historical personalities has led to the presumption that they may have had some form of depression. People who may have had depression include English author Mary Shelley, American-British writer Henry James, and American president Abraham Lincoln. Some well-known contemporary people with possible depression include Canadian songwriter Leonard Cohen and American playwright and novelist Tennessee Williams. Some pioneering psychologists, such as Americans William James and John B. Watson, dealt with their own depression.
There has been a continuing discussion of whether neurological disorders and mood disorders may be linked to creativity, a discussion that goes back to Aristotelian times. British literature gives many examples of reflections on depression. English philosopher John Stuart Mill experienced a several-months-long period of what he called "a dull state of nerves", when one is "unsusceptible to enjoyment or pleasurable excitement; one of those moods when what is pleasure at other times, becomes insipid or indifferent". He quoted English poet Samuel Taylor Coleridge's "Dejection" as a perfect description of his case: "A grief without a pang, void, dark and drear, / A drowsy, stifled, unimpassioned grief, / Which finds no natural outlet or relief / In word, or sigh, or tear." English writer Samuel Johnson used the term "the black dog" in the 1780s to describe his own depression, and it was subsequently popularized by depression sufferer former British Prime Minister Sir Winston Churchill.Social stigma of major depression is widespread, and contact with mental health services reduces this only slightly. Public opinions on treatment differ markedly to those of health professionals; alternative treatments are held to be more helpful than pharmacological ones, which are viewed poorly. In the UK, the Royal College of Psychiatrists and the Royal College of General Practitioners conducted a joint Five-year Defeat Depression campaign to educate and reduce stigma from 1992 to 1996; a MORI study conducted afterwards showed a small positive change in public attitudes to depression and treatment.
Trials are looking at the effects of botulinum toxins on depression. The idea is that the drug is used to make the person look less frowning and that this stops the negative facial feedback from the face. In 2015 results showed, however, that the partly positive effects that had been observed until then could have been due to placebo effects.MRI scans of patients with depression have revealed a number of differences in brain structure compared to those who are not depressed. Meta-analyses of neuroimaging studies in major depression reported that, compared to controls, depressed patients had increased volume of the lateral ventricles and adrenal gland and smaller volumes of the basal ganglia, thalamus, hippocampus, and frontal lobe (including the orbitofrontal cortex and gyrus rectus). Hyperintensities have been associated with patients with a late age of onset, and have led to the development of the theory of vascular depression.
Depression is especially common among those over 65 years of age and increases in frequency beyond this age. In addition, the risk of depression increases in relation to the frailty of the individual. Depression is one of the most important factors which negatively impact quality of life in adults, as well as the elderly. Both symptoms and treatment among the elderly differ from those of the rest of the population.As with many other diseases, it is common among the elderly not to present with classical depressive symptoms. Diagnosis and treatment is further complicated in that the elderly are often simultaneously treated with a number of other drugs, and often have other concurrent diseases. Treatment differs in that studies of SSRIs have shown lesser and often inadequate effects among the elderly, while other drugs, such as duloxetine (a serotonin-norepinephrine reuptake inhibitor), with more clear effects have adverse effects, such as dizziness, dryness of the mouth, diarrhea and constipation, which can be especially difficult to handle among the elderly.Problem solving therapy was, as of 2015, the only psychological therapy with proven effect, and can be likened to a simpler form of cognitive behavioral therapy. However, elderly with depression are seldom offered any psychological treatment, and the evidence proving other treatments effective is incomplete. ECT has been used in the elderly, and register-studies suggest it is effective, although less so as compared to the rest of the population.The risks involved with treatment of depression among the elderly as opposed to benefits are not entirely clear.
Models of depression in animals for the purpose of study include iatrogenic depression models (such as drug-induced), forced swim tests, tail suspension test, and learned helplessness models. Criteria frequently used to assess depression in animals include expression of despair, neurovegetative changes, and anhedonia, as many other criteria for depression are untestable in animals, such as guilt and suicidality.

Major urinary proteins (Mups), also known as α2u-globulins, are a subfamily of proteins found in abundance in the urine and other secretions of many animals. Mups provide a small range of identifying information about the donor animal, when detected by the vomeronasal organ of the receiving animal. They belong to a larger family of proteins known as lipocalins. Mups are encoded by a cluster of genes, located adjacent to each other on a single stretch of DNA, that varies greatly in number between species: from at least 21 functional genes in mice to none in humans. Mup proteins form a characteristic glove shape, encompassing a ligand-binding pocket that accommodates specific small organic chemicals.
Urinary proteins were first reported in rodents in 1932, during studies by Thomas Addis into the cause of proteinuria. They are potent human allergens and are largely responsible for a number of animal allergies, including to cats, horses and rodents. Their endogenous function within an animal is unknown but may involve regulating energy expenditure. However, as secreted proteins they play multiple roles in chemical communication between animals, functioning as pheromone transporters and stabilizers in rodents and pigs. Mups can also act as protein pheromones themselves. They have been demonstrated to promote aggression in male mice, and one specific Mup protein found in male mouse urine is sexually attractive to female mice. Mups can also function as signals between different species: mice display an instinctive fear response on the detection of Mups derived from predators such as cats and rats.
Humans in good health excrete urine that is largely free of protein. Therefore, since 1827 physicians and scientists have been interested in proteinuria, the excess of protein in human urine, as an indicator of kidney disease. To better understand the etiology of proteinuria, some scientists attempted to study the phenomenon in laboratory animals. Between 1932 and 1933 a number of scientists, including Thomas Addis, independently reported the surprising finding that some healthy rodents have protein in their urine. However, it was not until the 1960s that the major urinary proteins of mice and rats were first described in detail. It was found that the proteins are primarily made in the liver of males and secreted through the kidneys into the urine in large quantities (milligrams per day).Since they were named, the proteins have been found to be differentially expressed in other glands that secrete products directly into the external environment. These include lacrimal, parotid, submaxillary, sublingual, preputial and mammary glands. In some species, such as cats and pigs, Mups appear not to be expressed in urine at all and are mainly found in saliva. Sometimes the term urinary Mups (uMups) is used to distinguish those Mups expressed in urine from those in other tissues.
Between 1979 and 1981, it was estimated that Mups are encoded by a gene family of between 15 and 35 genes and pseudogenes in the mouse and by an estimated 20 genes in the rat. In 2008 a more precise number of Mup genes in a range of species was determined by analyzing the DNA sequence of whole genomes.
The mouse reference genome has at least 21 distinct Mup genes (with open reading frames) and a further 21 Mup pseudogenes (with reading frames disrupted by a nonsense mutation or an incomplete gene duplication). They are all clustered together, arrayed side by side across 1.92 megabases of DNA on chromosome 4. The 21 functional genes have been divided into two sub-classes based on position and sequence similarity: 6 peripheral Class A Mups and 15 central Class B Mups. The central Class B Mup gene cluster formed through a number of sequential duplications from one of the Class A Mups. As all the Class B genes are almost identical to each other, researchers have concluded that these duplications occurred very recently in mouse evolution. Indeed, the repetitive structure of these central Mup genes means they are likely to be unstable and may vary in number among wild mice. The Class A Mups are more different from each other and are therefore likely to be more stable, older genes, but what, if any, functional differences the classes have are unknown. The similarity between the genes makes the region difficult to study using current DNA sequencing technology. Consequently, the Mup gene cluster is one of the few parts of the mouse whole genome sequence with gaps remaining, and further genes may remain undiscovered.Rat urine also contains homologous urinary proteins; although they were originally given a different name, α2u-globulins, they have since become known as rat Mups. Rats have 9 distinct Mup genes and a further 13 pseudogenes clustered together across 1.1 megabases of DNA on chromosome 5. Like in mice, the cluster formed by multiple duplications. However, this occurred independently of the duplications in mice, meaning that both rodent species expanded their Mup gene families separately, but in parallel.
Most other mammals studied, including the pig, cow, cat, dog, bushbaby, macaque, chimpanzee and orangutan, have a single Mup gene. Some, however, have an expanded number: horses have three Mup genes, and gray mouse lemurs have at least two. Insects, fish, amphibia, birds and marsupials appear to have disrupted synteny at the chromosomal position of the Mup gene cluster, suggesting the gene family may be specific to placental mammals. Humans are the only placental mammals found not to have any active Mup genes; instead, they have a single Mup pseudogene containing a mutation that causes missplicing, rendering it dysfunctional.
Mups are members of a large family of low-molecular weight (~19 kDa) proteins known as lipocalins. They have a characteristic structure of eight beta sheets arranged in an anti-parallel beta barrel open on one face, with alpha helices at both ends. Consequently, they form a characteristic glove shape, encompassing a cup-like pocket that binds small organic chemicals with high affinity. A number of these ligands bind to mouse Mups, including 2-sec-butyl-4,5-dihydrothiazole (abbreviated as SBT or DHT), 6-hydroxy-6-methyl-3-heptanone (HMH) and 2,3 dihydro-exo-brevicomin (DHB). These are all urine-specific chemicals that have been shown to act as pheromones—molecular signals excreted by one individual that trigger an innate behavioural response in another member of the same species. Mouse Mups have also been shown to function as pheromone stabilizers, providing a slow release mechanism that extends the potency of volatile pheromones in male urine scent marks. Given the diversity of Mups in rodents, it was originally thought that different Mups may have differently shaped binding pockets and therefore bind different pheromones. However, detailed studies found that most variable sites are located on the surface of the proteins and appear to have little effect on ligand binding.Rat Mups bind different small chemicals. The most common ligand is 1-Chlorodecane,  with 2-methyl-N-phenyl-2-propenamide, hexadecane and 2,6,11-trimethyl decane found to be less prominent. Rat Mups also bind limonene-1,2-epoxide, resulting in a disease of the host's kidney, hyaline-droplet nephropathy, that progresses to cancer. Other species do not develop this disorder because their Mups do not bind that particular chemical. Accordingly, when transgenic mice were engineered to express the rat Mup, their kidneys developed the disease.
The Mup found in pigs, named salivary lipocalin (SAL), is expressed in the salivary gland of males where it tightly binds androstenone and androstenol, both pheromones that cause female pigs to assume a mating stance.Isothermal titration calorimetry studies performed with Mups and associated ligands (pyrazines, alcohols, thiazolines, 6-hydroxy-6-methyl-3-heptanone, and N-phenylnapthylamine,) revealed an unusual binding phenomena. The active site has been found to be suboptimally hydrated, resulting in ligand binding being driven by enthalpic dispersion forces. This is contrary to most other proteins, which exhibit entropy-driven binding forces from the reorganisation of water molecules. This unusual process has been termed the nonclassical hydrophobic effect.
Studies have sought to find the precise function of Mups in pheromone communication. Mup proteins have been shown to promote puberty and accelerate the estrus cycle in female mice, inducing the Vandenbergh and Whitten effects. However, in both cases the Mups had to be presented to the female dissolved in male urine, indicating that the protein requires some urinary context to function. In 2007 Mups normally found in male mouse urine were made in transgenic bacteria, and therefore created devoid of the chemicals they normally bind. These Mups were shown to be sufficient to promote aggressive behaviour in males, even in the absence of urine. In addition, Mups made in bacteria were found to activate olfactory sensory neurons in the vomeronasal organ (VNO), a subsystem of the nose known to detect pheromones via specific sensory receptors, of mice and rats. Together, this demonstrated that Mup proteins can act as pheromones themselves, independent of their ligands.
Consistent with a role in male-male aggression, adult male mice secrete significantly more Mups into their urine than females, juveniles or castrated male mice. The precise mechanism driving this difference between the sexes is complex, but at least three hormones—testosterone, growth hormone and thyroxine—are known to positively influence the production of Mups in mice. Wild house mouse urine contains variable combinations of four to seven distinct Mup proteins per mouse. Some inbred laboratory mouse strains, such as BALB/c and C57BL/6, also have different proteins expressed in their urine. However, unlike wild mice, different individuals from the same strain express the same protein pattern, an artifact of many generations of inbreeding. One unusual Mup is less variable than the others: it is consistently produced by a high proportion of wild male mice and is almost never found in female urine. When this Mup was made in bacteria and used in behavioural testing, it was found to attract female mice. Other Mups were tested but did not have the same attractive qualities, suggesting the male-specific Mup acts as a sex pheromone. Scientists named this Mup darcin as a humorous reference to Fitzwilliam Darcy, the romantic hero from Pride and Prejudice. Taken together, the complex patterns of Mups produced has the potential to provide a range of information about the donor animal, such as gender, fertility, social dominance, age, genetic diversity or kinship. Wild mice (unlike laboratory mice that are genetically identical and which therefore also have identical patterns of Mups in the urine) have individual patterns of Mup expression in their urine that act as a "barcode" to uniquely identify the owner of a scent mark.In the house mouse, the major MUP gene cluster provides a highly polymorphic scent signal of genetic identity.  Wild mice breeding freely in semi-natural enclosures showed inbreeding avoidance.  This avoidance resulted from a strong deficit in successful matings between mice sharing both MUP haplotypes (complete match). In another study, using white-footed mice, it was found that when mice derived from wild populations were inbred, there was reduced survival when such mice were reintroduced into a natural habitat.  These findings suggest that inbreeding reduces fitness, and that scent signal recognition has evolved in mice as a means of avoiding inbreeding depression.
In addition to serving as social cues between members of the same species, Mups can act as kairomones—chemical signals that transmit information between species. Mice are instinctively afraid of the smell of their natural predators, including cats and rats. This occurs even in laboratory mice that have been isolated from predators for hundreds of generations. When the chemical cues responsible for the fear response were purified from cat saliva and rat urine, two homologous protein signals were identified: Fel d 4 (Felis domesticus allergen 4), the product of the cat Mup gene, and Rat n 1 (Rattus norvegicus allergen 1), the product of the rat Mup13 gene. Mice are fearful of these Mups even when they are made in bacteria, but mutant animals that are unable to detect the Mups showed no fear of rats, demonstrating their importance in initiating fearful behaviour. It is not known exactly how Mups from different species initiate disparate behaviours, but mouse Mups and predator Mups have been shown to activate unique patterns of sensory neurons in the nose of recipient mice. This implies the mouse perceives them differently, via distinct neural circuits. The pheromone receptors responsible for Mup detection are also unknown, though they are thought be members of the V2R receptor class.
Along with other members of the lipocalin protein family, major urinary proteins can be potent allergens to humans. The reason for this is not known; however, molecular mimicry between Mups and structurally similar human lipocalins has been proposed as a possible explanation. The protein product of the mouse Mup17 gene, known as Mus m 1, Ag1 or MA1, accounts for much of the allergenic properties of mouse urine. The protein is extremely stable in the environment; studies have found 95% of inner city homes and 82% of all types of homes in the United States have detectable levels in at least one room. Similarly, Rat n 1 is a known human allergen. A US study found its presence in 33% of inner city homes, and 21% of occupants were sensitized to the allergen. Exposure and sensitization to rodent Mup proteins is considered a risk factor for childhood asthma and is a leading cause of laboratory animal allergy (LAA)—an occupational disease of laboratory animal technicians and scientists. One study found that two-thirds of laboratory workers who had developed asthmatic reactions to animals had antibodies to Rat n 1.Mup genes from other mammals also encode allergenic proteins, for example Fel d 4 is primarily produced in the submandibular salivary gland and is deposited onto dander as the cat grooms itself. A study found that 63% of cat allergic people have antibodies against the protein. Most had higher titres of antibodies against Fel d 4 than against Fel d 1, another prominent cat allergen. Likewise, Equ c1 (Equus caballus allergen 1) is the protein product of a horse Mup gene that is found in the liver, sublingual and submaxillary salivary glands. It is responsible for about 80% of the antibody response in patients who are chronically exposed to horse allergens.
While the detection of Mups excreted by other animals has been well studied, the functional role in the producing animal is less clear. However, in 2009, Mups were shown to be associated with the regulation of energy expenditure in mice. Scientists found that genetically induced obese, diabetic mice produce thirty times less Mup RNA than their lean siblings. When they delivered Mup protein directly into the bloodstream of these mice, they observed an increase in energy expenditure, physical activity and body temperature and a corresponding decrease in glucose intolerance and insulin resistance. They propose that Mups' beneficial effects on energy metabolism occurs by enhancing mitochondrial function in skeletal muscle. Another study found Mups were reduced in diet-induced obese mice. In this case, the presence of Mups in the bloodstream of mice restricted glucose production by directly inhibiting the expression of genes in the liver.
Fear Signals from Predators on YouTube, a video describing the research that determined Mups were kairomones
Majungasaurus (; "Mahajanga lizard") is a genus of abelisaurid theropod dinosaur that lived in Madagascar from 70 to 66 million years ago, at the end of the Cretaceous Period. The genus contains a single species, Majungasaurus crenatissimus. This dinosaur was briefly called Majungatholus, a name which is now considered a junior synonym of Majungasaurus.
Like other abelisaurids, Majungasaurus was a bipedal predator with a short snout. Although the forelimbs are not completely known, they were very short, while the hind limbs were longer and very stocky. It can be distinguished from other abelisaurids by its wider skull, the very rough texture and thickened bone on the top of its snout, and the single rounded horn on the roof of its skull, which was originally mistaken for the dome of a pachycephalosaur. It also had more teeth in both upper and lower jaws than most abelisaurids.
Known from several well-preserved skulls and abundant skeletal material, Majungasaurus has recently become one of the best-studied theropod dinosaurs from the Southern Hemisphere. It appears to be most closely related to abelisaurids from India rather than South America or continental Africa, a fact that has important biogeographical implications. Majungasaurus was the apex predator in its ecosystem, mainly preying on sauropods like Rapetosaurus, and is also one of the few dinosaurs for which there is direct evidence of cannibalism.
Majungasaurus was a medium-sized theropod that typically measured 6–7 meters (19.7–23.0 ft) in length, including its tail. Fragmentary remains of larger individuals indicate that some adults reached lengths of more than 8 meters (26.2 ft). An allometric study in 2016 found it to be 5.6 meters (18.4 ft) long. Sampson and Witmer estimated an average weight for an adult Majungasaurus of 1,100 kilograms (2,400 lb). The specimen they based it on (FMNH PR 2100) was not the largest one discovered. Larger specimens of Majungasaurus crenatissimus could have been similar in size to its relative Carnotaurus, which has been estimated to weigh 1,500 kilograms (3,300 lb).The skull of Majungasaurus is exceptionally well-known compared to most theropods and generally similar to that of other abelisaurids. Like other abelisaurid skulls, its length was proportionally short for its height, although not as short as in Carnotaurus. The skulls of large individuals measured 60–70 centimeters (24–28 in) long. The tall premaxilla (frontmost upper jaw bone), which made the tip of the snout very blunt, was also typical of the family. However, the skull of Majungasaurus was markedly wider than in other abelisaurids. All abelisaurids had a rough, sculptured texture on the outside faces of the skull bones, and Majungasaurus was no exception. This was carried to an extreme on the nasal bones of Majungasaurus, which were extremely thick and fused together, with a low central ridge running along the half of the bone closest to the nostrils. A distinctive dome-like horn protruded from the fused frontal bones on top of the skull as well. In life, these structures would have been covered with some sort of integument, possibly made of keratin. Computed tomography (CT scanning) of the skull shows that both the nasal structure and the frontal horn contained hollow sinus cavities, perhaps to reduce weight. The teeth were typical of abelisaurids in having short crowns, although Majungasaurus bore seventeen teeth in both the maxilla of the upper jaw and the dentary of the lower jaw, more than in any other abelisaurid except Rugops.
The postcranial skeleton of Majungasaurus closely resembles those of Carnotaurus and Aucasaurus, the only other abelisaurid genera for which complete skeletal material is known. Majungasaurus was bipedal, with a long tail to balance out the head and torso, putting the center of gravity over the hips. Although the cervical (neck) vertebrae had numerous cavities and excavations (pleurocoels) to reduce their weight, they were robust, with exaggerated muscle attachment sites and ribs that interlocked for strength. Ossified tendons attached to the cervical ribs, giving them a forked appearance, as seen in Carnotaurus. All of these features resulted in a very strong and muscular neck. Uniquely, the cervical ribs of Majungasaurus had long depressions along the sides for weight reduction. The humerus (upper arm bone) was short and curved, closely resembling those of Aucasaurus and Carnotaurus. Also like related dinosaurs, Majungasaurus had very short forelimbs with four extremely reduced digits, first reported with only two very short external fingers and no claws. The hand and finger bones of Majungasaurus, like other majungasaurines, lacked the characteristic pits and grooves where claws and tendons would normally attach, and its finger bones were fused together, indicating that the hand was immobile. In 2012, a better specimen was described, showing that the lower arm was robust, though short, and that the hand contained four metatarsals and four, probably inflexible and very reduced, fingers, with small claws on the second and third finger. The phalanx formula was 1-2-2-1-0.Like other abelisaurids, the hindlimbs were stocky and short compared to body length. The tibia (lower leg bone) of Majungasaurus was even stockier than that of its relative Carnotaurus, with a prominent crest on the knee. The astragalus and calcaneum (ankle bones) were fused together, and the feet bore three functional digits, with a smaller first digit that did not contact the ground.
French paleontologist Charles Depéret described the first theropod remains from northwestern Madagascar in 1896. These included two teeth, a claw, and some vertebrae discovered along the Betsiboka River by a French army officer and deposited in the collection of what is now the Université Claude Bernard Lyon 1. Depéret referred these fossils to the genus Megalosaurus, which at the time was a wastebasket taxon containing any number of unrelated large theropods, as the new species M. crenatissimus. This name is derived from the Latin word crenatus ("notched") and the suffix -issimus ("most"), in reference to the numerous serrations on both front and rear edges of the teeth. Depéret later reassigned the species to the North American genus Dryptosaurus, another poorly known taxon.
Numerous fragmentary remains from Mahajanga Province in northwestern Madagascar were recovered by French collectors over the next 100 years, many of which were deposited in the Muséum National d'Histoire Naturelle in Paris. In 1955, René Lavocat described a theropod dentary (MNHN.MAJ 1) with teeth from the Maevarano Formation in the same region where the original material was found. The teeth matched those first described by Depéret, but the strongly curved jaw bone was very different from both Megalosaurus and Dryptosaurus. Based on this dentary, Lavocat created the new genus Majungasaurus, using an older spelling of Mahajanga as well as the Greek word σαυρος/sauros (meaning "lizard"). Hans-Dieter Sues and Philippe Taquet described a dome-shaped skull fragment (MNHN.MAJ 4) as a new genus of pachycephalosaur (Majungatholus atopus) in 1979. This was the first report of a pachycephalosaur in the Southern Hemisphere.In 1993, scientists from the State University of New York at Stony Brook and the University of Antananarivo began the Mahajanga Basin Project, a series of expeditions to examine the fossils and geology of the Late Cretaceous sediments near the village of Berivotra, in Mahajanga Province. Among these scientists was paleontologist David W. Krause of Stony Brook.  The first expedition turned up hundreds of theropod teeth identical to those of Majungasaurus, some of which were attached to an isolated premaxilla that was described in 1996. The following seven expeditions would turn up tens of thousands of fossils, many of which belonged to species new to science. The Mahajanga Basin Project claims credit for quintupling the known diversity of fossil taxa in the region.
Fieldwork in 1996 turned up a spectacularly complete theropod skull preserved in exquisite detail (FMNH PR 2100). On top of this skull was a dome-shaped swelling nearly identical to the one described by Sues and Taquet as Majungatholus atopus. Majungatholus was redescribed as an abelisaurid rather than a pachycephalosaur in 1998. Although the name Majungasaurus crenatissimus was older than Majungatholus atopus, the authors judged the type dentary of Majungasaurus too fragmentary to confidently assign to the same species as the skull. Further fieldwork over the next decade turned up a series of less complete skulls, as well as dozens of partial skeletons of individuals ranging from juveniles to adults. Project members also collected hundreds of isolated bones and thousands of shed Majungasaurus teeth. Taken together, these remains represent nearly all the bones of the skeleton, although most of the forelimbs, most of the pelvis and the tip of the tail are still unknown. This fieldwork culminated in a 2007 monograph consisting of seven scientific papers on all aspects of the animal's biology, published in the Society of Vertebrate Paleontology Memoirs. The papers are in English, although each has an abstract written in Malagasy. In this volume, the dentary described by Lavocat was re-evaluated and determined to be diagnostic for this species. Therefore, the name Majungatholus was replaced by the older name Majungasaurus. Although the monograph is comprehensive, the editors noted that it describes only material recovered from 1993 through 2001. A significant quantity of specimens, some very complete, were excavated in 2003 and 2005 and await preparation and description in future publications. The dentary was made the neotype specimen after a 2009 petition to the ICZN.
Majungasaurus is classified as a member of the theropod clade Abelisauridae, which is considered a family in Linnaean taxonomy. Along with the family Noasauridae, abelisaurids are included in the superfamily Abelisauroidea, which is in turn a subdivision of the infraorder Ceratosauria. Abelisaurids are known for their tall skulls with blunt snouts, extensive sculpturing on the outer surfaces of the facial bones (convergent with carcharodontosaurids), very reduced (atrophied) forelimbs (convergent with tyrannosaurids), and stocky hindlimb proportions, among other features.As with many dinosaur families, the systematics (evolutionary relationships) within the family Abelisauridae are confused. Several cladistic studies have indicated that Majungasaurus shares a close relationship with Carnotaurus from South America, while others were unable to firmly place it in the phylogeny. The most recent analysis, using the most complete information, instead recovered Majungasaurus in a clade with Rajasaurus and Indosaurus from India, but excluding South American genera like Carnotaurus, Ilokelesia, Ekrixinatosaurus, Aucasaurus and Abelisaurus, as well as Rugops from mainland Africa. This leaves open the possibility of separate clades of abelisaurids in western and eastern Gondwana.
A cladogram by Tortosa et al. 2013 places Majungasaurus in a new subfamily, Majungasaurinae. A simplified version showing the taxa within the group is shown below.
Majungasaurus is perhaps most distinctive for its skull ornamentation, including the swollen and fused nasals and the frontal horn. Other ceratosaurs, including Carnotaurus, Rajasaurus, and Ceratosaurus itself bore crests on the head. These structures are likely to have played a role in intraspecific competition, although their exact function within that context is unknown. The hollow cavity inside the frontal horn of Majungasaurus would have weakened the structure and probably precluded its use in direct physical combat, although the horn may have served a display purpose. While there is variation in the ornamentation of Majungasaurus individuals, there is no evidence for sexual dimorphism.
Scientists have suggested that the unique skull shape of Majungasaurus and other abelisaurids indicate different predatory habits than other theropods. Whereas most theropods were characterized by long, low skulls of narrow width, abelisaurid skulls were taller and wider, and often shorter in length as well. The narrow skulls of other theropods were well equipped to withstand the vertical stress of a powerful bite, but not as good at withstanding torsion (twisting). In comparison to modern mammalian predators, most theropods may have used a strategy similar in some ways to that of long- and narrow-snouted canids, with the delivery of many bites weakening the prey animal.Abelisaurids, especially Majungasaurus, may instead have been adapted for a feeding strategy more similar to modern felids, with short and broad snouts, that bite once and hold on until the prey is subdued. Majungasaurus had an even broader snout than other abelisaurids, and other aspects of its anatomy may also support the bite-and-hold hypothesis. The neck was strengthened, with robust vertebrae, interlocking ribs and ossified tendons, as well as reinforced muscle attachment sites on the vertebrae and the back of the skull. These muscles would have been able to hold the head steady despite the struggles of its prey. Abelisaurid skulls were also strengthened in many areas by bone mineralized out of the skin, creating the characteristic rough texture of the bones. This is particularly true of Majungasaurus, where the nasal bones were fused and thickened for strength. On the other hand, the lower jaw of Majungasaurus sported a large fenestra (opening) on each side, as seen in other ceratosaurs, as well as synovial joints between certain bones that allowed a high degree of flexibility in the lower jaw, although not to the extent seen in snakes. This may have been an adaptation to prevent the fracture of the lower jaw when holding onto a struggling prey animal. The front teeth of the upper jaw were more robust than the rest, to provide an anchor point for the bite, while the low crown height of Majungasaurus teeth prevented them from breaking off during a struggle. Finally, unlike the teeth of Allosaurus and most other theropods, which were curved on both the front and back, abelisaurids like Majungasaurus had teeth curved on the front edge but straighter on the back (cutting) edge. This structure may have served to prevent slicing, and instead holding the teeth in place when biting.
Majungasaurus was the largest predator in its environment, while the only known large herbivores at the time were sauropods like Rapetosaurus. Scientists have suggested that Majungasaurus, and perhaps other abelisaurids, specialized on hunting sauropods. Adaptations to strengthen the head and neck for a bite-and-hold type of attack might have been very useful against sauropods, which would have been tremendously powerful animals. This hypothesis may also be supported by the hindlegs of Majungasaurus, which were short and stocky, as opposed to the longer and more slender legs of most other theropods. While Majungasaurus would not have moved as fast as other similar-sized theropods, it would have had no trouble keeping up with slow-moving sauropods. The robust hindlimb bones suggest very powerful legs, and their shorter length would have lowered the animal's center of gravity. Thus Majungasaurus may have sacrificed speed for power. Majungasaurus tooth marks on Rapetosaurus bones confirm that it at least fed on these sauropods, whether or not it actually killed them.
Although sauropods may have been the prey of choice for Majungasaurus, discoveries published in 2007 detail finds in Madagascar that indicate the presence of other Majungasaurus in their diet. Numerous bones of Majungasaurus have been discovered bearing tooth marks identical to those found on sauropod bones from the same localities. These marks have the same spacing as teeth in Majungasaurus jaws, are of the same size as Majungasaurus teeth, and contain smaller notches consistent with the serrations on those teeth. As Majungasaurus is the only large theropod known from the area, the simplest explanation is that it was feeding on other members of its own species. Suggestions that the Triassic Coelophysis was a cannibal have been recently disproven, leaving Majungasaurus as the only non-avian theropod with confirmed cannibalistic tendencies, although there is some evidence that cannibalism may have occurred in other species as well.It is unknown if Majungasaurus actively hunted their own kind or only scavenged their carcasses. However, some researchers have noted that modern Komodo monitors sometimes kill each other when competing for access to carcasses. The lizards will then proceed to cannibalize the remains of their rivals, which may suggest similar behavior in Majungasaurus and other theropods.
Scientists have reconstructed the respiratory system of Majungasaurus based on a superbly preserved series of vertebrae (UA 8678) recovered from the Maevarano Formation. Most of these vertebrae and some of the ribs contained cavities (pneumatic foramina) that may have resulted from the infiltration of avian-style lungs and air sacs. In birds, the neck vertebrae and ribs are hollowed out by the cervical air sac, the upper back vertebrae by the lung, and the lower back and sacral (hip) vertebrae by the abdominal air sac. Similar features in Majungasaurus vertebrae imply the presence of these air sacs. These air sacs may have allowed for a basic form of avian-style 'flow-through ventilation,' where air flow through the lungs is one-way, so that oxygen-rich air inhaled from outside the body is never mixed with exhaled air laden with carbon dioxide. This method of respiration, while complicated, is highly efficient.The recognition of pneumatic foramina in Majungasaurus, besides providing an understanding of its respiratory biology, also has larger-scale implications for evolutionary biology. The split between the ceratosaur line, which led to Majungasaurus, and the tetanuran line, to which birds belong, occurred very early in the history of theropods. The avian respiratory system, present in both lines, must therefore have evolved before the split, and well before the evolution of birds themselves. This provides further evidence of the dinosaurian origin of birds.
Computed tomography, also known as CT scanning, of a complete Majungasaurus skull (FMNH PR 2100) allowed a rough reconstruction of its brain and inner ear structure. Overall, the brain was very small relative to body size, but otherwise similar to many other non-coelurosaurian theropods, with a very conservative form closer to modern crocodilians than to birds. One difference between Majungasaurus and other theropods was its smaller flocculus, a region of the cerebellum that helps to coordinate movements of the eye with movements of the head. This suggests that Majungasaurus and other abelisaurids like Indosaurus, which also had a small flocculus, did not rely on quick head movements to sight and capture prey.Inferences about behavior can also be drawn from examination of the inner ear. The semicircular canals within the inner ear aid in balance, and the lateral semicircular canal is usually parallel to the ground when the animal holds its head in an alert posture. When the skull of Majungasaurus is rotated so that its lateral canal is parallel to the ground, the entire skull is nearly horizontal. This contrasts with many other theropods, where the head was more strongly downturned when in the alert position. The lateral canal is also significantly longer in Majungasaurus than in its more basal relative Ceratosaurus, indicating a greater sensitivity to side-to-side motions of the head.
A 2007 report described pathologies in the bones of Majungasaurus. Scientists examined the remains of at least 21 individuals and discovered four with noticeable pathologies. While pathology had been studied in large tetanuran theropods like allosaurids and tyrannosaurids, this was the first time an abelisauroid had been examined in this manner. No wounds were found on any skull elements, in contrast to tyrannosaurids where sometimes gruesome facial bites were common. One of the specimens was a phalanx (toe bone) of the foot, which had apparently been broken and subsequently healed.Most of the pathologies occurred on the vertebrae. For example, a dorsal (back) vertebra from a juvenile animal showed an exostosis (bony growth) on its underside. The growth probably resulted from the conversion of cartilage or a ligament to bone during development, but the cause of the ossification was not determined. Hypervitaminosis A and bone spurs were ruled out, and an osteoma (benign bone tumor) was deemed unlikely. Another specimen, a small caudal (tail) vertebra, was also found to have an abnormal growth, this time on the top of its neural spine, which projects upwards from the vertebrae, allowing muscle attachment. Similar growths from the neural spine have been found in specimens of Allosaurus and Masiakasaurus, probably resulting from the ossification of a ligament running either between the neural spines  (interspinal ligament) or along their tops (supraspinal ligament).The most serious pathology discovered was in a series of five large tail vertebrae. The first two vertebrae showed only minor abnormalities with the exception of a large groove that extended along the left side of both bones. However, the next three vertebrae were completely fused together at many different points, forming a solid bony mass. There is no sign of any other vertebrae after the fifth in the series, indicating that the tail ended there prematurely. From the size of the last vertebrae, scientists judged that about ten vertebrae were lost. One explanation for this pathology is severe physical trauma resulting in the loss of the tail tip, followed by osteomyelitis (infection) of the last remaining vertebrae. Alternatively, the infection may have come first and led to the end of the tail becoming necrotic and falling off. This is the first example of tail truncation known in a non-avian theropod dinosaur.
Majungasaurus, being known from many well-preserved specimens of different ages, is well studied in regards to its growth and development. Throughout ontogeny, the skull of Majungasaurus (more specifically, the jugal, postorbital, and quadratojugal) seems to have become taller and more robust; additionally, the skull bones became more fused and the eye sockets became proportionally smaller. This indicates a shift in dietary preferences between juveniles and adults.Research by Michael D'Emic et al indicates that it was among the slowest-growing theropods. Based on studies of the lines of arrested growth in several bones, it was found that Majungasaurus took twenty years to reach maturity, which may have been a result of the harsh environment in which it lived. However, other abelisaurids have also been found to have comparably slow growth rates.
All specimens of Majungasaurus have been recovered from the Maevarano Formation in the Mahajanga Province in northwestern Madagascar. Most of these, including all of the most complete material, came from the Anembalemba Member, although Majungasaurus teeth have also been found in the underlying Masorobe Member and the overlying Miadana Member. While these sediments have not been dated radiometrically, evidence from biostratigraphy and paleomagnetism suggest that they were deposited during the Maastrichtian stage, which lasted from 70 to 66 Ma (million years ago). Majungasaurus teeth are found up until the very end of the Maastrichtian, when all non-avian dinosaurs became extinct.
Then as now, Madagascar was an island, having separated from the Indian subcontinent less than 20 million years earlier. It was drifting northwards but still 10–15° more southerly in latitude than it is today. The prevailing climate of the time was semi-arid, with pronounced seasonality in temperature and rainfall. Majungasaurus inhabited a coastal flood plain cut by many sandy river channels. Strong geological evidence suggests the occurrence of periodic debris flows through these channels at the beginning of the wet season, burying the carcasses of organisms killed during the preceding dry season and providing for their exceptional preservation as fossils. Sea levels in the area were rising throughout the Maastrichtian, and would continue to do so into the Paleocene Epoch, so Majungasaurus may have roamed coastal environments like tidal flats as well. The neighboring Berivotra Formation represents the contemporaneous marine environment.Besides Majungasaurus, fossil taxa recovered from the Maevarano include fish, frogs, lizards, snakes, seven distinct species of crocodylomorphs, five or six species of mammals, Vorona and several other birds, the possibly flighted dromaeosaurid Rahonavis, the noasaurid Masiakasaurus and two titanosaurian sauropods, including Rapetosaurus. Majungasaurus was by far the largest carnivore and probably the dominant predator on land, although large crocodylomorphs like Mahajangasuchus and Trematochampsa might have competed with it closer to water.

Make Way for Ducklings is a children's picture book written and illustrated by Robert McCloskey. First published in 1941, the book tells the story of a pair of mallards who decide to raise their family on an island in the lagoon in Boston Public Garden, a park in the center of Boston.
Make Way for Ducklings won the 1942 Caldecott Medal for McCloskey's illustrations, executed in charcoal then lithographed on zinc plates. As of 2003, the book had sold over two million copies. The book's popularity led to the construction of a statue by Nancy Schön in the Public Garden of the mother duck and her eight ducklings, which is a popular destination for children and adults alike. In 1991, Barbara Bush gave a duplicate of this sculpture to Raisa Gorbachev as part of the START Treaty, and the work is displayed in Moscow's Novodevichy Park.The book is the official children's book of the Commonwealth of Massachusetts. Praise for the book is still high over 70 years since its first publication, mainly for the enhancing illustrations and effective pacing. It was criticised for having a loose plot, however. The book is popular worldwide.
Make Way for Ducklings, published in 1941, was McCloskey's second book and was the winner of the Caldecott Medal in 1942. In his acceptance speech, McCloskey explained his motivation for the story. While attending the Vesper George Art School in the early 1930s, he would spend time in the Public Garden feeding the ducks. After some time away, he returned to Boston to paint a mural and created a draft of the book after inspiration from May Massee. To better illustrate the story, McCloskey spent time at the American Museum of Natural History in New York, visited an ornithologist, and eventually brought home for models six ducklings to live in his studio at 280 West 12th Street, apartment 4C, in New York's West Village.
The story begins as two ducks (Mr. and Mrs. Mallard) fly over various potential locations around the city of Boston, Massachusetts (United States) to start a family. Each time Mr. Mallard selects a location, Mrs. Mallard finds something wrong with it. Tired from their search, the mallards land at the Public Garden Lagoon to spend the night. In the morning, a swan boat passes by the mallards. The mallards mistake the swan boat for a real bird and enjoy peanuts thrown by the people on the boat. Mrs. Mallard suggests that they build their nest in the Public Garden. However, just as she says this, her husband is nearly run down by a passing bicyclist. The mallards continue their search, flying over Boston landmarks such as Beacon Hill, the Massachusetts State House, and Louisburg Square. The Mallards finally decide on an island in the Charles River. From this island, the Mallards visit a policeman named Michael on the shore, who feeds them peanuts every day.
Shortly thereafter, the Mallards molt, and will not be able to fly until their new feathers grow again, and Mrs. Mallard hatches eight ducklings named Jack, Kack, Lack, Mack, Nack, Ouack, Pack, and Quack. After the ducklings are born, Mr. Mallard decides to take a trip up the river to see what the rest of it is like. Mr. and Mrs. Mallard agree to meet at the Public Garden in one week. In the meantime, Mrs. Mallard teaches the eight ducklings all they need to know about being ducks, such as swimming, diving, marching along, and to avoid dangers such as bicycles and other wheeled objects.
One week later, Mrs. Mallard leads the ducklings ashore and straight to the highway in hopes of crossing to reach the Garden, but she has trouble crossing as the cars will not yield to her. Michael, the policeman who the Mallards visited, stops traffic for the family to cross. Michael calls police headquarters and instructs them to send a police car to stop traffic along the route for the ducks. The ducks cross the highway, Embankment Road (Storrow Drive had yet to be constructed when the book was written), then proceed down Mount Vernon Street to Charles Street where they head south to the Garden. The people on the streets admire the family of ducks. When the family must cross Beacon Street to enter the Garden, there are four policemen standing in the intersection stopping traffic to make way for the ducklings. Mr. Mallard is waiting in the Public Garden for the rest of the family. Finally, the family decides to stay in the Garden and lives happily ever after. They end each day searching for peanuts and food, and when night falls, they swim to their little island and go to sleep.
Illustrated with sepia drawings rather than the traditional black-and-white pictures found in most children's books of the day, the book, which received the 1942 Caldecott Medal for its illustrations, has continued to garner praise years after its first publishing.
The drawings of Boston represent a duck's eye view of the city. Each of the individual ducklings is "bored, inquisitive, sleepy, or they are scratching, talking over their backs one to another, running to catch up with the line". Children identify with the ducklings because they behave as children do. The comforting message shows parents as caretakers, protectors, and teachers.
According to fellow Caldecott winner Paul O. Zelinsky, "I realized that if the action in the drawings were to move from left to right, the ducks could not have been shown passing the Corner Book Shop, which is a wonderful detail in the story ... He clearly knew these streets very well."
"'Robert McCloskey's unusual and stunning pictures have long been a delight for their fun as well as their spirit of place.'-- The Horn Book"
Make Way for Ducklings has been continuously in print since it was first published. As of 2003, the book had sold over two million copies. In September 2006 the hardcover edition of the book ranked #2,182 in sales at Amazon.com and #1,838 in sales at Barnes & Noble. The story has also been published in paperback and audiobook.When it was first released in 1941, Ellen Buell of The New York Times called the book "one of the merriest we have had in a long time", praising the understated comedic aspect of the procession down Beacon Street, as well as McCloskey's "fine large pictures" which simultaneously demonstrate "economy of line" and "wealth of detail".Viking publishing planned to release a 75th-anniversary edition of the classic in March 2016.
One critic, Alice Fannin, says the "loosely plotted" story gives no true explanation for why Mr. Mallard leaves the island in the Charles River or why the Mallards did not simply stay on the lagoon island in the first place and avoid the bicyclists on the shore. However, McCloskey has stated himself that he thinks of himself as an artist who writes children's books and not vice versa. Fannin also finds the characterization lacking, that is, the Mallards represent "rather stereotypically concerned parents", often showing the same facial expressions and rarely showing expressiveness.Another critic has positively commented on McCloskey's use of page breaks as a pacing technique. McCloskey's use of one-sentence pages forces the reader to quickly turn the page, enhancing the sense of motion, especially during the home search and when Mrs. Mallard teaches the ducklings their basic skills. McCloskey also employs this page break method to heighten surprise. When searching for a home, Mr. and Mrs. Mallard seem to have found a home on page eleven.
'Good,' said Mr. Mallard, delighted that at last Mrs. Mallard had found a place that suited her. But — (p. 11)
Then, they encounter a sudden problem with the chosen location when Mrs. Mallard is nearly run over by a bicyclist on page thirteen.
Make Way for Ducklings was published in the 1940s. Many books of the time portray a male dominated society, a trend which Make Way for Ducklings does not follow. In context, the story takes place during wartime as fathers were being drafted and sent to Europe, requiring more social support for single parent families. McCloskey presented Mrs. Mallard as an "independent and nonsubmissive female character." This strong portrayal has led one critic to label the book as "pre-feminist."
Based on a 2007 online poll, the National Education Association named Make Way for Ducklings one of its "Teachers' Top 100 Books for Children." In 2012 it was ranked number six among the "Top 100 Picture Books" in a survey published by School Library Journal.
The city of Boston, the setting of the book, has whole-heartedly embraced the story. In the Public Garden, where the Mallards eventually settled, a bronze statue has been erected of Mrs. Mallard and her eight ducklings. While the tallest statue stands only 38 inches (.97 meters) tall, the caravan of bronze ducks set in Boston cobblestone spans 35 feet (10.67 m) from front to back. The statue, installed October 4, 1987, was a tribute to Robert McCloskey "whose story ... has made the Boston Public Garden familiar to children throughout the world."Since 1978, the city has hosted an annual Duckling Day parade each spring, with children and their parents dressed as ducklings.  Part of the route retraces the path taken by Mrs. Mallard and her ducklings to get to the Public Garden.In 2000, schoolchildren from Canton, Massachusetts decided that the book was worthy of being the official children's book of the Commonwealth of Massachusetts and went to their state legislature to get a bill passed declaring it so. However, legislators from Springfield, Massachusetts blocked the legislation on the grounds the official book should be by Springfield native Dr. Seuss. Legislators reached a compromise when they agreed to make Dr. Seuss the official children's author of the commonwealth and Make Way for Ducklings the official children's book.A statue similar to the one in the Boston Public Garden was erected in Novodevichy Park in Moscow as part of the START Treaty by Acton, Massachusetts landscape and construction company Capizzi & Co. Inc. on July 30, 1991. The equipment, statues, cobblestones, and workers were all flown by the US Air Force in a C5 containing the heavy equipment, diesel fuel, and other assorted tools they would need. The individual statues, which were in total length, 40 feet (12 m) long, were presented by then United States First Lady Barbara Bush to Russian First Lady Raisa Gorbachev as a gift to the children of the Soviet Union. Four of the ducks were stolen, one in 1991 and three in February 2000. Thieves hoping to sell the ducks as scrap metal cut the statues off at the legs. The ducks were replaced in September 2000 at a rededication ceremony attended by former President of the Soviet Union Mikhail Gorbachev.
Make Way for Ducklings was adapted into an 11-minute black-and-white cartoon created by Weston Woods in 1955. Founded by educator Morton Schindel in 1953, Weston Woods Studios, Inc.(named after the wooded area outside his home in Weston, Connecticut) specializes in animating children's picture books on film. The program is centered toward young struggling readers who, after watching the movie, are encouraged to read the books themselves.In 1964, Schindel and Weston Woods Studios made the 18-minute Robert McCloskey, a documentary which is sometimes screened in art schools. The film shows McCloskey sitting in Boston Public Garden and intercuts pages from his sketchbook drawings for Make Way for Ducklings. The illustrator discusses experiences that have influenced his work and the relationship of craftsmanship to inspiration.

Makemake (minor-planet designation 136472 Makemake) is a dwarf planet (and plutoid) and perhaps the second largest Kuiper belt object in the classical population, with a diameter approximately two-thirds that of Pluto. Makemake has one known satellite, S/2015 (136472) 1. Makemake's extremely low average temperature, about 30 K (−243.2 °C), means its surface is covered with methane, ethane, and possibly nitrogen ices.Makemake was discovered on March 31, 2005, by a team led by Michael E. Brown, and announced on July 29, 2005. Initially, it was known as 2005 FY9 and later given the minor-planet number 136472. Makemake was recognized as a dwarf planet by the International Astronomical Union (IAU) in July 2008. Its name derives from Makemake in the mythology of the Rapa Nui people of Easter Island.
Makemake was discovered on March 31, 2005, by a team at the Palomar Observatory, led by Michael E. Brown, and was announced to the public on July 29, 2005. The team had planned to delay announcing their discoveries of the bright objects Makemake and Eris until further observations and calculations were complete, but announced them both on July 29 when the discovery of another large object they had been tracking, Haumea, was controversially announced on July 27 by a different team in Spain.Despite its relative brightness (it is about a fifth as bright as Pluto), Makemake was not discovered until after many much fainter Kuiper belt objects. Most searches for minor planets are conducted relatively close to the ecliptic (the region of the sky that the Sun, Moon and planets appear to lie in, as seen from Earth), due to the greater likelihood of finding objects there. It probably escaped detection during the earlier surveys due to its relatively high orbital inclination, and the fact that it was at its farthest distance from the ecliptic at the time of its discovery, in the northern constellation of Coma Berenices.Precovery images have been identified back to January 29, 1955.Besides Pluto, Makemake is the only other dwarf planet that was bright enough that Clyde Tombaugh could have detected it during his search for trans-Neptunian planets around 1930. At the time of Tombaugh's survey, Makemake was only a few degrees from the ecliptic, near the border of Taurus and Auriga, at an apparent magnitude of 16.0. This position, however, was also very near the Milky Way, and Makemake would have been almost impossible to find against the dense background of stars. Tombaugh continued searching for some years after the discovery of Pluto, but he did not find Makemake or any other trans-Neptunian objects.
The provisional designation 2005 FY9 was given to Makemake when the discovery was made public. Before that, the discovery team used the codename "Easterbunny" for the object, because of its discovery shortly after Easter.In July 2008, in accordance with IAU rules for classical Kuiper belt objects, 2005 FY9 was given the name of a creator deity. The name of Makemake, the creator of humanity and god of fertility in the myths of the Rapa Nui, the native people of Easter Island, was chosen in part to preserve the object's connection with Easter.
As of  December 2015, Makemake is 52.4 AU (7.84×109 km) from the Sun, almost as far from the Sun as it ever reaches on its orbit. Makemake follows an orbit very similar to that of Haumea: highly inclined at 29° and a moderate eccentricity of about 0.16. Nevertheless, Makemake's orbit is slightly farther from the Sun in terms of both the semi-major axis and perihelion. Its orbital period is nearly 310 years, more than Pluto's 248 years and Haumea's 283 years. Both Makemake and Haumea are currently far from the ecliptic—the angular distance is almost 29°. Makemake is approaching its 2033 aphelion, whereas Haumea passed its aphelion in early 1992.Makemake is a classical Kuiper belt object (KBO), which means its orbit lies far enough from Neptune to remain stable over the age of the Solar System. Unlike plutinos, which can cross Neptune's orbit due to their 2:3 resonance with the planet, the classical objects have perihelia further from the Sun, free from Neptune's perturbation. Such objects have relatively low eccentricities (e below 0.2) and orbit the Sun in much the same way the planets do. Makemake, however, is a member of the "dynamically hot" class of classical KBOs, meaning that it has a high inclination compared to others in its population. Makemake is, probably coincidentally, near the 11:6 resonance with Neptune.
Makemake is currently visually the second-brightest Kuiper belt object after Pluto, having a March opposition apparent magnitude of 17.0 in the constellation Coma Berenices. This is bright enough to be visible using a high-end amateur telescope.
Combining the detection in infrared by the Spitzer Space Telescope and Herschel Space Telescope with the similarities of spectrum with Pluto yielded an estimated diameter from 1,360 to 1,480 km. From the 2011 stellar occultation by Makemake, its dimensions have been initially measured to be (1502 ± 45) × (1430 ± 9) km. However, this analysis of the occultation data was later reanalyzed, which led to the dimension estimate of (1434+48−18) × (1420+18−24 km) without a pole-orientation constraint. Makemake was the fourth dwarf planet recognized, because it has a bright V-band absolute magnitude of −0.44. Makemake has a high geometrical albedo of 0.81+0.01−0.02.The rotation period of Makemake is estimated at 7.77 hours. Its lightcurve amplitude is small, only 0.03 mag. This was thought to be due to Makemake currently being viewed pole on from Earth; however, S/2015 (136472) 1's orbital plane (which is probably orbiting with little inclination relative to Makemake's equator due to tides resulting from its rapid rotation) is edge-on from Earth, implying that Makemake is really being viewed equator-on.
Like Pluto, Makemake appears red in the visible spectrum, and significantly redder than the surface of Eris (see colour comparison of TNOs). The near-infrared spectrum is marked by the presence of the broad methane (CH4) absorption bands. Methane is observed also on Pluto and Eris, but its spectral signature is much weaker.Spectral analysis of Makemake's surface revealed that methane must be present in the form of large grains at least one centimetre in size. In addition to methane, large amounts of ethane and tholins as well as smaller amounts of ethylene, acetylene and high-mass alkanes (like propane) may be present, most likely created by photolysis of methane by solar radiation. The tholins are probably responsible for the red color of the visible spectrum. Although evidence exists for the presence of nitrogen ice on its surface, at least mixed with other ices, there is nowhere near the same level of nitrogen as on Pluto and Triton, where it composes more than 98 percent of the crust. The relative lack of nitrogen ice suggests that its supply of nitrogen has somehow been depleted over the age of the Solar System.The far-infrared (24–70 μm) and submillimeter (70–500 μm) photometry performed by Spitzer and Herschel telescopes revealed that the surface of Makemake is not homogeneous. Although the majority of it is covered by nitrogen and methane ices, where the albedo ranges from 78 to 90%, there are small patches of dark terrain whose albedo is only 2 to 12%, and that make up 3 to 7% of the surface. These studies were made before S/2015 (136472) 1 was discovered; thus, these small dark patches may actually have been the dark surface of the satellite rather than any actual surface features on Makemake.However, some experiments have refuted these studies. Spectroscopic studies, collected from 2005 to 2008 using the William Herschel Telescope (La Palma, Spain) were analyzed together with other spectra in the literature, as of 2014. They show some degree of variation in the spectral slope, which would be associated with different abundance of the complex organic materials, byproduct of the irradiation of the ices present on the surface of Makemake. However, the relative ratio of the two dominant icy species, methane and nitrogen, remains quite stable on the surface revealing a low degree of inhomogeneity in the ice component. These results have been recently confirmed when the Telescopio Nazionale Galileo acquired new visible and near infra-red spectra for Makemake, between 2006 and 2013, that covered nearly 80% of its surface; this study found that the variation in the spectra were negligible, suggesting that Makemake's surface may indeed be homogenous.
Makemake was expected to have an atmosphere similar to that of Pluto but with a lower surface pressure. However, on 23 April 2011 Makemake passed in front of an 18th-magnitude star and abruptly blocked its light. The results showed that Makemake presently lacks a substantial atmosphere and placed an upper limit of 4–12 nanobar on the pressure at its surface.The presence of methane and possibly nitrogen suggests that Makemake could have a transient atmosphere similar to that of Pluto near its perihelion. Nitrogen, if present, will be the dominant component of it. The existence of an atmosphere also provides a natural explanation for the nitrogen depletion: because the gravity of Makemake is weaker than that of Pluto, Eris and Triton, a large amount of nitrogen was probably lost via atmospheric escape; methane is lighter than nitrogen, but has significantly lower vapor pressure at temperatures prevalent at the surface of Makemake (32–36 K), which hinders its escape; the result of this process is a higher relative abundance of methane. However, studies of Pluto's atmosphere by New Horizons suggest that methane, not nitrogen, is the dominant escaping gas, suggesting that the reasons for Makemake's absence of nitrogen may be more complicated.
It was calculated that a flyby mission to Makemake could take just over 16 years using a Jupiter gravity assist, based on a launch date of 21 August 2024 or 24 August 2036. Makemake would be approximately 52 AU from the Sun when the spacecraft arrives.
S/2015 (136472) 1, nicknamed MK2 by the discovery team, is the only known moon of Makemake. It is estimated to be 175 km (110 mi) in diameter (for an assumed albedo of 4%) and has a semi-major axis at least 21,000 km (13,000 mi) from Makemake. Its orbital period is ≥ 12 days (the minimum values are those for a circular orbit; the actual orbital eccentricity is unknown). Observations leading to its discovery occurred in April 2015, using the Hubble Space Telescope's Wide Field Camera 3, and its discovery was announced on 26 April 2016.Most other large trans-Neptunian objects have at least one known satellite: Eris has one, Haumea has two, Pluto has five, and 2007 OR10 has one satellite. 10% to 20% of all trans-Neptunian objects are expected to have one or more satellites. Because satellites offer a simple method to measure an object's mass, Makemake's satellite should lead to better estimates of its mass.
A preliminary examination of the imagery suggests that MK2 has a reflectivity similar to charcoal, making it an extremely dark object. This is somewhat surprising because Makemake is the second-brightest-known object in the Kuiper belt. One hypothesis to explain this is that its gravity is not strong enough to prevent bright but volatile ices from being lost to space when it is heated by the distant Sun.Further observations will be needed in order to determine MK2's orbit. If it is circular, it would suggest that MK2 was formed by an ancient impact event, but if it is elliptical, it suggests that it may have been captured.Alex Parker, the leader of the team that performed the analysis of the images at the Southwest Research Institute, said that MK2's orbit appears to be aligned edge-on to Earth-based observatories. This would make it much more difficult to detect because it would be lost in Makemake's glare much of the time, which, along with its dark surface, would contribute to previous surveys failing to observe it.

Making Waves is a British television drama series produced      by Carlton Television for ITV. It was created by Ted Childs and chronicles the professional and personal lives of the crew of the Royal Navy frigate HMS Suffolk. The series remained in development hell for several years and was first broadcast on 7 July 2004. However, due to low ratings it was removed from the schedules after only three episodes, the remainder of the series going unaired on television in the United Kingdom.
The series starred Alex Ferns as Commander Martin Brooke and Emily Hamilton as Lieutenant Commander Jenny Howard. The frigate HMS Grafton stood in for Suffolk and additional filming took place around HMNB Portsmouth with the full co-operation of the Royal Navy. A limited-edition DVD of all six episodes was released in December 2004.
Following the success of his previous series Soldier Soldier, Ted Childs, Richard Maher and Carlton began planning the series in the late 1990s, with Carlton's controller of drama and continuing series expecting something "in the London's Burning, Peak Practice vein". It was turned down by ITV because it was "old-fashioned" and did not fit in with the network's existing portfolio of dramas. Despite this, Childs and Carlton continued to develop the series and they brought it to the commissioning editors of BBC One, who negotiated with Carlton to broadcast it. By that time, the top levels of the ITV drama department had changed and due to Carlton's links with ITV, the BBC was unable to take the series. A six-episode series was commissioned in July 2002 by Nick Elliott at ITV.The project was initially managed by the MoD's Directorate of Corporate Communications (Navy), headed by Commodore Richard Leaman.  Lieutenant Commander Steve Tatham undertook much of the initial scoping and planning work before, in August 2002, Commander Kevin Fincher was appointed as the specialist project officer for the series; he would acquire the necessary ships, locations and personnel as well as advising the production team on and off set. Throughout pre-production Fincher negotiated a legal agreement with Carlton, whereby a financial recovery was made for anything they used that was taxpayer-funded. This included use of ships, fuel, and personnel. Another clause gave the Royal Navy a share of any royalties from the series, including advertising revenue and sales.
There were four credited writers on the Carlton staff, with writers from the ITV network centre involved in the development of the scripts. The first episode was written in 2002 by Terry Cafolla, who later wrote Messiah IV: The Harrowing. The second episode was written by Damian Wayling of The Bill, and the third by Niall Leonard. Matthew Bardsley was the credited writer of the three unaired episodes. Although a second series was not made, storylines were planned for a potential commission. Warrant Officer Dave Allport and Leading Seaman Sarah Worthy joined Fincher as advisers in January 2003.
Actors were auditioned and hired in late 2002, and included Alex Ferns, well-known to British audiences for his role as Trevor Morgan in the BBC soap opera EastEnders. Making Waves was the first lead role for Ferns, who had taken time off from television acting since leaving EastEnders to avoid being typecast in a soap role, and was pleased not to be playing "a psychotic rapist". Emily Hamilton, cast as executive officer Jenny Howard, was largely unknown to British television audiences at the time; her only notable role was in Russell T Davies's The Grand in the late 1990s. Lee Boardman (who later appeared in Rome) took the role of the chef Art Francis to distance himself from his most well-known role, drug-dealer Jez Quigley in the soap opera Coronation Street.Stephen Kennedy was already known for his role as Ian Craig in the BBC Radio 4 soap opera The Archers. He mentioned in an interview the differences between the two roles, before quipping that they were not that dissimilar. The crew of Grafton appeared as extras throughout the series and schoolchildren from St. Jude's School, Southsea appeared in the families day scenes in episode six.
The series producers scouted Portsmouth in 2002 for ship locations. Potential main settings HMS Marlborough and HMS Dryad were put aside in favour of HMS Grafton and filming commenced on 24 March 2003 with 30 actors and 60 crew moving onto the ship for the shoot. Alex Ferns arrived two days earlier than the rest of the cast to settle in, and made a trip to Yeovilton to meet Harrier pilots, while Emily Hamilton prepared for her role by shadowing Vanessa Spiller, XO of HMS Kent. The series was directed by Matthew Evans and Nigel Douglas and was shot on digital DV cameras. The production staff filmed approximately three hours of footage on every 12-hour day, editing it using Avid systems in the production offices at the naval base.Other vessels made cameos in the series; filming took place around and aboard HMS Victory for scenes in episode two, and aerial footage of HMS Invincible and HMS Gloucester was done for the war games scenes in episode four. HMS Lindisfarne appeared for the funeral scenes in episode five. Health and safety regulations required that Grafton was shadowed at all times by a support vessel during filming at sea, to intervene in the event of a member of the production team falling overboard.Location filming lasted until 26 June 2003 before post-production was completed in London, and the series was delivered to ITV in August 2003 for broadcast in the autumn schedules, though it would be held back for several months. A special preview screening of episode one was held aboard HMS Richmond on 13 February 2004, while the ship was in Aberdeen on a recruitment and promotional tour of the UK.  It was also premièred aboard HMS Northumberland during the same month whilst at the London Boat Show, and was attended by much of the cast and crew.
HMS Suffolk is due for Flag Officer Sea Training in four weeks but an accident during training results in the dismissal of the executive officer and the resignation of the captain. The series follows his replacement, Commander Martin Brooke, in his attempts to get his vessel and crew ship-shape for final assessment. Other storylines follow Leading Marine Engineer Artificer (LMEA) Dave Finnan's relationship with his Charge Chief's daughter Teresa, the emotional state of Mickey Sobanski after a blundered rescue operation, new rating Rosie Bowen settling into life on her first ship and the budding cross-ranks relationship between officer Sam Quartermaine and medic Anita Cook. Comic relief is provided in the characters of "Scouse" Phillips and Leading Chef Art Francis. Suffolk is mainly based at Portsmouth but engages in exercises such as war games throughout the series, as well as undertaking hazardous rescues of other vessels in the English Channel.
Making Waves featured an ensemble of actors but followed a core cast, with supporting players appearing in only a few episodes or having secondary storylines.
Commander Martin Brooke (played by Alex Ferns) is the son of a car mechanic and his naval background is based on piloting, rather than commanding a ship. He is assisted by Lt Cdr Jenny Howard (played by Emily Hamilton), who is initially his temporary XO, but eventually accepts Brooke's offer to stay on the ship. Lieutenant Commander William Lewis, the Marine Engineering Officer (played by Ian Bartholomew), is the superior of Charge Chief Marine Engineering Artificer (CCMEA) Andy Fellows (played by Steve Speirs) and Lewis's refusal to give the engines full maintenance regularly infuriates him, though not as much as LMEA Dave Finnan (played by Paul Chequer) who has just had a baby with his daughter Teresa (played by Chloe Howman). New Operator Mechanic Rosie Bowen (played by Joanna Page) settles into her first posting and attracts the attention of OM Mickey Sobanski (played by Lee Turnbull), who is contemplating his future in the Navy after an incident in the first episode.
The second episode introduces the new navigating officer Lieutenant Sam Quartermaine (played by Adam Rayner) and a subplot involving his relationship with LMA Anita Cook (played by Angel Coulby) runs through the series and is eventually discovered by Lieutenant James Maguire, the Principal Warfare Officer (played by Stephen Kennedy). Terry "Buffer" Duncan's (played by Geoff Bell) career is in jeopardy when an accusation of assault is thrown at him in the third episode, while Leading Regulator Liz Wilson (played by Diane Beck) develops an unreciprocated crush on Bowen. Leading Chef Art Francis (played by Lee Boardman) must successfully prepare dinner for the captain and crew before it is stolen or ruined by Steward Tim "Scouse" Phillips (played by Darren Morfitt).
Carlton delivered the series to ITV in August 2003 for broadcast in the autumn schedules, but it was then rescheduled four times over the next several months, before ITV eventually set a premiere date of 11 July 2004. It was then rescheduled to the preceding Wednesday, in the 9 pm slot. The series lost two million viewers over two weeks and ITV pulled episode four from the schedules on the morning before it was due for broadcast, replacing it with It Shouldn't Happen on a TV Soap, a bloopers programme, which returned ITV's ratings to above the five million mark.Writing in The Guardian a fortnight later, ITV head of drama David Liddiment defended the decision, stating that the network had planned to let Making Waves profit from The Bill's (the lead-in) high ratings at a time when BBC One Wednesday night ratings were suffering, but the series just "wasn't good enough" to hold an audience. Ted Childs later responded that Making Waves had received little publicity compared to Channel 4's ratings smash Supernanny, which aired opposite his series, and that because that programme had ended its run, the ratings for the last three episodes might have improved. He went on to question why ITV had spent £5 million on a series they knew would not be a hit. Making Waves had been another in a line of expensive series which had been cancelled because they performed below ITV's expectations in the ratings, following Sweet Medicine and Family the previous year.The cancellation of the series also drew criticism from the Royal Navy, with a source telling The Sun that it was "a kick in the teeth to our sailors". In its end-of-year review, ITV described the series as having "quality and distinctiveness" but failing "to find a mass audience".
Due to the series coming from the producer of Soldier Soldier and Kavanagh QC, the close involvement of the Royal Navy and the lead being taken by Alex Ferns, there was a strong media interest in the series, which only intensified as the series went through its many reschedulings. Positive previews were run by the Manchester Evening News and The Sun. The Times predicted "ITV has a ratings winner" and called the series a "classic military soap opera". The tabloid press was keen for the series to succeed; The Daily Mirror described it as promising and conjectured that it would be the defining role of Alex Ferns's career, although its Sunday equivalent did not share the sentiment: it described the series as a "seabed-bound disaster". The same writer criticised the script and directing of episode three and suggested that viewers were not interested in a naval series set in peacetime.The Scotsman dismissed it as little more than a six-part recruitment video, comparing scenes of refugees being lifted to real advertisements that showcased the navy's role in humanitarian crises, and concluded that the drama was a "collection of clichés and stilted dialogue". The Independent on Sunday compared the series to the sea-based soap opera Triangle and noted "an overdependence on claustrophobic interiors". However, the series was wryly praised for casting Alex Ferns instead of Ross Kemp in the lead role, bucking the trend of recent ITV military series and commented on the difficult time slot the series had been given. Over two years after the series was pulled, Alex Ferns admitted that it was formulaic, but blamed its failure on the constant rescheduling.
The BBFC passed the series for video release on 3 December 2004, rating episodes three and four as PG, and the rest as 12. Granada Ventures pressed approximately 2,500 two-disc sets of the series for sale exclusively on the Navy News website and it went on sale in December 2004, with 2,000 sets being purchased within a month.
