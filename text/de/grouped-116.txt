
Diffusion (lat. diffundere ‚ausgießen‘, ‚verstreuen‘, ‚ausbreiten‘) ist der ohne äußere Einwirkung eintretende Ausgleich von Konzentrationsunterschieden als natürlich ablaufender physikalischer Prozess aufgrund der brownschen Molekularbewegung. Er führt mit der Zeit zur vollständigen Durchmischung zweier oder mehrerer Stoffe durch die gleichmäßige Verteilung der beweglichen Teilchen und erhöht damit die Entropie des Systems. Bei den Teilchen kann es sich um Atome, Moleküle, Ladungsträger oder auch um freie Neutronen handeln. Meist ist zumindest einer der Stoffe ein Gas oder eine Flüssigkeit, doch können auch Feststoffe und Plasmen ineinander diffundieren.
Diffusion beruht auf der ungerichteten Zufallsbewegung von Teilchen aufgrund ihrer thermischen Energie („thermische Bewegung“, s. u.). Bei ungleichmäßiger Verteilung bewegen sich statistisch mehr Teilchen aus Bereichen hoher in Bereiche geringer Konzentration bzw. Teilchendichte, als umgekehrt.  Dadurch wird netto ein makroskopischer Stofftransport bewirkt. Unter Diffusion versteht man in der Regel diesen Netto-Transport. Der Begriff wird aber auch für den zugrundeliegenden mikroskopischen Prozess verwendet.
In einem abgeschlossenen System bewirkt Diffusion den Abbau von Konzentrationsunterschieden bis hin zur vollständigen Durchmischung. Die Zeit, die dafür benötigt wird, wächst im 
  -ten Potenz des Abstands. Diffusion ist daher vor allem auf Nano- bis Millimeter-Skalen wirksam; auf größeren Skalen dominiert in Flüssigkeiten und Gasen in der Regel Stofftransport durch Strömung (Konvektion). Die Diffusion in Feststoffen wird zudem häufig durch andere chemisch-physikalische Vorgänge wie Absorption, Adsorption, Resorption und Kapillartransport überlagert. 
Diffusion ist nicht von der Luftdurchlässigkeit eines Materials abhängig. Bei der Osmose diffundieren kleine Moleküle durch eine geschlossene Membran, die für größere Moleküle undurchlässig ist. Entscheidend ist die Diffusivität des Materials in Bezug auf den diffundierenden Stoff.
Kollektive Diffusion ist die Diffusion mehrerer Teilchen entlang eines Konzentrationsgradienten, darunter fallen beispielsweise die Fickschen Gesetze.
Bei der Selbstdiffusion werden dagegen einzelne Teilchen betrachtet, deren Verhalten u. a. von der Einsteinrelation beschrieben wird. Der Selbstdiffusionskoeffizient 
Für extrem kurze Zeiten kleiner der Brownschen Relaxationszeit spricht man vom ballistischen Regime.
Einer der Ersten, die systematisch Diffusionsversuche in größerem Umfang durchführten, war Thomas Graham. Aus seinen Experimenten zur Diffusion von Gasen leitete er das nach ihm benannte Grahamsche Gesetz ab:
In Hinblick auf Diffusion in Lösungen konnte Graham zeigen, dass die Diffusionsrate proportional zu Konzentrationsdifferenz und abhängig von der Temperatur ist (schnellere Diffusion bei höheren Temperaturen).
Weiterhin zeigte Graham die Möglichkeit auf, Mischungen von Lösungen oder Gasen mittels Diffusion zu trennen.Thomas Graham hatte die grundlegenden Gesetze der Diffusion noch nicht ermitteln können. Dies gelang nur wenige Jahre später Adolf Fick. Er postulierte, dass das gesuchte Gesetz analog zu den Gesetzmäßigkeiten der Wärmeleitung, die Jean Baptiste Joseph Fourier ermittelt hatte, sein müsse:
Fick führte Experimente durch, deren Ergebnisse die Gültigkeit des später nach ihm benannten Ersten Fickschen Gesetzes belegten. Die Gültigkeit des Zweiten Fickschen Gesetzes konnte er nur aus der Gültigkeit des Ersten herleiten. Der direkte Nachweis scheiterte an seinen begrenzten analytischen und mathematischen Möglichkeiten.
Albert Einstein gelang es Anfang des 20. Jahrhunderts, die Fickschen Gesetze aus den Gesetzen der Thermodynamik abzuleiten und so der Diffusion ein sicheres theoretisches Fundament zu geben. Dabei leitete er auch die Stokes-Einstein-Beziehung zur Berechnung des Diffusionskoeffizienten her:
Einstein zeigte auch, wie man die Bewegung eines einzelnen diffundierenden Teilchens erfassen kann und damit die Brown’sche Molekularbewegung als ein Fluktuationsphänomen verstehen kann. Er berechnete die mittlere quadratische Verschiebung 
  . Kurze Zeit nach Einstein kam Smoluchowski auf einem anderen Weg ebenfalls zu praktisch derselben Beziehung, und daher wird diese Gleichung heute als Einstein-Smoluchowski-Gleichung bezeichnet.
Ein oft genanntes Experiment zur Veranschaulichung der Ausbreitung durch Diffusion ist die allmähliche Einfärbung von lauwarmem Wasser durch einen Tropfen Tinte, den man hineingibt, das Wasser aber weder umrührt noch den Behälter schüttelt. Nach einiger Zeit hat sich die Tintenfarbe im ganzen Wasser gleichmäßig verteilt. Die Ausbreitung der Tinte im Wasser kann allerdings auch durch Dichte- und Temperaturunterschiede begünstigt werden. Diese Einflüsse lassen sich verringern, indem man eine farbige Flüssigkeit mit höherer Dichte mit einer Flüssigkeit mit niedrigerer Dichte überschichtet und sehr viskose Flüssigkeiten verwendet, z. B. farbigen Sirup und Honig. Die dann beobachtete allmähliche Einfärbung des Honigs erklärt sich nahezu ausschließlich durch Diffusion, wobei sowohl Sirup in den Honig als auch Honig in den Sirup diffundiert.
Beim Sintern spielt die Diffusion eine sehr wichtige Rolle beim Zusammenwachsen der Pulverbestandteile.
Stahl kann durch Eindiffundieren von Kohlenstoff und/oder Stickstoff randschichtgehärtet werden oder zu Metal Dusting führen.
In Diffusionsöfen werden bei Temperaturen um 1000 °C Dotanden in das Halbleitermaterial eingebracht, um dort gezielt die elektrische Leitfähigkeit oder mechanische Eigenschaften für Bauelemente der Mikrosystemtechnik zu beeinflussen.
Die Diffusion spielt in der Technischen Chemie eine zentrale Rolle. Häufig tritt sie hier gekoppelt mit Konvektion und chemischen Reaktionen auf. Typische Anwendungen sind Reaktor- und Katalysatordesign. In der chemischen Verfahrenstechnik erfolgt die selektive Trennung von Stoffgemischen häufig mittels Molekularsieb- und/oder Membrantechnik. Beide Verfahren beruhen auf „kinetischer Separation“, wobei die Differenzen der Diffusion der einzelnen Stoffe in Nanoporen eine wesentliche Rolle spielen und sich dadurch Steuerungsmöglichkeiten eröffnen.
Bei der Baukonstruktion muss zum Feuchteschutz die Wasserdampfdiffusion berücksichtigt werden, um unzulässig große Tauwasserkondensation zu vermeiden. Dazu werden Dampfsperren und Dampfbremsen mit definiertem Wasserdampfdiffusionswiderstand eingesetzt.
Mit der sogenannten diffusion-ordered-spectroscopy (DOSY) kann in Mischungen die translatorische Beweglichkeit einzelner Moleküle gemessen und anhand des Diffusionskoeffizienten das Molekülgewicht bestimmt werden.
Die Diffusion bei einer bestimmten konstanten Temperatur erfolgt ohne weitere Energiezufuhr und ist in diesem Sinne passiv; vor allem in der Biologie wird die Diffusion vom aktiven Transport unterschieden.
Theoretisch ist Diffusion ein unendlich lange dauernder Vorgang. Im Rahmen der Messbarkeit kann sie jedoch häufig als in endlicher Zeit abgeschlossen betrachtet werden.
Die thermische Bewegung, auf der die Diffusion beruht, kann je nach betrachtetem System sehr unterschiedlichen Charakter haben. In Gasen besteht sie aus geradliniger Bewegung, unterbrochen von gelegentlichen Stößen. Die schnelle thermische Bewegung von Flüssigkeitsteilchen bewirkt durch häufige Stöße die wesentlich langsamere, unter dem Mikroskop beobachtbare Brownsche Bewegung mesoskopischer Objekte.
In Festkörpern erfolgen gelegentliche Ortswechsel, z. B. durch den Platztausch zweier benachbarter Teilchen, oder das „Wandern“ von Leerstellen. Bei Ladungsträgern (z. B. Ionen, Elektronen, Löchern) ist der Wärmebewegung jedoch ein Drift durch die elektrostatischen Kräfte überlagert.
Die Bewegungsrichtung eines einzelnen Teilchens ist vollkommen zufällig. Aufgrund der Wechselwirkung mit anderen Teilchen erfolgen ständige Richtungsänderungen. Über einen längeren Zeitraum bzw. über viele Teilchen gemittelt kann sich dennoch ein Transport in eine bestimmte Richtung ergeben, z. B. wenn ein Sprung in eine bestimmte Richtung eine, vielleicht nur geringfügig, größere Wahrscheinlichkeit hat. Dies ist der Fall, wenn ein Konzentrationsunterschied (auch Konzentrationsgradient) vorhanden ist. Es entsteht dann ein Nettofluss an Teilchen, bis sich ein stationärer Zustand, das thermodynamische Gleichgewicht, einstellt. Zumeist ist der Gleichgewichtszustand die Gleichverteilung, bei der die Konzentration aller Teilchen an jedem Punkt im Raum gleich hoch ist.
Wahrscheinlichkeit und Diffusion – ein Erklärungsversuch: Angenommen 1000 Teilchen eines Stoffes wären nur in der rechten Hälfte eines Gefäßes, und 10 Teilchen in der linken Hälfte; außerdem bewegt sich jedes Teilchen durch die Brownsche Molekularbewegung eine bestimmte Strecke in eine völlig zufällige Richtung. Dann folgt: Die Wahrscheinlichkeit, dass sich eines der 1000 Teilchen zufälligerweise von der rechten in die linke Hälfte bewegt ist 100-mal größer als die Wahrscheinlichkeit, dass sich eines der nur 10 Teilchen von links nach rechts bewegt. Also werden nach einer gewissen Zeit mit hoher Wahrscheinlichkeit netto Teilchen von rechts nach links gewandert sein. Sobald die Wahrscheinlichkeit des Wanderns auf beiden Seiten gleich groß ist, sich also rechts und links je 505 Teilchen befinden, wird netto kein Massenfluss mehr stattfinden und die Konzentration bleibt überall (im Rahmen statistischer Schwankungen) gleich groß. Selbstverständlich wandern nach wie vor Teilchen von links nach rechts und umgekehrt; da es aber nun gleich viele Teile sind, lässt sich kein Unterschied in der Konzentration feststellen. Wenn man sich jetzt „rechts“ und „links“ als besonders kleine Teilräume z. B. des Tintenversuches vorstellt und alle diese Teilräume irgendwann alle die gleiche Tintenkonzentration aufweisen, hat sich die Tinte gleichmäßig verteilt.
Systeme, in denen die Teilchen regellos über das ganze Volumen verteilt sind, haben eine höhere Entropie als geordnetere Systeme, in denen sich die Teilchen bevorzugt in bestimmten Bereichen aufhalten. Diffusion führt damit zu einer Entropieerhöhung. Sie ist nach dem Zweiten Hauptsatz der Thermodynamik ein freiwillig ablaufender Prozess, der sich nicht ohne äußere Einwirkung umkehren lässt.
Die größere Entropie bei einer Verteilung über das gesamte Volumen ergibt sich auch aus der größeren Anzahl von Verteilungsmustern (oder Mikrozuständen), die die Teilchen bilden können, wenn sie mehr Platz zur Verfügung haben. Die Anzahl der Mikrozustände, die denselben Makrozustand bilden, heißt sein statistisches Gewicht 
  . Damit hat eine großräumige Verteilung auch ein höheres statistisches Gewicht im Vergleich zu einer räumlich konzentrierten Anordnung und ist deshalb auch wahrscheinlicher. Die Entropie (
Die Diffusion folgt Gesetzmäßigkeiten, die denen der Wärmeleitung äquivalent sind. Daher kann man Gleichungen, die den einen Prozess beschreiben, für den anderen übernehmen.
  : ein Koeffizient in mol2 · s · kg−1 · m−3Für einfache Anwendungsfälle kann anstelle des chemischen Potentials die Stoffmengenkonzentration 
   verwendet werden. Diese ist einfacher zugänglich als das chemische Potential eines Stoffes. Für ein ideales Gas ist das chemische Potential gegeben durch
  ,für einen nicht-idealen Stoff müssen zusätzliche excess-Terme berücksichtigt werden, da Teilchenwechselwirkungen vorliegen (siehe Chemisches Potential).
   das chemische Potential unter Standarddruck. Hängt die Temperatur nicht explizit vom Ort ab, so gilt:
    {\displaystyle {\frac {\partial \mu (x,t)}{\partial x}}={\frac {\partial \mu ^{\circ }}{\partial x}}+{\frac {\partial RT\ln(c(x,t)/c^{\circ })}{\partial x}}=RT{\frac {\partial \ln(c(x,t)/c^{\circ })}{\partial x}}={\frac {RTc^{\circ }}{c(x,t)}}{\frac {\partial c(x,t)/c^{\circ }}{\partial x}}={\frac {RT}{c(x,t)}}{\frac {\partial c(x,t)}{\partial x}}}
    {\displaystyle J=-K{\frac {\partial \mu }{\partial x}}=-K{\frac {RT}{c}}{\frac {\partial c}{\partial x}}=-D{\frac {\partial c}{\partial x}}}
  : Universelle Gaskonstante in J · K−1 · mol−1Bei sehr geringen Konzentrationen (einzelne Moleküle) ist diese Betrachtung nicht mehr ohne weiteres zulässig, da die klassische Thermodynamik Lösungen als Kontinuum betrachtet. Bei hohen Konzentrationen beeinflussen sich die Teilchen gegenseitig, so dass bei anziehender Wechselwirkung der Konzentrationsausgleich langsamer, bei abstoßender schneller erfolgt. Das chemische Potential ist in diesen Fällen nicht mehr logarithmisch von der Konzentration abhängig.
Die Teilchenstromdichte macht eine quantitative Aussage über die (im statistischen Mittel) gerichtete Bewegung von Teilchen, d. h. wie viele Teilchen einer Stoffmenge sich pro Zeit durch eine Fläche, die senkrecht zur Diffusionsrichtung liegt, netto bewegen. Die angegebene Gleichung gilt auch für den allgemeinen Fall, dass der Diffusionskoeffizient nicht konstant ist, sondern von der Konzentration abhängt (das ist aber streng genommen nicht mehr die Aussage des Ersten Fickschen Gesetzes).
    {\displaystyle {\frac {\partial c}{\partial t}}={\frac {\partial }{\partial x}}\left(D{\frac {\partial c}{\partial x}}\right),}
  .Sie stellt eine Beziehung zwischen zeitlichen und örtlichen Konzentrationsunterschieden dar und eignet sich somit zur Darstellung instationärer Diffusion, im Gegensatz zum 1. Fickschen Gesetz, das einen zeitlich konstanten Diffusionsfluss beschreibt. Es existieren für diese Differentialgleichung zahlreiche analytische und numerische Lösungsansätze, die jedoch stark von den Anfangs- und Randbedingungen abhängen.
Mathematisch gesehen ist die Diffusionsgleichung identisch mit der Wärmeleitungsgleichung, ihre mathematischen Eigenschaften und Lösungsansätze werden im dortigen Artikel behandelt.
Der Fall der dreidimensionalen Diffusion lässt sich mit dem Zweiten Fickschen Gesetz in seiner allgemeinsten Form beschreiben:
  . Mathematisch gesehen ist auch diese Diffusionsgleichung identisch mit der (dreidimensionalen) Wärmeleitungsgleichung, ihre mathematischen Eigenschaften und Lösungsansätze werden im dortigen Artikel behandelt. Die Lösung dieser Gleichung ist in der Regel aufwändig und je nach betrachtetem Gebiet nur numerisch möglich.
  Wenn nun zusätzlich der Diffusionskoeffizient isotrop ist, erhält man eine Differentialgleichung vom Laplace-Typ.
Ist neben der Diffusion auch ein gerichteter Transport beteiligt, so wird die Konzentrationsdynamik durch die Konvektions-Diffusions-Gleichung beschrieben.
Es ist üblich, vier Arten der Diffusion zu unterscheiden. Die Diffusionskoeffizienten unterscheiden sich bei unterschiedlichen Diffusionsarten, auch wenn gleiche Teilchen unter Standardbedingungen diffundieren.
Wenn in einem Gas, einer reinen Flüssigkeit oder einer Lösung kein makroskopischer Gradient existiert, findet ausschließlich echte Selbstdiffusion (engl.: self diffusion) statt. Selbstdiffusion (oft auch als Intradiffusion bezeichnet) ist der Transport von Teilchen innerhalb derselben Substanz, beispielsweise Wassermoleküle in reinem Wasser oder Natriumionen in einer NaCl-Lösung. Da dieses wegen der schwierigen Unterscheidbarkeit physikalisch und chemisch gleicher Teilchen allenfalls mit großem Aufwand zu beobachten ist, nähert man Selbstdiffusion oft mit isotopischen Tracern desselben Stoffes an, beispielsweise 22Na+ für Natriumionen. Dabei geht man davon aus, dass der Gradient, der durch Zugabe des Tracers entsteht, vernachlässigbar klein ist.
Selbstdiffusion ist ein Modell zur Beschreibung der Brownschen Molekularbewegung. Die gemessenen Diffusionskoeffizienten lassen sich über 
Eine besonders geeignete Methode zur Messung von Selbstdiffusionskoeffizienten stellt die Feldgradienten-NMR dar. Hier werden keine isotopischen Tracer benötigt, da physikalisch und chemisch gleiche Teilchen mittels der Kernspin-Präzessionsphase eines im Teilchen befindlichen Atomkerns unterscheidbar werden. Mit dieser NMR-Technik können sowohl Selbstdiffusionskoeffizienten in reinen Flüssigkeiten, wie auch in komplexen, fluiden Gemischen sehr präzise ermittelt werden.
Der Selbstdiffusionskoeffizient des reinen Wassers wurde äußerst genau gemessen und dient daher häufig als Referenzwert. Er beträgt 2,299·10−9 m²·s−1 bei 25 °C und 1,261·10−9 m²·s−1 bei 4 °C.
Tracerdiffusion ist die Diffusion geringer Konzentrationen eines Stoffes in einer Lösung einer zweiten Substanz. Tracerdiffusion unterscheidet sich von der Selbstdiffusion dahingehend, dass ein markiertes Teilchen eines anderen Stoffes als Tracer benutzt wird, z. B. 42K+ in NaCl-Lösung. Häufig werden radioaktiv oder fluoreszenzmarkierte Tracer verwendet, da man diese sehr gut detektieren kann. Bei unendlicher Verdünnung sind die Diffusionskoeffizienten von Selbst- und Tracerdiffusion identisch.
Dies bezeichnet die Diffusion entlang eines relativ starken Gradienten. Bei dieser Art der Diffusion ist eine Approximation des Diffusionskoeffizienten am besten möglich.
Gegendiffusion (engl.: counter diffusion) tritt auf, wenn entgegengesetzte Gradienten vorhanden sind, so dass Teilchen in entgegengesetzte Richtungen diffundieren.
Prinzipiell unterscheidet sich die Diffusion von Teilchen in Gasen hinsichtlich ihrer Gesetzmäßigkeiten nicht von der Diffusion gelöster Teilchen in Flüssigkeiten. Allerdings ist die Geschwindigkeit der Diffusion (bei vergleichbaren Gradienten) hier um Größenordnungen höher, da auch die Bewegung einzelner Teilchen in Gasen erheblich schneller ist.
Die Diffusion verdünnter Gase in Multikomponentensystemen lässt sich mit dem Modell der Maxwell-Stefan-Diffusion beschreiben.
In einem perfekten Kristallgitter schwingt jedes Gitterteilchen um seinen festen Gitterplatz, kann diesen aber nicht verlassen. Eine notwendige Voraussetzung für Diffusion in einem kristallinen Festkörper ist daher das Vorliegen von Fehlern im Gitter. Nur durch Gitterfehler können Platzwechsel von Atomen oder Ionen als Bedingung für Stofftransport stattfinden. Es sind verschiedene Mechanismen denkbar:
Die Teilchen „springen“ in Leerstellen des Gitters, sodass sich Leerstellen durch das Gitter bewegen und ein Nettofluss von Teilchen stattfindet. Dieser Mechanismus wurde durch den Kirkendall-Effekt nachgewiesen.
Kleinere Teilchen bewegen sich durch die Gitterzwischenräume. Auch dieser Mechanismus wurde experimentell nachgewiesen. Er führt im Vergleich zur Diffusion über Leerstellen zu sehr hohen Diffusionskoeffizienten.
Zwei Teilchen tauschen die Plätze oder es finden Ringtausche zwischen mehreren Teilchen statt. Dieser hypothetische Mechanismus konnte experimentell nicht bestätigt werden.
Falls freie Ladungsträger in Halbleitern hinreichend viel Streuung erfahren (z. B. an Phononen, Elektronen und Störstellen), propagieren sie ebenfalls diffusiv.Auch die Diffusion in Kristallen lässt sich durch die Fickschen Gesetze beschreiben. Allerdings können Diffusionskoeffizienten hier von der Raumrichtung abhängen (Anisotropie). Die im isotropen Fall skalaren Diffusionskoeffizienten werden dann zu einem Tensor zweiter Stufe, genannt Diffusionstensor. Deshalb ist der Diffusionsweg eine wichtige Größe zur Beschreibung von Diffusionsvorgängen in Festkörpern.
    {\displaystyle {\overline {\overline {D}}}={\begin{bmatrix}D_{x}&D_{xy}&D_{xz}\\D_{yx}&D_{y}&D_{yz}\\D_{zx}&D_{zy}&D_{z}\end{bmatrix}}}
  eine 3×3-Matrix ist, die als Diffusions-Tensor (oder Diffusionsmatrix) bezeichnet wird. Diese Matrix ist symmetrisch und hat daher jedoch nur sechs unabhängige Komponenten.
Die Diffusion in nichtkristallinen (amorphen) Festkörpern ähnelt in mechanistischer Hinsicht der in Kristallen, wobei allerdings die Unterscheidung zwischen regulären und irregulären Gitterplätzen entfällt. Mathematisch können solche Prozesse gut wie die Diffusion in Flüssigkeiten beschrieben werden.
Eine zusätzliche treibende Größe durch ein vorhandenes Potential führt dazu, dass die Gleichverteilung nicht mehr dem stationären Zustand entspricht. Die Theorie dazu liefert die Fokker-Planck-Gleichung.
Bei den vorstehend beschriebenen Diffusionsprozessen, die durch die Ficksche Diffusionsgleichung beschrieben werden können, steigt die mittlere quadratische Auslenkung 
   der diffundierenden Teilchen (also der mittlere Abstand der Teilchen zu ihrem Startpunkt nach der Zeit 
  Diese Gesetzmäßigkeit folgt aus der Theorie der Brown’schen Molekularbewegung. In Zellen können aber auch andere Gesetzmäßigkeiten beobachtet werden, beispielsweise bei der Bewegung von Makromolekülen durch das Cytoplasma der Zelle. Dieses mit Organellen und (Makro)molekülen dicht besetzte Medium führt zu einer gebremsten Diffusionsbewegung, die einem Potenzgesetz folgt. Es gilt dann:
Die erleichterte Diffusion oder Permeabilität beschreibt in der Biologie die Möglichkeit für bestimmte Stoffe, eine Biomembran leichter zu durchdringen, als dies eigentlich aufgrund ihrer Größe, Ladung, Polarität etc. möglich wäre. Bestimmte Proteine, sogenannte Tunnelproteine, bilden einen Tunnel durch die Zellmembran, der durch seinen Durchmesser und/oder seine Ladungsverteilung bestimmte Stoffe leichter passieren lässt als durch die „geschlossene“ Membran (etwa Ionenkanäle).
Die Begriffe Diffusion und Diffusivität werden auf dem Gebiet der Akustik häufig statt des deutschen Wortes Diffusität verwendet. Die falsche Übersetzung aus dem Englischen trägt dazu bei.
Peter W. Atkins, Charles A. Trapp: Physikalische Chemie. 3. korrigierte Auflage. Wiley-VCH, Weinheim u. a. 2001, ISBN 3-527-30236-0.
E. L. Cussler: Diffusion. Mass Transfer in Fluid Systems. 2nd edition. Cambridge University Press, Cambridge u. a. 1997, ISBN 0-521-56477-8.
J. Crank: The Mathematics of Diffusion. 2nd revised edition. Oxford University Press, Oxford u. a. 1980, ISBN 0-19-853411-6.
Paul Heitjans, Jörg Kärger (Hrsg.): Diffusion in Condensed Matter. Methods, Materials, Models. Greatly enlarged and completely revised edition. Springer, Berlin u. a. 2005, ISBN 3-540-20043-6.
Wilhelm Jost: Diffusion in solids, liquids, gases (= Physical chemistry 1, ISSN 0079-1881). 6th printing. Academic Press, New York NY 1970.
H. J. V. Tyrrell, K. R. Harris: Diffusion in Liquids. A theoretical and experimental Study. Butterworth, London 1984, ISBN 0-408-17591-5.
Die Fickschen Diffusionsgesetze, H. Föll, Technische Fakultät der Christian-Albrechts-Universität zu Kiel
Diffusion in Physikalische Werkstoffeigenschaften (PDF-Datei, 160 kB) beim Leibniz-Institut für Festkörper- und Werkstoffforschung
3D-Gitter-Monte-Carlo-Simulation der Diffusion auf einer vizinal geschnittenen kfz-(100)-Oberfläche von Lars Röntzsch
Video: Diffussion und Ficksche Gesetze – wie schnell geht Stofftransport ohne Strömung?. Jakob Günter Lauth (SciFox) 2013, zur Verfügung gestellt von der Technischen Informationsbibliothek (TIB), doi:10.5446/15654.

Die Digger-Kiefer oder Sabines Kiefer (Pinus sabiniana) ist ein immergrüner Nadelbaum aus der Gattung der Kiefern (Pinus) mit meist zu dritt wachsenden, 20 bis 28 Zentimeter langen Nadeln und sehr großen und schweren, 17 bis 25 Zentimeter langen Samenzapfen. Das natürliche Verbreitungsgebiet liegt in Kalifornien. Die Art wird in der Roten Liste der IUCN als nicht gefährdet eingestuft. Sie wird selten als Holzlieferant genutzt.
Die Digger-Kiefer wächst als immergrüner, bis zu 25 Meter hoher Baum. Der Stamm wächst gerade oder gekrümmt, als Monopodium oder gegabelt und erreicht Brusthöhendurchmesser von 100 Zentimetern. Das größte vermessene Exemplar erreichte 1986 eine Höhe von 49 Metern mit einem Stammdurchmesser von 1,5 Metern und einen Kronendurchmesser von 24 Metern. Dieser Baum existiert mittlerweile nicht mehr, das seither größte bekannte Exemplar erreicht nur noch eine Höhe von 37 Metern.Die Stammborke ist dick, rau und schuppig. Sie ist in braungraue bis schwarzgraue, unregelmäßige, längliche Platten geteilt, die durch rötlich braune Risse getrennt sind. Die Hauptäste sind lang, stehen waagrecht oder aufgerichtet. Die Äste bleiben häufig auch im unteren Bereich des Stamms erhalten und fallen nicht ab. Die wenigen Äste höherer Ordnung bilden eine breite, unregelmäßige und offene Krone. Die Zweige können sowohl dünn als auch dick sein. Sie sind unbehaart, anfangs blass graubraun und später dunkler braun, mit einer dünnen bläulichen Wachsschicht umgeben und nur spärlich benadelt. Sie sind durch vorstehende, herablaufende Pulvini, Reste von abgefallenen Nadelbündeln, rau.
Die Knospen sind harzig, eiförmig-konisch und haben ein spitzes Ende. Endständige Knospen sind 15 bis 25 Millimeter lang, seitständige Knospen sind kleiner. Die um die Knospen wachsenden Niederblätter sind angedrückt, rötlich braun mit blasserem Rand. Die Nadeln wachsen zu dritt in einer anfangs 20 Millimeter langen, sich auf 5 bis 7 Millimeter verkürzenden aber bleibenden, hellbraunen Nadelscheide. Sie sind graugrün, abstehend oder hängend, biegsam, etwas entlang der Längsachse verdreht, meist 20 bis 28 Zentimeter lang, seltener ab 15 und bis 32 Zentimeter und 1,5 Millimeter dick. Der Nadelrand ist fein gesägt, das Ende spitz-stechend bis pfriemförmig. Auf allen Nadelseiten gibt es deutliche Spaltöffnungsstreifen. Es werden meist zwei oder drei seltener bis zu zehn Harzkanäle gebildet. Die Nadeln bleiben drei bis vier Jahre am Baum.
Die Pollenzapfen sind anfangs gelb und werden später orangebraun. Ihre Form ist eiförmig bis ellipsoid und 10 bis 15 Millimeter lang. Die Samenzapfen wachsen meist einzeln, selten in Paaren an der Basis neuer Triebe auf kräftigen, 2 bis 5 Zentimeter langen Stielen. Ausgewachsene Zapfen sind sehr groß, sehr harzig, breit-eiförmig und beinahe symmetrisch. Geöffnete Zapfen sind bei Durchmessern von 15 bis 20 Zentimetern mit flacher oder mehr oder weniger konvexer Basis 17 bis 25 Zentimeter lang. Die 90 bis 120 Samenschuppen sind dick holzig, steif und matt braun. Die Apophyse ist deutlich ausgeprägt, 20 Millimeter breit, schokoladebraun, scharf quer gekielt und geht plötzlich oder graduell in den Umbo über. Dieser liegt dorsal, und ist 10 bis 20 Millimeter lang, an der Basis bis zu 12 Millimeter breit, seitlich gekielt und endet in einem scharfen, hakenförmigen Stachel.Die Zapfen erreichen ein Gewicht von 300 bis 600 Gramm und in Ausnahmefällen auch über 1 Kilogramm. Sie reifen nach zwei Jahren, öffnen sich dann langsam und bleiben noch bis zu fünf weitere Jahre am Baum. Die Samen sind schmal verkehrt-eiförmig, etwas abgeflacht, 15 bis 20 Millimeter lang, 7 bis 10 Millimeter breit, glatt und dunkelbraun. Die Samenflügel sind kurz und breit und etwa 10 Millimeter lang.
Das natürliche Verbreitungsgebiet liegt in Kalifornien im Westen der Vereinigten Staaten. Es werden auch gefährdete Bestände in Oregon angegeben. Die Digger-Kiefer wächst in den Gebirgen und Vorbergen, die das Kalifornische Längstal umgeben, vom Rand der Mojave-Wüste bis zu den Berghängen am Pazifischen Ozean. Man findet sie in Höhen von 50 bis 1800 Metern. Das Verbreitungsgebiet wird der Winterhärtezone 8 zugerechnet, wo im Mittel jährliche Minimaltemperaturen zwischen −12,2 und −6,7° Celsius vorherrschen. Das Klima ist sommertrocken, wobei die jährliche Niederschlagsmenge stark variiert und von 250 Millimeter am Rand der Wüste bis 1780 Millimeter in der Sierra Nevada reicht.In Küstennähe wächst die Digger-Kiefer im brand-anfälligen Chaparral zusammen mit Heidekräutern (Erica) und ähnlichen Arten. Auf den niedrigeren Hängen der Sierra Nevada und den höher liegenden Hängen der Küstengebirge findet man sie mit verschiedenen Eichenarten und häufig mit der Coulter-Kiefer (Pinus coulteri), und im Norden des Verbreitungsgebiets mit dem Westamerikanischen Wacholder (Juniperus occidentalis). Die Digger-Kiefer wächst meist in offenem Waldland, mit einzelnen Bäumen, die aus einer Buschschicht herausragen, oder in hauptsächlich von Gräsern und Kräutern bewachsenen Gebieten. Die schweren Zapfen werden von verschiedenen Hörnchen (Sciuridae) und Hähern als Nahrungsquelle benutzt. Die Hörnchen holen sich die Zapfen von den Bäumen und nagen sich durch die dicken Schuppen, um zu den Samen zu kommen. Die Häher spielen eine wichtige Rolle bei der Verteilung der Samen.Die Digger-Kiefer wird vom Rostpilz Peridermium harknessii befallen, der im gesamten Verbreitungsgebiet Gallbildung an den Zweigen, aber keine schweren Schäden verursacht. Die Zwergmistel Arceuthobium occidentale befällt besonders Bäume in lockeren Wäldern und breitet sich sehr rasch aus. Befall führt zu Wachstumseinbußen, Deformierungen und auch zum Absterben von Bäumen. Die Wurzeln werden vom Gemeinen Wurzelschwamm (Heterobasidion annosum) befallen, der in lockeren Wäldern kaum Schaden verursacht, sich in Plantagen jedoch rasch ausbreiten kann. Zapfen, Zweige und Nadeln werden von einer Vielzahl von Insekten angegriffen. Der Käfer Ips spinifer befällt die Borke und verursacht häufig das Absterben schon durch Trockenheit oder Feuer geschwächter Bäume. Durch die erhöhte Produktion von Harz können Käfer abgewehrt werden, für manche sind auch die Harzdämpfe giftig. Das Harz zieht jedoch den Wickler Petrova sabiniana an, der sich im Harz verpuppt. Die Samen werden durch ihre Schale vor Insekten geschützt, doch wird ein großer Teil der Samen von Nagetieren und Vögeln gefressen.In der Roten Liste der IUCN wird Pinus sabiniana als nicht gefährdet („Lower Risk/least concern“) eingestuft. Es wird jedoch darauf hingewiesen, dass eine Neubeurteilung notwendig ist.
Die Digger-Kiefer (Pinus sabiniana) ist eine Art aus der Gattung der Kiefern (Pinus), in der sie der Untergattung Pinus, Sektion Trifoliae und Untersektion Ponderosae zugeordnet ist. Erstmals entdeckt wurde sie 1826 von David Douglas im Gebiet der Umpqua, einem im südlichen Oregon lebenden Indianerstamm, doch verlor er die Proben beim Überqueren des Santiam River. Er fand die Art erst 1831 auf einer Reise durch die Gabilan-Berge wieder. 1832 wurde die Art von David Don in Description of the Genus Pinus erstmals formal wissenschaftlich beschrieben, wobei er David Douglas’ Beschreibung wörtlich übernahm.Der Gattungsname Pinus wurde schon von den Römern für mehrere Kiefernarten verwendet. Das Artepitheton sabiniana könnte sich auf Joseph Sabine (1770–1837) beziehen, einen Finanzbeamten und Sekretär der Horticultural Society, der Douglas' Sammeltätigkeit förderte. Der Name könnte allerdings auch seinen jüngerer Bruder Edward Sabine (1788–1883) ehren, den früheren Präsidenten der Royal Society. Manchmal wird das Artepitheton auch sabineana geschrieben.Die Digger-Kiefer bildet nur sehr schwer Hybride mit anderen Pinus-Arten: Lediglich ein Versuch der Hybridisierung mit der Coulter-Kiefer war erfolgreich (Stand 2010). Hingegen gab es mit Pinus torreyana mehrere erfolgreiche Hybridisierungsversuche. Anders als mit der Coulter-Kiefer gelang keine Hybridisierung mit Jeffreys Kiefer (Pinus jeffreyi) oder einer anderen Art der Subsektion Ponderosae.
Die Digger-Kiefer hat wegen des unregelmäßigen Wuchses und des hohen Harzanteils nur eine geringe wirtschaftliche Bedeutung. Das Holz wird zur Herstellung von Bahnschwellen, Paletten und Hackschnitzeln verwendet. Als Zierbaum tritt die Digger-Kiefer kaum in Erscheinung:  Man findet sie selten in Arboreten und Pineten in Gebieten mit geeignetem Klima wie in England, Westfrankreich, im Mittelmeerraum und in Australien.
Die indigene Bevölkerung nutzte die Samen als Nahrungsmittel und das Harz sowohl zur Herstellung von Trommeln als auch zum Abdichten von Körben. Der Nahrungswert der Samen ist vergleichbar mit dem anderer essbarer Kiefernkerne, doch werden die Samen kaum wirtschaftlich genutzt. Aus den Zweigen und Nadeln werden Öle und Terpentin gewonnen.
Aljos Farjon: A Handbook of the World's Conifers. Band 2. Brill, Leiden-Boston 2010, ISBN 90-04-17718-3, S. 756–757. 
James E. Eckenwalder: Conifers of the World. The Complete Reference. Timber Press, Portland, OR/London 2009, ISBN 978-0-88192-974-4, S. 476. 
Andreas Roloff, Andreas Bärtels: Flora der Gehölze. Bestimmung, Eigenschaften und Verwendung. Mit einem Winterschlüssel von Bernd Schulz. 3., korrigierte Auflage. Eugen Ulmer, Stuttgart (Hohenheim) 2008, ISBN 978-3-8001-5614-6, S. 775.
Russell H. Burns: Silvics of North America. Band 1 Conifers. United States Government Printing, 1991, ISBN 978-0-16-027145-8. 
Flora of North America Editorial Committee (Hrsg.): Flora of North America North of Mexico. Volume 2: Pteridophytes and Gymnosperms. Oxford University Press, New York/Oxford u. a. 1993, ISBN 0-19-508242-7 (englisch). 
Helmut Genaust: Etymologisches Wörterbuch der botanischen Pflanzennamen. 3. Auflage. Nikol, Hamburg 2005, ISBN 3-937872-16-7, S. 487.

Digitale Rechteverwaltung (auch Digitale Beschränkungsverwaltung oder Digitale Rechteminderung oder Digitales Rechtemanagement bzw. engl. Digital Rights Management oder kurz DRM) bezeichnet Verfahren, mit denen die Nutzung (und Verbreitung) digitaler Medien kontrolliert werden soll.
Vor allem bei digital vorliegenden Film- und Tonaufnahmen, aber auch bei Software, elektronischen Dokumenten oder elektronischen Büchern findet die digitale Nutzungsverwaltung Verwendung. Sie ermöglicht Anbietern, die solche DRM-Systeme einsetzen, prinzipiell neue Abrechnungsmöglichkeiten, um beispielsweise mittels Lizenzen und Berechtigungen sich Nutzungsrechte an Daten, anstatt die Daten selbst, vergüten zu lassen. Für den Endnutzer bedeutet das eine Beschränkung.
Im Gegensatz zu analogen Informationen lassen sich digitalisierte Inhalte jeglicher Art problemlos vervielfältigen und prinzipiell unbeschränkt weiterverbreiten. Dieser oftmals unkontrollierte Informationsfluss führt allerdings zwangsläufig zu Konflikten zwischen den Nutzern und den Urhebern bzw. den Rechteinhabern digitaler Inhalte, da eine unkontrollierte Nutzung gegen das Urheberrecht verstößt und sich in der Regel negativ auf das zugrunde liegende Geschäftsmodell auswirkt. Es ist daher aus Sicht der Urheber und Verwerter essentiell, Schranken zu definieren, die den Zugriff auf geschütztes geistiges Eigentum reglementieren und auch nach einer Weitergabe beschränken können. Ein DRM-System (DRMS) soll dabei helfen, indem es die Verwendung von Daten nur in dem von den jeweiligen Rechteinhabern definierten Rahmen (Lizenz) ermöglicht.
Mechanismen der digitalen Rechteverwaltung sind allgemein jedoch stark umstritten. Befürworter sehen in Systemen der digitalen Rechteverwaltung hauptsächlich die Eröffnung neuer Geschäftsmodelle mit bedarfsgerechterer Abrechnung (Pay-per-View) sowie den potentiellen Wegfall von Pauschalabgaben auf Leermedien wie CD-Rohlinge und der damit einhergehenden Entlastung der Verbraucher. Zudem können DRMS (DRM-Systeme) auch zum Schutz kritischer Daten wie zum Beispiel Unternehmensinterna eingesetzt werden (Enterprise Rights Management). Kritiker warnen vor allem vor Datenschutzproblemen und möglichen Einschränkungen bei der Benutzerfreundlichkeit und Archivierung sowie davor, dass es unmöglich wird, die Schranken des Urheberrechts geltend zu machen. Als problematisch wird angesehen, dass durch Verwendung dieses Systems die Interoperabilität der Geräte und digitaler Inhalte eingeschränkt wird.Zu einem ernsten Problem aus Sicht vieler Vertreter der Musikindustrie und Verwerter wurde die beliebige Kopierbarkeit von digitalen Inhalten erstmals Mitte der 1990er Jahre, als CD-Brenner für Endverbraucher erschwinglich und Personal Computer leistungsfähig genug für den Umgang mit im MP3-Format komprimierter Musik wurden. Ende der 1990er Jahre erfuhren außerdem die so genannten Internet-Tauschbörsen immer stärkeren Zulauf, da Internet-Benutzer dort prinzipiell kostenlos Dateien von der Festplatte anderer Benutzer kopieren können. Oft handelt es sich dabei um urheberrechtlich geschützte Musik, Filme oder Software. Dies führte laut Angaben der Medienindustrie zu teils erheblichen Umsatzrückgängen. Aufgrund der unbegrenzten Vervielfältigungsmöglichkeiten nutzten Medienunternehmen die durch das Internet ermöglichten neuen digitalen Vertriebswege lange Zeit nicht. Die wachsende Bedeutung des Internets brachte die Unternehmen jedoch zunehmend in Handlungszwang, der sich in der Entwicklung von DRM-Systemen (genauer: Multimedia Rights Management) niederschlug. Erst im Jahr 2003 gewann schließlich mit der Eröffnung des iTunes Music Store ein Vertriebsweg mit integrierter digitaler Rechteverwaltung an kommerzieller Bedeutung.
Im Allgemeinen bezeichnet man eine Bandbreite von Technologien mit dem Begriff „Digital Rights Management“. Hauptanreiz für die Entwicklung von Digital-Rights-Management-Systemen war der Schutz von Verwertungsrechten an Bild-, Ton- oder Videoaufnahmen.
Mittlerweile finden DRMS aber auch in vielen anderen Bereichen Anwendung, zum Beispiel in Unternehmen, um Dokumente zu schützen.
Die Vielzahl der Definitionen lassen sich in weit umfassende und engere Definitionen unterteilen. Hier werden zwei vorgestellt:
Als DRMS bezeichnet man technische Sicherheitsmaßnahmen, die es den Rechteinhaber von Informationsgütern ermöglichen, die Art der Nutzung seines Eigentums zu beschränken und so die Einhaltung einer zuvor getroffenen Nutzungsvereinbarung zu erzwingen.
Zu DRMS gehören im Allgemeinen auch Technologien, die digitale Wasserzeichen nutzen. Diese bieten nur eingeschränkte Möglichkeiten zur Nutzungskontrolle (zum Beispiel Einsatz von fragilen Wasserzeichen, welche die Darstellung oder das Abspielen von kopierten Inhalten in besonderen Abspielgeräten verhindern).
Fränkl/Karpf (2003) definieren DRMS als „technische Lösungen zur sicheren zugangs- und nutzungskontrollierten Distribution, Abrechnung und Verwaltung von digitalem und physischem Content“.
Elektronische Schutzmechanismen für digitale Informationen nennt man DRMS. Sie ermöglichen die Verwertung von digitalen Inhalten über eine reine Pauschalvergütung hinaus und erlauben zusätzlich die individuelle Lizenzierung/Abrechnung nach Häufigkeit, Dauer oder Umfang der Nutzung. Damit wird einerseits die unbegrenzte Nutzung einschränkbar, andererseits werden On-Demand-Geschäftsmodelle ermöglicht, die vorher kaum zu realisieren waren.
Digital copy, das Recht, eine legale Kopie auf einem PC und einem Portable Media Player anzufertigen (Film).
OMA DRM 1.0 und 2.0 – Spezifikationen für mobile Endgeräte, teils geeignet für alle IT-Plattformen (implementiert in zahlreichen Handys)
Microsoft Windows Media Digital Rights Management Version 10 – Für Windows Media Audio (WMA) und Windows Media Video (WMV) Dateien.
DRM wird hauptsächlich bei digitalen Inhalten wie Software, Filmen oder Musik eingesetzt. Am weitesten verbreitet sind die DRMS „FairPlay“ von Apple, „Windows Media DRM“ von Microsoft und das OMA DRM der Open Mobile Alliance. Diese ermöglichen eine genaue Einstellung der Berechtigungen und können für verschiedene Audio- und Videodateien verwendet werden. Marktführer Apple nutzt FairPlay im iTunes Store, andere Onlineshops wie Napster und Musicload, aber auch „Video-on-Demand“-Dienste verwenden vornehmlich das DRM-System von Microsoft. Das OMA DRM wird in fast jedem Mobiltelefon für Klingeltöne, Bilder, aber auch für mobile Musik- und Fernsehübertragungen (mobile TV) z. B. von Vodafone oder T-Mobile eingesetzt. Häufig werden die Systeme des OMA DRM und des Windows Media DRM kombiniert, um eine Interoperabilität zwischen Mobiltelefonen und PCs zu ermöglichen. Beispiele sind hier Musicload und Vodafone.
Neuerdings werden DRM-Techniken von der Industrie auch bei traditionellen nicht-digitalen Produkten eingesetzt. Beispiele sind Kaffeemaschinen von Keurig Green Mountain und Traktoren von John Deere.
DRM-Systeme verwirklichen die Idee der Zugriffskontrolle digitaler Inhalte mit Hilfe kryptografischer Verfahren. Realisiert wird dies, indem ein beliebiger digitaler Inhalt durch Verschlüsselung eindeutig an eine Lizenz gebunden wird. Ohne die zum digitalen Inhalt gehörige gültige Lizenz kann der Benutzer zwar das Gerät oder den Datenträger erwerben, nicht jedoch auf den Inhalt zugreifen.
Der Inhalteserver verwaltet die zu schützenden digitalen Inhalte und verschlüsselt diese mit Hilfe des DRM-Verpackers zur Verwendung in einem DRMS, wodurch die Inhalte vorerst unlesbar werden. Der Lizenzserver erzeugt auf Anforderung die erforderlichen Lizenzen zusammen mit den zugehörigen Schlüsseln für die Benutzerauthentifizierung und Inhalteentschlüsselung, welche aus den entsprechenden Kennungen (Benutzer- oder Gerätkennung, Inhaltekennung) und den Beschreibungen der Rechte berechnet werden. Möchte der Benutzer auf einen per DRM geschützten Inhalt zugreifen, fordert die DRM-Steuerung vom Lizenzserver die zur Wiedergabe notwendige Lizenz an. Werden Authentizität und Integrität des Wiedergabeprogramms verifiziert, werden die Inhalte mit dem in der Lizenz enthaltenen Schlüssel entschlüsselt, auf diese Weise wieder lesbar gemacht und an das Wiedergabeprogramm weitergegeben.
In Zukunft können Techniken des Trusted Computing verwendet werden, um die Einhaltung der Rechte zu gewährleisten.
DRMS sollten vorrangig die Weitergabe von und Zugriff auf digitale Inhalten auf offenen Plattformen kontrollierbar machen. DRMS sollten daher insbesondere Funktionen zur Zugangs- und zur Nutzungssteuerung bereitstellen. Während es bei der Zugangssteuerung um die Bestimmung des Personenkreises („Wer?“) geht, steht bei der Nutzungssteuerung die Art der Nutzung („Wie?“) im Mittelpunkt. Beide Funktionen greifen auf Lizenzdaten zu, die in unterschiedlicher Granularität die notwendigen Nutzungsrechte definieren.
Um digitalen Inhalten auch außerhalb eines DRMS einen gewissen Schutz zu ermöglichen, kann eine möglichst nicht leicht zu entfernende Kennzeichnung der Inhalte mögliche Lizenzverletzungen auch nachträglich erkennen.
Insbesondere sollen DRMS neue Optionen bei der Gestaltung von Erlösmodellen eröffnen. DRMS können diese einerseits durch die Bereitstellung einer Abrechnungsfunktion unterstützen. Die mit Hilfe der Abrechnungsfunktion erfassten Nutzungsdaten werden gesammelt und können dann von einem Abrechnungssystem beliebiger Art (wie zum Beispiel einem Micropayment-System) weiterverarbeitet werden. Andererseits kann die bereits erwähnte Nutzungssteuerung eine gruppen- oder selbst personenbezogene Differenzierung von Rechten und Preisen unterstützen. In der rechten Abbildung ist der logische Aufbau eines DRMS im Überblick dargestellt.
Ziel dieser Funktion ist es sicherzustellen, dass der Zugriff auf geschützte Inhalte nur entsprechend lizenzierten Personen und/oder Endgeräten gewährt wird.
Dabei kann der Zugriff auf digitale Inhalte neben dem berechtigten Subjekt auch hinsichtlich Zeitpunkt und Standort eingegrenzt werden. Hierbei wird der Benutzer im ersten Schritt mittels eines Authentifizierungsverfahrens identifiziert. Danach werden seine Zugriffsrechte geprüft.
Für die Identifizierung des Benutzers gibt es unterschiedliche Lösungsansätze: Das Spektrum an Verfahren reicht von Passwörtern (z. B. Software-ID) oder Hardware-Authentifikation (z. B. X.509 oder CPU) bis hin zu biometrischen Verfahren. Passwort-basierte Systeme sind zwar einfach und kostengünstig zu implementieren, eignen sich aber durch die Möglichkeit der Weitergabe des Passworts nicht zuverlässig für die Identifizierung eines Benutzers. Aufwändigere Verfahren, bis hin zur Biometrie, erhöhen zwar die Implementierungskosten, bieten dafür aber eine zuverlässigere Möglichkeit zur Benutzerauthentifizierung, wobei die Nachteile biometrischer Verfahren nicht außer Acht gelassen werden dürfen.
Die Durchsetzung einer entsprechenden Lizenz muss auch nach einer erfolgreichen Zugriffautorisierung gewährleistet werden. Die zum Zugriff auf die geschützten Inhalte verwendeten Programme müssen daher eine Beschreibung der berechtigten Verfügungsformen (Lizenz) verstehen und geeignet durchsetzen können.
Recht, abgeleitete Werke zu erstellen (extrahieren, editieren und einfügen)So könnte beispielsweise das Ausdrucken und die Ausgabe eines Dokumentes auf dem Bildschirm erlaubt (als positives Wiedergaberecht), aber die Weitergabe durch einen lokalen Speicherschutz unterbunden werden (als Einschränkung der Transportrechte). In ihrer einfachsten Form umfassen Nutzungssteuerungssysteme damit einen simplen Kopierschutzmechanismus (wie zum Beispiel beim „Digital Audio Tape“ (DAT) oder beim DVD-Standard). In der Regel ist es jedoch nicht das Ziel, das Kopieren völlig zu unterbinden, sondern Kopiervorgänge im Sinne einer Kopierkontrolle steuern zu können.
DRMS ermöglichen nicht nur den Schutz digitaler Inhalte, sondern auch durch die oftmals vorhandene Möglichkeit der Überwachung der Nutzung der DRM-geschützten Daten, die Etablierung nutzungsabhängiger Bezahlmodelle (Pay-per-View, Pay-per-Click etc.). Verbraucher können so nicht nur pauschal, sondern auch selektiv und in kleinen Mengen Inhalte erwerben. Inhalteanbieter wiederum erhoffen sich eine maximale Ausschöpfung ihrer Verwertungsrechte.
Technisch gesehen ist bei der Einzelnutzungsabrechnung eine enge Verzahnung von Systemkomponenten auf Anbieter- und auf Nutzerseite erforderlich. Dies kann soweit gehen, dass die Nutzung der Inhalte in Echtzeit detailliert mitprotokollieren und diese Informationen per Rückkanal an das Abrechnungssystem des Anbieters weitergeben. Neben der Protokollierungsfunktion und Rückkanalfähigkeit ist zusätzlich die Integration von sicheren, elektronischen Zahlungssystemen notwendig.
In DRMS, die auch Superdistributionsfunktionen implementieren, können Konsumenten erworbene Inhalte, die entsprechenden Rechte vorausgesetzt, beispielsweise selbst weiterverkaufen oder durch erfolgreiche Vermittlung neuer Kunden eine entsprechende Vermittlungsprämie verdienen.
Vollkommener Schutz ist auch durch DRMS nicht durchsetzbar: Auch wenn die technischen Schutzmöglichkeiten den Angriffstechniken und -werkzeugen der Cracker einen Schritt voraus bleiben sollten, besteht beispielsweise oftmals das „Problem der analogen Lücke“, d. h. die Möglichkeit, Analogkopien hochwertig zu redigitalisieren und ungeschützt weiterzuverbreiten.
Dementsprechend ergreifen Inhalteanbieter nicht nur vorbeugende, sondern auch reaktive Maßnahmen zum Schutz ihrer Inhalte. Diese beugen zwar nicht direkt Lizenzverletzungen vor, können aber durch den Abschreckungseffekt mögliche Lizenzverletzungen einschränken. Voraussetzung für die Identifizierung von Analogkopien sind entsprechend gesetzte Markierungen oder die Abwesenheit von Markierungen als Zeichen für kompromittierte Medienprodukte. Es lassen sich auch hier verschiedene Verfahren unterscheiden:
Zu den schwachen Markierungsverfahren zählen das Labeling und das Tatooing, welche im ersten Fall die lizenzrechtlichen Informationen in bestimmten Abschnitten des Medienproduktes (üblicherweise im Header) platzieren und im letzteren Fall einen Lizenzvermerk sicht- bzw. hörbar in das Medienprodukt einfügen. Diese Verfahren sind jedoch leicht überwindbar, weil die Metainformationen nicht versteckt werden. Außerdem sinkt durch sie die Qualität des Medienproduktes, da solche Maßnahmen häufig störend wirken. Zu den harten Markierungsverfahren zählen Wasserzeichen, welche die versteckte Einbettung von Metadaten in Medienprodukten ermöglichen.
Es ist auch eine Kombination verschiedener Verfahren außerhalb eines DRMS möglich. Für elektronische Bücher hat der Internet-Verlag tredition neben Wasserzeichen und dem Standard-Adobe-PDF-Kopierschutz zusätzlich eine „psychologische“ Barriere eingebaut, indem der Name des legalen Erwerbers zuzüglich weiterer persönlicher Daten für jeden sichtbar implementiert werden. Beim Weiterleiten werden die Daten des Urheberrechtsverletzers automatisch versendet.
Die Identifikation von nicht lizenzierten Medienprodukten kann beispielsweise automatisiert durch Internet-Suchroboter erfolgen. Diese können anhand der charakteristischen Bitmuster eines Medienproduktes und gesetzter oder fehlender Markierungen nicht lizenzierte Inhalte finden. Bei Verwendung von entsprechenden digitalen Fingerabdrücken in den Mediendateien kann sogar der ursprüngliche Käufer aufgespürt werden.
Zugangs- und Nutzungssteuerung benötigen die Basistechniken der Kryptografie, Rechtedefinitionssprachen und ggf. Abrechnungsfunktionen. Wasserzeichen sollen die lizenzrechtlichen Bestimmungen auch außerhalb eines DRMS zumindest nachträglich erkennbar machen.
Um die unberechtigte Nutzung, Veränderung oder Verfälschung geschützter Inhalte zu verhindern, können eine Vielzahl von kryptografischen Techniken verwendet werden. Kryptografische Verfahren kommen insbesondere im Rahmen der Zugriffs- und Nutzungskontrolle sowie der sicheren Abrechnung zum Einsatz. Digitale Signaturen können beispielsweise die Authentizität eines Berechtigten sicherstellen.
Im Rahmen elektronischer Zahlungssysteme helfen Verschlüsselungsverfahren (insbesondere das Secure-Electronic-Transaction-(SET-)System) bei der sicheren Übertragung von sensiblen Abrechnungsdaten (z. B. Kreditkartennummern) über das Internet.
Weiterhin können symmetrische Authentifikationssysteme im Rahmen von so genannten Challenge-Response-Verfahren einen Beitrag zur Identifikation und Ausschaltung (device revocation) von manipulierten DRMS-Geräten und -Programmen und damit gegen unautorisierten Medienkonsum leisten.
Ziel der verschiedenen Wasserzeichenverfahren ist es, bestimmte Informationen unwiderruflich mit einem Medienprodukt zu verbinden. Zu unterscheiden sind drei Varianten:
Bei sichtbaren Wasserzeichen wird eine klar erkennbare Urheberrechts-Markierung an das zu schützende Objekt angebracht, was die nicht autorisierte Nutzung unattraktiv machen soll und in jedem Fall zu einem (wenn auch manchmal marginalen) Qualitätsverlust führt. Nach dem legitimen Kauf eines Medienprodukts werden sichtbare Wasserzeichen in der Regel entfernt bzw. unsichtbare Wasserzeichen neu eingesetzt.
In (unsichtbar-)robusten Wasserzeichen werden rechtebezogene Informationen im Inhalt „versteckt“, d. h. unsichtbar gespeichert und untrennbar mit dem Werk verbunden. Derartige Informationen werden häufig zur Überprüfung von Zugangs- und Nutzungsrechten und für Abrechnungszwecke genutzt. Gelegentlich umfassen robuste Wasserzeichen auch Informationen zum Lizenznehmer. Im letzten Fall spricht man von digitalen Fingerabdrücken, die sich zur Rechtsverfolgung einsetzen lassen.
(Unsichtbar-)fragile Wasserzeichen dienen dem Nachweis der Unverfälschtheit (Unversehrtheit und Integrität), um Manipulationen zu erkennen. Hierbei wird überprüft, ob eine Mediendatei manipuliert wurde. Dabei sollen fragile Wasserzeichen nur gegen Verarbeitungsoperationen (Komprimierung, Skalierung etc.) robust sein, während bei inhaltlichen Änderungen (z. B. Bildmanipulationen) das Wasserzeichen zerstört werden soll. Daher lassen sich fragile Wasserzeichen für die Verfolgung von Rechtsverletzungen einsetzen.Sowohl bei den robusten als auch bei den unsichtbaren Wasserzeichen kommen steganografische Algorithmen zum Einsatz.
Rechtedefinitionssprachen erlauben die Beschreibung des Umfangs der eingeräumten Rechte und ggf. die gewählte Form der Abrechnung. Hierzu werden durch das DRMS je nach Anforderung die lizenzierten Nutzungsmöglichkeiten abgebildet und ggf. mit Preisen hinterlegt. Je nachdem wie mächtig die Rechtedefinitionssprache ist, können Nutzungsrechte sehr differenziert abgebildet und abgerechnet werden: Nutzungszeitraum, -häufigkeit, -qualität (Bild- und Hörqualität), -operationen (drucken, ändern, kopieren etc.) und weitere Bedingungen bzw. Einschränkungen (geographischer, sprachlicher oder endgeräte-spezifischer Natur) können granular definiert werden und ermöglichen eine zielgerichtete Nutzungskontrolle. Rechtedefinitionssprachen sollen dabei idealerweise alle denkbaren (also sowohl bestehende als auch neue) Rechtedimensionen über alle Auswertungsformen, Medienformen (Print, Audio, Bewegtbild) und Abrechnungsmodalitäten in maschinenlesbarer Form abbilden.
Die Möglichkeit der individuellen Steuerung und Abrechnung des Gebrauchs ermöglicht so bisher nicht realisierbare digitale und nutzungsabhängige Geschäftsmodelle. Die hierfür benutzte Sprache kann entweder proprietär oder offen sein. Eine offene und damit standardisierte Sprache ist notwendig, wenn eine plattformübergreifende, interoperable Nutzung anvisiert wird. Beispiele für etablierte Standards sind die durch die Organization for the Advancement of Structured Information Standards (OASIS) vorangetriebene eXtensible rights Markup Language (XrML) sowie die von der ODRL Initiative entwickelte Open Digital Rights Language (ODRL). Das XrML-Datenmodell besteht aus vier Entitäten sowie deren Beziehungen zueinander. Die dargestellte Hauptbeziehung zwischen den vier Entitäten wird durch die so genannte „Grant Assertion“ definiert, bestehend aus „Principal“ (Lizenznehmer), „Right“ (Nutzungsumfang), „Resource“ (lizenziertes Werk) und „Condition“ (Vorbedingung, die erfüllt sein muss bevor das Recht ausgeübt werden kann).
Rechteinformationen können entweder mittels steganografischer Verfahren untrennbar an die Medienprodukte angefügt oder separat zu diesen geliefert werden. Der Vorteil der ersteren Variante ist, dass es zu keiner ungewünschten Entkopplung zwischen Medienprodukt und Nutzungskontrollinformationen kommt. Bei der zweiten Form können Rechteinformationen flexibler geändert werden, was dezentralen Geschäftsmodellen (insbesondere Superdistribution) entgegenkommt.
Ähnlich wie bei Verschlüsselungstechniken kommen Rechtedefinitionssprachen im Rahmen von DRMS umfassend zum Einsatz: Sie unterstützen mittels Einbringung von Kundeninformationen die Zugangssteuerung, indem das lokale Abgreifen der Medienprodukte nur vorab autorisierten Nutzern gestattet wird. Primärzweck ist jedoch die Realisierung einer flexiblen Nutzungssteuerung sowie nutzungsabhängiger Abrechnung durch Rechte- und Abrechnungsinformationen.
In der abgebildeten Tabelle ist der funktionale Beitrag der drei dargestellten Techniken noch einmal im Überblick dargestellt. Die Darstellung ist nicht vollständig, sondern will lediglich zeigen, dass Basistechniken nicht isoliert, sondern kombiniert eingesetzt werden müssen, um die funktionalen Anforderungen zu realisieren. Eine effiziente Nutzungssteuerung wird zum Beispiel erst durch die Kombination aller drei Kerntechniken erzielt.
Die Wirksamkeit solcher Systeme wird häufig durch nationale Gesetze erweitert. In den USA wurde zu diesem Zweck der Digital Millennium Copyright Act (DMCA) verabschiedet. Dieses Gesetz verbietet dort die Umgehung solcher Systeme unter Androhung von Geldstrafen und/oder Freiheitsentzug je festgestelltem Einzelfall.
Auch in Deutschland (1. und 2. Korb der Urheberrechtsnovelle) und der EU (Informationsrichtlinie) wurde die Rechtsgrundlage in diesem Sinne verschärft, so dass nun die Umgehung von wirksamen Schutzmechanismen mit Freiheitsstrafe bis zu drei Jahren oder mit einer Geldstrafe belegt werden kann, falls die Tat nicht im privaten Rahmen geschieht (vgl. § 108b UrhG). Eine solche Umgehung ausschließlich zum Erstellen einer Privatkopie im Sinne der festgelegten Schranken des Urheberrechts ist jedoch straffrei. Darüber hinaus darf bei Computerprogrammen eine Kopiersperre zwar umgangen werden (§69a UrhG), jedoch ist eine Vervielfältigung nur mit Zustimmung des Rechtsinhabers zulässig (§ 69c UrhG).
In der Musikindustrie konnte sich DRM nicht durchsetzen. Mittlerweile verkaufen alle vier Major-Labels ihre Musik im Internet ohne DRM. Ausgangspunkt war, dass Verbraucher den Kauf von DRM-Musiktiteln teilweise ablehnten. Als erstes Major-Label verkaufte EMI ab April 2007 mit Apples iTunes Musik im Internet ohne DRM, mit großem Erfolg. Das zwang die anderen Major-Labels kurze Zeit später, ebenfalls DRM fallen zu lassen. Die Independent-Labels verkauften von Anfang an ohne DRM mit großem Erfolg, was die Major-Labels zum Nachziehen zwang.
In der Film- bzw. Video-Industrie verbreiten sich dagegen DRM-Techniken, so bei der Einführung der HDMI- und DisplayPort-Schnittstellen. Letztere führte etwa im Falle der 2008 vorgestellten MacBooks von Apple zu einiger Irritation unter Benutzern, als über DisplayPort angeschlossene Displays auch bei gekauften Filmen schwarz blieben.
Um DRM-Systeme herrscht eine intensive Diskussion zwischen Befürwortern und Gegnern. Unterstützer sind weitestgehend im Bereich der Inhalteanbieter zu finden, während sich ein Großteil der Kritiker aus Verbraucher- und Datenschützern zusammensetzt.
Anfang Februar 2007 hat sich jedoch Apple-Chef Steve Jobs, dessen Unternehmen mit FairPlay als erstes ein DRM-System am Massenmarkt etablieren konnte, gegen die Verwendung solcher Systeme ausgesprochen, da sie sowohl den Konsumenten als auch den Musikanbietern wie Apple zahlreiche Nachteile brächten. Nach seinen Angaben sei die Verwendung von DRM-Systemen beim digitalen Musikvertrieb von den vier größten Tonträgerunternehmen Universal Music, Sony BMG, Warner und EMI Group erzwungen worden. Am 2. April 2007 hat schließlich EMI als erste der vier angesprochenen Unternehmen auf einer gemeinsamen Pressekonferenz mit Steve Jobs angekündigt, von nun an auch den Verkauf ihrer Musik in DRM-freien Formaten durch ihre Händler zu unterstützen. Im August 2007 hat Universal Music angekündigt, testweise bis Januar 2008 DRM-freie Musikdownloads über zahlreiche Vertriebskanäle, allerdings explizit nicht über Apple, anzubieten. Mittlerweile haben alle Studios den Vertrieb von DRM-geschützten Inhalten abgebrochen.
Im PC-Computerspiel-Bereich trat 2008 mit GOG.com ein digitaler Distributor auf, welcher offen DRM kritisierte und seine Angebote explizit als „DRM-frei“ vermarktet.
Die beim Begriff „Digital Rights Management“ angesprochenen Rechte beziehen sich nicht notwendigerweise auch auf Rechte im Sinne des Gesetzes. Das englische Wort right ist eher mit Berechtigung zu übersetzen. Weil bestimmte andere Berechtigungen zur Benutzung von geschützten Daten durch DRM eingeschränkt werden können, interpretieren die FSF/GNU und andere Kritiker die Abkürzung DRM auch als Digital Restrictions Management (dt. Digitale Restriktionsverwaltung) bzw. als Digitale Rechteminderung.
Ein Nachteil von DRM mit Verschlüsselung ist die Inkompatibilität mit manchen (vor allem älteren oder preisgünstigen) Wiedergabegeräten. So lässt sich eine durch DRM geschützte Mediendatei trotz erworbener Lizenz nicht auf allen mobilen Geräten wiedergeben, sondern nur mit solchen, die das jeweilige DRM unterstützen. Dies trifft auch auf die Wiedergabe vom PC zu: nur spezielle Software kann dort die Medien wiedergeben. Der zusätzliche Abgleichvorgang mit dem Lizenzierungsserver und Entschlüsselungvorgang erschwert teilweise auch das Handling mit entsprechenden Medien.
Darüber hinaus gibt es zahlreiche Kritikpunkte an der Implementierung und den Umgang der Entwickler entsprechender Software mit DRM. So kommt es insbesondere im Zusammenhang mit Windows häufiger zu beschädigten DRM-Datenbanken, welche eine Wiedergabe trotz erworbener Lizenz selbst auf einem PC unmöglich machen. Die Lizenzübertragung kann sich bei hochfrequentierten Diensten aufgrund der Auslastung einiger Anbieter geschützter Inhalte als langwierig erweisen. In Zusammenhang mit der mangelhaften Implementierung entsprechender Wiedergabesoftware kommt es häufig zu – für den Durchschnittsnutzer – nicht aussagekräftigen Fehlermeldungen.
Ein einfaches Kopieren von DRM-Medien mit Verschlüsselung reicht für eine Datensicherung nicht aus, da die jeweiligen Lizenzinformationen mit gesichert werden müssen. Nicht jeder DRM-fähige Mediaplayer (zum Beispiel Vorabversion des Microsoft Mediaplayer 11) verfügt über eine für die Sicherung notwendige Funktion.
Der Käufer eines digitalen Musikabspielgeräts könnte wegen DRM-Restriktionen nicht frei wählen, wo er seine Musik einkauft, wenn sein Player nicht eines der DRM-Systeme unterstützt, die vom Hersteller freigegeben wurden. So wäre es für einen Marktführer im Online-Musikhandel, der zugleich Marktführer für Festplatten-Musikabspielgeräte ist, möglich, seine Kunden an sein System zu binden, wie beispielsweise Apple dies mit dem proprietären Kopierschutzverfahren FairPlay bei iTunes und dem iPod – mittlerweile nur noch im Bereich der Videos – versucht. DRM gewänne so in der Praxis mehr Bedeutung als künstliche „Konsum-Leitplanke“ denn als Mittel, um die Rechte von Künstlern zu wahren.
Durch DRM-Maßnahmen entstehen zusätzliche Kosten. So steigt – beispielsweise während der Entschlüsselung von geschützten Inhalten – die Prozessorlast und damit sowohl die Leistungsanforderungen an den PC, als auch der Stromverbrauch. Die zusätzliche Komplexität kann zudem die Systemstabilität beeinträchtigen und erhöht allgemein die Herstellungskosten für Computer-Komponenten durch zusätzlich notwendige Hardwarebausteine, umfangreichere Treiber, zusätzliche Lizenzen und Zertifizierungen – auch für Nutzer, die die DRM-Funktionalität gar nicht nutzen.
Aus der Verknüpfung von Technik und Anwendungsebene resultieren bei DRM-Systemen eine große Anzahl an noch offenen Fragen: So lassen sich Benutzerprofile erstellen, wenn Schlüssel und eindeutige Geräte-IDs zentral verwaltet werden. Es gibt beispielsweise DRM-Systeme, die bei jeder Benutzung des Mediums bei einer zentralen Stelle anfragen, ob der betreffende Benutzer überhaupt zur Benutzung berechtigt ist (DIVX in den USA, ein ehemaliges DVD-Miet-System).
Durch kritische Veränderungen des Inhalteanbietermarktes (Unternehmensübernahmen, -aufgaben, Insolvenz) bei DRM-Systemen ist nicht gesichert, dass sich DRM-geschützte Medien auch in Zukunft abspielen lassen, ähnlich der fehlenden Unterstützung von Software heute nicht mehr existierender Hersteller. Bei einer hohen Marktdurchdringung von DRM-Systemen könnte der Fortbestand der mit Hilfe dieser Technik gespeicherten Information ungewiss sein. Beispielsweise schalteten MSN-Music und Yahoo ihre Systeme zum 31. August 2008 beziehungsweise 30. September 2008 ab. Danach verliert ein Kunde seine dort gekaufte Musik, sobald sich an seinem PC etwas ändert. Von Amazon wurden bereits E-Books für den Kindle nach dem Kauf von den Geräten der Kunden gelöscht.
Es könnten Schwierigkeiten beim Abspielen neuerer oder inkompatibler Formate auftreten. Dies betrifft grundsätzlich auch das Anfertigen von Privatkopien bzw. Kopien für wissenschaftliche und Ausbildungszwecke.
Nach Ansicht der Freie-Software-Bewegung entzieht ein DRM-System Menschen prinzipbedingt die Möglichkeit vollständiger Kontrolle über Daten und Programme auf ihren Computern und schränkt somit ihre Freiheit ein.
Richard Stallman, Präsident der Free Software Foundation, bezeichnete DRM 2006 als „the functionality of refusing to function“ (deutsch: „die Funktionalität, das Funktionieren zu verweigern“)  und ist der Meinung „Defending freedom means thwarting DRM“ (deutsch: „Freiheit zu verteidigen bedeutet DRM zu vereiteln“) Laut Stallman kann Software, die DRM implementiert, die gewünschten Einschränkungen nur dann verlässlich durchsetzen, wenn sie nicht die Freiheit gewährt, beliebig verändert werden zu können, da diese Möglichkeit auch die Umgehung des DRM einschließen würde.
Diesen Widerspruch haben Gerätehersteller, die trotzdem unter Copyleft-Lizenzen lizenzierte freie Software wie zum Beispiel Linux für ihre DRM-Systeme einsetzen wollten, in der Vergangenheit dadurch umgangen, dass ihre Geräte (der TiVo ist ein bekanntes Beispiel) die Funktion verweigern, sobald eine veränderte Softwareversion installiert wird. Somit wird den Nutzern zwar theoretisch die Freiheit gewährt, die Software zu verändern und damit die Copyleft-Lizenz eingehalten, sinnvoll ausführen kann man die geänderte Software in der Praxis jedoch nicht.
Die 2007 aktualisierten Versionen der am weitesten verbreiteten Copyleft-Lizenzen GNU GPL und GNU LGPL enthalten Klauseln, die diese Möglichkeit des Missbrauchs von freier Software durch Gerätehersteller unmöglich machen sollen.
In vielen Ländern erlischt der urheberrechtliche Schutz eines Werks nach einer bestimmten Frist. In der Europäischen Union ist dies in der Regel 70 Jahre nach dem Tod des Urhebers der Fall.Nach Ablauf dieser Frist darf jedermann das entsprechende Werk nach Belieben kopieren und verkaufen. Ein Beispiel ist die 1911er Ausgabe der Encyclopædia Britannica, deren Inhalt wegen des Ablaufs der Schutzfrist unter anderem ohne Einschränkungen für die Wikipedia verwendet werden darf. Bislang erlaubt jedoch kein einziges DRM-System eine solche Freigabe von bisher urheberrechtlich geschützten Werken. Dies hat zur Folge, dass erworbene DRM-geschützte Dateien auch nach Ablauf der Schutzfrist nicht beliebig verwendet werden können, obwohl dies rechtlich ausdrücklich erlaubt ist. Dies könnte das digitale Vergessen beschleunigen.
Während ein DRMS zwar die Kontrolle von Medien in digitaler Form umsetzen kann, werden analoge Kopien oftmals möglich sein. Es besteht zum Beispiel die Möglichkeit, einen DRM-geschützten Inhalt durch eine analoge Aufnahme (Fotografie, Mikrofon) in eine ungeschützte Form, wenngleich auch häufig mit Qualitätseinbußen verbunden, zu redigitalisieren. Bei DRM-geschützten Audiodateien lässt sich dieser Qualitätsverlust jedoch auf ein Minimum beschränken. Kann die geschützte Datei legitim auf dem PC wiedergegeben werden, können die Audiosignale verlustfrei mit einer entsprechenden Software aufgezeichnet werden. Das Konzept der Aufnahme von Audio- und/oder Video-Signalen wird allgemein bei Screencasts genutzt, kann aber auch zweckentfremdet werden.
Diese Form der Umgehung eines DRMS ist zumindest in Deutschland kein Verstoß gegen § 108b des UrhG, da ein DRMS in der Regel keine „wirksame technische Maßnahme“ zur Vermeidung analoger Kopien darstellt.Mögliche technische Maßnahmen zur Kontrolle auch analogen Kopierens als Ergänzung oder Alternative zum DRM sind zum einen der Einsatz von Digitalen Wasserzeichen zur Identifizierung der Quelle, welche die analoge Kopie erstellt hat, und zum anderen fest eingebaute Erkennungsmechanismen in möglichst vielen Aufnahmegeräten, die dann die Aufnahme erkannter, geschützter Inhalte verweigern (zum Beispiel Macrovision).
In der Vergangenheit ist es schon gelungen, DRM-Systeme zu umgehen. Das bekannteste Beispiel war das vom norwegischen Programmierer Jon Lech Johansen entwickelte Programm QTFairUse. Dieses Programm nutzte eine durch Reverse Engineering von iTunes gefundene Lücke in Apples DRM-System FairPlay aus und war in der Lage, aus DRM-geschützten Audiodateien nicht DRM-geschützte Audio-Rohdaten im AAC-Format zu generieren. Weiterentwicklungen dieser Software wie iOpener oder JHymn erlaubten es, den DRM-Schutz von FairPlay-geschützten Dateien komfortabel zu entfernen. Allen diesen Programmen ist gemein, dass sie den kryptografischen Schlüssel desjenigen Benutzers verwenden, der die Audio-Dateien zuvor legal im iTunes Music Store erworben hat. Anfang 2006 ist das DRM des Windows Media Players 10 und 11 mit dem Programm drmdbg umgehbar geworden. Im August 2006 wurde eine einfach zu benutzende grafische Benutzeroberfläche mit dem Namen FairUse4WM für dieses Programm veröffentlicht. Damit wird es möglich, Musik von Diensten wie Napster to Go oder Yahoo! Unlimited to Go auch nach Ablauf des Abonnements abzuspielen.
Um die Benutzung und Entwicklung solcher Programme in Zukunft zu erschweren oder zu verhindern, können Computer mit kombinierten Hard- und Software-Mechanismen ausgestattet werden, die es ermöglichen, nur solchen Programmen, deren Integrität durch das Betriebssystem verifiziert wurde, bestimmte Funktionen einzuräumen. Im Zusammenhang mit DRM wären dabei zum Beispiel der gesicherte und authentisierte Zugang auf Schlüssel und Zertifikate oder auf Spezial-Hardware und Dekodiermodule zu nennen. Dieser Ansatz wird als Trusted Computing bezeichnet und ist ebenfalls umstritten, da er Anwendern den Zugriff auf bestimmte Daten einschränken kann.
Christian Arlt: Digital Rights Management Systeme. Der Einsatz technischer Maßnahmen zum Schutz digitaler Inhalte (= Information und Recht 60). Beck, München 2006, ISBN 3-406-54410-X (Zugleich: Göttingen, Univ., Diss., 2005).
Eberhard Becker, Willms Buhse, Dirk Günnewig, Niels Rump: Digital Rights Management. Technological, Economic, Legal and Political Aspects (= Lecture Notes in Computer Science 2770). Springer Berlin 2003, ISBN 3-540-40465-1.
Gerald Fränkl: Digital-rights-Management. Hintergründe, Instrumente, Perspektiven, (und) Mythen. VDM Verlag, Berlin 2005, ISBN 3-936755-93-0.
T. Hess, W.-I. Faecks, F. Rauchfuß, V. Ünlü: Rechtemanagement als Lösungsansatz aus dem Digitalen Dilemma.
Arnold Picot, Heinz Thielmann (Hrsg.): Distribution und Schutz digitaler Medien durch Digital Rights Management. Springer, Berlin u. a. 2005, ISBN 3-540-23844-1.
Bill Rosenblatt, Bill Trippe, Stephen Mooney: Digital Rights Management. Business and Technology. M & T Books, New York NY u. a. 2002, ISBN 0-7645-4889-1.
copyrightandtechnology.com – Ehemals DRM WATCH. Großes, wissenschaftliches (englischsprachiges) DRM Technologieportal
tatup-journal.de – Schwerpunkt zu DRM in der Zeitschrift „Technikfolgenabschätzung – Theorie und Praxis“ (2006)

Die Dimensionsanalyse ist ein mathematisches Verfahren, um das Zusammenspiel physikalischer Größen bei Naturphänomenen zu erfassen, ohne die einem physikalischen Vorgang zugrundeliegende Formel oder eine exakte Gesetzmäßigkeit zu kennen. Ihre Anwendung beruht auf angewandter Mathematik, praktischer Beobachtungsgabe, der Durchführung und Auswertung von Versuchen und auf intuitivem physikalischen Verständnis. Sie hat sich insbesondere in der Strömungsmechanik bewährt.
Für wirklichkeitsnahe Probleme in Technik und Wissenschaft sind die zugeordneten mathematischen Gleichungen in den meisten Fällen aufgrund komplexer Randbedingungen nicht analytisch, sondern nur numerisch lösbar, also durch Computerverfahren. Die Anwendung der Dimensionsanalyse auf geometrisch ähnliche, jedoch labortechnisch oder numerisch leichter handhabbare Modelle erlaubt hier häufig sehr genaue Rückschlüsse auf die Lösung des hochkomplexen Ausgangsproblems.
Die Dimensionsanalyse findet hauptsächlich in der experimentellen Physik, im Ingenieurwesen, aber auch in der Medizin und Biologie Anwendung.
Aerodynamik und das Verhalten von Körpern in strömenden Fluiden im Allgemeinen. Etwa die Untersuchung und Optimierung der aerodynamischen Eigenschaften von Flugzeugen und Hängebrücken.
Geowissenschaftlich interessant sind Auswirkungen von Erdbeben (etwa für Hochhäuser), Durchsickerungsvorgänge im Erdreich, Tragfähigkeit von Gründungen für Bauwerke oder Hangrutschungen und Lawinen.
In Medizin und Biologie das Themengebiet der Bionik, der Blutkreislauf oder das Pflanzenwachstum.Eine Dimensionsanalyse dieser Vorgänge liefert nützliche Proportionalitäten, Vorgaben zur Kalibrierung von Modellversuchen (s. Modellgesetze) und konkrete Anhaltspunkte für Variantenstudien. Wiederholt reicht das aus, um daraus funktionale Zusammenhänge abzuleiten. In jedem Falle trägt sie zum besseren Verständnis des Problems bei.
Bereits Physiker wie Ludwig Prandtl, Theodore von Kármán, Albert Shields, Johann Nikuradse und John William Strutt, 3. Baron Rayleigh, die sich Ende des 19. und zu Beginn des 20. Jahrhunderts als erste tiefergehend mit den Eigenschaften von Strömungen und bewegten Körpern in Fluiden beschäftigten, nutzten die Dimensionsanalyse, um vom Laborexperiment mit kontrollierbaren Randbedingungen auf das Verhalten physikalischer Probleme mit geometrisch ähnlichen Körpern oder mit Fluiden anderer Zähigkeit und Dichte zu schließen. Dieses Ähnlichkeitsprinzip, also die Möglichkeit, physikalische Phänomene in unterschiedlichen Maßstäben untersuchen zu können, bildet die Grundlage der Ähnlichkeitstheorie. Häufig wird diese Theorie auch als Modelltheorie bezeichnet.
Die der Ähnlichkeitstheorie zugrundeliegende Dimensionsanalyse besagt, dass sich jede dimensionsgebundene physikalische Formel 
   in eine dimensionslose, d. h. von physikalischen Einheiten bereinigte Gestalt überführen lässt. Dazu werden 
    {\displaystyle {\frac {y}{x_{1}^{k_{1}}\cdot x_{2}^{k_{2}}\cdot \ldots \cdot x_{n}^{k_{n}}}}={\frac {f(x_{1},x_{2},\dots ,x_{n})}{x_{1}^{k_{1}}\cdot x_{2}^{k_{2}}\cdot \ldots \cdot x_{n}^{k_{n}}}},\,k_{i}\in \mathbb {R} \neq 0,}
  so dass die linke und die rechte Seite der Gleichung dimensionslos werden. Die Dimensionsreinheit und damit die Korrektheit jeder physikalischen Beziehung lässt sich anhand dieser Aussage prüfen. Genügt eine Formel nicht diesen Kriterien, dann ist sie physikalisch nicht exakt. Dies gilt für viele Näherungsformeln, die bewusst bestimmte Größen vernachlässigen. Auch ist klar, dass nur Größen gleicher Dimension addiert und subtrahiert werden können, also untereinander vergleichbar sind. Die Argumente etwa trigonometrischer oder anderer transzendenter Funktionen müssen folglich dimensionslose Zahlen sein.
Das wichtige, auf der Dimensionsanalyse aufbauende, und unabhängig voneinander von Aimé Vaschy (1890), Dmitri Pawlowitsch Rjabuschinski (1911) und Edgar Buckingham (1915) bewiesene Π-Theorem, erweitert obige Aussage dahingehend, dass sich die Funktion 
    {\displaystyle {\frac {y}{x_{1}^{k_{1}}\cdot x_{2}^{k_{2}}\cdot \ldots \cdot x_{n}^{k_{n}}}}=G(\pi _{1},\pi _{2},\dots ,\pi _{p}),\,k_{i}\in \mathbb {R} \neq 0}
Durch die Dimensionsanalyse ist es möglich, die funktionale Gestalt physikalischer Formeln bis auf eine reellwertige Konstante 
   zu „erraten“, sofern nur wenige physikalische Größen Einfluss nehmen, wie beispielsweise beim erstmals von Galilei formulierten Fallgesetz
Messen einer physikalischen Größe heißt Größenarten (z. B. Geschwindigkeit, Druck) mit etwas vergleichen.
Für solche Vergleiche benötigt man nie mehr als sieben Grundgrößenarten, die man Basisgrößenarten nennt. Für sie sind über Prototypen Basiseinheiten (z. B. Meter, Sekunde) definiert. Jede Basisgrößenart stellt eine eigene Dimension dar, die nicht über die restlichen Basisgrößenarten beschrieben werden kann. Sie sind alle voneinander unabhängig.
Ein Grundgrößensystem beinhaltet alle Dimensionen, in denen ein Messvorgang stattfindet. Ein System, das alle bekannten Dimensionen – L = Länge, M = Masse, T = Zeit, I = Stromstärke, Θ = thermodynamische Temperatur, A = Stoffmenge und J = Lichtstärke – enthält, heißt {L,M,T,I,Θ,A,J}-System. Es ist ausreichend, um alle Vorgänge in der Natur zu erfassen. In der Mechanik, dem Hauptanwendungsgebiet der Dimensionsanalyse, kann man sich meist auf ein {L,M,T}-System beschränken.
Im Grundgrößensystem selbst ist die explizite Wahl einer Basiseinheit belanglos. Die Länge [L] wird so etwa mit den Basiseinheiten Meter, Fuß, Zentimeter, Yard etc. gemessen. Die Basiseinheit dient aber nur einem Vergleichszweck, sie ist nicht mit der Dimension zu verwechseln.
Grundgrößensysteme können nicht nur aus denjenigen Grundgrößenarten gebildet werden, die auch gleichzeitig Basisgrößenarten sind, sondern auch mit allen anderen. So ist nach Newton die Kraft als zusammengesetzte Größe aus Masse und den Grundgrößenarten Länge und Zeit geeignet, die Masse in einem {L,M,T}-System zu ersetzen. Dann entsteht ein {L,F,T}-System mit der Grundgrößenart Kraft [F] an Stelle der Masse, über die sie definiert ist. Die Kraft besitzt hier als Dimensionsbegriff eine eigene, unabhängige Dimension, welche den Massenbegriff einschließt.
Alle Größenarten eines {M,L,T}-System lassen sich auch in einem {F,L,T}-System angeben. Ein {M,F,L,T}-System darf es wegen der Abhängigkeit von Masse und Kraft nicht geben. Die Forderung nach voneinander unabhängigen Dimensionen wäre verletzt.
Man kann alternativ Grundgrößensysteme wählen, in denen der Druck, die Geschwindigkeit oder die Frequenz Grundgrößen sind. Bedingung ist, dass jede Grundgröße für sich eine von den anderen verwendeten Grundgrößen unabhängige Dimension darstellt.
Man nennt alle Grundgrößensysteme, in denen dieselben Größen dargestellt werden können, äquivalent. Für das Auffinden von so genannten Π-Faktoren ist die explizite Wahl von Grundgrößen belanglos. Sie ist nur eine Frage der bevorzugten Darstellungsweise.
In der Mechanik gebräuchliche Größenarten in einem {M,L,T}-System sind nachfolgend mit ihren Dimensionsformeln aufgelistet. Ihre Einheiten sind Potenzprodukte der Basiseinheiten. Ihre Dimensionsformeln sind Potenzprodukte der Dimensionen, innerhalb derer diese Einheiten beschrieben sind.
Formulierungen, wie „maßgebliche Größe der Dichte“ oder „Einfluss der Größen Geschwindigkeit und Beschleunigung“, sind umgangssprachlich. Diese Verwendung des Begriffs Größe ist im physikalischen Sinne nicht korrekt. Dichte, Geschwindigkeit, Beschleunigung usw. sind Größenarten. Erst in einer Gleichung der Art:
   (man kann auch von Messgröße sprechen) über eine (Maß-)Einheit [m/s] und eine Maßzahl 3 beschrieben. Für technische Zwecke ist dies aber nicht relevant.
Jedes Grundgrößensystem kann mithilfe einer Übergangsmatrix, welche die Exponenten der Dimensionen enthält, in ein dazu äquivalentes überführt werden. Möchte man in einem Grundgrößensystem beispielsweise die Dimension der Kraft 
    {\displaystyle D_{1}={\begin{pmatrix}&\mathbf {F} &\mathbf {L} &\mathbf {T} \\\mathbf {M} &1&-1&2\\\mathbf {L} &0&1&0\\\mathbf {T} &0&0&1\\\end{pmatrix}}}
  Die Transformation der Grundgrößen des {M,L,T}-Systems zum {F,L,T}-Grundgrößensystem ist durch die Matrizenmultiplikation
    {\displaystyle R\cdot D_{1}={\begin{pmatrix}&\mathbf {M} &\mathbf {L} &\mathbf {T} \\m&1&0&0\\l,b,h,\dots &0&1&0\\t&0&0&1\\f&0&0&-1\\\omega &0&0&-1\\v&0&1&-1\\a&0&1&-2\\\rho &1&-3&0\\F&1&1&-2\\\gamma &1&-2&-2\\p,E&1&-1&-2\\W&1&2&-2\\P&1&2&-3\\\mu &1&-1&-1\\\nu &0&2&-1\\\end{pmatrix}}\cdot {\begin{pmatrix}&\mathbf {F} &\mathbf {L} &\mathbf {T} \\\mathbf {M} &1&-1&2\\\mathbf {L} &0&1&0\\\mathbf {T} &0&0&1\\\end{pmatrix}}={\begin{pmatrix}&\mathbf {F} &\mathbf {L} &\mathbf {T} \\m&1&-1&2\\l,b,h,\dots &0&1&0\\t&0&0&1\\f&0&0&-1\\\omega &0&0&-1\\v&0&1&-1\\a&0&1&-2\\\rho &1&-4&2\\F&1&0&0\\\gamma &1&-3&0\\p,E&1&-2&0\\W&1&1&0\\P&1&1&-1\\\mu &1&-2&1\\\nu &0&2&-1\\\end{pmatrix}}=Q}
   die Exponenten aller Dimensionsformeln des {M,L,T}-Systems enthält. Die gesuchten Exponenten der Dimensionsformeln im {F,L,T}-System finden sich dann in der Dimensionsmatrix 
Da Länge und Zeit durch die Transformation unberührt bleiben, ändern sich lediglich die Exponenten derjenigen Größen, die mit der Dimension der Masse 
  , die Dimensionsformeln vereinfachen. Für andere hingegen, wie die direkt von der Masse abhängende Dichte 
  , jedoch verkomplizieren. Es ist nützlich ein solches Grundgrößensystem zu bilden, in dem sich die Größen des konkreten Problems möglichst einfach darstellen lassen.
  -Faktoren nennt man diejenigen Produkte, die sich aus einer Matrix wie der obigen Dimensionsmatrix 
   ergeben, wenn man einzelne Größen in beliebige Potenzen erhebt und sie mit anderen in der Matrix vorkommenden Größen derart multipliziert, dass das Produkt dimensionslos wird bzw. die Dimension 1 besitzt. Die Dimension einer Größe 
    {\displaystyle [\pi ^{2}]=[{\sqrt {\pi }}]=[\pi ^{\lambda }]=1,\,\lambda \in \mathbb {R} \neq 0}
Man sieht, dass beliebig viele Darstellungen eines einmal gefundenen Faktors möglich sind. Die Anzahl der 
  -Faktoren, die nicht als Potenz eines vorher gefundenen Faktors oder als Produkt von in Potenzen erhobenen Faktoren geschrieben werden können, ist allerdings beschränkt. Über die Existenz dieser 
    {\displaystyle (1)\quad A={\begin{pmatrix}&Y_{1}&Y_{2}&Y_{3}\\x_{1}&a_{11}&a_{12}&a_{13}\\x_{2}&a_{21}&\ddots &\ddots \\x_{3}&a_{31}&\ddots &\ddots \\\vdots &\ddots &\ddots &\vdots \\x_{n}&a_{n1}&a_{n2}&a_{n3}\\\end{pmatrix}}}
    {\displaystyle (2)\quad k\cdot A={\begin{pmatrix}k_{1}&k_{2}&k_{3}&\dots &k_{n}\end{pmatrix}}\cdot {\begin{pmatrix}a_{11}&a_{12}&a_{13}\\a_{21}&\ddots &\vdots \\a_{31}&\ddots &\vdots \\\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&a_{n3}\\\end{pmatrix}}={\begin{pmatrix}0&0&0\\\end{pmatrix}}}
    {\displaystyle (4)\quad \mathrm {rg} (K)={\begin{pmatrix}k_{11}&k_{12}&k_{13}&\dots &k_{1n}\\\vdots &\vdots &\vdots &\vdots &\vdots \\k_{p1}&k_{p2}&k_{p3}&\dots &k_{pn}\\\end{pmatrix}}=p}
   mit der Anzahl der gewählten Dimensionen (hier: 3) als Spalten und der Anzahl der Vektoren als Zeilen.
    {\displaystyle (5)\quad {\begin{pmatrix}k_{11}&k_{12}&k_{13}&\dots &k_{1n}\\\vdots &\vdots &\vdots &\vdots &\vdots \\k_{p1}&k_{p2}&k_{p3}&\dots &k_{pn}\\\end{pmatrix}}\cdot {\begin{pmatrix}a_{11}&a_{12}&a_{13}\\a_{21}&\ddots &\vdots \\a_{31}&\ddots &\vdots \\\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&a_{n3}\\\end{pmatrix}}={\begin{pmatrix}0&0&0\\\vdots &\vdots &\vdots \\0&0&0\\\end{pmatrix}}}
  Aus der Matrixalgebra ergibt sich, dass auch jede beliebige Linearkombination der gefundenen Zeilenvektoren Gleichung (2) löst, und damit einen 
   ergibt, indem man Zeilen mit beliebigen reellen Zahlen verschieden von Null multipliziert und mit anderen Zeilen addiert oder subtrahiert. Am Rang der Matrix 
    {\displaystyle (6)\quad \Pi =\Pi _{1}^{\lambda _{1}}\cdot \Pi _{2}^{\lambda _{2}}\cdot \quad \cdot \Pi _{p}^{\lambda _{p}},\,\lambda _{i}\in \mathbb {R} \neq 0,}
  wobei deren zugehörige Zeilenvektoren homogene Lösungen von (2) wären. Es sind allerdings weiterhin nur genau 
Mit einem beliebigen Fundamentalsystem sind über (6) alle existierenden Lösungen von (2) bestimmt. Dabei sind beliebig viele Lösungen darstellbar.
Dimensionslose Zahlenkonstanten, die oft schon Verhältnisgrößen sind, bleiben bei dieser Rechnung dimensionslos und stellen automatisch einen dimensionslosen Π-Faktor dar.
   im Gleichungssystem, das sich aus (2) ergibt, beliebige Werte außer Null annehmen zu lassen und den Rang der Zeilenmatrix nach (4) zu prüfen. Die Anzahl der unabhängigen Variablen ist identisch mit der Anzahl der 
Unabhängig oder frei wählbar sind im Gleichungssystem diejenigen Variablen, denen man beliebige Zahlenwerte zuweisen kann, ohne in der Lösung einen Widerspruch herbeizuführen. Eine geschickte Wahl ist es beispielsweise, immer einer unabhängigen Variablen den Zahlenwert Eins zuzuweisen und die anderen unabhängigen Variablen auf Null zu setzen. Die fehlenden abhängigen Variablen ergeben sich durch die Lösung des verbleibenden Gleichungssystems.
Der Nachteil dieser Methode besteht jedoch darin, dass man recht wenig Einfluss auf das Aussehen dieses Fundamentalsystems hat und unter Umständen eine Vielzahl von Gleichungssystemen lösen muss.
  -Faktoren schlichtweg aus (1) zu erraten. Dazu muss man die Zeilen der Größen in der Dimensionsmatrix 
Will man eine Größe im Zähler, muss man ihre Zeile mit „+1“ multiplizieren, andernfalls mit „−1“. (Die Zeilen mit Zahlen zu multiplizieren bedeutet die Größen in die entsprechenden Potenzen zu erheben.)
  -Faktoren zu beeinflussen. Allerdings muss man im Nachhinein den Rang der resultierenden Zeilenmatrix bestätigen, beispielsweise indem man eine nichtverschwindende Unterdeterminante findet, also zeigt, dass (4) erfüllt ist.
Meist führt das Erraten der Faktoren bei geschickter Wahl des Grundgrößensystems und übersichtlichen Verhältnissen wesentlich schneller zum Ziel als ein formales Vorgehen.
  -Faktoren demonstriert um das Gleichungssystem aus (2) möglichst geschickt zu lösen (z. B. Gauß’sches Eliminationsverfahren). Näheres dazu findet sich in jedem Mathematikbuch über Lineare Algebra.
Ist man zu einem Fundamentalsystem gelangt, befriedigt dies oftmals nicht den Wunsch nach einer physikalischen Aussagekraft der einzelnen 
Durch geschicktes Kombinieren der Faktoren untereinander und ihre Erhebung in beliebige Potenzen kann leicht ein neuer, physikalisch ergiebigerer Faktor gebildet werden. Soll dieser in einem neuen Fundamentalsystem vorhanden sein, ist lediglich einer der Faktoren zu streichen, durch deren Kombination man den neuen gebildet hatte. Dadurch wird der neu erlangte 
Für Modelluntersuchungen ist es nützlich, solche Faktoren gebildet zu haben, die immer eine charakteristische Größe enthalten, die dann nur in einem einzigen Faktor vorkommt. Dies muss nicht unbedingt möglich sein. Gleichung (6) erlaubt aber das zu prüfen.
    {\displaystyle (7)\quad y=f(x_{1},x_{2},\dots ,x_{n});\,\,[y]=[f(x_{1},x_{2},\dots ,x_{n})]\neq 1}
    {\displaystyle (8)\quad {\frac {y}{x_{1}^{k_{1}}\cdot x_{2}^{k_{2}}\cdot \ldots \cdot x_{n}^{k_{n}}}}={\frac {f(x_{1},x_{2},\dots ,x_{n})}{x_{1}^{k_{1}}\cdot x_{2}^{k_{2}}\cdot \ldots \cdot x_{n}^{k_{n}}}},\,k_{i}\in \mathbb {R} \neq 0}
    {\displaystyle \left[{\frac {y}{x_{1}^{k_{1}}\cdot x_{2}^{k_{2}}\cdot \dots \cdot x_{n}^{k_{n}}}}\right]=\left[{\frac {f(x_{1},x_{2},\dots ,x_{n})}{x_{1}^{k_{1}}\cdot x_{2}^{k_{2}}\cdot \dots \cdot x_{n}^{k_{n}}}}\right]=1}
Das so genannte Π-Theorem (in der Literatur auch oft Buckingham-Theorem), leitet einen Schritt weiter. Seine Hauptaussage ist, dass sich jede dimensionsgebundene Gleichung
    {\displaystyle (9)\quad {\frac {y}{x_{1}^{k_{1}}\cdot x_{2}^{k_{2}}\cdot \ldots \cdot x_{n}^{k_{n}}}}=G(\pi _{1},\pi _{2},\dots ,\pi _{p})}
    {\displaystyle \left[{\frac {y}{x_{1}^{k_{1}}\cdot x_{2}^{k_{2}}\cdot \ldots \cdot x_{n}^{k_{n}}}}\right]=\left[G(\pi _{1},\pi _{2},\dots ,\pi _{p})\right]=1,\,k_{i}\in \mathbb {R} \neq 0,\,p\,{\text{wie vorher}}}
  überführen lässt und damit nur aus dimensionslosen Potenzprodukten (und Zahlenkonstanten) aufgebaut ist. Dabei kann es sein, dass es mehrere Möglichkeiten gibt, die linke Seite der Gleichung in dimensionsloser Form darzustellen. Gelegentlich wird in der Literatur die linke Seite ebenfalls als Π-Faktor bezeichnet. Dies ist legitim, aber nicht konsequent, denn durch die Trennung in linke und rechte Seite erhält man die präzisere Aussage
    {\displaystyle {\frac {y}{x_{1}^{k_{1}}\cdot x_{2}^{k_{2}}\cdot \ldots \cdot x_{n}^{k_{n}}}}-G(\pi _{1},\pi _{2},\dots ,\pi _{p})=0}
    {\displaystyle F\left(\pi _{1},\pi _{2},\dots ,\pi _{p},{\frac {y}{x_{1}^{k_{1}}\cdot x_{2}^{k_{2}}\cdot \dots \cdot x_{n}^{k_{n}}}}\right)=0}
  Die Bedeutung des Theorems liegt darin, dass eine Aussage über den funktionalen Zusammenhang dimensionsbehafteter physikalischer Größen gemacht werden kann, der sich vielleicht nicht explizit formelmäßig angeben lässt. Dies gilt für viele komplexe Sachverhalte in der Natur (z. B. Turbulenz, Kármánsche Wirbelstraße).
Da Größen nur noch in bestimmten Relationen, den vorgestellten Π-Faktoren, zueinander auftreten können, erreicht man gleichzeitig eine nützliche Reduktion der Funktionsvariablen in 
  ), dann kann ein funktionaler Zusammenhang, etwa aus experimentellen Ergebnissen oder durch pure Intuition, nur erraten werden. Explizit herleiten lässt er sich nicht.Häufig trifft im zweiten Fall der Produktansatz nach Rayleigh zu, also dass die gefundenen Π-Faktoren miteinander multipliziert und in eine entsprechende, oft ganzzahlige Potenz erhöht, das gesuchte Endergebnis liefern.
   ab, oder der gedachte funktionale Zusammenhang muss um mindestens eine weitere Größe erweitert werden.
   dimensionslos gemacht werden kann, dann ist die Dimensionsmatrix unvollständig oder falsch.Das bedeutet, dass man in jedem Falle bei dimensionsbehafteten Gleichungen, was physikalische Formeln immer sind,
zu einer vorteilhaften, dimensionslosen Darstellung gelangen kann, in der die Einheiten der Größen keine Rolle spielen.
An ihre Grenzen stößt die Dimensionsanalyse, wenn nicht nur skalare Größen wie Druck oder Temperatur oder eindimensionale, gerade Bewegungsvorgänge behandelt werden, sondern Vektoren und Tensoren ins Spiel kommen.
Da nur eine physikalische Dimension der Länge zur Verfügung steht, für die Beschreibung von räumlichen Vorgängen aber ein dreidimensionales kartesisches Koordinatensystem nötig ist (wobei Vektoren ins Spiel kommen), müsste für den zweidimensionalen Parabelflug einer Kanonenkugel etwa deren zeitabhängige Höhe und Weite getrennt untersucht werden. Dies schließt nicht aus, dass man über die Kenntnis von Symmetrien in beiden Formeln und dem nötigen Hintergrundwissen die eine gültige Bestimmungsgleichung für den Flug innerhalb eines rechtwinkligen und unbewegten Koordinatensystems herleiten kann. Wird die Kugel zusätzlich noch durch Seitenwind abgelenkt, und das Problem damit dreidimensional, steigt die zu erfassende Komplexität weiter.
Der scheinbare Widerspruch zwischen den drei Dimensionen des Raumes und der einen zur Verfügung stehenden Dimension der Länge löst sich auf, wenn man diese Längendimension gedanklich in einem mitwandernden Koordinatensystem an der Flugkurve selbst ausrichtet. Folgt man der Bahn, ist die Kurve und die Kugelgeschwindigkeit eindimensional. Die Dimensionsanalyse ist also durchaus gültig. Die Bahngeschwindigkeit entlang der Kurve lässt sich eindimensional, nämlich über den Betrag des Geschwindigkeitsvektors, erfassen. Dies ist nur für einen unbewegten Beobachter wenig hilfreich, der nicht nur Kenntnis über Beträge der Geschwindigkeit oder den zurückgelegten Weg der Kugel, sondern auch über die Richtung der Geschwindigkeit und die Kugelposition im Raum erlangen möchte.
Ähnliches gilt etwa für dreidimensionale Spannungszustände (etwa bei Untersuchung von Materialfestigkeiten), die mit einem Spannungstensor erfasst werden müssten.
Die dritte wichtige Schlussfolgerung, die das Π-Theorem in der experimentellen Versuchstechnik bedeutsam macht, ist diejenige:
    {\displaystyle {\frac {y}{x_{1}^{k_{1}}\cdot x_{2}^{k_{2}}\cdot \ldots \cdot x_{n}^{k_{n}}}}=G(\pi _{1},\pi _{2},\dots ,\pi _{p}),\,k_{i}\in \mathbb {R} \neq 0}
   alle Π-Faktoren auf der rechten Seite der Gleichung konstant gehalten werden, dann wird auch das dimensionslose Funktionsergebnis auf der linken Seite immer dasselbe sein.Satz 3 ist entscheidend für die gesamte Ähnlichkeitstheorie. Alle Randbedingungen, die in realistischen Modellversuchen zu wählen sind, gehen hieraus hervor (s. vollständige und teilweise Modellähnlichkeit).
Als Beispiel für einen in Modellversuchen bedeutenden Π-Faktor sei die Reynolds-Zahl genannt. Diese ist:
   eingehen, ist es möglich, maßstabsgetreue kleinere Modelle (etwa Flugzeuge im Strömungskanal) zu untersuchen, und dennoch ein korrektes Ergebnis auf der linken Seite der obigen dimensionslosen Funktionsgleichung zu erhalten, indem man bei der Untersuchung des Modells 
Bei vielen Problemstellungen tauchen dieselben charakteristischen Π-Faktoren wiederkehrend auf. So sind viele, unter dem Stichwort dimensionslose Kennzahl, nach ihren Entdeckern und Erforschern benannt.
Wenn es gelingt, alle Π-Faktoren in einem physikalisch interessierenden Wertebereich konstant zu halten, spricht man von vollständiger Modellähnlichkeit, ansonsten von teilweiser Modellähnlichkeit.
Oftmals glückt die vollständige Modellähnlichkeit allerdings nicht, und man ist gezwungen, den mehr oder weniger großen Nebeneffekt auf das letztendliche Messergebnis abzuschätzen. Nebeneffekte können auch anderweitig auftreten, nämlich wenn eine Größe, deren Einfluss auf den Prototyp belanglos wäre, das Modell unerwünscht stark beeinflusst (s. Froude-Zahl im Schiffsmodell).
Über die Gleichsetzung der Π-Faktoren von Modell und Prototyp ergeben sich Modellgesetze. Variiert man in der Reynoldszahl des Modells gegenüber dem Prototyp die Länge, kann man dies, wie oben erklärt, durch Anpassung der Viskosität und/oder der Geschwindigkeit ausgleichen.
Um die Modellgesetze in eine vorteilhafte Form zu bringen, ist man immer bestrebt, nur diejenigen Größen in die Π-Faktoren zu übernehmen, die man auch im Modell variieren kann und nicht diejenigen, die sich aus der Konsequenz dieser Variation ergeben würden. Die praktisch sinnvollste Form erreicht man, wenn es möglich ist, diese Gleichungen derart zu schreiben, dass beim Einsetzen der Größenwerte des Prototyps immer eine eindeutige Aussage über eine einzelne Versuchseinstellung im Modell möglich ist. Also dergestalt, dass sich bei jeder Änderung der Ausgangssituation im Prototyp immer die erforderliche Versuchseinstellung im Modell offenbart.
Ein nicht zu unterschätzender Vorteil liegt überdies noch darin, in einem Modellversuch nicht mehr alle einfließenden Größen einzeln variieren zu müssen, sondern nur noch die aus ihnen gebildeten, und von der Anzahl her geringeren, Π-Faktoren. Auch für die Darstellung der späteren Versuchsergebnisse ist dies von entscheidender Bedeutung. Indem man nur noch Π-Faktoren statt einzelner, dimensionsbehafteter, Größen aufträgt, gelangt man zu einer wesentlich knapperen und übersichtlicheren Veranschaulichung der Messgrößen (man spart Dimensionen). Alle Diagramme, in denen die Achsen dimensionslos dargestellt sind, basieren auf der Grundlage der Dimensionsanalyse.
Beim Bau eines Modells und der späteren Versuchsdurchführung muss man sorgfältig alle relevanten Größen im Voraus überlegen. Nur über die richtigen Parameter gelangt man auf die richtigen oder einen vollständigen Satz von Π-Faktoren und kann eine realistische Simulation durchführen. Bei Auswahl zu vieler Größen, die möglicherweise nur geringe Bedeutung auf die Messung haben, steigt jedoch die Anzahl der Versuche gewaltig. Dies erfordert physikalischen Sachverstand.
Vielleicht stellt sich im Nachhinein heraus, dass eine Größe, der man eine Bedeutung zugedacht hatte, wesentlich weniger Einfluss auf das Ergebnis hat als angenommen. Falls diese Größe nur in einem einzigen Faktor vorkommt, ist es möglich, diesen zu streichen. Ansonsten empfiehlt es sich, mit einem neuen Satz von Größen die Dimensionsmatrix zu bilden und ein passendes Fundamentalsystem zu finden.
Um die Anwendung der Formeln aus den vorhergehenden Kapiteln zu demonstrieren, folgen einige Rechenbeispiele.
    {\displaystyle A={\begin{pmatrix}&\mathbf {M} &\mathbf {L} &\mathbf {T} \\g&0&1&-2\\t&0&0&1\\m&1&0&0\\\end{pmatrix}}}
   kann nicht dimensionslos gemacht werden und ist folglich physikalisch nicht korrekt. Eine Abhängigkeit des Fallwegs von der Masse führt erst dann zu einer richtigen Beschreibung, wenn die Luft berücksichtigt wird. Denn die für die bremsende Reibung verantwortliche Luftdichte enthält die Dimension der Masse.
Galilei stand die Differentialrechnung nicht zur Verfügung. Ihm war unbekannt, dass die Fallgeschwindigkeit 
Vertikal belastete Stäbe einer bestimmten Länge sind knickgefährdet, d. h. ihr Versagen erfolgt häufig bevor die eigentliche Bruchlast des Querschnitts erreicht ist. Die so genannte Knicklast 
   für den zweiten Fall der nebenstehenden Abbildung ergibt sich für ein {F,L,T}-System in ausführlicher Schreibweise zu
    {\displaystyle A={\begin{pmatrix}&\mathbf {F} &\mathbf {L} &\mathbf {T} \\E&1&-2&0\\l&0&1&0\\h&0&1&0\\d&0&1&0\\\end{pmatrix}}}
  . Bei diesen beiden leicht zu erratenden Π-Faktoren handelt es sich um die so genannten geometrischen Ähnlichkeiten 
  gelten, womit die Dimensionsanalyse gezeigt hat, dass man in Laborversuchen lediglich die so genannte Schlankheit des Stabes 
Nach Gleichung 6 im Abschnitt Existenz und Anzahl von Π-Faktoren lässt sich ein weiterer Π-Faktor bilden:
    {\displaystyle \pi _{3}=\pi _{1}\cdot \pi _{2}^{-1}={\frac {h}{l}}\cdot {\frac {1}{\frac {h}{d}}}={\frac {d}{l}}}
    {\displaystyle {\frac {F}{E\cdot l^{2}}}=C\cdot \left({\frac {h}{l}}\right)^{3}\cdot {\frac {d}{l}}}
  analog, d. h. von gleicher funktionaler Gestalt ist. Die Knicklast lässt sich in Versuchen an Stäben beliebiger Länge und Elastizität, und nicht nur auf die Rechteckform beschränkt, leicht verifizieren und in Diagrammform dargestellten. Die Kenntnis geschlossener Formeln, wie etwa der von Euler, ist nicht nötig. Bemerkenswert ist die gewonnene Erkenntnis, dass Elastizitätsmodul und Länge eines feststehenden Querschnitts für einen Knickversuch prinzipiell frei wählbar sind. Die Proportionalität zwischen 
Das Standardproblem in der Anfangszeit der Strömungsmechanik war die Bestimmung des Widerstands eines in einem Fluid umströmten Körpers. Dieses lässt sich mit Hilfe der Dimensionsanalyse erfassen.
   einer Kugel und jedes anderen Körpers hängt von seiner Form, hier präzisiert durch den Kugeldurchmesser 
    {\displaystyle A={\begin{pmatrix}&\mathbf {M} &\mathbf {L} &\mathbf {T} \\v&0&1&-1\\\rho &1&-3&0\\\mu &1&-1&-1\\d&0&1&0\\\end{pmatrix}}\,{\text{bzw.}}\,A={\begin{pmatrix}0&1&-1\\1&-3&0\\1&-1&-1\\0&1&0\\\end{pmatrix}}}
   Π-Faktor, die berühmte Reynolds-Zahl, benannt nach dem Erkenner dieses Prinzips, Osborne Reynolds und damit:
    {\displaystyle {\frac {F}{\rho \cdot v^{2}\cdot d^{2}}}=G\left({\frac {v\cdot d\cdot \rho }{\mu }}\right)=G(Re)}
   durch die Stirnfläche des Körpers ersetzt wird und der Proportionalitätsfaktor 1/2 aus dem Staudruck zugefügt wird. Auch mit dieser Umformulierung gilt der Zusammenhang
   wird als Strömungswiderstandskoeffizient bezeichnet. Er kann durch Versuche bestimmt werden und ist, wie im dimensionslosen Diagramm zu erkennen, geschwindigkeitsabhängig und keinesfalls konstant. Mit dem durch Messungen ermittelten Zusammenhang zwischen 
   gilt das analytisch schwer herzuleitende lineare Stokes-Gesetz. Anschließend, bei höheren Geschwindigkeiten, variiert 
  , bedingt durch Wirbelbildung auf der Kugelrückseite. Ähnliche Diagramme lassen sich mit Versuchen für beliebige geometrische Formen und Körper ermitteln.
  , denn an der Wasseroberfläche entstehen dem Gesetz der Schwerkraft unterliegende Wellen. Das Wasser ist ausreichend tief gegenüber 
    {\displaystyle A={\begin{pmatrix}&\mathbf {M} &\mathbf {L} &\mathbf {T} \\v&0&1&-1\\g&0&1&-2\\\rho &1&-3&0\\\mu &1&-1&-1\\L&0&1&0\\D&0&1&0\\t&0&1&0\\\end{pmatrix}}\,\,{\text{bzw.}}\,A={\begin{pmatrix}0&1&-1\\0&1&-2\\1&-3&0\\1&-1&-1\\0&1&0\\0&1&0\\0&1&0\\\end{pmatrix}}}
   sind geometrische Ähnlichkeiten. Maßstabsgetreu wiedergegebene Rundungen der Schiffsform werden vorausgesetzt.
    {\displaystyle {\frac {F}{g\cdot \rho \cdot L\cdot D\cdot t}}=G\left({\frac {L}{D}},{\frac {t}{D}},Re={\frac {\rho \cdot v\cdot D}{\mu }},Fr={\frac {v^{2}}{g\cdot D}}\right)}
Vollständige Modellähnlichkeit ist erreicht, wenn alle Π-Faktoren in Modell und Prototyp konstant gehalten werden können. Bei 
   anzupassen, was ohne Zentrifuge auf der Erde nicht realisierbar ist. Vollständige Modellähnlichkeit ist nicht zu erreichen, nur 
   können konstant sein. Alternativ kann das Modell in einer anderen Flüssigkeit mit entsprechender Dichte und Zähigkeit untersucht werden.
   eine Rolle, wird im Regelfall keine vollständige Modellähnlichkeit erreicht. Sehr kleine Modelle verlangen außerdem große Anströmgeschwindigkeiten. Viele Modelle sind deshalb nur realistisch, wenn sie entsprechend groß sind.
Bei Strömungsvorgängen, in denen die freie Oberfläche des Fluids keine Rolle spielt, ist die Froude-Zahl mangels Oberflächenwellenbildung nicht relevant. Modelle von U-Booten oder Flugzeugen (unterhalb der Schallgeschwindigkeit) können im Prinzip bei vollständiger Modellähnlichkeit untersucht werden. Entscheidend ist nur die Reynolds-Zahl.
Um riesige, nicht realisierbare Strömungsgeschwindigkeiten im Windkanal zu umgehen, werden Flugzeugmodelle oft in dichteren Medien angeströmt. Bewegt sich ein Objekt so schnell, dass der Kompressionsmodul 
   ist die Geschwindigkeit von Longitudinalwellen in elastischen Medien, in Luft die sogenannte Schallgeschwindigkeit. Die Mach-Zahl ist ab Werten von etwa 
Ein berühmtes Beispiel für die Anwendung der Dimensionsanalyse stammt vom britischen Physiker Geoffrey Ingram Taylor. Nachdem er eine Bilderserie mit genauen Zeitintervallen der ersten Atombombenexplosion 1945 in New Mexico erhalten hatte (Trinity-Test), konnte er die freigesetzte Energie der dortigen Nuklearexplosion ermitteln. Die vor Ort gemessene Sprengkraft war von den Entwicklern in Los Alamos gegenüber den außenstehenden Briten geheim gehalten worden.
    {\displaystyle A={\begin{pmatrix}&\mathbf {M} &\mathbf {L} &\mathbf {T} \\t&0&0&1\\\rho &1&-3&0\\E&1&2&-2\\\end{pmatrix}}\,{\text{bzw.}}\,A={\begin{pmatrix}0&0&1\\1&-3&0\\1&2&-2\\\end{pmatrix}}}
    {\displaystyle {\frac {R^{5}\cdot \rho }{E\cdot t^{2}}}=c^{5}\to R(t)=c\cdot {\sqrt[{5}]{\frac {E\cdot t^{2}}{\rho }}}\to E(R,t)={\frac {R^{5}\cdot \rho }{c^{5}\cdot t^{2}}}}
  Bei einer geschätzten Temperatur zum Explosionszeitpunkt um etwa 6 Uhr morgens in New Mexico von 
   ließe sich aus einer Vergleichsexplosion mit konventionellem Sprengstoff (mehrere kg TNT) bestimmen. Taylor besaß genug Hintergrundwissen, um 
    {\displaystyle E={\frac {130^{5}\cdot 1{,}204}{1{,}0^{5}\cdot 0{,}025^{2}}}=7{,}15\cdot 10^{13}\,\mathrm {J} }
    {\displaystyle E={\frac {7{,}15\cdot 10^{13}}{4{,}18\cdot 10^{9}}}=17.095\,{\text{Tonnen TNT}}}
  Trinity hatte nach offiziellen Angaben eine Energie von annähernd 19.000–21.000 Tonnen TNT. Die Abweichung zu oben erklärt sich dadurch, dass der Radius 
   in der 5. Potenz eingeht. Das Ergebnis ist bemerkenswert genau. Taylor selbst errechnete ca. 19.000 Tonnen TNT.
H. L. Langhaar: Dimensional Analysis and Theory of Models. John Wiley & Sons, New York London 1951, ISBN 0-88275-682-6.
Henry Görtler: Dimensionsanalyse, Theorie der physikalischen Dimensionen mit Anwendungen. Springer-Verlag, Heidelberg 1975, ISBN 3-540-06937-2.
W. J. Duncan: Physical Similarity and Dimensional Analysis. Edward Arnold & Co., London 1951, ISBN 0-7131-3042-3.
Wilfred E. Baker, Peter S. Westine, Franklin T. Dodge: Similarity Methods in Engineering Dynamics, Theory and Practice of Scale Modeling. Second Edition, Elsevier Science Publishers, Amsterdam 1991 (Neuauflage), ISBN 0-444-88156-5.
Joseph H. Spurk: Dimensionsanalyse in der Strömungslehre. Springer, Berlin 1999, ISBN 3-540-54959-5.
Edgar Buckingham: On Physically Similar Systems. Illustrations of the Use of Dimensional Equations. In: Physical Review. Series II. Band 4, Nr. 4. American Physical Society, Oktober 1914, S. 345–376, doi:10.1103/PhysRev.4.345. 
Edgar Buckingham: The Principle of Similitude. In: Nature. Band 96, Nr. 2406, 9. Dezember 1915, S. 396–397, doi:10.1038/096396d0 (nature.com [PDF; abgerufen am 1. Juni 2012]). 
Helmut Kobus: Anwendung der Dimensionsanalyse in der experimentellen Forschung des Bauingenieurwesens. In: Die Bautechnik. Heft 3, Ernst & Sohn, Berlin 1974.
Aerospaceweb, mit Darstellungen von Strömungswiderständen mittel Ähnlichkeitskennzahlen. (englisch)
Dimensionsanalytische Methoden zur Optimierung zerstäubungstechnischer Prozesse in der Verfahrenstechnik (PDF; 246 kB)

Die Dinosaurier (Dinosauria, von altgriechisch δεινός deinós, deutsch ‚schrecklich, gewaltig‘ und altgriechisch σαῦρος sauros, deutsch ‚Eidechse‘) sind die Gruppe der Landwirbeltiere (Tetrapoda), die im Mesozoikum (Erdmittelalter) von der Oberen Trias vor rund 235 Millionen Jahren bis zur Kreide-Paläogen-Grenze vor etwa 66 Millionen Jahren die festländischen Ökosysteme dominierte.
In der klassischen Systematik werden die Dinosaurier als ausgestorbener Zweig der Reptilien betrachtet, aus kladistischer Sicht jedoch schließen die Dinosaurier als systematische Einheit die Vögel, die aus kleinen theropoden Dinosauriern hervorgingen, mit ein. Somit sind nicht alle Dinosaurier während des Massenaussterbens am Ende des Mesozoikums untergegangen, sondern mit den Vögeln überlebte eine spezielle Entwicklungslinie der Dinosaurier bis heute. Diese Linie erwies sich als außerordentlich anpassungsfähig und erfolgreich: Die Vögel stellen etwa ein Drittel aller rezenten (heute lebenden) Landwirbeltierarten, sind in faktisch allen terrestrischen Ökosystemen vertreten und weisen zudem mit den Pinguinen eine Gruppe auf, die sekundär stark an ein Leben an und im Wasser angepasst ist.
In der Zoologie, die sich vorwiegend mit rezenten Tieren beschäftigt, und speziell in der Ornithologie (Vogelkunde) werden die Vögel jedoch aus pragmatischen Gründen nach wie vor als eigenständige Klasse und nicht als Dinosaurier oder Reptilien betrachtet. Gleiches gilt für den allgemeinen Sprachgebrauch. Auch in der modernen Wirbeltierpaläontologie ist eine informelle Trennung von Vögeln und Dinosauriern im klassischen Sinn durchaus üblich. Letztgenannte werden, um der kladistischen Sichtweise gerecht zu werden, auch als „Nichtvogeldinosaurier“ (engl. non-avian dinosaurs) bezeichnet. Deshalb sind stets „Nichtvogeldinosaurier“ gemeint, wenn im Folgenden von „Dinosauriern“ die Rede ist.
Das Wissen über die Dinosaurier erhalten Paläontologen durch die Untersuchung von Fossilien, die in Form von versteinerten Knochen, Haut- und Gewebeabdrücken überliefert sind – und durch Spurenfossilien, also Fußspuren, Eier, Nester, Magensteine oder versteinerten Kot. Überreste von Dinosauriern sind auf allen Kontinenten gefunden worden, einschließlich Antarktikas, da die Dinosaurier zu einer Zeit entstanden, als alles Festland im Superkontinent Pangaea vereinigt war.
In der ersten Hälfte des 20. Jahrhunderts galten Dinosaurier als wechselwarme, träge und wenig intelligente Tiere. Zahlreiche Studien seit den 1970er Jahren haben jedoch gezeigt, dass es sich um aktive Tiere mit erhöhten Stoffwechselraten und soziale Interaktionen ermöglichenden Anpassungen handelte. Dinosaurier sind zu einem Teil der weltweiten Populärkultur geworden und spielen in einigen außergewöhnlich erfolgreichen Büchern und Filmen eine Rolle (unter anderem in Jurassic Park). Neue Entdeckungen erscheinen regelmäßig in den Medien.
Der englische Anatom Richard Owen stellte das Taxon „Dinosauria“ im April 1842 auf. Nach der heutigen kladistischen Auffassung schließen die Dinosaurier alle Nachfahren des letzten gemeinsamen Vorfahren von Triceratops und der Vögel ein. Alternativ wurde vorgeschlagen, die Dinosaurier als alle Nachfahren des jüngsten gemeinsamen Vorfahren von Megalosaurus und Iguanodon zu definieren, da dies zwei der drei Gattungen sind, die Owen bei seiner Erstbeschreibung der Dinosaurier nannte. Beide Definitionen fassen dieselben Taxa zur Gruppe der Dinosaurier zusammen: Die Theropoden (zweibeinige Karnivoren), Sauropodomorphen (mehrheitlich große Pflanzenfresser mit langen Hälsen und Schwänzen), Ankylosaurier (vierbeinige Pflanzenfresser mit massiven Hautpanzern), Stegosaurier (vierbeinige, Knochenplatten tragende Pflanzenfresser), Ceratopsia (vierbeinige Pflanzenfresser mit Hörnern und Nackenschilden) und Ornithopoden (zwei- oder vierbeinige Pflanzenfresser).Die Dinosaurier wiesen eine immense Formenvielfalt auf. Einige waren Herbivoren (Pflanzenfresser), andere Karnivoren (Fleischfresser); einige waren quadruped (vierbeinig), andere biped (zweibeinig), und wieder andere, wie zum Beispiel Iguanodon, konnten sich sowohl biped als auch quadruped fortbewegen. Viele hatten eine Panzerung, Hörner, Knochenplatten, Schilde oder Rückensegel. Obwohl sie für eine gigantische Größe bekannt sind, variierte ihre Größe beträchtlich; so waren viele Dinosaurier nur so groß wie ein Mensch oder kleiner.
Jedoch beschränkte sich diese Formenvielfalt faktisch ausschließlich auf terrestrische Lebensräume. Nur einige wenige Arten zeigen gewisse Anpassungen an ein Leben an und in Süßgewässern. Auch den Luftraum beherrschten sie nicht wirklich. Wie gut der oft als erster Vogel bezeichnete, aber noch recht dinosaurierhafte Archaeopteryx aktiv fliegen (das heißt dynamischen Auftrieb erzeugen) konnte, ist umstritten. Entsprechend muss davon ausgegangen werden, dass seine bislang unbekannten, möglicherweise schon relativ vogelähnlichen unmittelbaren Vorfahren nicht gut oder gar nicht aktiv fliegen konnten. Auch die einer anderen Entwicklungslinie als Archaeopteryx angehörenden „vier-flügeligen Dinosaurier“ der Unterkreide (Microraptor, Changyuraptor) konnten wahrscheinlich nicht aktiv fliegen, sondern sich nur von Baum zu Baum gleitend fortbewegen.
Bis 2006 wurden 527 Dinosauriergattungen von einer geschätzten Gesamtzahl von etwa 1850 Gattungen wissenschaftlich beschrieben. Eine Studie von 1995 schätzt die Gesamtanzahl auf 3400 – wovon jedoch viele nicht als Fossilien überliefert seien. Im Mittel kommen gegenwärtig pro Monat zwei neue Gattungen und pro Jahr mindestens 30 neue Arten hinzu.
Gegenüber ihren nächsten Verwandten innerhalb der Archosaurier (Lagosuchus, Scleromochlus und den Flugsauriern) zeichnen sich die Vertreter aller Dinosauriergruppen durch eine Anzahl gemeinsamer abgeleiteter Merkmale (Synapomorphien) aus:
Schädelmerkmale: das Postfrontale fehlt; im Gaumen überlappt das Ectopterygoid das Flügelbein (Pterygoid); der Kopf des Quadratums ist in seitlicher Ansicht exponiert; Verkleinerung der Posttemporalöffnung (Fenestra posttemporalis, einer Hinterhauptsöffnung).
Merkmale des postkranialen Skeletts (das Skelett ohne den Schädel (cranium)): die Schultergelenkpfanne ist rückwärtig orientiert; die Hand ist asymmetrisch mit verkürzten äußeren Fingern (IV und V) – bei höheren Theropoden fehlen diese völlig; das Schienbein zeigt am vorderen oberen Ende eine kammartige Erhebung (Cnemialkamm); der Astragalus (ein Fußwurzelknochen) weist einen aufwärts abgehenden Fortsatz auf; der mittlere Mittelfußknochen (an dem Zehe III ansetzt) ist S-förmig gebogen.Während urtümliche Dinosaurier alle diese Merkmale aufweisen, kann der Knochenbau späterer Formen stark abweichen, so dass manche der aufgeführten Charakteristika nicht mehr vorhanden oder nachzuvollziehen sind.
Darüber hinaus gibt es verschiedene weitere Merkmale, die viele Dinosaurier gemeinsam haben, aber nicht als gemeinsam abgeleitete Merkmale bezeichnet werden, da sie sich gleichfalls bei einigen Nicht-Dinosauriern finden oder nicht bei allen frühen Dinosauriern auftreten. Dies sind unter anderem das verlängerte Schulterblatt (Scapula), drei oder mehr Kreuzbein-Wirbel im Bereich des Beckengürtels (drei Kreuzbeinwirbel wurden bei einigen anderen Archosauriern gefunden, jedoch nur zwei bei Herrerasaurus) oder eine offene (perforierte) Hüftgelenkpfanne (geschlossen bei Saturnalia).Bei den Dinosauriern standen die Beine senkrecht unter dem Körper, ähnlich wie bei den meisten Säugetieren – aber anders als bei den meisten anderen Reptilien, deren Beine gespreizt nach außen hin abstehen (Spreizgang). Durch ihre aufrechte Haltung konnten Dinosaurier beim Bewegen leichter atmen, was wahrscheinlich Ausdauer- und Aktivitätslevel erlaubte, welche die anderer Reptilien mit gespreizten Beinen übertrafen. Außerdem hat die gerade Stellung der Beine eventuell die Evolution des Gigantismus unterstützt, da so die Beine entlastet wurden.
Das Wissen über die Dinosaurier erhalten Paläontologen durch die Untersuchung von Fossilien, wobei Knochenfunde eine herausragende Rolle spielen – durch sie werden wichtige Daten über Verwandtschaftsbeziehungen, Anatomie und Körperbau, Biomechanik und vieles mehr gewonnen.
Weitere Hinweise, besonders über das Verhalten der Dinosaurier, liefern die Spurenfossilien, etwa Zahnabdrücke an Knochen von Beutetieren, Hautabdrücke, Schwanzabdrücke, und vor allem fossile Fußspuren, die mit Abstand häufigsten Spurenfossilien. Spurenfossilien ermöglichen es, Dinosaurier aus einer anderen Perspektive zu studieren, da das Tier lebte, als die Spuren hinterlassen wurden – während Knochen immer von toten Tieren stammen. Weitere Informationen werden aus fossilen Eiern und Nestern, aus Koprolithen (versteinerten Kot) und Gastrolithen (Magensteine, die zur Zerkleinerung der Nahrung von einigen Dinosauriern verschluckt wurden) gewonnen.
Viele Wissenschaftler dachten lange, Dinosaurier seien eine polyphyletische Gruppe und bestünden aus miteinander nicht näher verwandten Archosauriern – heute werden Dinosaurier als selbstständige Gruppe angesehen.
Die ersten Dinosaurier gingen möglicherweise schon während der mittleren Trias vor etwa 245 Millionen Jahren aus ursprünglichen Vertretern der Avemetatarsalier-/Ornithodiren-Linie der Archosaurier hervor, wie der ostafrikanische Nyasasaurus bezeugt, der entweder der frühste Dinosaurier oder ihr nächster bekannter Verwandter ist. Die Fossilien der ältesten unzweifelhaften Dinosaurier Eoraptor und Herrerasaurus entstammen der etwa 230 Millionen Jahre (späte Trias) alten Ischigualasto-Formation in Argentinien. Eoraptor gilt als der ursprünglichste Vertreter und sah wahrscheinlich dem gemeinsamen Vorfahren aller Dinosaurier sehr ähnlich. Somit dürften die ersten Dinosaurier kleine, bipede Fleischfresser gewesen sein.
Diese Sichtweise wird bestätigt durch Funde primitiver, dinosaurierähnlicher Ornithodiren wie Marasuchus und Lagerpeton. Diese Gattungen werden zwar außerhalb der Dinosaurier klassifiziert, waren aber wahrscheinlich mit dem gemeinsamen Vorfahren aller Dinosaurier nahe verwandt.
Als die ersten Dinosaurier erschienen, waren die Nischen der terrestrischen Ökosysteme von verschiedenen Arten urtümlicher Archosaurier und Therapsiden besetzt: Aetosaurier, Cynodonten, Dicynodonten, Ornithosuchiden, Rauisuchier sowie Rhynchosaurier. Die meisten dieser Gruppen starben noch in der Trias aus; so gab es am Übergang zwischen Karnium und Norium ein Massenaussterben, bei dem die Dicynodonten und verschiedene basale Archosauromorphen wie die Prolacertiformen und Rhynchosaurier verschwanden. Darauf folgte ein weiteres Massenaussterben am Übergang zwischen Trias und Jura, bei dem die meisten anderen frühen Archosauriergruppen, wie Aetosaurier, Ornithosuchier, Phytosaurier und Rauisuchier ausstarben. Diese Verluste hinterließen eine Landfauna, die aus Crocodylomorphen, Dinosauriern, Säugetieren, Pterosauriern und Schildkröten bestand.Die frühen Dinosaurier besetzten wahrscheinlich die Nischen, die durch die ausgestorbenen Gruppen frei wurden. Früher wurde davon ausgegangen, dass die Dinosaurier die älteren Gruppen in einem langen Konkurrenzkampf zurückdrängten – dies wird heute aus mehreren Gründen als unwahrscheinlich angesehen: Die Zahl der Dinosaurier nahm nicht allmählich zu, wie es bei einem Verdrängen anderer Gruppen der Fall gewesen wäre; vielmehr machte ihre Individuenzahl im Karnium lediglich 1–2 % der Fauna aus, während sie nach dem Aussterben einiger älterer Gruppen im Norium bereits 50–90 % ausmachte. Ferner war die senkrechte Stellung der Beine, die lange als Schlüsselanpassung der Dinosaurier galt, ebenso in anderen zeitgenössischen Gruppen ausgeprägt, die nicht so erfolgreich waren (Aetosaurier, Ornithosuchier, Rauisuchier und einige Crocodylomorphen).
Die Überordnung der Dinosaurier wird, wie die meisten heutigen Reptilien, zu den Diapsiden gezählt. Diese unterscheiden sich von den Synapsiden (aus denen die Säugetiere hervorgingen) und von den Anapsiden (die heutigen Schildkröten) durch zwei paarweise angeordnete Schädelfenster hinter den Augen. Innerhalb der Diapsiden werden sie zu den Archosauriern, den Herrscherreptilien, gezählt, die mit zwei weiteren zusätzlichen Schädelfenstern ausgestattet sind. Heutige Überbleibsel dieser Reptiliengruppe sind neben den Krokodilen die Vögel. Die Dinosaurier selber werden traditionell in zwei Ordnungen, Saurischia (auch Echsenbeckendinosaurier) und Ornithischia (auch Vogelbeckendinosaurier), aufgeteilt. Diese unterscheiden sich vorrangig an der Beckenstruktur. Die Saurischia haben die Beckenstruktur ihrer Vorfahren beibehalten und sind durch voneinander abstehende Pubis- und Ischiumknochen zu erkennen. Die Pubis- und Ischiumknochen der Ornithischia jedoch verlaufen beide parallel zueinander schräg nach hinten.
Es folgt eine vereinfachte Klassifikation von Dinosauriern auf Familienebene. Eine detailliertere Aufstellung bis auf die Ebene der Gattungen findet sich im Artikel Systematik der Dinosaurier.
Ceratosauria (Ceratosaurus und Abelisauriden – letztere waren wichtige Prädatoren der späten Kreide in den südlichen Kontinenten)
Spinosauroidea (Fleisch- und eventuell Fischfresser; einige hatten einen krokodilähnlichen Schädel und knöcherne Rückensegel)
Troodontidae (ähnlich wie die Dromaeosauriden, aber leichter gebaut, und möglicherweise Allesfresser)
Prosauropoda (frühe Verwandte der Sauropoden; klein bis recht groß; einige waren eventuell Allesfresser, biped und quadruped)
Ornithopoda (divers, waren gleichzeitig quadruped und biped, entwickelten Fähigkeit zu kauen, große Anzahl von Zähnen)
Ceratopsia (quadrupede Dinosaurier mit Hörnern und Nackenschildern, obwohl frühe Formen nur Andeutungen dieser Merkmale hatten)
Die Evolution der Dinosaurier nach der Trias wurde durch Veränderungen der Vegetation und der Lage der Kontinente beeinflusst. In der späten Trias und im frühen Jura waren alle Kontinente zu der großen Landmasse Pangaea vereinigt, wodurch es eine weltweit einheitliche Dinosaurierfauna gab, die sich hauptsächlich aus karnivoren Coelophysoideen und herbivoren Prosauropoden zusammensetzte. Nacktsamige Pflanzen, insbesondere Koniferen, verbreiteten sich während der späten Trias als mögliche Futterquelle. Prosauropoden konnten das Pflanzenmaterial nicht im Mund verarbeiten und waren auf andere Mittel zur Aufschlüsselung der Nahrung im Verdauungstrakt angewiesen. Die Homogenität der Dinosaurierfaunen setzte sich bis in den mittleren und späten Jura fort: Unter den karnivoren Theropoden dominierten die Ceratosaurier, die Spinosauroideen und die Carnosaurier, während unter den Herbivoren die Stegosaurier, die Ornithischier und die Sauropoden verbreitet waren. Wichtige, gut bekannte Faunen des späten Jura schließen die der Morrison-Formation in Nordamerika und der Tendaguru Beds in Tansania mit ein. Faunen aus China zeigen jedoch bereits einige Unterschiede wie die spezialisierten Sinraptoriden unter den Karnivoren und ungewöhnliche, langhalsige Sauropoden wie Mamenchisaurus unter den Herbivoren. Ankylosaurier und Ornithopoden verbreiteten sich zunehmend, die Prosauropoden jedoch starben aus. Koniferen und andere Pflanzengruppen wie Farne und Schachtelhalme waren die dominierenden Pflanzen. Anders als Prosauropoden und Sauropoden haben die Ornithischier Mechanismen entwickelt, die eine Verarbeitung von Nahrung im Mund erlaubten. So hielten backenähnliche Organe die Nahrung im Mund, und durch Kieferbewegungen konnte die Nahrung zermahlen werden.Während der frühen Kreide setzte sich das Auseinanderbrechen Pangaeas fort, wodurch sich die Dinosaurierfaunen verschiedener Kontinente mehr und mehr unterschieden. Die Ankylosaurier, Iguanodonten und Brachiosauriden verbreiteten sich über Europa, Nordamerika und Nordafrika. Später kamen besonders in Afrika Theropoden wie die großen Spinosauriden und Carcharodontosauriden hinzu; außerdem gewannen Sauropodengruppen wie die Rebbachisauriden und die Titanosaurier an Bedeutung. In Asien wurden Maniraptoren wie die Dromaeosauriden, Troodontiden und Oviraptorosaurier häufig, Ankylosaurier und frühe Ceratopsier wie Psittacosaurus wurden wichtige Herbivoren. Währenddessen wurde Australien die Heimat ursprünglicher Ankylosaurier, Hypsilophodonten und Iguanodonten. Die Stegosaurier sind anscheinend in der späten Unterkreide oder der frühen Oberkreide ausgestorben. Eine große Veränderung in der Unterkreide brachte das Auftreten der Blütenpflanzen. Zur selben Zeit entwickelten verschiedene Gruppen von Herbivoren Zahnbatterien, die aus übereinander gestapelten Ersatzzähnen bestanden. Den Ceratopsiern dienten die Zahnbatterien zum Schneiden, während sie besonders bei Hadrosauriden zum Mahlen eingesetzt wurden. Einige Sauropoden haben ebenfalls Zahnbatterien entwickelt, am deutlichsten sind sie bei Nigersaurus ausgeprägt.In der Oberkreide gab es drei große Dinosaurierfaunen. In Nordamerika und Asien dominierten unter den Karnivoren die Tyrannosaurier und verschiedene Typen kleinerer Maniraptoren, die Herbivoren waren überwiegend Ornithischier und setzten sich aus Hadrosauriden, Ceratopsiern, Ankylosauriern und Pachycephalosauriern zusammen. In den südlichen Kontinenten waren die Abelisauriden die vorherrschenden Predatoren und Titanosaurier die vorherrschenden Herbivoren. Die Fauna Europas schließlich setzte sich aus Dromaeosauriden, Rhabdodontiden (Iguanodontia), Nodosauriden (Ankylosauria) und Titanosauriern zusammen. Blütenpflanzen breiteten sich weiter aus, und die ersten Gräser tauchten am Ende der Kreide auf. Hadrosauriden, welche Nahrung zermahlten, und Ceratopsier, welche Nahrung lediglich abschnitten, erlangten gegen Ende der Kreide in Nordamerika und Asien eine große Häufigkeit und Vielfalt. Einige Theropoden entwickelten sich derweil zu Herbivoren oder Omnivoren (Allesfressern), wie die Therizinosaurier und die Ornithomimosaurier.
Der erste als Vogel geltende Dinosaurier, Archaeopteryx, lebte im späten Jura Mitteleuropas (siehe Solnhofener Plattenkalk). Er entwickelte sich wahrscheinlich aus frühen Vertretern der Maniraptoren, einer Gruppe sehr vogelähnlicher Theropoden aus der relativ modernen Untergruppe Coelurosauria. Archaeopteryx weist ein Mosaik aus Merkmalen der Vögel und der Theropoden auf, ähnelt jedoch so sehr den Theropoden, dass mindestens ein Fossil ohne klar erkennbare Federabdrücke fälschlicherweise dem Compsognathus, einem kleinen Nichtvogeldinosaurier, zugeschrieben wurde. Obwohl das erste fast vollständige Exemplar von Archaeopteryx bereits im Jahr 1861 gefunden wurde, fand die Idee, dass Vögel von Dinosauriern abstammen, erst wesentlich später allgemeine Anerkennung, nachdem sie 1970 von John Ostrom neu aufgegriffen worden war. Bis heute wurden zahlreiche anatomische Gemeinsamkeiten zwischen theropoden Dinosauriern und Archaeopteryx nachgewiesen. Ähnlichkeiten zeigen sich besonders im Bau der Halswirbelsäule, des Schambeins (Pubis), des Handgelenks, des Schultergürtels, des Gabelbeins und des Brustbeins.
Ab den 1990er Jahren wurde eine Reihe von gefiederten coelurosauriden Theropoden entdeckt, die weitere Anhaltspunkte auf die enge Verwandtschaft zwischen Dinosauriern und Vögeln liefern. Die meisten dieser Funde stammen aus der Jehol-Gruppe im Nordosten Chinas, einer mächtigen Sedimentabfolge, die durch exzellent erhaltene Fossilien berühmt ist. Einige der in der Jehol-Gruppe gefundenen Vertreter mit daunenartigen Federn (Protofedern) sind relativ ursprüngliche und mit den Vögeln nicht sonderlich nahe verwandte Coelurosaurier, zum Beispiel der Compsognathide Sinosauropteryx, der Therizinosauroide Beipiaosaurus sowie die Tyrannosauroiden Dilong und Yutyrannus; letzterer der nach heutigem Kenntnisstand (Stand Juni 2015) mit Abstand größte gefiederte Dinosaurier, der aus der Yixian-Formation (Barremium bis frühes Aptium) der Provinz Liaoning stammt. Dies zeigt, dass ein primitives Gefieder ein ursprüngliches Merkmal der Theropoden, zumindest aber der Coelurosaurier zu sein scheint und dass viele andere Theropoden, von denen heute nur Skelettteile bekannt sind, zu Lebzeiten ebenfalls gefiedert waren, ihr Gefieder jedoch nicht fossil überliefert wurde. Daran ändert auch nichts, dass die Sedimente der Jehol-Gruppe erst in der Unterkreide abgelagert worden sind und damit geologisch jünger als der Solnhofener Plattenkalk und Archaeopteryx sind. Funde „gefiederter Dinosaurier“ sind generell selten, was sehr wahrscheinlich daran liegt, dass Weichteile wie Haut und Federn nur unter besonders günstigen Bedingungen, wie sie in den Sedimenten der Jehol-Gruppe und im Solnhofener Plattenkalk offenbar herrschten, fossilieren. Viele der gefiederten Nichtvogeldinosaurier in der Jehol-Gruppe wären demnach die ältesten mit Gefieder überlieferten Vertreter ihrer Entwicklungslinien, obwohl alle geologisch älteren Vertreter dieser Linien sowie deren gemeinsamer Vorfahr ebenfalls Federn hatten.
Die für Vögel so typischen Konturfedern finden sich jedoch nur bei Vertretern der Coelurosaurier-Untergruppe Maniraptora, welche die Oviraptorosaurier, die Troodontiden, die Dromaeosauriden und die Vögel umfasst. Protofedern entwickelten sich ursprünglich wahrscheinlich zur Wärmeisolierung, die Konturfedern speziell für die optische Kommunikation mit Artgenossen. Die Funktion des Erzeugens von Auftrieb beim Fliegen oder Segeln durch die Luft übernahmen sie erst später in der Evolution. Bei Microraptor gui, einem etwa huhngroßen Dinosaurier aus der Jehol-Gruppe, sind nicht nur die Arme und Hände, sondern auch die Beine als mit Konturfedern bestückte Flügel ausgebildet, und es wird angenommen, dass Microraptor auf diesen vier Schwingen von Baum zu Baum gesegelt ist. Allerdings gilt Microraptor, obwohl geologisch jünger als Archaeopteryx, nicht als Vogel. Sein vogelähnliches Äußeres geht stattdessen auf eine Parallelentwicklung (Konvergenz) innerhalb der Maniraptora zurück. Mit Jeholornis gab es aber auch einen frühen „echten“ Vogel in der Jehol-Gruppe.
Dass Microraptor nicht als Vogel betrachtet wird, liegt nicht daran, dass er lediglich zur Fortbewegung per Gleitflug fähig war, sondern an seiner Skelettanatomie, die ihn als Dromaeosauriden ausweist. Schließlich sind auch die fliegerischen Fähigkeiten von Archaeopteryx durchaus umstritten. Eine Hypothese zur Entwicklung der Flugfähigkeit der „echten“ Vögel besagt, dass diese sich aus dem Gleitflug entwickelt hat. Die Vorfahren von Archaeopteryx, möglicherweise sogar Archaeopteryx selbst, wären demnach ebenfalls Gleitflieger gewesen. Jedoch gibt es auch andere Ansichten.
Obwohl Dinosaurier in der Größe erheblich variierten, waren sie als Gruppe groß. Nach einer Schätzung wog der durchschnittliche Dinosaurier ein bis zehn Tonnen, das durchschnittliche Säugetier des Känozoikums nur zwei bis fünf Kilogramm. Einige Dinosaurier waren gigantisch; insbesondere die langhalsigen Sauropoden, zu denen die größten Landtiere der Erdgeschichte gehörten.
Laut der Cope’schen Regel besteht im Laufe der Evolution einer Tiergruppe infolge von zwischen- und innerartlicher Konkurrenz eine generelle Tendenz zur Zunahme der Körpergröße ihrer Vertreter. Dies allein erklärt jedoch nicht den in der Erd- und Evolutionsgeschichte beispiellosen Gigantismus der Dinosaurier. Diesbezüglich stellt sich sowohl die Frage, warum die Tiere so groß wurden, als auch, wie diese Größe erreicht werden konnte. Hinsichtlich des Warum wird für die Sauropoden angenommen, dass ihr Riesenwachstum Vorteile für die Verdauung brachte. Da Sauropoden ihre eher schwer verdauliche pflanzliche Nahrung nicht kauten, ermöglichte ein längeres Verweilen in einem ausladenden Verdauungstrakt eine intensivere Nutzung (einen besseren Aufschluss) der Nahrung als bei kleineren Pflanzenfressern. Dies hätte eine Spezialisierung auf sehr nährstoffarme Pflanzenkost erlaubt. Hinsichtlich des Wie haben ein vogelähnliches Luftsacksystem einschließlich luftgefüllter Knochen (Pneumatisierung, bei den großen Theropoden besonders im Schädel) den Gigantismus der Dinosaurier ermöglicht. Die Sauropoden hätten zudem durch die Nahrungsaufnahme ohne Kauen Kiefermuskulatur eingespart, was einen verhältnismäßig kleinen leichten Kopf zuließ, wodurch wiederum die Hälse so lang werden konnten, wobei die Halslänge als Merkmal unter anderem möglicherweise einer sexuellen Selektion unterworfen war, d. h. Männchen mit den längsten Hälsen hätten den höchsten Paarungserfolg gehabt (eine Form von innerartlicher Konkurrenz). Des Weiteren sei bei den Sauropoden eine Gewichtszunahme von bis zu 30 Tonnen in 20 Jahren nur realisierbar, wenn die Stoffwechselraten bei Jungtieren sehr hoch (vogel- oder säugerähnlich) gewesen seien, mit zunehmender Körpergröße aber immer weiter abgenommen hätten, um eine Überhitzung zu vermeiden (vgl. Oberflächenregel; siehe auch Dinosaurier-Physiologie).Welches die größten oder kleinsten Dinosaurier waren, wird wahrscheinlich nie mit Sicherheit gesagt werden können. Die Überlieferung durch Fossilien ist oft sehr unvollständig, nur die wenigsten Dinosaurier sind durch vollständige Skelette bekannt – von vermutlich besonders großen Arten werden meist lediglich Knochenfragmente gefunden. Paläontologen können zwar die Form und Größe der Knochen mit denen besser bekannter Arten vergleichen, um die Größe zu schätzen – dies ist allerdings ungenau. Noch schlechter lässt sich das Gewicht der Tiere schätzen, da es unter anderem davon abhängt, wie ein Modell mit Muskeln und Sehnen versehen wird, deren Lage Paläontologen anhand von Muskelansatzstellen an den Knochen herausfinden müssen.
Der größte und schwerste Dinosaurier, der durch gute Skelettfunde bekannt ist, ist der Sauropode Brachiosaurus (auch bekannt als Giraffatitan). Ein Skelett, das aus den Knochen verschiedener etwa gleich großer Individuen besteht, ist im Berliner Naturkundemuseum ausgestellt und hat eine Höhe von über 13 Metern und eine Länge von mehr als 23 Metern; ein solches Tier hätte wahrscheinlich ein Gewicht von 30 bis 60 Tonnen gehabt. Der längste durch vollständige Skelette bekannte Dinosaurier ist Diplodocus, ein Skelettfund zeigt eine Länge von 27 Metern. Noch größere Sauropoden sind nur durch Skelettfragmente bekannt. Einer der größten Dinosaurier könnte Argentinosaurus gewesen sein, der manchmal auf ein Gewicht von bis zu hundert Tonnen geschätzt wird; der 33,5 Meter lange Diplodocus hallorum (früher Seismosaurus) könnte zusammen mit dem 33 Meter langen Supersaurus vielleicht zu den längsten Dinosauriern gehört haben.
Unter den fleischfressenden Dinosauriern gab es ebenfalls Riesen. Der größte durch fast vollständige Skelettfunde bekannte Theropode ist der etwa 12 Meter lange Tyrannosaurus rex, jedoch gibt es hier ebenso Skelettfragmente, die auf noch größere Gattungen schließen lassen. Der größte bekannte Theropode war vielleicht Spinosaurus mit einer Länge von 16 bis 18 Metern und einem Gewicht von acht Tonnen, weitere sehr große Theropoden schließen Giganotosaurus, Mapusaurus und Carcharodontosaurus mit ein.
Die kleinsten Dinosaurier hatten die Größe eines Huhns – so waren die Theropoden Microraptor und Parvicursor beide weniger als 60 Zentimeter lang. Die kleineren Dinosaurier ernährten sich fast ausschließlich karnivor.
Interpretationen über das Verhalten der Dinosaurier basieren meistens auf der Haltung von Skelettfunden, auf Spurenfossilien wie fossilen Fußspuren, auf dem Habitat, in welchem die Tiere lebten, auf Computersimulationen der Biomechanik und auf Vergleichen mit rezenten Tieren ähnlicher ökologischer Nischen. Viele Hypothesen über das Verhalten der Dinosaurier sind lediglich spekulativ, einige finden aber Zustimmung bei den meisten Forschern.
Der Fund eines Iguanodon-Massengrabs in Bernissart (Belgien) im Jahr 1878 gab einen ersten Hinweis auf Herdenleben bei Dinosauriern. Heute sind viele weitere Hinweise auf ein Herdenleben bei vielen Dinosaurierarten bekannt, so wurden neben weiteren Massengräbern auch viele parallel verlaufende Fährtenfolgen entdeckt. Hadrosauriden wanderten vermutlich in großen Herden, ähnlich wie die heutigen Springböcke oder Amerikanischen Bisons; so enthält ein Massengrab von Maiasaura aus Montana (USA) die Überreste von mindestens 10.000 Individuen. Sauropodenspuren aus Oxford (England) zeigen, dass diese Sauropoden in gemischten Herden mit unterschiedlichen Arten wanderten. Vielleicht bildeten Dinosaurier Herden zur Verteidigung gegen Fressfeinde, zum Schutz der Jungtiere oder für periodische Wanderungen. Einige karnivore Dinosaurier werden ebenso oft gesellig dargestellt, wobei sie in einer Gruppe selbst größere Beute erlegt haben könnten. Jedenfalls ist das Jagen in Gruppen bei den nächsten lebenden Verwandten der Dinosaurier, den Vögeln und den Krokodilen, recht ungewöhnlich, und vermeintliche Nachweise für ein Jagen in Gruppen bei den Theropoden Deinonychus und Allosaurus könnten die Ergebnisse von tödlichen Auseinandersetzungen zwischen fressenden Tieren sein, wie bei modernen Reptilien häufig zu beobachten.
Eine Maiasaura-Nestkolonie, die Jack Horner im Jahr 1978 in Montana (USA) entdeckt hat, zeigt, dass einige Dinosaurier ihre Jungen noch lange nach dem Schlüpfen betreuten und beschützten. In der Mongolei wurde 1993 das Skelett des Oviraptoriden Citipati in einer brütenden Position über seinen Eiern entdeckt; dies könnte auf isolierende Federn hinweisen, welche die Eier warm hielten. Andere Funde zeigen ebenfalls elterliche Fürsorge. So wurde zum Beispiel in Liaoning (China) ein erwachsenes Exemplar des Ceratopsier Psittacosaurus zusammen mit 34 Jungtieren gefunden – die große Anzahl des Nachwuchses könnte darauf hindeuten, dass das erwachsene Tier den Nachwuchs von verschiedenen Individuen betreut hat, ähnlich wie bei heutigen Straußen. Die Auca-Mahuevo-Fundstelle in Patagonien barg tausende Nester mit Eiern, die Sauropoden zugeschrieben werden und Hinweise auf große Nistkolonien dieser Dinosaurier geben, ähnlich denen der heutigen Pinguine. Sauropoden betrieben allerdings wahrscheinlich keine elterliche Fürsorge, was nicht zuletzt wegen der Größe der Elterntiere im Vergleich zu den Jungtieren angenommen wird.Die mannigfaltigen Kämme und Schilde einiger Dinosaurier, wie die der Marginocephalia, der Theropoden und der Lambeosaurinen, waren zur aktiven Verteidigung vielleicht zu zerbrechlich. Wahrscheinlicher ist, dass sie zur sexuellen Zurschaustellung dienten oder Artgenossen einschüchtern sollten – jedoch ist nur wenig über die Paarung und das Territorialverhalten der Dinosaurier bekannt. Bisswunden an den Schädeln von einigen Theropoden lassen aktive aggressive Konfrontationen zumindest bei diesen Dinosauriern vermuten. Die Kommunikation der Dinosaurier untereinander bleibt ebenfalls mysteriös, ist aber ein aktives Gebiet der Forschung. Beispielsweise haben jüngere Studien gezeigt, dass die Kopfkämme der Lambeosaurinen als Resonanzverstärker für ein breites Spektrum von Rufen gedient haben könnten.Ein Fossil eines Troodontiden aus China zeigte, dass dieser kleine Theropode den Kopf beim Schlafen unter die Arme steckte, um ihn warm zu halten; ähnlich wie heutige Vögel. Eines der für die Verhaltensforschung wertvollsten Dinosaurier-Fossilien wurde im Jahr 1971 in der Wüste Gobi entdeckt und beinhaltet einen Velociraptor, der einen Protoceratops attackiert hat; der Fund zeigt die Tiere annähernd in Lebenddarstellung. Weitere Hinweise auf das Jagen lebender Beute liefert eine teilweise verheilte Schwanzverletzung eines zu den Hadrosauriern zählenden Edmontosaurus – der Schwanz wurde von einem Tyrannosaurier gebissen, das Tier überlebte aber. Kannibalismus konnte bei einigen Theropoden wie Majungasaurus nachgewiesen werden; so wurden im Jahr 2003 in Madagaskar entsprechende Bissspuren gefunden.Neue Funde wie Oryctodromeus zeigen, dass einige herbivore Arten anscheinend in einem Bau unter der Erde lebten, während einige vogelähnliche Arten vielleicht baumbewohnend waren, wie Microraptor und die rätselhaften Scansoriopterygiden. Die meisten Dinosaurier bewegten sich jedoch auf dem Boden fort. Ein gutes Verständnis der Art der Fortbewegung ist ein Schlüssel für die Verhaltensforschung, und die Biomechanik hat bedeutende Fortschritte auf diesem Gebiet gebracht. So gab es Studien über die von Muskeln ausgeübten Kräfte und über das auf dem Skelett lastende Gewicht, wodurch geschätzt wurde, wie schnell Dinosaurier rennen konnten. Weiter wurde untersucht, ob Diplodociden mit ihrem Schwanz einen Überschallknall erzeugen konnten oder ob Sauropoden schwimmen konnten. Für Theropoden wurde anhand von Kratzspuren im Sediment eines Sees nachgewiesen, dass sie schwimmen konnten.
Seit den 1960er Jahren läuft eine energische Diskussion über die Temperatur-Regulierung der Dinosaurier. Obwohl die Theorie, dass Dinosaurier ihre Körpertemperatur überhaupt regulieren konnten, ursprünglich von Wissenschaftlern abgelehnt wurde, ist die Warmblütigkeit (Endothermie) der Dinosaurier die heute gängige Sichtweise, und die Debatte hat sich mehr auf die Mechanismen der Thermoregulation fokussiert.
Als die ersten Dinosaurier entdeckt wurden, glaubten die Wissenschaftler, Dinosaurier seien wechselwarme (ektotherme) Tiere – „schreckliche Echsen“, wie ihr Name nahelegt. Man stellte sich Dinosaurier als langsame, träge Tiere vor und verglich sie mit Reptilien, die sich erst durch die Sonne aufwärmen müssen, um sich aktiv bewegen zu können. Die Vorstellung von wechselwarmen Dinosauriern herrschte vor, bis Robert „Bob“ Bakker, ein früher Verfechter der Warmblütigkeit der Dinosaurier, eine einflussreiche Arbeit zum Thema veröffentlichte.
Hinweise auf Warmblütigkeit liefern Entdeckungen aus Antarktika und Australien, wo „Polar-Dinosaurier“ gefunden wurden, die dort einen kalten, sechsmonatigen Winter überstehen mussten. Erst kürzlich wurden Funde aus der Kreidezeit in Nordalaska gemacht, die zeigen, dass in diesen schon damals kalten Gebieten sogar dieselben Arten wie im übrigen Nordamerika lebten. Zusätzlich lässt der Skelettbau vieler Dinosaurier – insbesondere der Theropoden – auf eine hohe Aktivität schließen, die ebenfalls für eine hohe Stoffwechselrate spricht. Ebenso konnten die für warmblütige Tiere typischen Blutgefäß-Strukturen in Dinosaurierknochen nachgewiesen werden. Eine nicht unbedeutende Anzahl von kleineren Dinosauriern verfügte außerdem über ein isolierendes Federkleid. Sauropoden zeigen jedoch weniger Anzeichen für Warmblütigkeit. Es ist daher möglich, dass einige Dinosaurier warmblütig waren, andere aber nicht.Die Debatte wird dadurch verkompliziert, dass Warmblütigkeit auf mehr als nur einen Mechanismus beruhen kann. In den meisten Diskussionen wird die Dinosaurier-Warmblütigkeit mit der von durchschnittlich großen Vögeln oder Säugetieren verglichen, welche Energie aufwenden, um ihre Körpertemperatur über der Umgebungstemperatur zu halten. Kleine Säugetiere und Vögel besitzen außerdem eine Isolierung in Form von Fett, Fell oder Federn, die den Wärmeverlust verringert. Jedenfalls haben große Tiere wie Elefanten ein ganz anderes Problem – wird ein Tier größer, vergrößert sich das Volumen viel schneller als die Hautfläche (Haldanes Prinzip). Ab einem gewissen Punkt übersteigt die vom Körper produzierte Wärme den Wärmeverlust über die Haut, sodass den Tieren Überhitzung droht. Besonders in Bezug auf Sauropoden wird daher die Theorie diskutiert, dass große Dinosaurier durch ihre schiere Größe wärmer als die Umgebung waren (gigantotherm), ohne dass sie spezielle Anpassungen wie Säugetiere oder Vögel besessen hätten. 2011 hatten Forscher in Science berichtet, dass die Körpertemperatur für einige große pflanzenfressende Dinosaurier auf 36 bis 38 Grad bestimmt wurde.Theropoden und wahrscheinlich ebenso Sauropoden besaßen Luftsäcke, die wie Blasebälge Luft durch die Lunge führten. Da dieses Merkmal unter heute lebenden Tieren nur bei den Vögeln bekannt ist, gilt es als weiterer Hinweis auf eine Abstammung der Vögel von den Dinosauriern. Außerdem wird es als Hinweis gedeutet, dass ebenso größere Dinosaurier wie die Sauropoden warmblütig gewesen sein könnten, da Verdunstung in den Luftsäcken einen effektiven Kühlmechanismus darstellt.
Computertomografische Untersuchungen von Hohlräumen in der Brustgegend des Ornithopoden Thescelosaurus zeigten im Jahr 2000 die Überreste eines komplexen, vierkammerigen Herzes. In Fachkreisen besteht zwar Uneinigkeit über die Richtigkeit der Ergebnisse, das Vorkommen eines vierkammerigen Herzens sowohl bei Vögeln als auch bei Krokodilen könnte jedoch darauf hindeuten, dass Dinosaurier ebenfalls ein solches besessen haben.Histologische Untersuchungen an Dinosaurierknochen lieferten Hinweise auf enge physiologische Parallelen zwischen weiblichen Dinosauriern und Vögeln bei der Eierschalenproduktion. Bei weiblichen Vögeln wächst infolge der Ausschüttung von Östrogen im Zeitraum vor der Eiablage eine sehr calciumreiche Knochensubstanz in den Beinknochen, an der Innenseite der harten Außenknochen (corticaler Knochen) in die Markhöhle hinein. Diese Knochensubstanz wird als medullärer Knochen bezeichnet und ist reich an Proteoglykanen und Glykoproteinen, an die die Calcium-Ionen gebunden werden, sowie arm an Kollagen. Medullärer Knochen dient als Reservoir, in dem Calcium gespeichert wird, das für die Bildung der Eierschalen nötig ist. Knochensubstanz, die als medullärer Knochen gedeutet wird, fand sich auch in den Beinknochen eines Tyrannosaurus rex. Dies lässt vermuten, dass die Calciumeinlagerung bei Dinosauriern und Vögeln ähnlich funktionierte. Zudem kann das Vorhandensein von medullärem Knochen bei fossilen Vögeln oder bei Dinosauriern für die Bestimmung weiblicher Tiere genutzt werden. Nach weiteren Forschungen wurde medullärer Knochen auch bei Allosaurus und dem Vogelbeckensaurier Tenontosaurus entdeckt. Da diese beiden Gattungen jeweils einer der beiden Hauptlinien der Dinosaurier angehören, wird davon ausgegangen, dass medullärer Knochen zur Calciumspeicherung bereits sehr früh in der Dinosaurierevolution entstanden ist und dass deshalb nahezu alle Dinosaurier dieses Merkmal besessen haben dürften. Dass diese Knochensubstanz auch bei noch nicht voll ausgewachsenen Tieren gefunden wurde, lässt den Schluss zu, dass Dinosaurier relativ früh in ihrer Individualentwicklung die Geschlechtsreife erreichten.
Eines der besten Beispiele für Abdrücke von weichem Gewebe in einem Dinosaurierfossil wurde in Petraroia, Italien entdeckt. Der im Jahr 1998 beschriebene Fund stammt von dem kleinen, sehr jungen Coelurosaurier Scipionyx und zeigt Abdrücke verschiedener Darmabschnitte, der Leber, der Muskeln und der Luftröhre.Im Jahr 2005 stellten Dr. Mary Higby Schweitzer und ihr Team erstmals flexibles Gewebematerial eines Dinosauriers vor, das in einem 68 Millionen Jahre alten Beinknochen eines Tyrannosaurus rex aus der Hell-Creek-Formation in Montana (USA) gefunden wurde. Nach einer Rehydrierung wurde das Material wieder elastisch, und nach einer mehrere Wochen währenden Behandlung zum Entfernen der Mineralien (Demineralisierung) konnten intakte Strukturen wie Blutgefäße, Knochenmatrix und Knochenfasern nachgewiesen werden. Genauere Untersuchungen unter dem Mikroskop zeigten darüber hinaus, dass sogar noch Mikrostrukturen auf zellulärer Ebene erhalten geblieben sind. Thomas Kaye et al. stellen die Ergebnisse in einer kürzlich veröffentlichten Studie jedoch in Frage – nach diesen Forschern handelt es sich bei dem vermeintlichen Gewebematerial um bakterielle Biofilme. Die Bakterien kolonisierten einst die Hohlräume des Knochens, die zuvor von echten Zellen besetzt waren. Die von Schweitzer aufgrund ihrer Eisenhaltigkeit als Blutgefäße interpretierten Strukturen deuten die Forscher zudem als Framboide – rundliche, mikroskopisch kleine Mineralstrukturen.Die erfolgreiche Gewinnung von Dinosaurier-DNA wurde zwar in zwei Fällen gemeldet, keine dieser beiden Studien konnte jedoch bestätigt werden. Jedenfalls wurde bereits das theoretische Peptid Rhodopsin eines Dinosauriers kloniert, wobei die Gene von Krokodilen und Vögeln, der nächsten heute lebenden Verwandten der Dinosaurier, als Grundlage für die phylogenetische Ableitung dieses Peptids dienten. Das in Zellkultur exprimierte und aufgereinigte Rhodopsin zeigte sich aktiv in funktionellen Tests. Des Weiteren wurden verschiedene Proteine in Dinosaurierfossilien entdeckt, inklusive des roten Blutfarbstoffs Hämoglobin. Die Möglichkeit des Klonens von Dinosauriern, wie in Michael Crichtons berühmtem Roman DinoPark (1993 verfilmt als Jurassic Park), kann aufgrund mangelnder genetischer Information für die nähere Zukunft ausgeschlossen werden.
Im Zuge der mehr als einhundertjährigen Erforschung fossiler Überreste von Dinosauriern sind viele Informationen über Neurologie, mögliches Verhalten und Physiologie dieser urzeitlichen Lebewesen gewonnen worden. Empirische Daten zur Individualentwicklung (beispielsweise zum Erreichen der Geschlechtsreife und zu Wachstumsraten) und zur allgemeinen Lebenserwartung fehlten jedoch zu Beginn des 21. Jahrhunderts noch weitgehend. Um das Alter einzelner Individuen und darüber die mittlere Lebenserwartung der entsprechenden Spezies abzuschätzen, kamen und kommen histologischen Untersuchungen und insbesondere der Lebensaltersbestimmung anhand von Wachstumslamellen (Skeletochronologie) eine besondere Bedeutung zu.Bei den nächsten lebenden Verwandten der Dinosaurier, Vögeln und Krokodilen, korreliert die Lebenserwartung bei einer bestimmten Spezies mit der maximal erreichbaren Körpergröße ihrer Individuen (je größer desto älter) und die Vertreter einiger der größten Arten können erfahrungsgemäß sehr alt werden (in Gefangenschaft teils mehr als 100 Jahre). Daraus könnte man ähnliche Lebensspannen und eine ähnliche Größenkorrelation für Dinosaurier ableiten. Allerdings stimmen die Ergebnisse histologischer Untersuchungen nicht mit solch rein phylogeniebasierten Rückschlüssen überein; die Daten rezenter Archosaurier lassen sich also nicht ohne Weiteres auf Dinosaurier hochrechnen. Nichtsdestoweniger sind die Erkenntnisse aus histologischen Untersuchungen rezenter Archosaurier wichtig für die korrekte Interpretation der Beobachtungen an histologischen Schnitten fossiler Dinosaurierknochen (Aktualismusprinzip).
Dass viele Dinosaurierarten eine enorme Größe erreichten, weiß man bereits seit dem 19. Jahrhundert. Wie schnell sie wuchsen, mit welchem Alter sie ausgewachsen waren oder ob sie bis zu ihrem Tode kontinuierlich weiterwuchsen, war jedoch lange Zeit unbekannt. Skeletochronologische Untersuchungen an Dinosaurierknochen, das heißt das Zählen von Wachstumslamellen (engl.: lines of arrested growth, wörtlich: ‚Linien gestoppten/gebremsten Wachstums‘), die im Idealfall wie die Jahresringe eines Baumstamms das Lebensalter des Tieres anzeigen, konnten diese Wissenslücke mittlerweile ansatzweise füllen. So ist für den spätkreidezeitlichen riesenwüchsigen Theropoden Tyrannosaurus ermittelt worden, dass er in der „Pubertät“, im zweiten Lebensdrittel (zwischen 14 und 18 Jahren), besonders schnell wuchs, mit einer täglichen Gewichtszunahme um mindestens zwei Kilogramm, und dass sich im Alter von etwa 20 Jahren und bei einem Gewicht von 5 Tonnen das Wachstum drastisch verlangsamte, er faktisch ausgewachsen war. Sein Höchstalter soll 28 Jahre betragen haben, wäre also, angesichts seiner Größe, überraschend gering gewesen. Für kleinere, geologisch ältere Vertreter der Tyrannosauriden wurden noch geringere Höchstalter berechnet. Die Lebenserwartung des großwüchsigen triassischen „Prosauropoden“ Plateosaurus wird auf mindestens 27 Jahre geschätzt.Ob bei einem bestimmten Dinosauriertaxon die starke Verlangsamung des Körperwachstums (das Erreichen der sogenannten asymptotischen Größe oder somatischen Reife) auch mit dem Erreichen der Geschlechtsreife übereinstimmte, kann durch das Zählen von Wachstumsringen allein nicht festgestellt werden. Die Entdeckung von medullärem Knochen (einem kalziumreichen Knochengewebe in der Markhöhle der Beinknochen weiblicher Vögel) bei verschiedenen Dinosaurierarten ermöglicht allerdings nunmehr relativ eindeutig die Bestimmung des Alters der Geschlechtsreife, zumindest bei weiblichen Individuen. Medullärer Knochen dient als Kalziumspeicher für die Sekretion kalkiger Eierschalen und sollte folglich nur bei geschlechtsreifen Tieren zu finden sein. So ist in Kombination mit der Skeletochronologie für Tenontosaurus (ein Vogelbeckensaurier), Allosaurus und Tyrannosaurus (beides Echsenbeckensaurier) das Erreichen der Geschlechtsreife im Alter von 8, 10 bzw. 18 Jahren ermittelt worden. Dies legt nahe, dass Dinosaurier wahrscheinlich einige Jahre vor Erreichen der asymptotischen Größe geschlechtsreif wurden. Dass dieses Muster auch bei mittelgroßen bis großen Säugetieren auftritt und vor allem dass Dinosaurier die Geschlechtsreife und asymptotische Größe bedeutend früher erreichten, als es rezente Reptilien täten, wenn diese ähnlich großwüchsig wären, lässt den Schluss zu, dass Dinosaurier warmblütigen Amnioten wie Säugetieren physiologisch-metabolisch wahrscheinlich ähnlicher waren als rezenten Reptilien.
In den letzten 550 Millionen Jahren gab es fünf große Massenaussterben, bei denen jeweils mindestens 40 % aller Gattungen ausstarben. Am berühmtesten ist das Kreide-Paläogen-Massenaussterben vor etwa 66 Millionen Jahren, bei dem schätzungsweise 50 % der Gattungen und 20 % der Familien verschwanden, darunter alle Nicht-Vogel-Dinosaurier. Es wurden die verschiedensten Hypothesen aufgestellt, um die Ursachen dieses Massensterbens zu klären. Die meisten aktuellen Theorien sehen die Ursache in einem Meteoriteneinschlag oder einem gesteigerten Vulkanismus, einige schließen beide Ereignisse mit ein. Das rapide Absinken des Meeresspiegels könnte gleichfalls zum Massensterben beigetragen haben, da die großen Flachmeere verschwanden und sich Landbrücken bildeten.In einer dünnen, dunklen Ton­schicht aus der Zeit des Massenaussterbens an der Kreide-Paläogen-Grenze fanden der Physiker Luis Walter Alvarez und sein Sohn, der Geologe Walter Alvarez, eine Anreicherung des sonst in der Erdkruste sehr seltenen Schwermetalls Iridium (siehe Iridium-Anomalie). In ihrer im Juni 1980 veröffentlichten Studie postulierten sie, dass dieses Iridium durch einen Meteoriten auf die Erde gelangt sein könnte, dessen Einschlag das Massensterben auslöste. Die Hypothese wurde 1991 durch den Fund des 180 km durchmessenden Chicxulub-Kraters am Rande der Yukatan-Halbinsel im Golf von Mexiko untermauert, der zum passenden Zeitpunkt durch den Einschlag eines etwa 10 km großen Meteoriten entstanden ist. Zu den möglichen kurz- und mittelfristigen Folgen für die damaligen Ökosysteme sind vielfältige Überlegungen angestellt worden. So wird eine zunächst starke Aufheizung der Atmosphäre als unmittelbare Folge des Einschlags angenommen („Feuersturm“), gefolgt von einer inzwischen nachgewiesenen starken Abkühlung, weil die Atmosphäre durch Staub verdunkelt und so die Sonneneinstrahlung (Insolation) für einige Jahre um 10 bis 20 % reduziert wurde („nuklearer Winter“), mit negativen Auswirkungen auf die Photosyntheseraten von Landpflanzen und einzelligen Meeresalgen, den Primärproduzenten der terrestrischen und marinen Ökosysteme. Eine weitere unmittelbare und mittelbare Folge des Einschlages waren wahrscheinlich schwere und schwerste Erdbeben und Tsunamis, die noch tausende Kilometer vom Einschlagsort entfernt auftraten.Mehrere Hypothesen zu den Ursachen des Aussterbens stehen speziell mit dem Umstand in Zusammenhang, dass der Einschlag seinerzeit in eine karbonatische Schelfplattform erfolgte. Deren Sedimentabfolge beherbergt heute in unmittelbarer Nähe des Chicxulub-Kraters eine ergiebige Erdöllagerstätte, die wahrscheinlich auch schon am Ende der Kreidezeit existierte. Daher wurde, gestützt auf Isotopenuntersuchungen von organischem Kohlenstoff aus Sedimenten des Kreide-Paläogen-Grenzbereichs, eine These formuliert, die besagt, dass durch den Einschlag große Mengen von Erdöl verbrannt seien. Der dabei entstandene Ruß sei in die Stratosphäre aufgestiegen und habe sich dort weltweit verbreitet. Dies, und nicht der aus dem Einschlagskrater ausgeschleuderte feine Gesteinsstaub, sei Hauptursache für die nach dem Impakt erfolgte weltweite Verdunkelung und Abkühlung gewesen. Weil der mächtige Sedimentstapel der Yucatan-Karbonatplattform unter anderem auch Anhydrit und Gips (Kalziumsulfat, CaSO4) enthält, könnten durch die Einschlagshitze große Mengen Schwefeldioxid (SO2) und -trioxid (SO3) entstanden sein, was in den folgenden Wochen und Monaten weltweit zu saurem Regen geführt hätte, der eine Versauerung von Böden und Gewässern verursacht und so die Lebensbedingungen vor allem für Pflanzen und Algen erheblich verschlechtert hätte. Mithilfe eines computergestützten Klimamodells wurde ermittelt, dass nicht Staub oder Ruß, sondern vor allem langlebige Schwefelsäure-Aerosole in der Atmosphäre in den Monaten nach dem Einschlag ein Absinken der durchschnittlichen jährlichen globalen Oberflächentemperatur um mindestens 26 °C bewirkt hätten. Dadurch habe die Erde eine mehrjährige, nahezu globale Dauerfrostperiode erlebt, mit dramatischen Folgen für sowohl Land- als auch Meereslebewesen.Ein Indiz für die Vulkanismustheorie ist der gewaltige Dekkan-Trapp-Vulkanismus der indischen Dekkan-Hochebene, der mindestens zwei Millionen Kubikkilometer Basalt (Flutbasalt, Trapp) förderte. Neben einer verminderten Sonneneinstrahlung könnten große Mengen Kohlenstoffdioxid in die Atmosphäre gelangt sein, was einen starken Treibhauseffekt zur Folge gehabt hätte. Eine neuere Studie verknüpft den Chicxulub-Impakt unmittelbar mit dem Dekkan-Trapp-Vulkanismus. Demnach verzeichnete der lange „schwelende“ Dekkan-Trapp aufgrund der beim Meteoriteneinschlag freigesetzten Energie von mindestens 3×1023 Joule und der dadurch ausgelösten tektonischen Schockwellen ein neues Aktivitätsmaximum. Laut dieser Hypothese ist der geologisch kurzfristige, über Jahrtausende in das Paläogen reichende Ausstoß von 70 % aller Dekkan-Trapp-Flutbasalte auf dieses Ereignis zurückzuführen.Das eigentliche Aussterben vollzog sich wahrscheinlich in einem relativ kurzen Zeitraum und „zumindest in Nordamerika nach geologischen Maßstäben augenblicklich“. Zwar wurden einige Dinosaurierknochen in 64,5 Millionen Jahre alten känozoischen Schichten gefunden und daraufhin von einigen Forschern als Hinweis auf ein langsames Aussterben gedeutet, jedoch wurde dieser These heftig widersprochen mit der Begründung, die Knochen seien aus mesozoischen Ablagerungen erodiert und in den känozoischen Schichten neu eingebettet worden. Weit umstrittener ist die Frage, ob die Dinosaurier am Ende der Kreidezeit ohnehin im Niedergang begriffen waren oder ob sie in voller Blüte standen. Aktuelle Forschungen hierzu ergaben, dass die Artneubildungsrate der Dinosaurier schon 50 Millionen Jahre vor dem Ende der Kreidezeit von der normalen Hintergrundaussterberate übertroffen wurde. Ausnahmen von dieser allmählichen Abnahme der Artendiversität sind die Hadrosaurier und die Ceratopsia, spezialisierte Pflanzenfresser, die noch in der jüngsten Kreide zahlreiche neue Arten bildeten. Trotz dieser Ausnahmen waren die Dinosaurier als Taxon seinerzeit offenbar relativ anfällig gegenüber einer globalen Katastrophe.
Keine andere Gruppe ausgestorbener Tiere hat eine derart wichtige kulturelle Bedeutung wie die Dinosaurier. Seit die Dinosaurier im 19. Jahrhundert erstmals ins Licht der Öffentlichkeit rückten, erfreuen sie sich weltweit großen Interesses und so großer Beliebtheit, dass teilweise von einer bis heute anhaltenden „Dinomanie“ gesprochen wird.
Fast so bekannt wie die Dinosaurier selbst ist die Tatsache, dass sie innerhalb kürzester Zeit ausgestorben sind. Daher wird der Name Dinosaurier oft als Metapher für Denk- und Handelsweisen oder für Dinge benutzt, die als rückwärtsgewandt und nicht mehr zeitgemäß empfunden werden, wie etwa beim Dinosaurier des Jahres.
Die ersten Dinosaurierfossilien sind schon vor hunderten, wahrscheinlich tausenden von Jahren gefunden worden, wobei ihre wahre Natur nicht erkannt wurde. Im China der östlichen Jin-Dynastie berichtete Cháng Qú (常璩) in seinem Buch Huàyángguó zhì (chinesisch 華陽國志 / 华阳国志) im 4. Jahrhundert über die Entdeckung von „Drachenknochen“ aus der Provinz Sichuan, die vielleicht von Dinosauriern stammten. Dorfbewohner in Zentralchina haben derartige Drachenknochen seit Jahrzehnten ausgegraben, um aus ihnen traditionelle Medizin herzustellen. Die antiken Griechen und Römer fanden ebenso entsprechende Fossilien, die Stoff für ihre Legenden und Sagen boten.
Im Jahr 1677 fertigte Robert Plot die erste formelle Beschreibung eines Dinosaurierfossils an, das in der Nähe von Cornwell bei Oxfordshire (England) entdeckt und heute Megalosaurus zugeschrieben wird. Obwohl er den großen Knochen zuerst einem Elefanten zuordnete, der mit den Römern nach Britannien gekommen war, erkannte Plot später eine scheinbare Ähnlichkeit mit Menschenknochen und schrieb ihn einem Riesen der biblischen Vorsintflut zu. Einer der ersten, der über diese Gruppe urzeitlicher Riesenreptilien wusste und danach forschte, war der englische Arzt Gideon Mantell. Bereits im Jahre 1822 fand er den ersten fossilen Zahn, den er einige Jahre später und nach weiteren Funden Iguanodon nannte. 1824 beschrieb der Amateurpaläontologe William Buckland mit Megalosaurus erstmals einen Dinosaurier in einem wissenschaftlichen Journal. Den Begriff Dinosauria prägte jedoch ein anderer, der englische Anatom Richard Owen. Im Jahr 1842 fasste er Megalosaurus und Iguanodon mit einer weiteren Gattung, Hylaeosaurus, zu einer Gruppe zusammen, die er Dinosauria nannte.
Im Jahr 1858 wurde dann das erste fast vollständige Dinosaurierskelett in Nordamerika entdeckt. Dieser als Hadrosaurus foulkii beschriebene Fund aus Haddonfield (New Jersey) zeigte, dass dieser Dinosaurier wohl zweibeinig lief, und revolutionierte damit das öffentliche Bild der Dinosaurier – zuvor stellte man sich Dinosaurier wie Megalosaurus als riesige, auf vier Beinen laufende, waranähnliche Wesen vor. Diese Entdeckung löste eine wahre Dinomanie (Dinosaurier-Enthusiasmus) in den USA aus.
In den Folgejahren begann eine Feindschaft zwischen zwei berühmten Dinosaurierforschern, Edward Drinker Cope und Othniel Charles Marsh, die in den berühmten „Knochenkriegen“ eskalierte. Vielleicht begann der Streit, als Cope scharfe Kritik seitens Marsh erhielt, als er den Schädel des neu entdeckten, seltsamen Meeresreptils Elasmosaurus am falschen Ende des Körpers platzierte. Dies war der Beginn von Missgunst und Eifersucht zwischen den beiden Forschern, und ihr Streit endete erst 30 Jahre später im Jahr 1897 nach dem Tod Copes. Jeder der beiden Kontrahenten versuchte mit seinem Team, immer mehr Dinosaurierknochen zu finden als der andere – mit allen Mitteln. Sie zerstörten sich gegenseitig viele Knochenfunde, weitere Knochen fielen dem Dynamit zum Opfer, mit dem damals Knochen freigesprengt wurden. Das Resultat der Rivalität waren 142 neu entdeckte Dinosaurierspezies, wozu Marsh 86 Arten und Cope 56 Arten beitrug.Seitdem werden auf der ganzen Welt Dinosaurierfossilien gefunden: So startete das Berliner Museum für Naturkunde eine große Expedition unter der Leitung von Werner Janensch nach Deutsch-Ostafrika, dem heutigen Tansania, die einzigartige Funde wie Brachiosaurus oder Kentrosaurus zutage förderte. Weitere wichtige Entdeckungen wurden unter anderem in Südamerika, Madagaskar, Indien, der Mongolei, China und ebenso in Deutschland gemacht.
Obwohl Dinosaurier anfangs als lebhafte, agile Tiere galten, wurde dieses Bild durch die Entdeckungen von Marsh und Cope verändert; Dinosaurier wurden zunehmend als dumme, langsame und unbeholfene Kreaturen betrachtet. Einen Sauropoden beschrieb Marsh aufgrund seines im Vergleich zur Körpergröße lächerlich klein erscheinenden Kopfes sogar als Morosaurus („dumme Echse“), später hatte sich jedoch Copes Bezeichnung Camarasaurus durchgesetzt. Erst seit den 1970er Jahren näherte sich die wissenschaftliche Meinung wieder dem ursprünglichen Bild von lebhaften, aktiven Tieren an, nachdem John Ostrom Deinonychus beschrieben hatte und die Idee von einer Warmblütigkeit der Dinosaurier aufkam. Diese Entwicklung löste die Dinosaurier-Renaissance aus, eine bemerkenswerte Zunahme von Aktivitäten innerhalb der Dinosaurierforschung, die bis heute andauert. Derzeit stammen die meisten neuen Funde unter anderem aus China und Südamerika, insbesondere Argentinien.
Breites öffentliches Interesse an Dinosauriern erregte erstmals der Crystal Palace Park in London, wo im Jahr 1853 eine Urlandschaft mit verschiedenen, lebensgroßen Modellen ausgestorbener Tiere modelliert wurde, die noch heute zu bestaunen ist. Der Bildhauer Benjamin Waterhouse Hawkins fertigte, unter Beratung von Owen, vier Dinosauriermodelle an. Die Popularität der Dinosaurier im Crystal Palace Park wurde so groß, dass Hawkins für ein ähnliches Projekt im Central Park in New York engagiert wurde. Eine neue Verwaltung des Parks ließ diesen Plan jedoch fallen, und die halbfertigen Modelle wurden zerstört.
Seit Anfang des 19. Jahrhunderts zogen ausgestellte Skelette urzeitlicher Säugetiere eine Vielzahl von Besuchern an. Mit Reiseausstellungen, die durch die USA und Europa zogen, konnte viel Geld eingenommen werden. Daher sollten schon bald nach der Entdeckung vollständiger Dinosaurierskelette diese ausgestellt werden. Im Jahr 1868 wurde Hawkins beauftragt, erstmals Dinosaurier-Skelette (Hadrosaurus und Dryptosaurus) zu montieren und der Öffentlichkeit zu präsentieren. In einem New Yorker Museum gab es einen regelrechten Besucheransturm. In den folgenden Jahrzehnten wurden durch diese neuen Attraktionen in den Museen viele Skelette anderer urtümlicher Tiere in die Museumskeller verbannt, und in Deutschland konnten im Jahr 2006 allenfalls noch Relikte der Ägypter mit der Popularität von Dinosaurierskeletten konkurrieren.
Von Beginn des 20. Jahrhunderts an wurde das Motiv Dinosaurier in Literatur und Film wirtschaftlich immer bedeutender und ertragreicher. Eine der ersten und eine der berühmtesten Fantasy-Geschichten ist Arthur Conan Doyles Roman Die vergessene Welt („The Lost World“, 1912), der ab 1925 vielfach verfilmt wurde. Wie diese handeln viele andere solcher Geschichten von der Entdeckung eines bis dahin isolierten Gebietes, zum Beispiel im Regenwald oder auf einer Insel, wo Dinosaurier bis in unsere Zeit überlebt haben.
In der auch in Deutschland sehr erfolgreichen Zeichentrickserie Familie Feuerstein (1960–1966) gehörten Dinosaurier zum regelmäßigen Inventar. Später entstand eine Reihe von Hollywood-Action-Filmen, in denen die Dinosaurier jedoch, mit Ausnahme einiger, aber nicht aller Darstellungen in Filmen der Jurassic-Park- und Jurassic-World-Reihe, übertrieben und nicht den wissenschaftlichen Erkenntnissen entsprechend gezeigt werden. So wurden Dinosaurier meist ahistorisch als menschenfressende Ungetüme, oft mit Höhlenmenschen zusammenlebend, dargestellt. Im Jahr 1954 zeigte der preisgekrönte tschechoslowakische Fantasy-Film Reise in die Urzeit, in dem Kinder auf einem Fluss immer tiefer in die Vergangenheit reisen, erstmals die Urzeittiere in den richtigen Zeitaltern.
Neueste Dinosaurierfilme sind oft Dokumentarfilme, zum Beispiel die mit großem Aufwand produzierte BBC-Serie Dinosaurier – Im Reich der Giganten (englischer Titel: „Walking with Dinosaurs“), in der versucht wird, Dinosaurier in ihrem Lebensraum darzustellen. Erst durch die moderne und oft aufwändige dreidimensionale Bewegtgrafik (Computer Generated Imagery) ist es glaubhaft möglich, diese Kreaturen lebensecht bildlich darzustellen. Dennoch hat im wissenschaftlichen Bereich die handwerkliche, oftmals kunstfertige und ästhetische Illustration – im englischen Sprachraum wird diese Disziplin manchmal als Paleoart bezeichnet – ihre Bedeutung nicht verloren. Selbst in großen Kinoproduktionen wird heute immer mit Storyboards gearbeitet. Nach dem immensen wirtschaftlichen Erfolg des Films Jurassic Park, der in der Internet Movie Database 2007 als weltweit neunterfolgreichster Film geführt wird, erobert der Darsteller Dinosaurier ebenso die Welt des Computerspiels.
Ein Dino verkörperte die Hauptfigur der 1990–2006 produzierten Hörbuchserie Mit em Batino unterwägs der Zürcher Verkehrserziehung.
Bilder eines (regelmäßig grünhäutig dargestellten) Sauropoden im Stile einer Kinderbuchillustration werden in Leselern-Materialien (Anlauttabellen) als Beispielwort-Bild in der Bedeutung Dinosaurier oder Dino für den Buchstaben D verwendet, z. B. in einer 2014 in bayerischen Grundschulen verwendeten Ausgabe. Dies impliziert, dass heute der Begriff „Dino(saurier)“ verbunden mit dem Ikon des Sauropoden als Bestandteil des rezeptiven Wortschatzes deutschsprachiger Schulanfänger allgemein vorausgesetzt wird.
Im April 2016 wurden Bilder von Köpfen dreier als exemplarisch angesehener Dinosaurier-Taxa zur Aufnahme als Emojis in den Unicode-Standard vorgeschlagen. In dem Vorschlagsdokument werden die Köpfe von Tyrannosaurus rex und Brontosaurus („Bronto“) grünhäutig dargestellt, der von Triceratops bräunlich.
Philip J. Currie, Kevin Padian: Encyclopedia of Dinosaurs. Academic Press, San Diego, Calif. u. a. 1997, ISBN 0-12-226810-5.
James O. Farlow, Michael K. Brett-Surman (Hrsg.): The Complete Dinosaur. Indiana University Press, Bloomington IN u. a. 1997, ISBN 0-253-33349-0.
William J. T. Mitchell: The Last Dinosaur Book. The Life and Times of a Cultural Icon. University of Chicago Press, Chicago IL u. a. 1998, ISBN 0-226-53204-6 (Auszüge aus dem Buch).
Gregory S. Paul: The Scientific American Book of Dinosaurs. St. Martin’s Press, New York NY 2000, ISBN 0-312-26226-4.
Gregory S. Paul: Dinosaurs of the Air. The Evolution and Loss of flight in Dinosaurs and Birds. The Johns Hopkins University Press, Baltimore MD u. a. 2002, ISBN 0-8018-6763-0.
José Luis Sanz: Starring T. rex! dinosaur mythology and popular culture. Indiana University Press, Bloomington IN 2002, ISBN 0-253-34153-1.
David B. Weishampel, Peter Dodson, Halszka Osmólska (Hrsg.): The Dinosauria. 2. Ausgabe. University of California Press, Berkeley CA u. a. 2004, ISBN 0-520-24209-2.
David E. Fastovsky, David B. Weishampel: The Evolution and Extinction of the Dinosaurs. 2. Ausgabe. Cambridge University Press, Cambridge u. a. 2005, ISBN 0-521-81172-4.
Zeitreise ins Erdmittelalter – Die Paläobiologie der Dinosaurier. Von Martin Sander. In: Gerold Wefer (Hrsg.): expedition Erde. 2., überarbeitete und erweiterte Auflage, 2006 (PDF, 1,79 MB)
Dinosaurier-Interesse (Privates Projekt mit einführenden Informationen über Dinosaurier und andere Tiere des Mesozoikums für Erwachsene und Kinder)
Palaeos.com (Umfassendes privates Projekt mit hohem wissenschaftlichem Anspruch, über Dinosaurier und die gesamte Lebewelt der Erdgeschichte)
EnchantedLearning.com (Kommerzielle Internetpräsenz ohne Registrierungszwang mit knappen und sachlichen Informationen, richtet sich an Erwachsene und Kinder)
Fossilworks – Dinosaur facts and figures (Kuriose Statistiken der wissenschaftlichen Datenbank “The Paleobiology Database”)Bilder
Skeletal Drawing (Bilder zur Anatomie der Dinosaurier erstellt von dem Paläontologen Scott Hartman)

Der Diolkos (griechisch Δίολκος, von dia διά ‚hindurch‘ und holkos ὁλκός ‚Zug‘) war ein antiker griechischer Schiffkarrenweg über den Isthmus von Korinth, auf dem Schiffe vom Korinthischen zum Saronischen Golf transportiert wurden. Die Abkürzung über die Landenge erlaubte es, die gefährliche Umschiffung der Peloponnes zu vermeiden.
Die Hauptaufgabe des Ziehwegs galt dem Gütertransfer, während er in Kriegszeiten bevorzugt auch zur Beschleunigung militärischer Operationen genutzt wurde. Der 6 bis 8,5 km lange gepflasterte Rillenweg funktionierte nach dem Eisenbahnprinzip und war von ca. 600 v. Chr. bis zur Mitte des 1. Jahrhunderts n. Chr. in Betrieb. Er stellte eine in diesem Maßstab in der Antike einzigartige Kombination der Spurrillenführung und des Überlandtransports von Seefahrzeugen dar.
die Auslegung der antiken Quellen, insbesondere im Hinblick auf das Alter und die Gründe für die Inbetriebnahme des Ziehweges;
die Bewertung seiner militärischen und ökonomischen Funktion in der Antike.Auch technische Überlegungen zur Durchführbarkeit des Überlandtransports der Schiffe standen wiederholt im Mittelpunkt des Interesses, wobei auch theoretische Kalkulationen zur Berechnung der zu überwindenden physikalischen Kräfte versucht wurden.Bereits der Chefingenieur des Kanals von Korinth Béla Gerster führte vor dem Durchstich eine genaue Untersuchung der Topographie des Isthmus durch, fand aber den Diolkos nicht. Überreste des Schiffkarrenwegs wurden wahrscheinlich zuerst vom deutschen Archäologen Habbo Gerhard Lolling in der Baedeker-Ausgabe von 1883 identifiziert. 1913 berichtete J.G. Frazer in seinem Kommentar zu Pausanias über Spuren eines antiken Pflasterwegs über den Isthmus, während Teile des westlichen Kais 1932 durch Harold North Fowler entdeckt wurden.Systematische Ausgrabungen wurden schließlich von 1956 bis 1962 vom griechischen Archäologen Nikolaos Verdelis vorgenommen, der ein mehr oder weniger durchgehendes Stück von 800 m Länge freilegte und insgesamt 1100 m ausfindig machte. Zwar dient Verdelis’ Ausgrabungsbericht nach wie vor als Grundlage für moderne Interpretationen, aber die volle Publikation seiner Forschungsergebnisse wurde durch sein frühzeitiges Ableben verhindert, so dass viele Fragen hinsichtlich der genauen Natur des Diolkos offenblieben. Zusätzliche Forschungen vor Ort in Ergänzung zu Verdelis’ Arbeit wurden später von Georges Raepsaet und Walter Werner veröffentlicht.Heutzutage sind insbesondere im westlichen Abschnitt wesentliche Teile des Diolkos durch Erosion gefährdet, die vom Schiffsbetrieb im nahen Kanal herrührt. Kritiker, die dem griechischen Kulturministerium fortgesetzte Untätigkeit vorwerfen, haben eine Petition zur Bewahrung und Restaurierung der registrierten archäologischen Stätte eingebracht.
Der Diolkos erlaubte es Schiffen auf der Fahrt vom Ionischen Meer zur Ägäis, die gefährliche Umschiffung der Peloponnes mit ihren weit ins Meer ragenden Landspitzen zu vermeiden. Besonders Kap Malea war für starke Stürme gefürchtet, wohingegen der Golf von Korinth und der Saronische Meerbusen relativ ruhige Gewässer darstellten. Zudem ließ sich durch Verwendung des Überlandwegs die Schifffahrtsroute insbesondere von und nach Athen erheblich abkürzen.
Die antiken Quellen schweigen sich über das Alter des Diolkos aus. Für Thukydides (460–395 v. Chr.) schien der Diolkos bereits etwas Altes zu sein. Der archäologische Befund deutet auf einen Bau am Ende des 7. oder Anfang des 6. Jahrhunderts v. Chr. hin, als Periander Tyrann von Korinth war. Der Diolkos blieb Berichten zufolge bis zur Mitte des 1. Jahrhunderts regelmäßig in Betrieb; danach brechen die schriftlichen Überlieferungen ab. Möglicherweise wurde er durch Neros fehlgeschlagenes Kanalprojekt 67 n. Chr. dauerhaft unbrauchbar gemacht. Dafür spräche, dass die antiken Aushubarbeiten, die nachweislich eine Breite von 40–50 m und eine Tiefe von bis zu 30 m erreichten, entlang genau derselben Strecke wie der spätere Kanal von Korinth liefen, der heutzutage den antiken Pflasterweg durchschneidet. Deutlich später bezeugte Kriegsschifftransporte 868 und um 1150 wurden aufgrund der großen Zeitspanne, in der keine Nachrichten über die Nutzung des Schiffkarrenwegs erhalten sind, vermutlich nicht mehr über den Diolkos abgewickelt.
Der Diolkos spielte eine bedeutende Rolle im antiken Seekrieg. Griechische Historiker berichten von mehreren Marineoperationen zwischen dem 5. und 1. Jahrhundert v. Chr., bei denen Kriegsschiffe zur Zeitersparnis über den Diolkos gezogen wurden. Während des Peloponnesischen Kriegs planten die Spartaner, Kriegsschiffe zur Bedrohung Athens zum Saronischen Golf zu ziehen (428 v. Chr.), während sie später ein Geschwader hinüberschafften, das eilig zu Operationen in Chios weitersegelte (411 v. Chr.). 220 v. Chr. ließ Demetrios von Pharos seine Flotte von ungefähr 50 Schiffen über den Isthmus zur Bucht von Korinth ziehen. Drei Jahre später wurden 38 makedonische Kriegsschiffe von Philipp V. hinübergesandt, während die größeren Schiffe seiner Flotte um Kap Malea segelten. Nach seinem Sieg bei Actium (31 v. Chr.) rückte Octavian so schnell wie möglich gegen Marcus Antonius vor, indem er einen Teil seiner 260 Liburnen über den Isthmus schleppen ließ. 868 n. Chr. ließ der byzantinische General Niketas Oryphas seine gesamte Flotte von 100 Dromonen in einer zügig ausgeführten Operation über den Isthmus ziehen, nahm dabei aber aufgrund des wahrscheinlich verfallenen Zustands des Diolkos wohl einen anderen Weg.
Trotz der häufigen Erwähnung des Diolkos im Zusammenhang mit militärischen Operationen geht die moderne Forschung davon aus, dass der Hauptzweck des Schiffkarrenwegs der Gütertransfer war, da anzunehmen ist, dass Kriegsschiffe nicht sehr häufig transportiert zu werden brauchten, und weil die antike Geschichtsschreibung sich immer stärker für Krieg als für Handel interessierte. Bemerkungen von Plinius dem Älteren und Strabon, die in Friedenszeiten den Diolkos als regelmäßig in Betrieb beschrieben, deuten ebenfalls auf eine Handelsfunktion hin. Da der Bau des Diolkos zeitlich mit der Herausbildung der griechischen Monumentalarchitektur zusammenfällt, könnte der Überlandweg anfangs besonders für den West-Ost-Transport von Schwergütern wie Marmor, Monolithen und Bauholz gedient haben. Zwar ist nicht die Höhe der Zölle bekannt, die Korinth für den auf seinem Gebiet liegenden Diolkos erheben konnte, aber die Tatsache, dass der Fahrweg noch lange nach seiner Errichtung benutzt und unterhalten wurde, deutet darauf hin, dass er für antike Handelsschiffe lange Zeit eine attraktive Alternative zur Fahrt um Kap Malea geblieben ist.
Der Diolkos läuft über den engsten Teil der Landenge, wo er dem Gelände zur Vermeidung größerer Steigungen in einem gewundenen Kurs folgt. Der Fahrweg überquert mit einer durchschnittlichen Steigung von 1:70 den Isthmus auf einer Höhe von ca. 79 m, wobei der steilste Abschnitt bis zu 6 % Gefälle aufweist. Die Gesamtlänge des Diolkos wird abhängig von der Anzahl der angenommenen Wegbiegungen auf 6–7 km, 8 km oder 8,5 km geschätzt. Insgesamt 1,1 km davon konnten hauptsächlich am Westende archäologisch dokumentiert werden. Dort beginnt das bekannte Teilstück an einer Anlegestelle südlich des Kanals, läuft für einige hundert Meter parallel zum Wasserweg und wechselt dann auf die Nordseite, wo es in einem leichten Bogen ungefähr genauso lang am Kanal weiterläuft. In seinem weiteren Verlauf folgte der Diolkos entweder in gerader Linie dem Kurs des modernen Kanals oder schwang im weiten Bogen nach Süden. Der Karrenweg endet am Saronischen Golf am Dorf Schoinos (heute: Kalamaki), das von Strabo als östlicher Endpunkt beschrieben wurde. Einige Abschnitte des Diolkos wurden durch den Kanal aus dem 19. Jahrhundert oder andere moderne Bauten zerstört.
Der Diolkos war ein Pflasterweg aus hartem Kalkstein mit parallel laufenden Rillen im Abstand von ungefähr 160 cm. Der Weg war zwischen 3,4 und 6 m breit. Da sich aus antiken Quellen nur wenig darüber entnehmen lässt, wie die Schiffe hinüberbefördert wurden, muss die Art und Weise des Schifftransports größtenteils aus dem archäologischen Befund rekonstruiert werden. Die Rillen zeigen an, dass der Transport auf dem Diolkos mit einer Art Radfahrzeug abgewickelt wurde. Entweder wurden Schiff und Fracht auf verschiedenen Karren hinübergezogen, oder nur die Fracht wurde hinübergebracht, um auf der anderen Seite auf ein anderes Schiff verladen zu werden. Man nimmt an, dass es sich normalerweise eher um größere Boote als Schiffe gehandelt hat, obgleich eine technische Analyse ergeben hat, dass der Transport von Trieren (25 t, 35 m Länge, 5 m Breite) technisch machbar, aber schwierig war. Um der Gefahr vorzubeugen, dass der Kiel während des Transports in der Mitte durchbrach, müssen dicke Taue, so genannte hypozomata benutzt worden sein, die zur Stabilisierung des Schiffsrumpfs von Bug zu Heck gespannt wurden. Schiff und Fracht wurden vermutlich durch Menschen und Tiere mithilfe von Seilen, Flaschenzügen und möglicherweise auch Ankerwinden gezogen.
Der Wissenschaftler Tolley unternimmt den Versuch, die Mannschaftsstärken zu ermitteln, die damals für den Transport einer Triere über den Hügelrücken der Landenge erforderlich waren. Unter der Annahme, dass eine mit Wasser vollgesaugte Triere einschließlich fahrbarem Untersatz 38 t wog und ein Mensch über einen längeren Zeitraum eine Zugleistung von 300 N ausüben kann, sei – abhängig von der zu überwindenden Steigung und dem Zustand des Rillenwegs – eine Zugtruppe von 112 bis 142 Männern vonnöten gewesen (33 550 bis 42.500 N). Beim Anfahren wären es sogar knapp 180 Leute gewesen. Bei einer Ziehgeschwindigkeit von 2 km/h bei einer geschätzten Streckenlänge von 6 km wäre der Transfer über den Isthmus so in drei Stunden zu bewältigen gewesen. Raepsaet errechnet dagegen unter Zugrundelegung geringerer Transportlasten und Reibungswerte eine max. Zugkraft von 27.000 N, was deutlich kleinere Arbeitsgruppen erfordert hätte. Auch der Einsatz von Ochsgespannen, die Tolley in seinem Rechenmodell unter Verweis auf deren relativ abnehmende Arbeitsleistung ablehnt, wäre unter diesen Bedingungen sinnvoll gewesen. In beiden Kalkulationen muss der menschliche Kraftaufwand am Ziehweg in jedem Fall als beträchtlich eingestuft werden.
Nach Auffassung des britischen Technikhistorikers M.J.T. Lewis stellt der Diolkos eine „Eisenbahn“ (engl. railway) im Sinn eines präparierten Wegs dar, der ein Fahrzeug auf ihm so lenkt, dass es nicht die Spur verlassen kann. Zwischen 6 und 8,5 km lang, mindestens 650 Jahre lang regelmäßig in Betrieb, und für alle gegen Bezahlung offen, stellte der Diolkos sogar eine öffentliche Eisenbahn dar, eine Idee, die erst im 19. Jahrhundert wieder auftauchte. Auch in seiner Spurweite von 160 cm war der Diolkos modernen Eisenbahnen ähnlich.
Andererseits scheint eine eingehende archäologische Betrachtung des Rillenwegs ein differenziertes Bild zu vermitteln. So variiert die Erscheinungsform der Spurrillen im Verlauf der beiden Hauptabschnitte zwischen einem „V“-Profil und einem rechteckigen Querschnitt. Während Einigkeit darüber herrscht, dass die rechteckigen Spurrillen im östlichen Teil absichtlich zur Lenkung von Karrenrädern in die Pflastersteine geschlagen wurden, werden die V-förmigen im westlichen Bereich von einigen Autoren als das Ergebnis von Abnutzung interpretiert oder sind gar nicht zu erkennen. Allerdings lässt auch hier die deutliche Wölbung des Pflasterwegs auf absichtlich angelegte Rillen schließen. Zusätzlich zu den beiden parallelen Hauptrillen befinden sich in diesem Abschnitt an den Fahrbahnseiten streckenweise auch deutlich flachere Rillen. Diese werden als Abnutzungsspuren der Hinterräder des Transportuntersatzes interpretiert, die bei der Kurveneinfahrt aufgrund ihrer starren Hinterachse spurabweichend einen kleineren Radius beschrieben. Generell können die unterschiedlichen Rillenformen im West- und Ostteil durch die lange Betriebsdauer des Diolkos erklärt werden, in deren Verlauf sein Erscheinungsbild durch Ausbesserungen und andere bauliche Veränderungen deutlich verändert worden sein muss.Interpretationsschwierigkeiten bereitet die sogenannte Rampe, die in einem Steigungsbereich zwischen den beiden Streckenabschnitten liegt. Hierbei handelt es sich um zwei parallele, im Abstand von ca. 60 cm laufende Reihen von Steinblöcken mitten auf der Fahrbahn, mit einer Länge von ungefähr 15 m, aber ursprünglich wohl doppelt so lang, die von Raepsaet und Werner als Führungssteine für den Transport angesehen werden. Die weitergehende Interpretation der Konstruktion durch Raepsaet als Umladeplatz für zwei unterschiedliche Typen von fahrbaren Untersätzen wird allerdings von Lewis als unpraktikabel abgelehnt, da dafür eine erneute Verladung von Schiff bzw. Fracht bereits 600 m hinter der Schiffsanlegestelle notwendig gewesen wäre.Funktional kann der Diolkos-Schiffkarrenweg als eine Kombination zweier in der Antike durchaus verbreiteter Techniken betrachtet werden, nämlich der Spurlenkung nach dem Eisenbahnprinzip und des Schifftransports über Land. Über letzteres wissen antike Autoren vor allem im Zusammenhang mit militärischen Operationen zu berichten. So erachtete bereits Herodot den Bau des Athoskanals als überflüssig, da der persische König Xerxes I. seine Invasionsflotte „ohne irgendwelche Mühe auch über die Landenge hätte ziehen können“. Auf Sizilien ließ der Tyrann Dionysius I einmal ein 80 Trieren starkes Geschwader 20 Stadien (ca. 3,5 km) über Land transportieren, während Alexander der Große während seines Asienfeldzugs seine Schiffe sogar vom Mittelmeer zum Euphrat transferierte – allerdings im demontierten Zustand. Das bekannteste Beispiel dürfte das Ausweichmanöver von Hannibal sein, der 212 v. Chr. seine im Tarentiner Hafen von den Römern blockierte Flotte durch die Straßen der Stadt bis zum offenen Meer schleifen ließ. Zu den bei solchen Überlandtransporten als gesichert geltenden Maßnahmen gehörten die Verwendung von Hebemaschinen, um die Schiffe außer Wasser zu hieven, das vorherige Präparieren des Ziehwegs und der Einsatz von Rindern neben Menschen als Zugkräfte.Das Prinzip der Spurrillenführung fand hingegen im kleineren Maßstab im zivilen Bereich Anwendung. So wurden in den Theatern von Megalopolis (3. Jahrhundert v. Chr.), Sparta (ca. 30 v. Chr.) und Eretria kurze, maximal 34 m lange Doppelrillen quer über die Bühne entdeckt, die entweder zur Verschiebung des Bühnenbildes dienten oder eine fahrbare Plattform trugen, auf der Darsteller des theatralischen Effekts wegen von unsichtbarer Hand in den Saal gezogen wurden. Auf der Römerstraße über den Kleinen Sankt Bernhard lassen sich ebenfalls kurze, aus dem Fels geschlagene Spurrillen ausmachen, die – der Funktion einer modernen Leitplanke vergleichbar – das Abkommen der Karren vom Passweg verhindern sollten. Auch im antiken Bergbau machte man sich das Prinzip bereits zunutze: Ein 150 m langer Stollen in der römischen Goldmine in Três Minas in Portugal (1. Jh. n. Chr.) weist eine Rillentrasse mit einer Spurbreite von 1,20 m und periodischen Tunnelverbreiterungen vermutlich zum Durchlassen des Gegenverkehrs auf. In dem Maßstab, in dem die Verbindung beider Elemente bei Bau und Betrieb des Diolkos umgesetzt wurde, war der Schiffkarrenweg in der Antike allerdings wohl einzigartig.
Folgende antike und mittelalterliche Quellen erwähnen Schiffstransporte über den Isthmus. Da der Diolkos dabei nur selten direkt beim Namen genannt wird, kommt es für das Verständnis der Transferaktivitäten auf eine Analyse verwandter Ausdrücke und Umschreibungen an. In chronologischer Reihenfolge:
Abgesehen vom Diolkos bei Korinth existieren spärliche schriftliche Hinweise auf zwei weitere Schiffkarrenwege im römischen Ägypten, die beide ebenfalls unter der Bezeichnung „Diolkos“ firmierten. Der Arzt Oreibasios (c. 325–403 n. Chr.) überliefert zwei Textstellen aus dem 1. Jh. n. Chr., in denen sein Kollege Xenokrates von Aphrodisias beiläufig einen Diolkos nahe dem Hafen von Alexandria erwähnte, der an der Südspitze der Insel Pharos gelegen haben könnte. Ptolemäus berichtet in seinem Buch über Geographie kurz von einem anderen Diolkos, der eine versandete Mündung des Nils mit dem Mittelmeer verband. Weder Xenokrates noch Ptolemäus liefern nähere Angaben zu den jeweiligen Karrenwegen.
R. M. Cook: Archaic Greek Trade. Three Conjectures 1. The Diolkos. In: Journal of Hellenic Studies. Bd. 99, 1979, S. 152–155.
Jan Willem Drijvers: Strabo VIII 2,1 (C335). Porthmeia and the Diolkos. In: Mnemosyne. Bd. 45, 1992, S. 75–76.
P. M. Fraser: The ΔΙΟΛΚΟΣ of Alexandria. In: The Journal of Egyptian Archaeology. Bd. 47 (1961), S. 134–138.
Bela Gerster: L'Isthme de Corinthe: tentatives de percement dans l'antiquité. In: Bulletin de correspondance hellénique. Bd. 8, Nr. 1, 1884, S. 225–232 (229f.)
M. J. T. Lewis: Railways in the Greek and Roman world. In: A. Guy, J. Rees, (Hrsg.): Early Railways. A Selection of Papers from the First International Early Railways Conference. 2001, S. 8–19 (10–15) (PDF online)
Georges Raepsaet, Mike Tolley: Le Diolkos de l’Isthme à Corinthe. Son tracé, son fonctionnement. In: Bulletin de correspondance hellénique. Bd. 117, 1993, S. 233–261.
Walter Werner: Der Kanal von Korinth und seine Vorläufer. In: Das Logbuch. Sonderheft. Hamburg 1993.
Walter Werner: Der Diolkos. Die Schiffsschleppbahn am Isthmus von Korinth. In: Nürnberger Blätter zur Archäologie. Bd. 10 (1995), S. 103–118.
Walter Werner: The largest ship trackway in ancient times: the Diolkos of the Isthmus of Corinth, Greece, and early attempts to build a canal. In: International Journal of Nautical Archaeology. Bd. 26, Nr. 2, 1997, S. 98–119.

Diotima (altgriechisch Διοτίμα Diotíma, Betonung in heutigem Deutsch meist: Diótima) ist eine Figur in Platons Dialog Symposion, in dem die Gesprächsteilnehmer die Natur des Eros erörtern. Sie wird dort als weise Frau aus Mantineia in Arkadien vorgestellt. In dem Dialog tritt Diotima nicht selbst unter den Beteiligten auf, sondern der Philosoph Sokrates erzählt, wie er von ihr über den Eros belehrt wurde. In dem Gespräch legte ihm Diotima ihre Lehre von der rechten philosophischen Lenkung des erotischen Drangs dar. Das Eros-Konzept, das Platon ihr in den Mund legt, wird seit der Renaissance als „platonische Liebe“ bezeichnet.
Unbekannt ist, ob die Gestalt frei erfunden ist oder ein historisches Vorbild aus dem 5. Jahrhundert v. Chr. hat, das möglicherweise tatsächlich diesen Namen trug. Wegen der starken Nachwirkung des Dialogs bis in die Gegenwart ist der Name Diotima in der Neuzeit immer wieder aufgegriffen und als Pseudonym, als ehrender Alternativname oder zur Benennung einer literarischen Figur verwendet worden. Er steht traditionell für eine Frau, die in der Lage ist, auf erotischem Gebiet ein philosophisch untermauertes Wissen zu vermitteln.
Diotima ist die einzige weibliche Figur, die in einem platonischen Dialog zu Wort kommt. Sie tritt aber nicht direkt auf, denn an dem Symposion (Trinkgelage, Gastmahl), dessen Verlauf der Dialog schildert, nimmt sie nicht teil. Vielmehr berichtet Sokrates, die Hauptfigur des Dialogs, von einem Gespräch, in dem ihn Diotima über den Eros belehrt und von der Richtigkeit ihrer Sichtweise überzeugt hatte. Er hatte sie eigens zu dem Zweck, solche Aufklärung zu empfangen, aufgesucht, als sie sich zeitweilig in Athen aufhielt. Im Symposion rühmt er ihre vollendete Weisheit. Er gibt ihre Äußerungen in direkter Rede wieder und identifiziert sich mit deren Inhalt, statt eine eigene Theorie vorzutragen. Die Diotima-Rede bildet den philosophischen Höhepunkt des Gastmahls.
Ansonsten erfährt der Leser des Symposions über Diotima nur, dass sie eine Seherin war, deren Weisheit ihr außergewöhnliche Fähigkeiten verlieh; Sokrates berichtet, sie sei in der Lage gewesen, durch ein Opfer den Ausbruch der Pest in Athen um zehn Jahre zu verzögern. Daraus ist ersichtlich, dass sie als Priesterin fungierte. Gemeint ist die Pest, die in Athen im Jahr 430 v. Chr. ausbrach („attische Seuche“). Der Frauenname Diotima (Bedeutung: „die von Zeus Geehrte“ oder „die Zeus Ehrende“) war selten; häufig war hingegen die männliche Form Diotimos.In seiner Wiedergabe des Gesprächs mit Diotima (Symposion 201d–212c) schildert Sokrates zuerst die Wesensart des Eros, dann dessen Wirken. Dabei tritt er ihr gegenüber als Schüler auf. Indem sie Fragen stellt, die ihm zu Erkenntnissen verhelfen sollen, übernimmt sie die maieutische Rolle, die er sonst selbst in Platons Dialogen gegenüber seinen Gesprächspartnern spielt. Wo er bekennen muss, keine Antwort zu wissen, enthüllt sie ihm die Wahrheit.
Mit „Eros“ ist die mythische Gestalt gemeint, die als Urheber des erotischen Begehrens der Menschen betrachtet wurde. Damit ist stets die Vorstellung von Leidenschaft verbunden. Sokrates geht anfänglich von der Annahme aus, Eros sei ein großer Gott und müsse schön sein. Diotima widerlegt diese Meinung. Sie zeigt, dass Eros weder gut und schön noch schlecht und hässlich ist, sondern in einem Mittelbereich zu verorten ist. Wegen dieser Unvollkommenheit kann er kein Gott sein. Zu den Sterblichen zählt er aber auch nicht. Da er zwischen Gottheit und Mensch steht, ist er ein Daimon („Dämon“, aber nicht im heute gängigen, meist abwertenden Sinn dieses Begriffs). Damit fällt ihm – wie allen Dämonen – eine Mittlerrolle zwischen Göttern und Menschen zu. Diese Aufgabe erfüllt er in seinem Zuständigkeitsbereich, auf dem Gebiet des Erotischen. Er übermittelt den Menschen das, was ihnen diesbezüglich von den Göttern zukommen soll.
In Diotimas Mythos ist Eros nicht – wie in einer verbreiteten Überlieferung – der Sohn der Göttin Aphrodite, sondern er wurde bei dem Festmahl, das die Götter anlässlich von Aphrodites Geburt hielten, gezeugt. Seine Mutter Penia, die personifizierte Armut, kam als Bettlerin zu dem Mahl und traf dort den betrunkenen Poros („Wegfinder“). Poros ist die Personifikation der Findigkeit, die stets einen Ausweg findet und den Weg zu Fülle und Reichtum bahnt. Ihm fehlt aber, wie seine Betrunkenheit andeutet, die Fähigkeit des Maßhaltens. Um ihre Bedürftigkeit auszugleichen, wollte Penia von ihm ein Kind empfangen. So kam es zur Zeugung des Eros, der sich später der Göttin, deren Geburtsfest zur Begegnung seiner Eltern geführt hatte, anschloss und ihr Begleiter wurde. In seinem Naturell verbindet Eros die Eigenschaften seines Vaters mit denen seiner Mutter. Von der Mutter hat er das Prinzip des Mangels geerbt, daher ist er arm und unansehnlich, barfuß und obdachlos. Vom Vater hat er seine Tatkraft und Schlauheit, seine Zauberkunst und die starke Neigung zum Schönen und Guten, die ihn antreibt. Da die Weisheit zum Schönen zählt, ist er auch ein Philosoph („Weisheitsliebender“). Ihm fehlt Einsicht, doch strebt er eifrig danach, da er sich dieses Mangels bewusst ist.
Wie Eros trachten auch die von ihm ergriffenen Menschen nach dem Schönen und Guten und wollen es für sich erlangen. Sie möchten es dauerhaft besitzen, um glücklich zu sein.
Der Mensch verfügt über Zeugungskraft oder Fruchtbarkeit sowohl im körperlichen als auch im seelischen Sinne. Diese Fähigkeit des Hervorbringens ist ebenso wie die Schönheit von göttlicher Art, daher kann sie sich dort entfalten, wo sie auf Schönes trifft; mit Hässlichem harmoniert sie nicht, daher wird sie von ihm nicht aktiviert. Aus diesem Grund richtet sich das erotische Begehren auf das Schöne. Dabei wird aber das Schöne nicht als solches erstrebt. Der erotische Drang ist nicht Liebe zum Schönen, sondern ein Drang zum Zeugen und Hervorbringen im Schönen. Das Sterbliche strebt nämlich nach Unsterblichkeit. Mittels der Fortpflanzung können Sterbliche etwas von sich hinterlassen und so eine Dauerhaftigkeit erreichen, mit der sie gewissermaßen am Unsterblichen teilhaben. Analog dazu ist auch das Hervorbringen dauerhafter geistiger Werte, etwa in der Dichtung oder der Gesetzgebung, eine Art von Zeugung, die „unsterblichen“ Ruhm verschafft.Eine besondere Stärke erreicht die erotische Anziehungskraft, wenn die begehrte Person nicht nur körperlich schön ist, sondern auch seelisch, also tugendhaft. Hiervon ausgehend entwickelt Diotima ihre Lehre von der rechten philosophischen Lenkung des erotischen Drangs. In der Jugend soll man sich schönen Körpern zuwenden und dabei erkennen, dass es nicht um die Vorzüge eines bestimmten Körpers geht, sondern um die körperliche Schönheit an sich, die in allen schönen Körpern dieselbe ist. Später wird man sich der seelischen Schönheit zuwenden, die man zunächst in einer bestimmten Person wahrnimmt. Daher richtet sich nun die Liebe auf diese Person, auch wenn sie äußerlich unansehnlich ist. Das führt zu einer Ausrichtung auf die Ethik; der Liebende entdeckt das Schöne in schönen Handlungen. Später wird auch die Schönheit von Erkenntnissen für ihn wahrnehmbar. Dabei erhält er Gelegenheit zu entdecken, dass auch im geistig-seelischen Bereich die Schönheit nicht an etwas Einzelnes gebunden ist, sondern das Allgemeine ist, das sich jeweils im Besonderen zeigt. Von da aus gelangt der Liebende zur höchsten Erkenntnisstufe. Dort kommt es nicht mehr auf einzelne Tugenden oder auf einzelne schöne Taten oder Einsichten an, sondern auf Schönheit im allgemeinsten und umfassendsten Sinne: die vollkommene und unwandelbare Schönheit schlechthin, die allen Erscheinungsformen des Schönen letztlich als deren Quelle zugrunde liegt. Dieses Urschöne ist keine bloße Abstraktion, kein gedankliches Konstrukt, sondern für den, der die letzte Stufe erreicht hat, eine wahrnehmbare Wirklichkeit.
Sokrates stimmt Diotimas Ausführungen zu und ergänzt, dass Eros auf dem philosophischen Erkenntnisweg der beste Helfer des Menschen sei. Daher solle man ihn und die Erotik ehren und sich auf diesem Gebiet üben.
Platon stellt in seinen Dialogen Sokrates als Philosophen dar, der Erkenntnisse erlangt hat und anderen im Gespräch zu Einsichten verhilft, aber nicht mit dem Anspruch auftritt, über Wissen im Sinne eines abgeschlossenen, lückenlos begründbaren Lehrsystems zu verfügen. Daher lässt er ihn im Symposion nicht wie andere Gesprächsteilnehmer eine eigene Theorie des Eros in einer Rede vortragen, sondern weist ihm die Rolle des Berichterstatters zu, der nur fremde Weisheit darlegt. Aus diesem Grund benötigt Platon die Gestalt der Diotima, der er hier sein Konzept in den Mund legt. Als weise Seherin verfügt Diotima über eine Einsicht, die der philosophische Diskurs allein nicht vermitteln kann. Sie argumentiert zwar streckenweise philosophisch, aber hinsichtlich des Kerns ihrer Lehre beruft sie sich auf eine transzendente Erfahrung, die nach ihrer Darstellung den Höhepunkt und Abschluss eines philosophischen Schulungswegs darstellt.
Hiervon ausgehend haben manche Forscher Diotima zu einer frei erfundenen Gestalt erklärt, während andere einen Zusammenhang mit einer realen Person annehmen oder zumindest nicht ausschließen. Alle Angaben späterer antiker Autoren fußen auf denen Platons, die sie zum Teil mit erfundenen Ergänzungen ausschmücken. Falls es sich um eine fiktive Gestalt handelt, könnte ihre angebliche Herkunft aus Mantineia eine Anspielung auf ihre Funktion als Seherin (mántis) sein. Einer Hypothese zufolge gestaltete Platon die Figur der Diotima als Gegenbild zu derjenigen der Aspasia, die in seinem Dialog Menexenos als Rhetoriklehrerin eine Rolle spielt und nach der ein nur fragmentarisch erhaltener Dialog des Sokratikers Aischines benannt ist. Dabei sei es ihm darum gegangen, das Konzept von Aischines’ Aspasia zu überwinden und ihm ein überlegenes entgegenzustellen.Oft ist die Frage erörtert worden, warum Platon gerade bei diesem Thema ausnahmsweise die Darlegung seiner eigenen Auffassung einer Frau überträgt. Dabei haben auch mancherlei Spekulationen über seine eigene sexuelle Orientierung eine Rolle gespielt. Es ist sogar vermutet worden, Diotima vertrete ein sophistisches, von Platon oder zumindest seinem Sokrates abgelehntes Konzept. Dieser Ansatz gilt heute als verfehlt, was aber nicht bedeutet, dass Diotimas Lehre mit Platons eigener Überzeugung gänzlich identisch sein muss. Eine weitere Hypothese lautet, Platon habe eine „Selbstdemontage des weiblichen Prinzips“ beabsichtigt. Es sei Diotimas Aufgabe, den Eros aus der ihm traditionell zuerkannten göttlichen Position zu verdrängen. Dies bedeute eine Abweisung des Weiblich-Göttlichen, das endgültig vom Männlich-Göttlichen überwunden werde. Das Ziel sei eine vollständige „Entmachtung der Prinzipien der Aphrodite“. Im Sinne dieser Absicht sei es zweckmäßig, dass gerade bei diesem Machtwechsel „eine Frau das Kommando führt“.
Eine Darstellung Diotimas in der antiken bildenden Kunst konnte bisher nicht mit völliger Sicherheit ermittelt werden. Sehr wahrscheinlich handelt es sich bei der Frau, die auf einem Wandbild der frühen römischen Kaiserzeit aus Boscoreale neben dem sitzenden Sokrates steht, um Diotima. Das Wandbild, das nach einem Vorbild aus dem späten 4. Jahrhundert v. Chr. gestaltet wurde, befindet sich heute im J. Paul Getty Museum in Malibu.Zwei weitere Identifizierungsvorschläge sind nach heutigem Forschungsstand in Betracht zu ziehen: eine stehende Frau auf einem Weihrelief des 3. Jahrhunderts v. Chr. (mit Epimenides; heute in Rom, Konservatorenpalast) und eine stehende Frau, die eine Leber als Zeichen der Wahrsagung hält, auf einem 1887 in Mantineia gefunden Relief (jetzt im Archäologischen Nationalmuseum in Athen). Das Relief aus Mantineia stammt aus dem letzten Drittel des 5. Jahrhunderts v. Chr.; es ist also, falls es tatsächlich Diotima darstellt und diese eine historische Person ist, zeitgenössisch.
Früher wurde auch eine sitzende Frau mit Eros und einem Mann – mutmaßlich Sokrates – auf einem Bronzerelief aus Pompeji (heute in Neapel) sowie auf zwei versilberten Toneimern aus der Gegend von Orvieto für Diotima gehalten. Diese Identifizierung hat sich jedoch als falsch erwiesen.In der römischen Kaiserzeit fand die Gestalt der Diotima wenig Beachtung. Ihre im Symposion vorgetragene Lehre beschäftigte aber Plutarch und die Neuplatoniker. Plutarch und der Neuplatoniker Plotin setzten sich mit der Frage auseinander, wie der Eros-Mythos zu deuten sei. Im 5. Jahrhundert führte der Neuplatoniker Proklos in seinem Kommentar zu Platons Dialog Politeia Diotima unter den Pythagoreerinnen an, deren Tugend bekannt war. Er soll auch Diotimas Ausführungen im Symposion kommentiert haben. Im Mittelalter war Diotima weitgehend unbekannt, da Platons Schrift im Westen verschollen war.
Als in der Renaissance das Symposion der westlichen Gelehrtenwelt wieder im griechischen Originaltext zugänglich wurde, erweckte Diotima das Interesse der Humanisten. Der berühmte Gelehrte Marsilio Ficino, ein eifriger Erforscher des antiken Platonismus, übersetzte den Dialog ins Lateinische und machte ihn damit einem breiteren Lesepublikum zugänglich. Außerdem schrieb er dazu 1468/1469 einen lateinischen Kommentar (Commentarium in convivium Platonis de amore, kurz De amore „Über die Liebe“), der 1484 gedruckt wurde. Diesen Kommentar, von dem er auch eine italienische Fassung anfertigte, gestaltete er als Dialog mit sieben zeitgenössischen Teilnehmern, welche die Reden im Symposion erläutern. Die Rede des Sokrates deutet Tommaso Benci, der Diotima als göttlich inspirierte Seherin einführt; Sokrates habe zeigen wollen, dass die Menschen nur dank göttlicher Eingebung verstehen könnten, was wahre Schönheit und rechte Liebe sei.In der Folgezeit wurde Diotima für die Gebildeten das Muster einer Frau, die in der Philosophie mit eigenen Gedanken hervortritt. Im späten 16. Jahrhundert verfasste Francesco Patrizi da Cherso das vier Dialoge umfassende Werk L’amorosa filosofia, wobei er den Aufbau des Symposions nachahmte. Es ist wie bei Platon ein Bericht über ein Gastmahl, an dem von einem Gespräch mit einer Frau erzählt wurde, die Belehrung über Liebesangelegenheiten erteilte. Diese gebildete Dame, die als „neue Diotima“ bezeichnet wird – es handelt sich um die mit Patrizi befreundete Dichterin Tarquinia Molza – trug allerdings unplatonisches Gedankengut vor. Sie führte alle Formen der Liebe auf Selbstliebe zurück.
1775/1780 fertigte der französische Maler Jacques-Louis David eine Zeichnung an, die Sokrates und Diotima darstellt. Sie befindet sich heute in der National Gallery of Art in Washington, D. C.Diotima wurde literarisch als Deckname für zeitgenössische Damen verwendet, denen eine an das antike Vorbild erinnernde Rolle zugeschrieben wurde. So nannte Frans Hemsterhuis die gebildete Fürstin Amalie von Gallitzin „Diotima“, sich selbst im Umgang mit ihr „Sokrates“.
Gegen Ende des 18. Jahrhunderts erwachte in Deutschland ein neues Interesse an Platon, und die Rolle von Frauen im philosophischen Diskurs wurde vielfach thematisiert. Damit rückte Diotima als Vorbild für eigenständig philosophierende Frauen wieder ins Blickfeld. Friedrich Schlegel verfasste 1795 einen Aufsatz Über die Diotima, in dem er sie als Priesterin und als Pythagoreerin darstellte und als „Bild vollendeter Menschheit“ beschrieb, als eine Frau, „in welcher sich die Anmut einer Aspasia, die Seele einer Sappho, mit hoher Selbständigkeit vermählt“. Ausführlich trat Schlegel dem Verdacht entgegen, sie sei eine Hetäre gewesen, da damals nur Hetären über Bildung verfügt hätten und mit Männern auf die von Platon geschilderte Weise hätten Umgang pflegen können.
Die bekannteste und wirkungsmächtigste Diotima-Rezeption dieser Epoche und der gesamten Neuzeit ist diejenige Friedrich Hölderlins. Er war von Diotimas Ausführungen im Symposion stark beeindruckt und verwendete in seiner Liebeslyrik ihren Namen – mit der griechischen Betonung Diotíma – für die Geliebte. Den platonischen Gedanken, dass Eros über das vergängliche Individuelle emporheben kann, drückte er dichterisch in der Ode Der Abschied und in der Elegie Menons Klagen um Diotima aus. In seinem Briefroman Hyperion, an dem er in den letzten Jahren des 18. Jahrhunderts arbeitete, brachte er ebenfalls das Eros-Konzept von Platons Diotima zur Geltung. Die Handlung spielt im späten 18. Jahrhundert während der Kämpfe um die Befreiung Griechenlands von der osmanischen Herrschaft. Diotima, ein griechisches Mädchen, ist als Geliebte des Titelhelden eine zentrale Figur. Sie liebt Hyperion, ermutigt ihn aber auch zu der Einsicht, dass er seine Lebensaufgabe erst erfüllen kann, wenn er sich nicht mehr von einer einseitigen Bindung an die konkrete Einzelerscheinung beherrschen lässt, sondern den Weg in eine höhere Dimension findet. Zugleich ist sie selbst die Verkörperung seines Ideals vollendeter Schönheit. Er befindet sich in einer Illusion, denn er entwirft für sich ein idealisiertes Bild von Diotima, das sich stark von ihrer Selbstwahrnehmung unterscheidet. Nach ihrem Tod, an dem er Mitschuld trägt, fällt ihm die Aufgabe zu, sein Leben neu zu gestalten. Schließlich findet er Frieden in der Natur. Das Vorbild für Hölderlins literarische Frauengestalt war Susette Gontard, doch nicht in dem Sinne, dass die reale Person durchgängig mit der fiktiven gleichzusetzen wäre.Ganz anders war der Ansatz von Christoph Martin Wieland. Er setzte sich 1800/1801 mit den im Symposion vorgetragenen Liebeslehren kritisch auseinander. In seinem Briefroman Aristipp wird in einem Brief von einem Gastmahl berichtet, an dem neben der Gastgeberin Lais fünf Männer teilnahmen. Platons Symposion wurde vorgelesen und dann hinsichtlich seiner einzelnen Bestandteile erörtert. Dabei kamen die Gesprächsteilnehmer zu Ergebnissen, die der Auffassung Diotimas radikal widersprechen. Insbesondere die Lehre vom Urschönen stieß auf fundamentale Kritik, da das Urschöne außerhalb des Bereichs möglicher menschlicher Erfahrung liege. Daher kann es aus dieser kritischen Sicht nicht das Ziel der Liebe sein, sondern erscheint als unwirklich. Wieland konzipierte seine Lais als Gegenbild zu Diotima.
Im 19. und 20. Jahrhundert war das literarische Interesse an Diotima relativ gering. Die Dichterin Sophie Borries (1799–1841) und die polnische Schriftstellerin und Dichterin Jadwiga Łuszczewska (1834–1908) wählten den Namen als Pseudonym. Auch die Publizistin Lenore Kühn (1878–1955) verwendete dieses Pseudonym, als sie 1930 im Verlag Eugen Diederichs ihre Schule der Liebe veröffentlichte, ein Sachbuch über Geschlechterbeziehungen, das einen großen Verkaufserfolg erzielte. In Robert Musils Roman Der Mann ohne Eigenschaften erhält die Gastgeberin eines Salons von einem Bewunderer den Namen Diotima. Damit knüpft Musil an die antike Tradition an, allerdings mit ironischer Absicht: Er wendet sich gegen eine idealistische, romantische Überhöhung trivialer Verhältnisse.Von dem Schweizer Maler Hans Erni stammen mehrere Zeichnungen von Sokrates mit Diotima. Der Komponist Luigi Nono schuf 1979/1980 das Streichquartett Fragmente – Stille, An Diotima, mit dessen Benennung er auf Hölderlins Diotima-Gestalt Bezug nahm. Als Hommage an dieses Werk versteht sich der Name des französischen Streichquartetts „Quatuor Diotima“, das 1996 von Musikschulabsolventen von Paris und Lyon gegründet wurde.
Die italienische Philosophin Luisa Muraro hat 1983 in Verona eine feministische Philosophinnengemeinschaft namens Diotima gegründet. In Athen erscheint seit 1973 die philosophische Zeitschrift Diotima, herausgegeben von der Société Hellénique d’Etudes Philosophiques.
David M. Halperin: Why Is Diotima a Woman? Platonic Erōs and the Figuration of Gender. In: David M. Halperin u. a. (Hrsg.): Before Sexuality. The Construction of Erotic Experience in the Ancient Greek World. Princeton University Press, Princeton (N.J.) 1990, ISBN 0-691-03538-5, S. 257–308.
Kurt Sier: Die Rede der Diotima. Untersuchungen zum platonischen Symposion. Teubner, Stuttgart 1997, ISBN 3-519-07635-7.
Jürgen Wippern: Eros und Unsterblichkeit in der Diotima-Rede des Symposions. In: Hellmut Flashar, Konrad Gaiser (Hrsg.): Synusia. Festgabe für Wolfgang Schadewaldt zum 15. März 1965. Neske, Pfullingen 1965, S. 123–159.Rezeption
Pascal Firges: Eros im Hyperion. Platonisches und spinozistisches Gedankengut in Hölderlins Roman (= Kulturgeschichtliche Reihe, Band 11). Sonnenberg, Annweiler 2010, ISBN 978-3-933264-61-9.
Jean Firges: Friedrich Hölderlin: Trauer um Diotima. Der „Hyperion“-Roman (= Exemplarische Reihe Literatur und Philosophie, Band 10). Sonnenberg, Annweiler 2002, ISBN 978-3-933264-17-6.
