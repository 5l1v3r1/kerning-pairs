
Das Long-QT-Syndrom (LQTS, früher QT-Syndrom) ist eine seltene Krankheit, die bei sonst herzgesunden Menschen zum plötzlichen Herztod führen kann. Es ist entweder vererbt (kongenital) oder erworben, dann meist als Folge einer unerwünschten Arzneimittelwirkung. Die bekanntesten kongenitalen Long-QT-Syndrome sind das Romano-Ward-Syndrom (autosomal-dominant; syn.: Pseudohypokaliämie-Syndrom) und das Jervell- und Lange-Nielsen-Syndrom (autosomal-rezessiv; JLNS).
Das wegweisende und namensgebende Krankheitszeichen des Long-QT-Syndroms ist eine Verlängerung der QT-Zeit im Elektrokardiogramm (EKG) mit einer frequenzkorrigierten QT-Zeit (QTc) von über 440 Millisekunden (ms). Für das Long-QT-Syndrom typisch ist anfallsweise auftretendes Herzrasen (Tachykardie), oft in Form der lebensbedrohlichen Torsade-de-pointes-Tachykardien. Diese Herzrhythmusstörungen können zu Schwindelattacken, plötzlicher Bewusstlosigkeit (Synkope) und zum Herzstillstand durch Kammerflimmern führen. Viele Patienten leiden aber unter keinerlei Beschwerden, bleiben also asymptomatisch.
Sowohl die Tachykardien als auch die Synkopen treten bevorzugt bei körperlicher Belastung oder in Stresssituationen auf. Bei symptomatischen Patienten ist die Prognose ohne Behandlung schlecht, fast allen Patienten kann aber heutzutage eine adäquate Therapie angeboten werden.
Ursache des Long-QT-Syndroms sind geringfügige Abweichungen im Ablauf der elektrischen Signalübermittlung in den Zellen des Herzmuskels (Myokard). Dabei handelt es sich um eine verzögerte Repolarisation, hauptsächlich eine Verlängerung der als Phase 2 bezeichneten Plateauphase des Aktionspotentials. Während dieser früher oft als vulnerable Phase bezeichneten Zeit von etwa 300-400 Millisekunden können irreguläre Nachdepolarisationen bereits wieder ein Aktionspotenzial auslösen, welches dann länger anhaltende Arrhythmien triggern kann („getriggerte Aktivität“). Bei den kongenitalen Long-QT-Syndromen wird die Verlängerung der Plateauphase durch abnorme Eigenschaften der Ionenkanäle verursacht, entweder in Form eines verminderten Ionentransports („loss of function“ des Kalium-Ionenkanals beim LQTS1 und LQTS2) oder einer erhöhten Transportleistung („gain of function“ des Natrium-Ionenkanals beim LQTS3). Beim erworbenen Long-QT-Syndrom wird sie in erster Linie auf eine Hemmung des schnellen Anteils des Kalium-Ionenstromes IKr zurückgeführt.
Seit Mitte der 1960er Jahre wurden mit dem Romano-Ward- und dem Jervell- und Lange-Nielsen-Syndrom zunächst zwei klinische Erscheinungsformen (Phänotypen) des angeborenen Long-QT-Syndroms unterschieden. Heute sind molekularbiologisch eine Vielzahl verschiedener Syndrome identifiziert, wobei aktuell sechs Genotypen (LQTS1-LQTS6) dem Romano-Ward-Syndrom zugerechnet werden, zwei dem JLNS (JLNS1-JLNS2) und einer dem Andersen-Syndrom (LQTS7). Ihnen gemeinsam ist eine Mutation von Genen, welche die Ionenkanäle der Herzmuskelzellen kodieren.
Bei den kongenitalen Formen sollten alle blutsverwandten Familienmitglieder auf das Vorliegen eines QT-Syndroms untersucht werden.
Das Jervell-Lange-Nielsen-Syndrom wird autosomal-rezessiv vererbt mit Innenohrschwerhörigkeit und QT-Verlängerung. Etwa sieben Prozent der kongenitalen Long-QT-Syndrome werden dem JLNS zugerechnet.
Bei etwa 70 Prozent der vererbten Long-QT-Syndrome liegt eine der autosomal-dominanten Varianten ohne Hörstörung vor, das Romano-Ward-Syndrom.
Eine Verlängerung des QT-Intervalls im EKG kann auch durch den Einfluss einer Vielzahl von Arzneimitteln, durch Elektrolytstörungen und möglicherweise als Folge von Entzündungen (Myokarditis) und Durchblutungsstörungen (Ischämie) entstehen. Wenn dabei Torsades-de-pointes-Tachykardien oder gar Synkopen auftreten, spricht man von einem erworbenen Long-QT-Syndrom, wobei bis heute unklar ist, inwieweit diese Patienten in Wirklichkeit ein verborgenes, kongenitales Long-QT-Syndrom aufweisen.
In den 1960er Jahren wurden erste Berichte über eine Verlängerung der QT-Zeit durch das damals zur Behandlung von Rhythmusstörungen sehr gebräuchliche Chinidin veröffentlicht. Seit den 1990er Jahren hat das durch Medikamente hervorgerufene Long-QT-Syndrom zunehmende Beachtung gefunden, nachdem immer mehr in dieser Hinsicht gefährliche Substanzen identifiziert wurden. Mittlerweile umfasst diese Liste mehr als hundert zum Teil häufig eingesetzte Präparate der unterschiedlichsten Gruppen, wobei oft nur ein oder zwei Vertreter einer Stoffklasse betroffen sind. Darunter finden sich neben Antiarrhythmika wie Chinidin und Sotalol auch viele häufig verschriebene Medikamente, deren kardiale Nebenwirkung lange Zeit überhaupt nicht bekannt war. In den Fokus geraten sind auch Antibiotika wie Erythromycin und Trimethoprim-Sulfamethoxazol, einige Antihistaminika, viele Psychopharmaka, wie beispielsweise Neuroleptika. Zudem Parkinson- und Anti-Malaria-Mittel sowie Röntgenkontrastmittel und verschiedene Opioide. Mehrere Präparate (u. a. Astemizol, Cisaprid und Clobutinol, LAAM und Grepafloxacin) sind deswegen bereits vom Markt genommen worden. Eine ausführliche und aktuelle englischsprachige Liste der beachtenswerten Medikamente wird an der University of Arizona gepflegt.Gemeinsam ist diesen Substanzen, dass sie in der Herzmuskelzelle den Kaliumausstrom während der Repolarisation hemmen und so das QT-Intervall verlängern können. Der blockierte Kaliumkanal ist der HERG-Kanal.
Das Risiko für derartige unerwünschte Arzneimittelwirkungen (UAW) ist bei niedrigen Pulsfrequenzen (Bradykardie), weiblichem Geschlecht, erniedrigtem Kaliumspiegel im Blut (Hypokaliämie), Verdickung des Herzmuskels durch arterielle Hypertonie (Bluthochdruck), Herzmuskelschwäche und hohen Wirkstoffkonzentrationen auf Grund pharmakogenetischer Besonderheiten erhöht.
Plötzliche Todesfälle junger und sonst gesunder Menschen erregen Aufmerksamkeit, besonders wenn sie sich bei großen Sportveranstaltungen vor den Augen der Öffentlichkeit ereignen. Statistisch gesehen sind derartige Ereignisse aber selten, und noch seltener sind sie Folge eines Long-QT-Syndroms. Insgesamt wird für den plötzlichen Herztod von einer Prävalenz von 1-2 pro 1000 Einwohner und Jahr ausgegangen, bei Unter-30-jährigen nur etwa 0,5-1 pro 100.000 Einwohner und Jahr. Etwa sechs Prozent der plötzlich Verstorbenen weisen bei einer Obduktion keine Anzeichen einer organischen Herzkrankheit auf, sind also einer primären Rhythmusstörung erlegen. Es wird angenommen, dass davon etwa ein Drittel ein Long-QT-Syndrom aufwies. Diese zum Teil geschätzten Zahlen lassen vermuten, dass in Deutschland jährlich etwa zehn bis zwanzig Menschen im Alter von unter 30 Jahren an einem Long-QT-Syndrom sterben.
Der Verdacht, auch ein Teil der Fälle von plötzlichem Kindstod könnte durch ein kongenitales Long-QT-Syndrom verursacht sein, konnte in einer Untersuchung von 41 Fällen zumindest molekulargenetisch und anhand der Untersuchung von Familienmitgliedern nicht erhärtet werden.Angeborene Long-QT-Syndrome treten mit einer Häufigkeit von 1:5.000 bis 1:15.000 aller Lebendgeburten auf. Etwa 30-46 Prozent dieser Patienten erleiden vor dem 40. Lebensjahr eine Synkope. Die Häufigkeit eines plötzlichen Herztodes bei Heranwachsenden mit Long-QT-Syndrom liegt bei 1,6 % innerhalb der zehn Jahre zwischen dem Alter von 10-20. Diese Häufigkeit variiert stark innerhalb der unterschiedlichen Gruppen von Patienten und kann durch Medikamentierung deutlich gesenkt werden.Mit Hilfe des Ruhe-EKGs und einer molekulargenetischen Untersuchung können heute die Patienten mit einem besonders hohen Risiko besser identifiziert werden. Als Hochrisikogruppe gelten alle Patienten mit einem QTc-Intervall von mehr als 500 ms und den Genotypen LQTS1 und LQTS2 sowie LQTS3 bei männlichem Geschlecht. Sie haben unbehandelt ein Risiko von mehr als 50 Prozent, vor ihrem 40. Lebensjahr eine Synkope, einen Herzstillstand oder den plötzlichen Herztod zu erleiden. Ebenfalls besonders gefährdet sind Menschen, die mehr als eine der bekannten Mutationen aufweisen; damit ist bei knapp acht Prozent der angeborenen Long-QT-Syndrome zu rechnen. Diese als compound mutation bezeichneten Genotypen sind häufiger symptomatisch (100 vs. 72 Prozent) und erleiden häufiger einen Herzstillstand (56 vs. 27 Prozent) als solche mit weniger als zwei nachgewiesenen Mutationen.
Etwa zwei Prozent der Patienten, die Methadon erhalten, entwickeln eine verlängerte QT-Zeit. Für die Substitutionstherapie Opioidabhängiger wurden von einem US-Expertengremium entsprechende Richtlinien erarbeitet, die auch für den deutschsprachigen Raum von Bedeutung sind, da entsprechende Richtlinien hier noch fehlen.Eine Studie von 2013 stellt bei Krankenhauspatienten einen Zusammenhang von QT-Verlängerung und nahendem Tod fest. Unter allen Patienten, bei denen ein EKG geschrieben wurde, wiesen 2 % eine QT-Verlängerung über 500 Millisekunden (frequenzkorrigiert) auf. Die Todeshäufigkeit innerhalb dieser Gruppe lag für den gleichen Zeitraum bei 19 % gegenüber 5 % in der Gruppe ohne QT-Verlängerung.
Die Verlängerung der QT-Zeit selbst ist normalerweise nicht spürbar, mehr als die Hälfte der Patienten mit einem Long-QT-Syndrom leiden an keinerlei Beschwerden. Wenn Symptome auftreten, so sind sie bereits durch potentiell lebensbedrohliche (sog. maligne) Herzrhythmusstörungen verursacht, die schon als ein schwerwiegendes Krankheitszeichen gewertet werden müssen. Dabei handelt es sich um anhaltende (>30 Sekunden) oder nicht-anhaltende (≤ 30 Sekunden) ventrikuläre Tachykardien meist vom Typ der Torsade-de-pointes-Tachykardie. Je nach Dauer und Pulsfrequenz der Tachykardie, Körperposition und allgemeiner Verfassung können diese Tachykardien gar nicht bemerkt werden, zu Schwindel oder plötzlicher Bewusstlosigkeit (Synkope) oder gar zum Herzstillstand und damit zum plötzlichen Herztod führen.
Da die Tachykardien urplötzlich und bevorzugt bei körperlicher Belastung oder in Stresssituationen auftreten, werden auch die Symptome häufig unerwartet und aus völligem Wohlbefinden in den beschriebenen Situationen bemerkt.
Der wegweisende und namensgebende Befund des Long-QT-Syndroms ist die Verlängerung des QT-Intervalls im Ruhe-EKG.
Die in Millisekunden (ms) gemessene QT-Zeit ist für sich genommen wenig aussagekräftig, da sie beim Menschen u. a. von der Herzfrequenz, dem Alter und dem Geschlecht abhängig ist. Um eine abnormal lange QT-Zeit zuverlässig erkennen und verschiedene QT-Zeiten im Verlauf miteinander sinnvoll vergleichen zu können, ist eine rechnerische Korrektur der gemessenen QT-Zeit erforderlich. Am häufigsten wird die Bazett-Formel genutzt:
wobei die QT-Dauer in ms und der RR-Abstand in Sekunden anzugeben ist. Bei Herzfrequenzen über 100 pro Minute führt die Korrekturformel nach Bazett zu einer Überkorrektur, bei Herzfrequenzen unter 60 pro Minute zu einer Unterkorrektur. Bei Frequenzen über 80 pro Minute führt die in den letzten Jahren zunehmend häufiger angewandte Formel nach Fridericia zu exakteren Ergebnissen:
Für wissenschaftliche Zwecke ist eine genauere Korrektur der QT-Zeit erforderlich, die auch das Geschlecht und das Alter des Patienten berücksichtigt. Dies geschieht nach folgenden Formeln (nach Pfeufer u. a. 2005):
    {\displaystyle QTc{\textrm {[ms]}}=QT{\textrm {[ms]}}-(0{,}152\times {(RR{\textrm {[ms]}}-1000)})-(0{,}318\times {(Alter{\textrm {[a]}}-60)})}
    {\displaystyle QTc{\textrm {[ms]}}=QT{\textrm {[ms]}}-(0{,}154\times {(RR{\textrm {[ms]}}-1000)})-(0{,}207\times {(Alter{\textrm {[a]}}-60)})-4{,}58}
Nachteil der genannten Korrekturformeln ist der erforderliche Rechenschritt, der einen Rechner oder entsprechende Nomogramme erforderlich macht. Aus diesem Grund verwenden viele Ärzte spezielle „EKG-Lineale“, die zur ermittelten Herzfrequenz jeweils die normale QT-Zeit angeben.
Als oberer Grenzwert gilt eine QTc von 440 ms, ab 500 ms ist von einem hohen Risiko auszugehen. Für die Bewertung der „gemessenen“ QT-Zeit und damit auch der berechneten QTc ist das Wissen um die Fehlerquellen der Methode wichtig. Besonders bei niedrigen Amplituden der T-Wellen und gelegentlich nachfolgenden U-Wellen ist das Ende der T-Welle und damit der Endpunkt der Messung nicht exakt definiert und unterliegt der subjektiven Wahrnehmung des Untersuchers. Darüber hinaus unterscheiden sich die aus einer, drei oder zwölf gleichzeitig abgeleiteten EKG-Linien ermittelten QT-Intervalle signifikant, so dass die Messmethode bei Vergleichen berücksichtigt werden sollte.
Eine verlängerte QT-Zeit kann relativ einfach im Ruhe-EKG erkannt werden, wenn man den RR-Abstand zweier benachbarter QRS-Zacken betrachtet. Ist die QT-Zeit länger als der halbe RR-Abstand, dann ist die QT-Zeit auf jeden Fall verlängert.
Bei einem Score von 1 besteht eine geringe, bei einem Score bis 3 eine mittlere, ab 4 eine hohe Wahrscheinlichkeit für das Vorliegen eines Long-QT-Syndroms.
Da die Häufigkeit schwerwiegender Herzrhythmusstörungen unter einer Behandlung mit Betarezeptorenblockern eindeutig abnimmt, gehören sie zur Standardtherapie bei kongenitalem Long-QT-Syndrom. Patienten, bei denen trotzdem noch Synkopen auftreten und solche nach einem überlebten Herzstillstand sollten vorsorglich einen implantierbaren Defibrillator (ICD) erhalten. Möglicherweise profitieren Patienten mit einem besonders hohen Risiko bereits vor dem Auftreten von Symptomen von der Implantation eines ICD.
Bei einem durch Medikamente verursachten Long-QT-Syndrom steht das unverzügliche Absetzen der Substanz im Vordergrund. Betablocker gelten – im Gegensatz zur kongenitalen Form – hier als kontraindiziert, da sie eine Bradykardie hervorrufen oder verstärken und so das Risiko bedrohlicher Rhythmusstörungen eher erhöhen. Bewährt hat sich neben dem Ausgleich einer evtl. Hypokaliämie die Zufuhr von Magnesium, bei Bradykardie wird eine Steigerung der Herzfrequenz durch Medikamente (z. B. Orciprenalin) oder eine vorübergehende Schrittmacherstimulation empfohlen.
Körperliche Belastung ist für Patienten mit einem Long-QT-Syndrom problematisch. Besonders bei abruptem Belastungsbeginn oder -ende, Kälte, Druckschwankungen und lauten Geräuschen besteht ein erhöhtes Risiko für bedrohliche Rhythmusstörungen. Aus diesem Grund wird von Sportarten wie Basketball, Eishockey, Bodybuilding, Wellenreiten/Surfen, Schwimmen, Tauchen und Schnorcheln grundsätzlich abgeraten, ebenso vom wettkampfmäßigen Laufen, Gewichtheben, Motorradfahren, Squash- und Tennisspielen. Regelmäßige, moderate, körperliche Aktivität wie Joggen, Walking und Skaten hingegen wird befürwortet, und auch gegen Bowlen und Gewichtheben ist wenig einzuwenden, wenn es nicht leistungsorientiert erfolgt. Kinder werden oft vom Schulsport befreit, da dieser unter dem Aspekt der Benotung (Gefahr einer Überforderungssituation) steht, individuell kann aber über die Ausübung von anderen Freizeitsportarten entschieden werden.
W. Haverkamp u. a.: QT-Syndrome: Aspekte zur Pathogenese, molekularen Genetik, Diagnostik und Therapie. In: Deutsches Ärzteblatt. (1997) 94, S. A667–A672.
W. Haverkamp u. a.: Medikamentenbedingte QT-Verlängerung und Torsade de pointes: Ein multidisziplinäres Problem. In: Deutsches Ärzteblatt. (2002) 99, S. A1972–A1979.
L. Crotti, G. Celano, F. Dagradi, P. J. Schwartz: Congenital long QT syndrome. In: Orphanet J Rare Dis. 2008, Juli 7; 3, S. 18. Review. PMID 18606002, PMC 2474834 (freier Volltext)
Pfeufer u. a.: Common variants in myocardial ion channel genes modify the QT interval in the general population: results from the KORA study. In: Circ Res. 2005, April 1; 96 (6), S. 693–701.
Wie Stress das Herz aus dem Takt bringt; Auswirkung von Cortisol auf mutierte IKs-Kanäle bei LQT-Syndrom; Scinexx, Springer-Verlag, Heidelberg 2008

Die Quartärforschung betreibt systematisch Untersuchungen zum erdgeschichtlichen Zeitabschnitt des Quartärs bzw. der jüngsten geologischen Periode. Diese Periode ist durch eine Serie von Kaltzeiten mit großflächigen Vergletscherungen, die sich mit relativ warmen, interglazialen Zeitabschnitten abwechseln gekennzeichnet, wie dem gegenwärtigen Holozän. Die Erforschung des Quartärs begann im späten 18. Jahrhundert, wobei sich dieser Forschungszweig erst im Laufe des 19. Jahrhunderts zusammen mit der Paläontologie etablierte. Wie in vielen anderen Wissenschaftsdisziplinen kämpften auch die frühen Pioniere der Quartärforschung mit der Überwindung festgefahrener Ideen und dogmatischer Vorstellungen, die vor allem auf einer wörtlichen Auslegung der Bibel mit der Sintflut als reales weltweites Ereignis beruhten. Die moderne Quartärforschung ist stark interdisziplinär geprägt und integriert Informationen aus verschiedenen Wissenschaften, wie Paläoklimatologie, Geologie, Ozeanographie, aber auch aus der Archäologie oder Anthropologie. Die Einbeziehung dieser Forschungsbereiche bei der Auswertung der quartären geologischen Archive hat seit Beginn des 20. Jahrhunderts maßgeblich dazu beigetragen, wie die jüngere Erdgeschichte heute interpretiert wird.
Der Begriff Quartär wurde geprägt vom italienischen Bergbauingenieur Giovanni Arduino (1714–1795). Er unterschied vier geologische Ordnungen, die die gesamte Erdgeschichte umfassten: Primär, Sekundär, Tertiär und Quartär. Diese vier „Schichten“, die übereinander zu liegen schienen, manifestierten sich in Italien regional unterschiedlich. So identifizierte Arduino die Glimmerschiefer der Atesinischen Plattform im Umkreis der norditalienischen Städte Bozen und Trient als Primär, das Sekundär als die fossilreichen Ablagerungen der Südlichen Kalkalpen, das Tertiär als die fossilreichen Sedimentgesteine der Täler und das Quartär mit den Schottern der Po-Ebene. Der Begriff Quartär wurde danach erst wieder im Jahr 1829 vom französischen Geologen Jules Desnoyers aufgegriffen, um die tertiären von den jüngeren Ablagerungen im Pariser Becken zu unterscheiden. Der Begriff Quartär wurde kurze Zeit später im Jahr 1833 vom Franzosen Henri Reboul dahingehend beschrieben, dass die quartären Schichten die rezente Flora und Fauna aufweisen.Der erdgeschichtliche Zeitabschnitt des Quartärs ist gegenwärtig in die geochronologischen Epochen des Pleistozäns und des Holozäns unterteilt. Die Begriffsgeschichte dieser Zeitabschnitte gestaltete sich ebenfalls sehr langwierig. Die Bezeichnung Pleistozän wurde 1839 durch den schottischen Geologen Charles Lyell geprägt. Lyell definierte das Pleistozän als jüngste geologische Ära. Als sich die Theorie der Gletscherentstehung etablierte, wurde im Jahr 1846 das Pleistozän von Edward Forbes mit dem Zeitalter der Gletscher (Glacial epoch) gleichgesetzt. Moriz Hoernes führte 1853 den Begriff des Neogens ein und bildete damit das übergeordnete System zu Lyells Miozän und Pliozän. Darauf Bezug nehmend spezifizierte Lyell im Jahr 1873, dass der Begriff Pleistozän „strictly synonymous with post-Pliocene“ („streng synonym zu Postpliozän“) verwendet werden sollte. In derselben Publikation trennte Lyell explizit das Pleistozän (Glazial) von der gegenwärtigen Zeit (Postglazial). Paul Gervais ersetzte kurze Zeit später den Begriff gegenwärtig durch Holozän.Demzufolge bestand zum Ende des 19. Jahrhunderts bereits die stratigraphische Nomenklatur des Quartärs. Allerdings war zu diesem Zeitpunkt noch unbekannt, wann das Tertiär endete und das Quartär begann. In der Geologie werden zu diesem Zweck Typlokalitäten bestimmt, die Grenzen zwischen unterschiedlichen stratigraphischen Einheiten bilden. Während des 18th International Geological Congress in London im Jahr 1948 wurde beschlossen, eine solche Typlokalität für die Pliozän-Pleistozän-(Tertiär-Quartär-)Grenze zu finden. Nach knapp drei Jahrzehnten wurde im Jahr 1985 an der Lokalität Vrica in Kalabrien ein solches stratigraphisches Referenzprofil festgelegt und ursprünglich auf ca. 1,64 Millionen Jahre datiert. Eine genaue Altersbestimmung wurde erst durch die Einbeziehung radiometrischer Datierungsmethoden möglich, die seitdem einen zentralen Bestandteil der Quartärforschung bilden.
Die Forschungszweige, die zur heute anerkannten modernen Quartärforschung führten, waren breit gefächert, und die Erkenntnisse stammten aus vielen verschiedenen Disziplinen. Eine dieser Disziplinen war die Wirbeltierpaläontologie. Ebenso wie in anderen Wissenschaftszweigen führte eine zentrale Entdeckung zur Begründung einer neuen und eigenständigen Forschungslinie. Eine derartige Entdeckung geschah am Big Bone Lick, einem Fundort am Ohio River in Kentucky (USA). Dies war die erste bedeutende Fossillagerstätte in der Neuen Welt, die Europäern bekannt war. Baron Charles de Lougueuil, der Kommandant einer französischen Militärexpedition, war vermutlich der erste Europäer, der diese Fundstätte im Jahr 1739 besuchte. Er sammelte Fossilien des Amerikanischen Mastodons, die später von den französischen Naturforschern Louis Jean-Marie Daubenton, Georges-Louis Leclerc de Buffon und Georges Cuvier untersucht wurden. Letzterer veröffentlichte 1825 eine Beschreibung der Überreste des Big Bone Lick Mastodons.
Im Jahr 1803 erstanden die USA das Territorium Louisianas von den Franzosen. Dieses Territorium umfasste mehr als zwei Millionen Quadratkilometer und reichte vom Mississippi bis zu den Rocky Mountains. Als der damalige Präsident der Vereinigten Staaten Thomas Jefferson die Forscher Meriwether Lewis und William Clark aussandte, um dieses neue amerikanische Gebiet zu erkunden und zu kartieren, erwartete er möglicherweise die Entdeckung einiger lebender Exemplare des Mastodons bzw. anderer großer pleistozäner Säugetiere. Thomas Jefferson war ein begeisterter Naturforscher und zeigte großes Interesse an den fossilen Knochenfunden von Big Bone Lick. 1807 beauftragte Thomas Jefferson den Entdecker William Clark im Anschluss an die Lewis-und-Clark-Expedition zu einer umfassenden Grabungskampagne am Big Bone Lick, die etwa 300 Exemplare unterschiedlichster Fossilien zum Vorschein brachte. Diese Fossilienfunde bildeten die Grundlage für die pleistozäne Wirbeltierpaläontologie auf zwei Kontinenten. Die Entdeckung des Mastodons sowie anderer pleistozäner Vertreter der Megafauna an diesem Standort beflügelte die Phantasie sowohl von Wissenschaftlern als auch von Politikern.
Basierend auf Funden dieser Art nahm das Forschungsgebiet der Wirbeltierpaläontologie im späten 18. und frühen 19. Jahrhundert zunehmend Konturen an. Einer der führenden Wissenschaftler dieser neuen Disziplin war der Franzose Georges Cuvier (1769–1832). Zu Beginn des 19. Jahrhunderts war er Professor für Tieranatomie am Muséum national d’histoire naturelle in Paris. Cuvier galt als Gegner der Evolutionstheorie; sein wichtigster Beitrag zum damaligen Kenntnisstand der Wissenschaft war die Möglichkeit des Aussterbens von Lebensformen, basierend auf fossilen Knochenfunden. Bis in das 19. Jahrhundert hinein lehnten die meisten Philosophen und Naturforscher die Idee eines umfassenden Artensterbens ebenso ab wie die sich anschließende Entwicklung neuer Lebensformen. Vielmehr orientierten sie sich an der wörtlichen Auslegung der Bibel, mit einer Erschaffung der Erde vor wenigen tausend Jahren innerhalb eines Zeitraums von sechs Tagen (Kreationismus). Die pleistozänen Fossilfunde vieler Paläontologen führten daher nur allmählich zu einem Umdenken.Cuvier war der Begründer der Kataklysmentheorie und hielt an dieser Vorstellung sein Leben lang fest. Er erklärte das Aussterben von Arten mit periodisch auftretenden Revolutionen in der Erdgeschichte. Jede dieser Revolutionen war ein katastrophaler Umbruch, der zum Verschwinden eines Großteils der Arten führte. Im Gegensatz zu anderen Gelehrten seiner Zeit, die die Idee einer Sintflut vertraten (u. a. William Buckland), verband Cuvier diese Revolutionen nicht mit biblischen oder historischen Ereignissen. Statt dessen erwog er die Möglichkeit, dass die letzte große Revolution, die zum Aussterben von Spezies wie des Mastodons und des Mammuts führte, durch eine starke und plötzliche globale Abkühlung bewirkt wurde. Der schweizerisch-amerikanische Naturforscher Louis Agassiz griff diese Idee auf und entwickelte sie zum Konzept einer Großen Eiszeit weiter.
Louis Agassiz (1807–1873) präsentierte seine Theorie über die Große Eiszeit erstmals 1837 der Schweizerischen Naturforschenden Gesellschaft in Neuchâtel. Dies war eine ideale Umgebung, um die anwesenden Geologen und Naturhistoriker von seiner Theorie zu überzeugen und die Wirkung des Gletschereises unmittelbar im alpinen Gelände zu demonstrieren. Agassiz zeigte dazu auf große Gesteinsblöcke (Findlinge) und Geröllansammlungen (Moränen), die durch das Gletschereis transportiert beziehungsweise aufgeschüttet worden waren, sowie auf Gletscherschliffe am Festgestein, die er mit der Bewegung der Eismassen begründete. Agassiz veröffentlichte seine Eiszeitheorie in den Büchern Étude sur les glaciers im Jahr 1840 sowie in Système glaciare im Jahr 1847. Diese Bücher fassten seine Untersuchungen in Europa zusammen, wobei er später weitere Belege für eine großflächige Vergletscherung in Nordamerika fand. Agassiz' Theorie wurde anfangs von zahlreichen führenden Geologen abgelehnt, die nach wie vor die Ansicht vertraten, dass oberflächlich transportiertes Sediment auf die Sintflut zurückzuführen sei (Neptunismus). So sehr Agassiz' Theorie über die Vergletscherung überzeugte, so ablehnend wurden seine Aussagen zur pleistozänen Megafauna aufgenommen. Agassiz behauptete, man würde vermuten, dass Mammuts und andere ausgestorbene Arten ursprünglich an ein tropisches Klima angepasst gewesen seien. Jedoch sei das genaue Gegenteil der Fall gewesen, nämlich dass diese Arten in einer eiszeitlich geprägten Umwelt gelebt hatten.Andere europäische Wissenschaftler entdeckten ebenfalls Hinweise auf eine vergangene Kaltzeit, wie Jens Esmark in Norwegen oder Albrecht Reinhard Bernhardi in Deutschland, die jeweils deutliche Spuren ehemaliger Eisschilde dokumentierten. In der Schweiz fanden Ignaz Venetz und Johann von Charpentier geologische Belege für Gletschervorstöße weit über die Grenzen der gegenwärtigen alpinen Gletscher hinaus. Agassiz selbst sammelte zusätzliche Beweise für eine Vergletscherung in Großbritannien und Nordamerika. Von Agassiz′ Eiszeittheorie überzeugt forschten zahlreiche Geologen während des mittleren und späten 19. Jahrhunderts an der Rekonstruktion der eiszeitlichen Gletscherstände. Agassiz vermutete, dass die damaligen Eisschilde einen Großteil der mittleren und hohen Breiten der Erde bedeckt hatten. Kurz darauf mehrten sich die Hinweise auf verschiedene Vergletscherungsstadien, jeweils unterbrochen von kürzeren Warmzeiten. In den 1850er Jahren wurde von mindestens zwei großen Kaltzeiten im europäischen Raum ausgegangen. Bezugnehmend auf stratigraphische Befunde postulierte James Geikie 1877 vier oder fünf Vergletscherungsphasen während des Pleistozäns. Untersuchungen aus Nordamerika deuteten darauf hin, dass der letzte Gletschervorstoß nicht der ausgedehnteste war, da er die Relikte früherer Vergletscherungen nur zum Teil überlagert hatte. Geologen führten dazu die Begriffe Nebraskan, Kansan, Illinoian und Wisconsin ein, um die Abfolge von vier Vergletscherungen in Nordamerika zu klassifizieren. Diese waren durch drei Interglaziale voneinander getrennt (Aftonian, Yarmouthian und Sangamon). Als Meilenstein der Eiszeit- und Quartärforschung in Europa gilt das in den Jahren 1901 bis 1909 von Albrecht Penck und Eduard Brückner herausgegebene dreibändige Standardwerk Die Alpen im Eiszeitalter, das die vier alpinen Kaltzeiten Günz, Mindel, Riss und Würm umfassend beschrieb und eine wegweisende stratigraphische Grundlage zu diesem Themenbereich etablierte (basierend auf der Auswertung stratigraphischer Profile nordalpiner Flussterrassen).
Zum Ende des 19. Jahrhunderts wurde von der Wissenschaft die Tatsache großräumiger Vergletscherungen allgemein akzeptiert, allerdings blieben die Ursachen und die genaue zeitliche Dauer der glazialen und interglazialen Phasen weiterhin unklar. Als gesichert galt lediglich, dass die wechselnden Klimazustände der jüngeren Erdgeschichte viele Jahrtausende beanspruchten. Zur Erklärung dieser Zyklen wurden mehrere Hypothesen entwickelt, so zum Beispiel Änderungen der atmosphärischen Kohlenstoffdioxid-Konzentration oder periodische Schwankungen der Sonnenaktivität.
Eine der frühesten Theorien über den periodischen Wechsel zwischen Glazialen und Interglazialen stammt vom schottischen Naturforscher James Croll (1821–1890). In der Korrespondenz mit Charles Lyell schilderte Croll seine Idee über den Zusammenhang der Vergletscherungen mit Änderungen der orbitalen Bahnelemente. Lyell war von dieser Annahme beeindruckt und ermöglichte Croll im Jahr 1867 eine Anstellung beim Geological Survey of Scotland. Hier wurde er von dem Geologen Archibald Geikie ermutigt, seine Theorie weiter auszubauen. Croll korrespondierte zu dieser Zeit regelmäßig mit Charles Darwin, wovon beide Wissenschaftler profitierten. Croll begann seine Theorie ab dem Jahr 1867 im Rahmen mehrerer Abhandlungen und Werke zu veröffentlichen. Als seine bekanntesten Publikationen gelten Climate and Time, in their Geological Relations im Jahr 1875 und Climate and Cosmology im Jahr 1885.1846 publizierte der französische Astronom Urbain Le Verrier Formeln zur Berechnung der Bahnelemente. Croll benutzte diese Veröffentlichung zur Rekonstruktion des Erdorbits (Exzentrizität) während der vergangenen drei Millionen Jahre. Dabei entdeckte er, dass Muster hoher Exzentrizität über hunderttausend Jahre Bestand hatten und sich mit Mustern geringer Exzentrizität abwechselten, so wie sie zum Zeitpunkt seiner Berechnungen vorlagen. Je stärker der Orbit von einer Kreisform abwich, umso größer war der Unterschied der solaren Einstrahlung im jahreszeitlichen Wechsel. Croll begriff die Bedeutung der Saisonalität der solaren Einstrahlung und erzielte damit einen der wichtigsten Erkenntnisgewinne der Paläoklimatologie. Änderungen des Erdorbits führten demnach zu einer Verlängerung des Winters, so dass größere Schneemengen in den hohen Breiten fielen. Eine umfangreichere Schneedecke reflektiert mehr Sonnenstrahlung und verstärkt damit die orbitalen Effekte (Eis-Albedo-Rückkopplung). Croll sah in dieser Verstärkung den Auslöser für das Wachstum von Eisschilden. Crolls Theorie war von großer Bedeutung für die Klimatologie, allerdings zeigten nachfolgende Untersuchungen auch deutliche Mängel auf. Das Auftreten der pleistozänen Vergletscherungen konnte auf dieser Basis nur unzureichend dargestellt werden, zum anderen war Crolls Chronologie der Vergletscherungen fehlerhaft. Vor allem stufte er die jüngste Glazialphase weitaus älter ein, als die geologischen Untersuchungen von James Geikie (dem jüngeren Bruder von Archibald Geikie) und anderen zeigten. Croll konnte den Großteil seiner zeitgenössischen Kollegen nicht überzeugen, und in der Folge wurden seine Ideen bis in die 1940er Jahre weitgehend ignoriert.
Milutin Milanković (1879–1958) war ein jugoslawischer Mathematiker, der sich auf Geophysik und Astronomie spezialisierte. Im Jahr 1909 wurde er Mitglied an der Fakultät für Angewandte Mathematik an der Universität Belgrad. Durch seine Inhaftierung während des Ersten Weltkriegs durch Österreich-Ungarn konnte er seine Forschungstätigkeit über die mathematische Theorie des Klimawandels – basierend auf früheren Arbeiten von Joseph-Alphonse Adhémar und James Croll – erst 1920 fortsetzen und 1941 abschließen. Jedoch erklärte Adhémar das glaziale Klima ausschließlich über die Präzession, während Milanković die zyklischen Veränderungen der drei Bahnelemente des Erdorbits um die Sonne berücksichtigte: Exzentrizität, Ekliptik und Präzession. Auf der Grundlage dieser Orbitalparameter entwickelte er ein umfassendes mathematisches Modell, um damit die Abhängigkeit der solaren Einstrahlung von der geographischen Breite und den dazugehörigen Oberflächentemperaturen der letzten 600.000 Jahre zu berechnen.Sein nächster Schritt lag im Versuch der Korrelation der veränderlichen Orbitalparameter mit den Glazial-Interglazial-Zyklen. In Zusammenarbeit mit dem deutschen Klimatologen Wladimir Köppen ging Milanković davon aus, dass Schwankungen der Einstrahlung in gewissen Breitengraden und Jahreszeiten in der Lage sind, Vergletscherungsprozesse zu verstärken oder abzuschwächen. Dieser Ansatz fand in der Fachwelt jahrzehntelang nur geringe Resonanz und galt weitgehend als spekulativ. Erst im Jahr 1976 publizierte der Geologe James Hays eine interdisziplinäre Studie über Tiefseesedimentbohrkerne. Darin postulierten Hays und seine Forschungskollegen eine große Übereinstimmung mit Milankovićs Vorhersagen im Zusammenhang mit dem Zeitpunkt und der Intensität von veränderten klimatischen Bedingungen in den vergangenen 450.000 Jahren. Die Untersuchung belegte, dass signifikante Klimaveränderungen eng mit den orbitalen Parametern Exzentrizität, Ekliptik und Präzession verknüpft sind. Diese orbitalen Veränderungen sind heute bekannt als Milanković-Zyklen.
Allerdings gab es während des Quartärs mehrere Klimawandel-Ereignisse, die offenbar nicht mit allen astronomischen Parametern korrelierten, sondern nur mit einem einzigen Zyklus übereinstimmten, wobei auch ein „Umspringen“ vom 40.000-Jahre-Zyklus (Neigungswinkel der Erdachse) auf den 100.000-Jahre-Zyklus (Änderung der Exzentrizität) nachgewiesen wurde. Dessen ungeachtet ist die Theorie seit den 1980er Jahren in modifizierter und erweiterter Form ein fester Bestandteil von Paläoklimatologie und Quartärforschung und wird vielfach zur Rekonstruktion der Eiszeitphasen herangezogen.
Ohne eine absolute Altersbestimmung wäre es beispielsweise kaum möglich gewesen, die verschiedenen Komponenten der Milanković-Zyklen auf ihre klimatische Relevanz hin zu überprüfen. Bis in die zweite Hälfte des 20. Jahrhunderts konnte die Quartärforschung keine fundierten Aussagen über die genaue zeitliche Abfolge sowie über die Dauer der verschiedenen Kalt- und Warmzeiten treffen. Die Chronologie quartärer Ereignisse wurde ausschließlich über relative Datierungsmethoden bestimmt und beschränkte sich auf eine Sequenz von Ereignissen, die aufgrund von Fossilfunden einem bestimmten stratigraphischen Profil zugeordnet wurden. Dabei blieb jedoch zwangsläufig offen, ob ein geologisches Ereignis 50.000 oder 150.000 Jahre zurücklag.
Die Entwicklung der radiometrischen Datierung revolutionierte nicht nur die Quartärforschung, sondern führte auch zur Etablierung der Subdisziplinen Geochronologie und Chronostratigraphie und erlangte somit große Bedeutung für alle Perioden des 541 Millionen Jahre umfassenden Phanerozoikums und darüber hinaus. Die Anfänge dieser Analysemethode reichen bis in das Jahr 1902 zurück, als die Physiker Ernest Rutherford und Frederick Soddy die Zerfallsreihe von radioaktiven Elementen entdeckten. Die Möglichkeit einer praktischen Nutzung für die Altersdatierung wurde von Rutherford erstmals 1904 erwähnt. Zwei Jahre später begann Rutherford mit der Berechnung des radioaktiven Zerfalls des Elements Uran. Dabei bilden sich Nuklide mit unterschiedlichen Halbwertszeiten und darauf basierend weitere Zerfallsreihen. So entstehen aus dem Mutterisotop 234U verschiedene Tochterisotope wie zum Beispiel das Thorium-Isotop 230Th. Mittels der Zerfallsreihen von Uran war es fortan möglich, das numerische Alter von magmatischen Gesteinen und vulkanogenen Sedimenten zu bestimmen, die vor Jahrmillionen durch eruptive Prozesse ausgestoßen und abgelagert worden waren. Indirekt konnte dadurch auch das Alter von benachbarten fossil­führenden Sedimentgesteinen definiert und somit die geologische Zeitskala mit numerischen Altersdaten versehen werden. Gegenwärtig gebräuchliche Methoden sind die Uran-Thorium-Datierung oder die Uran-Blei-Datierung. Um möglichst genaue Resultate zu erzielen, werden vielfach Zirkonkristalle verwendet. Diese eignen sich aufgrund ihrer Hitzeresistenz und ihrer dadurch stabil gebliebenen Gitterstruktur zur präzisen Analyse der darin eingeschlossenen radioaktiven Nuklide (wie 235U, 238U oder 232Th).
Den wohl wichtigsten Beitrag zur Datierung quartärer Fossilien und Sedimente lieferte die Entdeckung der Radiokarbonmethode (auch 14C-Methode genannt). Im Jahr 1940 entdeckten die Physiker Martin Kamen und Sam Ruben das langlebige radioaktive Kohlenstoffisotop 14C. Kamen verwendete 14C als Tracer in biologischen Systemen und fand heraus, dass unter dem Einfluss von kosmischer Strahlung das Stickstoff-Isotop 14N in der Atmosphäre zu 14C umgewandelt wird. Die Existenz des Kohlenstoffisotops 14C wurde bereits 1934 postuliert, allerdings konnte es anfangs weder beobachtet noch charakterisiert werden. Kamen gelang es als erstem, die Halbwertszeit von 14C mit etwa 5.730 Jahren zu bestimmen.
Basierend auf Kamens Entdeckungen stellte der Chemiker Willard Libby 1947 fest, dass Pflanzen während ihrer Kohlenstoffaufnahme bei der Photosynthese Spuren von 14C absorbieren. Nach ihrem Absterben endet die Absorption von Kohlenstoff, und das enthaltene 14C zerfällt in seiner gewohnten Rate, ohne ersetzt zu werden. Im Jahr 1952 entdeckte Libby schließlich, dass durch die Messung der verbliebenen 14C-Konzentration in den pflanzlichen Überresten der Absterbezeitpunkt bestimmt werden konnte. Zusätzlich wurden Konzentrationen von 14C auch im Gewebe von Tieren entdeckt, da diese durch ihre Nahrung direkt oder indirekt pflanzliches Material aufgenommen hatten. Die Radiokarbondatierung ermöglicht eine absolute Altersbestimmung von fossilen tierischen oder pflanzlichen Funden der letzten ca. 50.000 Jahre, deckt damit jedoch nur einen relativ kleinen Bereich des Quartärs ab. Zusätzlich können aus den natürlichen Schwankungen des 14C-Isotops und des stabilen Kohlenstoffisotops 12C die Zyklen der Sonnenaktivität, Veränderungen des geomagnetischen Dipolfeldes sowie der Austausch zwischen Kohlenstoffsenken und Atmosphäre berechnet werden. Für die Entdeckung der Radiokarbondatierung erhielt Willard Libby 1960 den Nobelpreis für Chemie. Gegenwärtig führen die zunehmenden anthropogenen CO2-Emissionen zu einer deutlichen Verringerung der 14C-Anteile in der Atmosphäre. Dieser Effekt wird künftige Radiokarbondatierungen voraussichtlich beträchtlich erschweren beziehungsweise signifikant verfälschen.
Die Lumineszenzdatierung ist eine physikalische Altersbestimmung für quartäre Sedimente. Diese Methode basiert auf einem mit dem Probealter anwachsenden Strahlenschaden, der durch die emittierte Lumineszenz quantifiziert wird. Innerhalb dieser Form der Altersbestimmung wird je nach verwendeter Stimulationsenergie zwischen Thermolumineszenzdatierung (TL) und Optisch Stimulierter Lumineszenz (OSL) unterschieden. Die TL-Methode hat ihren Ursprung in den 1950er Jahren und wurde erstmals für die Datierung von gebrannter Keramik eingesetzt. Von großer Bedeutung speziell in der Quartärforschung ist die OSL-Methode. Sie basiert auf dem Prinzip, dass bei ausreichender Lichtexposition (beispielsweise Sonnenlicht) das gesamte Lumineszenzsignal zurückgesetzt wird. Das bedeutet, dass im Gegensatz zur Oberflächenexpositionsdatierung das Alter der Sedimentation bestimmt werden kann. Die OSL-Datierung wurde Mitte der 1980er Jahre entwickelt. Die Lumineszenzdatierung hat einen signifikanten Beitrag zur Quartärforschung geleistet, da es mit ihr erstmals möglich war, einzelne Mineralkörner zu datieren und nicht nur organische Bestandteile (wie bei der Radiokarbondatierung). Die Lumineszenzdatierung deckt einen Messbereich von einigen Jahrhunderten bis etwa 150.000 Jahren ab.
Die Kryptondatierung unter Verwendung des Isotops 81Kr in Verbindung mit dem stabilen Isotop 83Kr wird in der Praxis seit dem Jahr 2011 eingesetzt. Den Durchbruch brachte eine neue Detektortechnologie auf der Grundlage der Atom Trap Trace Analysis (ATTA). Mit einer Halbwertszeit von 230.000 Jahren eignet sich 81Kr innerhalb des quartären Zeitrahmens vor allem zur Untersuchung von Gletschern und alten Eisschichten, wie sie zum Beispiel auf Grönland und in der Antarktis vorkommen, und liefert dabei erheblich präzisere Resultate als herkömmliche Datierungsverfahren. Ein weiterer Anwendungsbereich dieser Methode ist die gegenwärtig sich noch im Anfangsstadium befindliche Detektierung des Argon-Isotops 39Ar zur Analyse von Gletschereis und ozeanischem Tiefenwasser. Bei der Atom Trap Trace Analysis handelt es sich um eine magneto-optische „Atomfalle“ (MOT) unter Einsatz von Laserphysik zur Spurenanalyse seltener Edelgasisotope. Dabei wird jedes Atom des Probenmaterials einzeln gezählt, wobei zum Beispiel auf eine Billiarde Argon-Atome lediglich ein 39Ar-Isotop entfällt.
Um fundierte Aussagen über Klima, Umweltbedingungen und geophysikalische Ereignisse früherer Epochen treffen zu können, verfügt die Quartärforschung über eine Vielzahl spezieller Mess- und Bestimmungsmethoden. Zum Standardinstrumentarium zählen Klimaproxys, die in natürlichen Archiven wie Baumringen, Tropfsteinen, Eisbohrkernen, Korallen, See- oder Ozeansedimenten zu finden sind. Diese werden nicht nur zur Rekonstruktion von Kalt- und Warmzeiten eingesetzt, sondern liefern darüber hinaus Informationen zur Sonnenaktivität, Niederschlagsintensität sowie zur Luftzusammensetzung. Um falsche Resultate möglichst auszuschließen, müssen Klimaproxys mit modernen, instrumentell ermittelten Datenreihen verglichen und an ihnen kalibriert werden. Nachfolgend ist eine Reihe von Proxys aufgeführt, die zu den Grundlagen der Quartärforschung zählen.
Mit der Dendrochronologie lässt sich durch eine Jahresring-Auswertung das jährliche Baumwachstum in Abhängigkeit von Witterung, Umwelt und Klima rekonstruieren. Für einzelne europäische Baumarten wurden lückenlose Jahresringtabellen über einen Zeitraum von 10.000 Jahren erstellt. Momentaner „Rekordhalter“ ist der Hohenheimer Jahrringkalender, an dem die mitteleuropäische Klimaentwicklung von der Gegenwart bis in die Jüngere Dryaszeit zurückverfolgt werden kann.
Die Palynologie (Pollenanalyse) ist unter der Bezeichnung Pollenstratigraphie ein Teilbereich der Paläontologie und hat in der Quartärforschung und Paläoklimatologie ebenfalls an Bedeutung gewonnen. Dank ihrer globalen Verbreitung und ihrer großen Widerstandsfähigkeit gegenüber Umwelteinflüssen und geologischen Prozessen eignen sich urzeitliche Pollen, Sporen und Mikrofossilien bis in die geologische Gegenwart als Leitfossilien. Darüber hinaus können anhand der lokalen Häufigkeit und Artenvielfalt der Pollen auch komplexe Ökosysteme rekonstruiert werden.
Die Warvenchronologie, auch Bändertondatierung genannt, basiert auf der genauen Zählung von Ablagerungsschichten (Warven) in Still- und Fließgewässern wie Seen oder Flüssen. Falls die Zählung in einen absoluten Zeitrahmen eingebunden werden kann, ermöglicht dies eine Altersangabe in Warvenjahren. Der Anwendungsbereich der Warvenchronologie erstreckt sich über einen Zeitrahmen von etlichen hundert bis etwa 30.000 Jahren und reicht in Einzelfällen darüber hinaus.
Eisbohrkerne gehören zu den genauesten Klimaarchiven und werden deshalb sehr methodisch analysiert und ausgewertet. Neben Gebirgsgletschern, aus deren Bohrkernen unter günstigen Bedingungen die regionalen Klimaverläufe der letzten Jahrtausende rekonstruiert werden können, eignen sich der grönländische und der antarktische Landeisschild zu detaillierten Analysen über längere Zeiträume. Während das bisher älteste untersuchte Grönland-Eis rund 123.000 Jahre abdeckt und damit die Eem-Warmzeit einschließt, konnte im Rahmen des Projekts EPICA ein Antarktis-Bohrkern mit einem Gesamtalter von über 800.000 Jahren geborgen werden. Die „fossilen“ Luftbläschen innerhalb eines Eisbohrkerns sind Archive für die Zusammensetzung der Atmosphäre und hier vor allem für die Kohlenstoffdioxid- und Methan-Konzentrationen, die, gekoppelt an die Kalt- und Warmphasen eines Eiszeitzyklus, starken Schwankungen unterlagen und zusammen mit den Milanković-Zyklen einen wesentlichen Klimafaktor bilden. Außerdem liefern Eisbohrkerne Daten zur Sonnenaktivität, zu Lufttemperaturen, zu Verdunstungs- und Kondensationsprozessen sowie zu Anomalien des Erdmagnetfeldes.
Ozeanische Sedimente. Die über längere Zeiträume auf den Kontinentalschelfen oder in der Tiefsee entstandenen Ablagerungsschichten werden in biogene, lithogene und hydrogene Sedimente unterteilt. Je nach Ursprung erlauben die Bohrkernproben Rückschlüsse auf die geographische Verbreitung von Lebewesen, Zustandsänderungen von Meeresströmungen oder Klimaschwankungen der Vergangenheit. Die genaue Datierung ozeanischer Bohrkernproben schwankt normalerweise sehr stark und ist abhängig von deren Alter und von der Geschwindigkeit der jeweiligen Sedimentationsprozesse. Ablagerungen aus dem Holozän erlauben unter günstigen Bedingungen eine zeitliche Auflösung von einigen Jahrzehnten.
Tropfsteine wie Stalagmiten und Stalaktiten kommen weltweit vor und sind fast zwangsläufig in den Höhlen von Karst- und Kalkgesteingebieten zu finden. Tropfsteine entstehen aus dem mit Kohlenstoffdioxid angereicherten Oberflächenwasser, das auf seinem Weg durch Spalten und poröses Material organische Säuren aufnimmt, die im Verbund mit dem Kohlenstoffdioxid das im Gestein enthaltene Calciumcarbonat lösen. Das Verhältnis der Sauerstoffisotope im Tropfsteinkalk, die Dicke der Wachstumslagen und die Anteile diverser Spurenelemente summieren sich zu einem auf Jahrzehnte genauen Umweltarchiv, das auch abrupte und kurzzeitige Umschwünge wie die Dansgaard-Oeschger-Ereignisse der letzten Kaltzeit verzeichnet. Tropfsteine können – je nach Dauer der Wasser- und damit der Calciumcarbonatzufuhr – sehr lange wachsen und erreichen mitunter ein Alter von mehreren Hunderttausend Jahren.
Das Paläothermometer δ18O (Delta-O-18), mit dem in den 1970er Jahren erstmals die Temperaturen im Verlauf des Känozoikums messtechnisch bestimmt wurden, basiert auf dem Verhältnis der stabilen Sauerstoff-Isotope 18O und 16O. Dieses vielfältig einsetzbare Verfahren eignet sich für die Rekonstruktion von Niederschlagstemperaturen und dient zudem als Indikator von Prozessen der Isotopenfraktionierung wie der Methanogenese. In der Quartärforschung werden 18O/16O-Daten als Temperaturproxy unter anderem von fossilen Foraminiferen, von Eisbohrkernen, Tropfsteinen und Süßwassersedimenten verwendet.
Die Quartärforschung ist ein stark interdisziplinär geprägtes Forschungsfeld, das sich mit Umweltveränderungen der letzten 2,6 Millionen Jahre (der Zeitspanne des Quartärs) beschäftigt. Ziel der Quartärforscher ist die Auswertung der geologischen Archive dieser Zeit, um jene Schlüsselfaktoren, die Veränderungsprozesse auslösen und steuern, auf verschiedenen räumlichen und zeitlichen Skalen zu erfassen. Die Schlüsselfaktoren können sowohl physikalischen, chemischen, biologischen, atmosphärischen, aber auch anthropogenen Ursprungs sein. Dieses breite Forschungsspektrum erfordert einen umfassenden Informationsaustausch zwischen zahlreichen naturwissenschaftlichen und technischen Disziplinen. Zum Beispiel wurden die Grundlagen einer genauen Altersdatierung erst durch die Entwicklungen der Atomphysik des frühen 20. Jahrhunderts geschaffen. Erkenntnisgewinne in der Evolutionsbiologie oder präzise Analysemöglichkeiten von Biomolekülen ließen eine verbesserte Interpretation von quartären Fossilfunden zu. Neuerungen aus dem Ingenieurwesen nach Ende des Zweiten Weltkrieges führten dazu, dass Bohrkerne wie EPICA, GISP und GRIP aus ozeanischen Sedimenten oder aus den grönländischen und antarktischen Eisschilden gewonnen werden konnten. Diese Bohrkerne lieferten unter anderem auch neue Hinweise zur Klimasensitivität der Erde. So kommen die Autoren einer 2016 veröffentlichten Studie nach einer Analyse der letzten 784.000 Jahre mit acht kompletten Warm-und-Kalt-Zyklen zu dem Ergebnis, dass die Klimasensitivität in hohem Maße temperaturabhängig ist. Danach liegt die Klimasensitivität während einer Kaltzeit wie dem Würm- beziehungsweise Weichsel-Glazial bei rund 2 °C und erhöht sich unter Warmzeitbedingungen um annähernd das Doppelte.Die Auswertung der quartären geologischen Archive trug seit Beginn des 20. Jahrhunderts maßgeblich dazu bei, wie die jüngere Erdgeschichte heute interpretiert wird. Das Instrumentarium zur Analyse dieser geologischen Gegebenheiten wuchs stetig an und ermöglichte Rekonstruktionen mit einer zeitlichen Auflösung, wie sie vor wenigen Jahrzehnten noch undenkbar war (vor allem im Hinblick auf abrupte Klimawandel-Ereignisse).
Während des Quartärs hatte die Erde ihr gegenwärtiges physisches Erscheinungsbild hinsichtlich der Größe und Verteilung der Kontinente und Gebirge, der Meeresströmungen, der Zusammensetzung der Erdatmosphäre oder der großen Biome im Wesentlichen bereits angenommen. Allerdings gaben die geologischen Archive auch darüber Aufschluss, dass sich das irdische Klimasystem oftmals und zum Teil markant verändert hatte. Im Laufe des Quartärs entstand zudem der moderne Mensch, der im Zuge seiner weltweiten Ausbreitung allmählich zum dominierenden Einflussfaktor auf die irdische Biosphäre wurde.
Die Tatsache des gegenwärtigen Klimawandels in Verbund mit anderen Faktoren wie Artensterben, Versauerung der Ozeane oder Reduzierung natürlicher Biotope führte zum Entwurf des Anthropozäns (altgriechisch: Das menschengemachte Neue), das nach den Vorstellungen britischer Geologen und des niederländischen Nobelpreisträgers für Chemie, Paul J. Crutzen, als jüngster Zeitabschnitt in das chronostratigraphische System der Erdgeschichte implementiert werden sollte. Auf dem 35. Internationalen Geologischen Kongress in Kapstadt 2016 schloss sich die 2009 gebildete Arbeitsgruppe zum Anthropozän dieser Position an, wobei das Jahr 1950 als Startpunkt der neuen Epoche favorisiert wurde. Die endgültige Entscheidung über den künftigen Status des Anthropozäns wird von der International Commission on Stratigraphy (ICS) getroffen.
Quartärforscher integrieren Informationen aus diversen Naturwissenschaften wie der Klimatologie, Ökologie, Geologie, physischen Geographie oder der Ozeanographie, aber auch aus Humanwissenschaften wie der Archäologie oder Anthropologie. Dahinter steht die Notwendigkeit eines inter- und multidisziplinären Ansatzes für das Verständnis des Erdsystems und schließt die Herausforderung mit ein, das Gefahrenpotenzial globaler Umweltveränderungen zu erkennen und einzugrenzen. Der deutsche Quartärforscher Paul Woldstedt merkte bereits im Jahr 1951 an, dass die Quartärforschung „zum Verständnis der Gegenwart und unserer Stellung in ihr“ beiträgt.
Scott Elias und Cary Mock (Hrsg.): Encyclopedia of Quaternary Science. 2. Auflage. Elsevier, 2013, ISBN 978-0-444-53643-3 (bedeutendes Nachschlagwerk zu dem Thema, die ersten Kapitel befassen sich mit der Entwicklung der Quartärforschung selbst). 
Vivien Gornitz (Hrsg.): Encyclopedia of Paleoclimatology and Ancient Environments (= Encyclopedia of Earth Sciences Series). 2009, ISBN 978-1-4020-4551-6, Abschnitte Glaciations, Quarternary u. a. 
Mike Walker: Quaternary Science 2007: a Fifty-Year Retrospective. In: Journal of the Geological Society. Dezember 2007, doi:10.1144/0016-76492006-195 (Überblick über die Forschungsgeschichte seit Mitte des 20. Jh.).
Jürgen Ehlers: Das Eiszeitalter. Spektrum Akademischer Verlag, 2011, ISBN 978-3-8274-2327-6 (einführendes Werk, das sich an ein breites Publikum richtet). 
Karl N. Thome: Einführung in das Quartär: Das Zeitalter der Gletscher. 2. Auflage. Springer, 2013, ISBN 978-3-642-58744-3 (unveränderte Neuauflage der Ausgabe von 1999, berücksichtigt nur die Literatur bis Anfang der 1990er Jahre). 
Albert Schreiner: Einführung in die Quartärgeologie. 2. Auflage. Schweizerbart, 1997, ISBN 3-510-65177-4. 
Jürgen Ehlers: Allgemeine und historische Quartärgeologie. Enke, 1994, ISBN 3-432-25911-5. englischsprachig
J. John Lowe und Michael J. C. Walker: Reconstructing Quaternary Environments. 3. Auflage. Routledge, 2014, ISBN 978-1-317-75371-1 (stark methodenorientierte Einführung und Übersicht). 
Neil Roberts: The Holocene: An Environmental History. Wiley, 2014, ISBN 978-1-4051-5521-2 (Einführung für Undergraduate-Studenten mit Schwerpunkt auf dem Holozän, auch mit Kapiteln über Methoden und das Pleistozän). 
Raymond S. Bradley: Paleoclimatology. Reconstructing Climates of the Quaternary (= International geophysics series. Band 68). 3. Auflage. Academic Press, 2013, ISBN 978-0-12-386995-1 (Schwerpunkt auf Übersicht über paläoklimatischen Methoden, häufig zitiertes Werk für fortgeschrittene Studenten und für Forscher, hat 2015 einen Textbook Excellence Award („Texty“) gewonnen). 
William F. Ruddiman: Earth's Climate: Past and Future. W. H. Freeman, 2008, ISBN 978-0-7167-8490-6 (Einführung in die Klimageschichte, häufig zitiert, für das Quartär sind besonders die Teile III und IV relevant). 
Harry John Betteley Birks und Hilary H. Birks: Quaternary Palaeoecology. Blackburn Press, 2004, ISBN 1-930665-56-3 (Einführung in die quartäre Paläoökologie, Kapitel 1 und Kapitel 2 online).
Mike Walker: Quaternary Dating Methods: An Introduction. Wiley, 2013, ISBN 978-1-118-70009-9 (häufig zitiertes einführendes Werk zu Datierungsmethoden). 
Jay Stratton Noller, Janet M. Sowers und William R. Lettis: Quaternary Geochronology: Methods and Applications. Wiley, 2000, ISBN 0-87590-950-7.
Tobias Krüger: Die Entdeckung der Eiszeiten – Internationale Rezeption und Konsequenzen für das Verständnis der Klimageschichte. Schwabe-Verlag, Basel, 2008, ISBN 978-3-7965-2439-4.
R. H. Grapes, David Oldroyd, A. Grigelis (Hrsg.): History of Geomorphology and Quarternary Geology, Geological Society of London Special Publication 301, 2008
E&G – Quaternary Science Journal – erschien zwischen 1951 und 2005 als „Eiszeitalter und Gegenwart“, herausgegeben von der Deutschen Quartärvereinigung (DEUQUA), freier Zugang, englisch- und deutschsprachige Artikel, darunter viele mit Bezug zu Deutschland
Quaternary Science Reviews – seit 1982 erscheinende englischsprachige Fachzeitschrift mit vielen Übersichtsarbeiten, hat unter den auf das Quartär spezialisierten Zeitschriften den mit Abstand höchsten Impact Factor
Quaternary Research – erscheint seit 1970, englischsprachig, international und interdisziplinär ausgerichtete Zeitschrift des Quaternary Research Center der University of Washington mit einem Schwerpunkt auf Mensch-Umwelt Wechselwirkungen im Quartär
Journal of Quaternary Science – englischsprachig, seit 1986, vor allem Originalarbeiten zur gesamten Quartärforschung, wird für die Quaternary Research Association (QRA) veröffentlicht, mit einem Schwerpunkt auf interdisziplinäre Arbeiten von internationalem Interesse
Quaternary International – erscheint seit 1989, englischsprachige Zeitschrift der International Union for Quaternary Research (INQUA), die das gesamte Spektrum der in der Quartärforschung zur Anwendung kommenden Naturwissenschaften abdecken will, publiziert vor allem Forschungsarbeiten, die zuvor auf Veranstaltungen der INQUA vorgestellt wurden
The Holocene – seit 1991, veröffentlicht ausschließlich Arbeiten zum Holozän, hat einen hohem Anteil an Beiträgen über das Wechselverhältnis von Mensch und Umwelt

Quarz, auch Tiefquarz oder α-Quarz genannt, ist ein Mineral mit der chemischen Zusammensetzung SiO2 und trigonaler Symmetrie. Er ist die auf der Erdoberfläche stabile Form (Modifikation) des Siliciumdioxids und nach den Feldspaten das zweithäufigste Mineral der Erdkruste. Bei einer Temperatur von über 573 °C (unter einem Druck von 1 bar) geht Tiefquarz durch Änderung der Kristallstruktur in Hochquarz über.
Mit einer Mohshärte von 7 gehört Quarz zu den harten Mineralen und dient als Bezugsgröße auf der bis 10 (Diamant) reichenden Skala nach Friedrich Mohs. Er bildet oft gut entwickelte Kristalle von großer Formen- und Farbenvielfalt, deren Kristallflächen Glasglanz aufweisen. Quarz besitzt keine Spaltbarkeit, bricht muschelig wie Glas und zeigt auf den Bruchflächen einen fettigen Glanz.
In der Industrie ist Quarz eines der wichtigsten Minerale und hat gleichermaßen als Baustoff wie als Rohstoff für die Keramik-, Glas- und Zementindustrie weltweite Bedeutung. Quarzkies und gebrochener Quarz sind Rohstoff zur Gewinnung von Silicium.
Darüber hinaus werden Quarz und seine farbigen Varietäten seit alters her als Schmuckstein geschätzt (siehe Verwendung).
Quarzkristalle werden auch künstlich hergestellt: Daraus geschnittene Schwingquarze dienen als Taktgeber in elektronischen Schaltungen und Quarzuhren.
Gelegentlich wird Quarz mit dem Calcit verwechselt, kann jedoch durch seine größere Härte, die niedrigere Doppelbrechung und die Reaktion des Calcits mit verdünnter Salzsäure leicht von diesem unterschieden werden.
Der Ausdruck ist erstmals in der 1. Hälfte des 14. Jahrhunderts im Ostmitteldeutschen als Fachwort des böhmischen Bergbaus bezeugt (quarz). Zu seiner Herkunft gibt es verschiedene Hypothesen. Nach einer dieser Hypothesen stammt er von mhd. querch (‚Zwerg‘) in Anlehnung an den Aberglauben, dass Berggeister das bergbaulich wertlose Mineral unterschöben, nachdem sie ein an gleicher Stelle ursprünglich vorhandenes wertvolles Erz „geraubt“ hätten (vgl. auch die Etymologie von Kobalt). Eine andere Hypothese besagt, dass Quarz von kwardy, einem Ausdruck aus einem polnischen Dialekt, stammt, welches der tschechischen Vokabel tvrdý (‚hart‘) entspricht. Einer Hypothese von Sergei Iwanowitsch Tomkeieff aus dem Jahr 1942 zufolge soll das Wort eine Zusammenziehung des sächsischen Bergmannsausdruckes „Querklufterz“ sein. Tomkeieff setzt dabei die Form quertz, querz als Grundform voraus, die sich in Schriften aus dem 16. Jahrhundert wie Ein nützlich Bergbüchlin und Agricolas De re metallica mit Bezug auf den Sprachgebrauch sächsischer Bergleute findet. Allerdings werden schon in dem 1889 erschienenen siebenten Band des Grimmschen Wörterbuchs ähnliche, seit dem 16. Jahrhundert existierende Herleitungen, die von einer Zusammenziehung aus quaterz, quaderz (‚böses Erz‘) oder aus gewarz, gewärze (Sammelbegriff zu Warze) ausgehen, als „spielende versuche, den ursprung des wortes zu ergründen“ bezeichnet.Quarz hat sich als Mineralbezeichnung international durchgesetzt, mit leichten, sprachspezifischen Abwandlungen wie beispielsweise quartz im Englischen und Französischen, kvarts im Schwedischen, quarzo im Italienischen oder кварц (kwarz) im Russischen.
Nach der 8. und 9. Auflage der strunzschen Systematik der Minerale gehört Quarz aufgrund seiner chemischen Zusammensetzung zur Mineralklasse der Oxide mit einem Metall-Sauerstoff-Verhältnis von 1:2.
In der 8. Auflage der Mineralsystematik ist er zudem Namensgeber für eine Gruppe chemisch ähnlicher oder gleicher Minerale, der Quarzgruppe, deren weitere Mitglieder Coesit, Cristobalit, Melanophlogit, Mogánit, Opal, Stishovit und Tridymit sind.
Die 9. Auflage der Mineralsystematik nach Strunz untergliedert die Oxide allerdings feiner. Quarz sowie die ihm verwandten Minerale Beta-Quarz (Existenz bisher nur als Synthese bekannt) Coesit, Cristobalit, Melanophlogit, Mogánit, Seifertit, Opal, Stishovit und Tridymit werden nun der Unterabteilung (Chemische Verbindungen) Mit kleinen Kationen: Kieselsäure-Familie zugerechnet. Das in der Systematik ebenfalls mit aufgeführte Lechatelierit (Kieselglas) hat allerdings nach wie vor einen fraglichen Mineralstatus und ist daher von der International Mineralogical Association (IMA) auch nicht als eigenständiges Mineral anerkannt.
Die Systematik von James Dana ordnet die Minerale nach ihrer Kristallstruktur. Im Quarz ist Silicium tetraedrisch von vier Sauerstoffatomen umgeben. Diese SiO4-Tetraeder sind über ihre Ecken zu einem dreidimensionalen Gerüst verknüpft, und Quarz wird daher in der Systematik von Dana den Gerüstsilikaten zugeordnet.
Quarz ist eine sehr reine Verbindung und baut andere Elemente nur in Spuren ins Kristallgitter ein. Natürliche Quarze können zwischen 13 und 15.000 ppm (meist aber nur einige 100 ppm) Al3+, zwischen 9 und 1400 ppm Na+, zwischen 3 und 300 ppm K+, sowie geringere Mengen an Fe3+, Ti4+, P5+, H+ und Li+ enthalten.
Der Einbau dieser Ionen erfolgt zumeist über einen gekoppelten Ersatz (Substitution) eines Si4+-Ions durch ein dreiwertiges und ein einwertiges Ion, so etwa Al3+ und Na+. Die Fremdionen werden sowohl auf den Si-Positionen im Gitter eingebaut wie auch auf ansonsten leeren Zwischengitterplätzen. Der Einbau von Eisen und Aluminium ist zusammen mit der Einwirkung von ionisierender Strahlung verantwortlich für die verschiedenen Farben der Quarzvarietäten.
Tiefquarz ist trigonal-trapezoedrisch (Kristallklasse 32) und kristallisiert in den enantiomorphen Raumgruppen P3121 (Nr. 152)Vorlage:Raumgruppe/152 und P3221 (Nr. 154)Vorlage:Raumgruppe/154. Die Maße der Elementarzelle sind a1 = a2 = 4,9124 Å und c = 5,4039 Å. Eine Elementarzelle enthält drei Formeleinheiten SiO2. Silicium (Si) und Sauerstoff (O) besetzen kristallographisch unterscheidbare Atompositionen:
O: x = 0,4139; y = 0,2674; z = 0,2144Jedes Sauerstoffion ist von zwei Siliciumionen im Abstand von 1,6054 Å und 1,6109 Å umgeben und sechs Sauerstoffionen im Abstand von ca. 2,62 Å. Die Si-O-Bindungen haben einen großen kovalenten Anteil, was die Ursache für die große Härte von Quarz ist. Der Si-O-Si-Bindungswinkel beträgt 143,61°.
Entsprechend ist jedes Siliciumion tetraedrisch von vier Sauerstoffionen umgeben, zwei im Abstand von 1,6054 Å und zwei im Abstand von 1,6109 Å.
SiO2-Gerüst: Die SiO4-Tetraeder sind untereinander über die Tetraederecken verknüpft, jeder Tetraeder mit vier benachbarten Tetraedern. In Richtung der c-Achse sind sie zu Paaren von spiralförmigen Ketten verknüpft. Diese SiO4-Tetraederhelixpaare, die untereinander nicht verbunden sind, bilden sechsseitige, offene Kanäle in Richtung der c-Achse.
α-Quarzkristalle der beiden enantiomorphen Raumgruppen unterscheiden sich im Drehsinn der Tetraederschrauben. Linkshändischer α-Quarz kristallisiert in der Raumgruppe P3121 (Nr. 152)Vorlage:Raumgruppe/152 und die Tetraederschrauben winden sich im Uhrzeigersinn um die c-Achse dem Betrachter entgegen, wenn man von oben auf die c-Achse schaut. Entsprechend winden sich die Tetraederschrauben des rechtshändigen α-Quarzes (Raumgruppe P3221 (Nr. 154)Vorlage:Raumgruppe/154) entgegen dem Uhrzeigersinn dem Betrachter entgegen. Die spiralförmigen Tetraederketten sind mit sechs benachbarten Tetraederspiralen so verknüpft, dass jeder SiO4-Tetraeder zu zwei benachbarten Tetraederketten gehört und an zwei der sechsseitigen Kanäle grenzt.
Quarz ist nur bei niedriger Temperatur in der trigonalen α-Quarz-Phase stabil. Bei 573 °C findet eine Phasenumwandlung in die hexagonale β-Quarz-Phase statt. Die höhere Symmetrie des β-Quarzes führt unter anderem zum Verlust der piezoelektrischen Eigenschaften. Den Übergang von der β-Quarz-Phase zum α-Quarz kann man sich leicht vereinfacht durch Kippen robuster Tetraeder um die <100>-Achse veranschaulichen. Die Kipprichtung entscheidet über die Orientierung des α-Quarzes.
Gut ausgebildete Kristalle sind verbreitet und ihre Form kann je nach Wachstumsbedingungen recht unterschiedlich sein. Die nebenstehende Abbildung illustriert die typische prismatische Kristallform von Linksquarz und wie sich diese Form aus den Grundkörpern der trigonal-trapezoedrischen Klasse (Klasse 32) zusammensetzt. Die in Klammern gesetzten Zahlen im Text und auf der Abbildung sind die Millerschen Indizes. Sie werden in der Kristallographie für die Bezeichnung von Kristallflächen verwendet. Indizes von Kristallflächen werden in runde Klammern gesetzt, Indizes von einer Flächengruppe, die einen Grundkörper bilden, in geschweifte Klammern und Indizes von Richtungen (Kristallachsen) in eckige Klammern.
Dominiert wird die Kristallform vom hexagonalen Prisma I. Stellung ({1010}). Die Prismenflächen liegen parallel zur kristallographischen c-Achse. Begrenzt wird das Prisma an den Enden vom positiven und negativen Rhomboeder ({1011} und {0111}), wobei das positive Hauptrhomboeder mit größeren Flächen auftritt.
Untergeordnet, d. h. kleiner ausgebildet, treten verschiedene trigonale Trapezoeder, meist {5161}, und trigonale Bipyramiden, meist {1121}, auf. Von diesen Polyedern gibt es in der Kristallklasse 32 jeweils zwei enantiomorphe (linke und rechte), ansonsten aber identische Formen. An einem unverzwillingten Quarzkristall treten entweder nur rechte oder nur linke Trapezoeder und Bipyramiden auf, am Linksquarz (Raumgruppe P3121 (Nr. 152)Vorlage:Raumgruppe/152) linke Formen und am Rechtsquarz (Raumgruppe P3221 (Nr. 154)Vorlage:Raumgruppe/154) rechte Formen. Unterschieden werden können Rechts- und Linksquarze anhand der Anordnung der Trapezoeder- und Bipyramidenflächen. Beim Linksquarz treten diese links von den Hauptrhomboederflächen {1011} auf und beim Rechtsquarz rechts von den Hauptrhomboederflächen.
Tessiner Habitus: Quarze, deren Kristallform von großen, sehr steilen Rhomboederflächen dominiert werden.
Skelettquarz: Bei schnellem Kristallwachstum in übersättigten Lösungen erfolgt das Wachstum besonders entlang der Kristallkanten und Ecken. Es bilden sich rahmenartig hervorgehobene Kanten um tiefer gelegene Kristallflächen (Rahmenquarz). Mitunter wachsen diese tiefer liegenden Kristallflächen von den hervorstehenden Kanten her wieder zu, wobei sich dünne Quarzscheiben über einem Hohlraum bilden (Fensterquarze).
Würfelquarz: Quarze, deren Kristallform von den Rhomboederflächen {1011} dominiert wird. Der Winkel zwischen diesen Flächen beträgt beim Quarz 85,5°, was diesen Kristallen einen würfeligen Habitus verleiht.
Zepterquarz: Wächst auf einem Quarzkristall in Richtung längs der Hauptachse eine zweite, junge Generation, bilden sich sogenannte Zepterquarze. Die „Töchter“ sind meist klarer als der Mutterkristall. Erfolgt das spätere Kristallwachstum nur an einem Ende des Kristalls, bildet sich die charakteristische, zepterförmige Kristallform heraus.
Fadenquarz: Ein Fadenquarz entsteht, wenn während des Kristallwachstums ein Kluftriss auftritt und den Kristall auseinanderreißt. Während des Öffnens der Kluft wächst der Kristall von beiden Seiten des Risses aus wieder zusammen. Der Riss selbst bleibt als dünner „Faden“ im Kristall sichtbar. Er tritt an der geschliffenen und polierten Oberfläche wie Schleifspuren und Anhäufung von feinen Löchern in Linien (Streifen) zu Tage.
Friedländer Quarz: Quarzkristalle mit Flächenstreifung auf den Flächen des sechsseitigen Prismas (1010) quer zur kristallographischen c-Achse bzw. zum Prisma.
Phantomquarz: Erfolgt das Kristallwachstum in mehreren Phasen, sind die verschiedenen Wachstumsstufen in klaren Kristallen durch einschlussreiche Zonen sichtbar.Weitere Namen sind für bestimmte Verwachsungen mehrerer Kristalle gebräuchlich:
Sprossenquarze oder Artischockenquarze: Quarze, die aufgrund von Gitterfehlern viele einzelne Tochterkristalle ausgebildet und so artischockenförmige Aggregate gebildet haben.
gewundene Quarze (Gwindel): Parallelverwachsung mehrerer plattiger Kristalle entlang einer Prismenfläche, wobei die kristallographischen Hauptachsen der Einzelkristalle nicht in einer Ebene liegen, sondern gegeneinander verdreht sind.
Die beiden chiralen Formen des Quarzes, Rechtsquarz und Linksquarz, treten zuweilen auch orientiert miteinander verwachsen auf.
Brasilianer Zwilling: Als Brasilianer Zwilling bezeichnet man die orientierte Verwachsung der beiden enantiomorphen Formen des Tiefquarzes, Rechts- und Linksquarz parallel zur Prismenfläche (1120). Brasilianer Zwillinge sind oft feinlamellar und typisch für Amethyst. Dort finden sich Brasilianer Zwillingslamellen konzentriert in den {101}-Rhomboedersektoren. Der Einbau von Eisenspuren in die Quarzstruktur scheint eine wichtige Rolle für die Bildung der feinlamellaren Brasilianerzwillinge von Amethysten zu spielen. Entsprechend der Konzentration der Zwillingslamellen in den {101}-Rhomboedersektoren zeigen Amethyste eine höhere Eisenkonzentration in diesen Sektoren. In der seltenen Varietät Ametrin (zweifarbige Quarzkristalle) wird diese Sektorzonierung sichtbar. Die etwas eisenärmeren Sektoren sind violett und die etwas eisenreicheren Zonen gelb.
Dauphinée-Zwilling (auch Schweizer oder alpines Zwillingsgesetz): Als Dauphinée-Zwilling bezeichnet man die Durchdringung von zwei Tiefquarzkristallen mit gleichen Drehsinn, so dass die Flächen der positiven Rhomboeder {h0hl} des einen Kristallindividuums mit den Flächen der negativen Rhomboeder {0hhl} des anderen Kristallindividuums zusammenfallen. Die Zwillingsachse ist entweder [0001] oder [1011]. Die pyro- und piezoelektrischen Effekte der beiden Kristallindividuen heben sich dabei gegenseitig auf. Dauphinée-Zwillinge sind daher für die meisten technischen Anwendungen ungeeignet.
Japaner Zwilling: Verzwillingung von Tiefquarz nach der Dipyramide II Stellung (1122). Die Prismenachsen der verzwillingten Kristalle schneiden sich hierbei im Winkel von 84° 33’, was den Zwillingen eine charakteristische, herzförmige Form verleiht.
Quarz zeigt einen starken piezoelektrischen Effekt senkrecht zur Prismenachse entlang der a-Achsen. Auf Druck oder Zug reagiert ein Quarzkristall mit einer elektrischen Polarisierung entlang der Kraftrichtung. Umgekehrt führt das Anlegen einer elektrischen Gleichspannung zu einer Dehnung oder Stauchung des Kristalls. Wird eine Wechselspannung mit geeigneter Frequenz angelegt, so kann der Kristall zu Resonanzschwingungen angeregt werden. Die Resonanzfrequenz ist dabei von der Geometrie (Form und Größe) des Kristalls abhängig. Aufgrund der Regelmäßigkeit und Genauigkeit dieser Schwingungen werden Schwingquarze in Quarzoszillatoren als Zeitbasis und Taktgeber für elektronische Schaltungen eingesetzt, zum Beispiel in Uhren, Computern, Geräten der Digitaltechnik und der Funktechnik.
Durch die Kristallisation des Quarzes in einer enantiomorphen Struktur wird die Schwingungsebene des Lichtes, das einen Tiefquarz in Richtung der c-Achse durchquert, gedreht. Die Angabe exakter Messergebnisse dieser Drehung erweist sich als schwierig, da Messergebnisse aufgrund verschiedener Störfaktoren wie unerkannter Verzwillingungen von Rechts- und Linksquarz oder kleinster Verunreinigungen stark streuen. Zusätzlich erschweren Fertigungstoleranzen die Herstellung exakt orientierter Quarzschnitte. Weiterhin ist die Stärke der Drehung der Schwingungsebene des Lichtes abhängig von der Wellenlänge des Lichtes (Beispiel: Natrium-D-Linie: 589,3 nm, Grünfilter für Quecksilberdampflampen: 546 nm). So schwankt die Angabe des optischen Drehvermögens bei Quarz je nach Quelle und Wellenlänge zwischen 21 und 28 °/mm. Andererseits eignet sich bearbeiteter Quarz in Form von Quarzplatten hervorragend zur Überprüfung von Polarimetern.
Quarz ist ein sehr häufiges Mineral und in zahlreichen Vertretern aller drei Gesteinsklassen zu finden.
So kristallisiert er bei der Abkühlung SiO2-reicher Schmelzen und ist primärer Bestandteil von SiO2-reichen Plutoniten (Quarzreiche Granitoide, Granite, Granodiorite, Tonalite, Quarz-Syenite, Quarz-Monzonite, Quarz-Diorite) sowie der entsprechenden Vulkanite (Rhyolithe, Dacite, Andesite, Quarz-Trachyte, Quarz-Latite). Die Quarzgehalte dieser Gesteine sind eines der Hauptkriterien für ihre Klassifikation nach dem Streckeisendiagramm.
Quarz ist auch in vielen metamorphen Gesteinen enthalten (z. B. in Hornfelsen, Phylliten und Gneisen). Dort ist er entweder vom Ausgangsgestein ererbt oder er wird über zahllose Mineralreaktionen während der Gesteinsmetamorphose gebildet. So markiert zum Beispiel die Reaktion von Chloritoid und Alumosilikat zu Staurolith und Quarz die Grenze zwischen Grünschieferfazies und Amphibolithfazies bei Metapeliten.
Wegen seiner Verwitterungsbeständigkeit ist Quarz überdies ein häufiger Konstituent feinkörniger (aber nicht der feinkörnigsten) klastischer Sedimentgesteine (in erster Linie zu nennen: Sandsteine) sowie von Böden, die sich auf einem quarzreichen Gestein entwickelt haben.
Gut ausgebildete Quarzkristalle mit Sammlerwert entstehen hingegen bevorzugt in Klüften, hydrothermalen Gängen (als sogenannte Gangart) und als Auskleidung natürlicher Höhlungen, sogenannter Geoden.
Quarz ist die auf der Erdoberfläche stabile Form (Modifikation) des kristallinen Siliciumdioxids. Zahlreiche weitere Modifikationen treten bei höheren Drücken und Temperaturen auf. Einige können metastabil an der Erdoberfläche erhalten bleiben.
Bei niedrigen Temperaturen (70–200 °C) kristallisiert aus SiO2-Gel ein Gemisch aus Quarz und Mogánit, einem charakteristischen Bestandteil von Quarzin und Chalcedon.
Bei Temperaturen oberhalb von 573 °C (bei 1013,2 hPa) wandelt sich Quarz in Hochquarz um. Die Phasenumwandlung erfolgt sehr schnell, und Hochquarz bleibt auch bei rascher Abkühlung nie metastabil erhalten. Zwar finden sich in einigen Magmatiten Quarzkristalle mit der Kristallform von Hochquarz (Paramorphose), strukturell handelt es sich jedoch um Quarz.
Bei höheren Temperaturen wandelt sich Hochquarz erst in Tridymit um (ab 867 °C), dann in Cristobalit um (ab 1470 °C). Cristobalit schmilzt bei 1727 °C (Temperaturen jeweils bezogen auf 1013,2 hPa).
Die Umwandlungstemperaturen sind abhängig vom Druck. Allgemein nehmen sie mit steigenden Drücken zu.
Bei hohem Druck, wie er im Erdmantel herrscht oder bei Meteoriteneinschlägen auftritt, bilden sich besonders dichte SiO2-Phasen. Ab 2 GPa bildet sich Coesit (3,01 g/cm³), ab 7,5 GPa Stishovit (4,3 g/cm³) und ab ca. 78 GPa Seifertit (4,12 g/cm³).
Reiner Quarz ist vollkommen transparent und farblos und wird, wenn er gut ausgebildete Kristalle entwickelt, als Bergkristall bezeichnet. Quarze sind meist durch mikroskopische Einschlüsse von Flüssigkeiten und Gasen milchig trüb (Milchquarz) und erscheinen im Gestein eingewachsen grau. Unter der Bezeichnung Rheinkiesel sind zudem durchsichtige bis milchig trübe Rollstücke aus Bergkristall bekannt, die vorwiegend aus dem Alpenraum stammen und im Rheinkies gefunden werden.Durch den Einbau färbender Ionen (im Allgemeinen Fe3+ oder Fe2+), Einschluss farbiger Minerale oder Einwirkung von ionisierender Strahlung können Quarze unterschiedlich gefärbt sein. Anhand der Farbe und deren Ursache werden folgende Varietäten unterschieden:
Amethyst: violette Färbung durch das Zusammenspiel von eingelagerten Eisenionen und Bestrahlung mit Gammastrahlen
Ametrin: seltene Quarzvarietät, die Sektoren mit Amethyst- und Citrinfärbung an einem Kristall zeigt
Prasiolith (Grünquarz): lauchgrüner und durchsichtiger Quarz, der selten natürlich vorkommt und auch durch Brennen von Amethyst oder gelblichen Quarzen künstlich erzeugt wird
Rauchquarz (Morion): durch natürliche oder künstliche Gammastrahlen graubraun (rauchfarben) bis schwarz (Morion) gefärbt
Blauquarz (Saphirquarz): blaues, undurchsichtiges Aggregat mit eingelagerten Krokydolith-Fasern oder Dumortierit. Je nach Art des Einschlusses wird Blauquarz auch präziser als Krokydolith-Quarz oder als Dumortierit-Quarz bzw. Dumortieritquarz bezeichnet.
Erdbeerquarz ist eine Varietät und Handelsbezeichnung für einen durch rotbraune Hämatiteinschlüsse unregelmäßig rosa bis rot gefärbten Quarz. Er ist meist transparenter und in der Farbe kräftiger als der Rosenquarz.
Prasem (Smaragdquarz): lauchgrünes, undurchsichtiges Aggregat, das seine Farbe durch Einschlüsse von Aktinolith erhält.
Rosenquarz: durch Dumortieriteinschlüsse trüber, rosa gefärbter Quarz, gelegentlich mit Asterismus durch Einlagerung feinster Rutilnadeln
Unter mikrokristallinem Quarz versteht man massige Aggregate von sehr feinkristallinem Quarz mit Kristallgrößen im Mikrometerbereich. Hier unterscheidet man drei Formen:
Chalcedon: mikrokristalliner, faseriger Quarz, faserig gewachsen entlang einer Prismenfläche [11-20] („length-fast“).
Achat, Onyx: mikrokristallin faserige Quarze mit parallelfaserigem (parabolischem) oder sphärolithischem Gefüge
Quarzin: mikrokristalliner, faseriger Quarz, faserig gewachsen entlang der Basisfläche (0001) des hexagonalen Prismas („length-slow“).Amethystquarz ist eine undurchsichtige, gebänderte Verwachsung von Amethyst und Milchquarz.
Alle Formen von mikrokristallinem Quarz weisen eine große Dichte an Gitterbaufehlern und Verzwillingungen auf.
Hornstein und Flint (Feuerstein) sind Verwachsungen von mikrokristallinem Quarz mit Mogánit in einem regellosen, granularen Gefüge. Hierbei handelt es sich strenggenommen nicht um Minerale und Mineralvarietäten, sondern um Gesteine, die auch unter dem Oberbegriff Chert zusammengefasst werden. Hierunter werden bisweilen auch Chalcedon und seine Erscheinungsformen sowie amorphes SiO2 (Opal) subsumiert.
Aventurin-Quarz, Falkenauge, Tigerauge, Katzenaugen-Quarz: Quarze mit Einschlüssen plattiger oder faseriger Minerale wie Fuchsit, Rutil, AsbestDer oft im Handel zu findende Aqua Aura ist keine Varietät, sondern meistens Bergkristall (oder ein anderer Quarz), der mit Metall (vorwiegend Gold) bedampft wurde. Resultat ist ein transparenter, blau gefärbter Kristall, zum Teil mit vielfarbigem Schimmer.
Brasilit ist dagegen die Handelsbezeichnung für eine durch Brennen grünlich-gelb bis blassgelb gefärbten Quarz.
Im Safiental (Graubünden, Schweiz) wurden die weltweit ersten Funde des Mantelquarzes gefunden, dessen Spitze ein wenig im Prisma versenkt ist.
Quarz besitzt eine relativ große Härte und die Eigenschaft, bei kurzzeitiger starker mechanischer Beanspruchung scharfkantig zu brechen. Daher wurde dieses Mineral in seinen verschiedenen Erscheinungsformen, einschließlich Hornfels, Quarzit und insbesondere Flint, schon in der Altsteinzeit von den Vertretern der Gattung Mensch als Rohstoff für vielerlei Werkzeuge und Waffen verwendet. Obwohl er schwieriger zu bearbeiten ist als Flint, dominieren in einzelnen Fundstätten Steinartefakte aus makrokristallinem Quarz (Gangquarz, sogar Bergkristall) sofern dieser in unmittelbarer Nähe verfügbar ist, speziell in Afrika südlich der Sahara.Bevorzugt unbearbeitete Quarzitknollen oder rohes Felsgestein wurden als Schlagsteine zur Steinbearbeitung verwendet. Größere Brocken aus makrokristallinem Quarz wurden zudem als Kochsteine bevorzugt, da sie infolge rascher Temperaturwechsel weniger leicht platzen können.
Quarzsand bzw. -pulver ist zusammen mit Kaolin und Feldspat ein Zuschlagstoff für Porzellan und eine Vielzahl weiterer Keramikwerkstoffe.
Quarzsand bzw. gemahlenes Quarzgestein wird geschmolzen zur Glas- und Quarzglas-Herstellung. Quarzglas ist ein aus (kristallinem) Quarz beziehungsweise Siliciumdioxid erschmolzener, glasartig erstarrter Feststoff; die korrekte Bezeichnung ist daher Kieselglas. Quarzglas und auch künstliche Quarz-Einkristalle (Reiner Bergkristall) werden zu optischen Prismen und Linsen geschliffen. Verwendet wird Quarzglas auch in Normmaßstäben und Normgewichten sowie als Faden für Torsionswaagen und als Lichtwellenleiter.
Quarz und Quarzglas reagieren nur mit wenigen Chemikalien. Flusssäure ist die einzige Säure, die Quarz aufzulösen vermag, dabei bilden sich Siliciumtetrafluorid beziehungsweise Hexafluorokieselsäure. Diese Eigenschaft ist förderlich für eine Vielzahl von Anwendungsgebieten:
Bei der Wirbelschichtverbrennung wird Quarzsand mit der Luft verwirbelt, um die Wärmeübertragung zu verbessern und den Verbrennungsvorgang zu optimieren.
Seine hohe Festigkeit, die Pflanzenbewuchs verhindert, führt zum Einsatz des Minerals als Eisenbahnschotterkörper. Quarz ist als Straßenschotter ungeeignet, da er zu hart ist, schlecht bindet und einen raschen Verschleiß der Autoreifen verursacht.
Quarzsand dient als Schleifmittel und Füllstoff sowie zur Lichtbogenlöschung in Schmelzsicherungen. Zudem wird er zur Herstellung von Wasserglas und Silikatfarben verwendet. Mit Polymeren gemischt dient er zudem als Werkstoff, um harte Oberflächen für Fußböden und Arbeitsplatten zu schaffen.
Die piezoelektrischen Eigenschaften des Quarzes werden bei Schwingquarzen ausgenutzt, die bei geeigneter Erregung durch eine elektrische Spannung mit einer festen Frequenz mechanisch schwingen. Der Bau sehr genau gehender Quarzuhren wurde so möglich. Heute finden sich in praktisch allen elektronischen Geräten Schwingquarze als Taktgeber. Daneben ist Quarz auch geeignet für Druckmessungen, in der Hochfrequenztechnik sowie als akustooptischer Güteschalter in Lasern.
Die beiden chiralen Formen des Quarzes, Rechtsquarz und Linksquarz, zeigen einen gegensätzlichen piezoelektrischen Effekt. In solchen Zwillingen heben sich daher die piezoelektrischen Effekte im Gesamtkristall auf, weshalb sie für piezoelektrische Anwendungen unbrauchbar sind und gegenüber synthetischen Quarzen seltener eingesetzt werden. Für technische Anwendungen werden die Zwillinge häufig parallel zur (01-1)-Ebene (AT-Schnitt) oder (023)-Ebene (BT-Schnitt) geschnitten, da der piezoelektrische Effekt senkrecht zu diesen Ebenen nahezu unabhängig von der Temperatur ist.
Quarzvarietäten wie der Achat, der violette Amethyst, der zitronengelbe Citrin, der blutrote Jaspis oder der schwarz-weiß gestreifte Onyx werden wegen der großen Härte und der guten Schneid- und Polierbarkeit des Minerals in der Schmuckindustrie zu Schmucksteinen verarbeitet.
Dringt kieselsäurereiches Grundwasser in das Gewebe abgestorbener holziger Pflanzen ein, so können diese durch das Auskristallisieren von mikrokristallinem Quarz (Si(OH)4 → SiO2 + 2 H2O) fossilisieren, wobei das holzige Gewebe zwar durch Quarz ersetzt wird, die ursprüngliche Zellstruktur jedoch oft erhalten bleibt. Paläobotaniker können daraus heute zum Beispiel Schlüsse zu den einstigen Wachstumsbedingungen der Pflanze ziehen. Bekannt sind auch versteinerte Araukarien-Zapfen aus Patagonien.
Verkieselungen gibt es auch von Tieren. Dabei wird oft ein vormals aus Calciumcarbonat (CaCO3) bestehendes Außenskelett bzw. Gehäuse durch mikrokristallinen Quarz ersetzt. Bekannt sind beispielsweise verkieselte Korallen aus dem Miozän von Florida und der Trias von British Columbia und Alaska, opaleszierende Schnecken, Muscheln und Wirbeltierreste aus der Unterkreide des Lightning Ridge in Australien sowie verkieselte Schnecken vom Dekkan-Trapp (Oberkreide) in Indien. Wenn das Gehäuseinnere dieser Schnecken nach der weitgehenden Zersetzung der Weichteile nicht vollständig mit Sediment verfüllt worden war, können darin auch achatartige Drusen ausgebildet sein.
Beim Abbau und der Verarbeitung von Quarz können erhebliche Mengen von Quarzfeinstaub entstehen, der, über Monate oder Jahre hinweg täglich eingeatmet, zu der unter Bergleuten gefürchteten Silikose und im Extremfall sogar zu Lungenkrebs führen kann.Jedoch kommt es beim Schleifen der Edelsteine nie zur Staubbildung, da der Schleifvorgang immer mit Wasser, Emulsion, Petroleum oder einem speziellen Schleiföl ausreichend gekühlt wird. Ein Trockenschliff würde auch die meisten Edelsteine beschädigen oder zerstören.
In der Esoterik gilt reiner Quarz (Bergkristall) als Heilstein, der vor schädlichen Strahlen bewahren, Kopfschmerzen und verschiedene Entzündungen lindern, Leber und Niere reinigen und die Durchblutung (Krampfadern) stärken soll.Quarz in der Varietät Bergkristall ist dem Tierkreiszeichen Löwe und in der Varietät Quarz-Katzenauge dem Steinbock zugeordnet. Alternativ kann Quarz beziehungsweise Bergkristall aber auch den Tierkreiszeichen Stier, Zwillinge oder Schütze zugeordnet sein. Als „Monatsstein“ steht Bergkristall für den April und als „Planetenstein“ nach Richardson und Huett (1989) neben dem Tigerauge für den Saturn und neben mehreren weiteren Mineralen für den Neptun.Den verschiedenen Varietäten wie dem gelben Citrin oder dem violetten Amethyst werden zudem überwiegend Eigenschaften zugeschrieben, die sich aus der Mythologie ihrer Farbe ableiten lassen, zum Beispiel gelb für Energie und violett für Spiritualität. In der biologisch-dynamischen Landwirtschaft wird das Präparat Hornkiesel eingesetzt.
Friedrich Klockmann: Klockmanns Lehrbuch der Mineralogie. Hrsg.: Paul Ramdohr, Hugo Strunz. 16. Auflage. Enke, Stuttgart 1978, ISBN 3-432-82986-8, S. 521–526 (Erstausgabe:  1891). 
P. J. Heaney, C. T. Prewitt, G. V. Gibbs (Hrsg.): Silica. Physical Behavior, Geochemistry and Materials Applications. Mineralogical Society of America, Washington 1994, ISBN 0-939950-35-9. 
Petr Korbel, Milan Novák: Mineralien-Enzyklopädie (= Dörfler Natur). Edition Dörfler im Nebel-Verlag, Eggolsheim 2002, ISBN 978-3-89555-076-8, S. 88–95.
David Barthelmy: Quartz Mineral Data. In: webmineral.com. Abgerufen am 21. Januar 2019 (englisch). 

Ein Qubit (für „Quantenbit“ selten auch Qbit) (['kju.bɪt] oder [k'bɪt]) ist ein beliebig manipulierbares Zweizustands-Quantensystem; also ein System, das nur durch die Quantenmechanik korrekt beschrieben wird und das nur zwei durch Messung sicher unterscheidbare Zustände hat.
Qubits bilden in der Quanteninformatik die Grundlage für Quantencomputer und die Quantenkryptografie. Das Qubit spielt dabei die analoge Rolle zum klassischen Bit bei herkömmlichen Computern: Es dient als kleinstmögliche Speichereinheit, und definiert gleichzeitig ein Maß für die Quanteninformation.
Als Zweizustands-Quantensystem ist das Qubit das einfachste nichttriviale Quantensystem überhaupt. Der Begriff „Zweizustandssystem“ bezieht sich hierbei nicht etwa auf die Zahl der Zustände, die das System annehmen kann. In der Tat kann jedes nichttriviale quantenmechanische System prinzipiell unendlich viele verschiedene Zustände annehmen. Allerdings kann im Allgemeinen der Zustand eines Quantensystems durch Messung nicht sicher bestimmt werden, sondern durch die Messung wird zufällig einer der möglichen Messwerte der gemessenen Observablen ausgewählt, wobei die Wahrscheinlichkeit jedes Messwertes durch den vor der Messung vorliegenden Zustand bestimmt wird. Da zudem die Messung den Zustand ändert, kann dieses Problem auch nicht durch mehrmaliges Messen am gleichen System umgangen werden.
Jedoch gibt es zu jeder Messung bestimmte Zustände, bei deren Vorliegen vor der Messung der Messwert mit absoluter Sicherheit vorausgesagt werden kann, die sogenannten Eigenzustände der Messung bzw. der gemessenen Observablen. Dabei gibt es zu jedem möglichen Ergebnis mindestens einen solchen Zustand. Die maximale Anzahl möglicher Messwerte erhält man dabei für Messungen, bei denen es jeweils nur genau einen Zustand gibt, der diesen Messwert sicher liefert. Darüber hinaus liegt nach jeder Messung ein zum erhaltenen Messwert zugehöriger Eigenzustand vor (Kollaps der Wellenfunktion); liegt jedoch bereits vor der Messung ein Eigenzustand der Messung vor, so wird dieser nicht verändert.
Zwei Zustände, die man durch Messung sicher unterscheiden kann, nennt man auch orthogonal zueinander. Die maximale Anzahl der möglichen Messwerte bei einer Messung, und somit auch die maximale Anzahl orthogonaler Zustände, ist eine Eigenschaft des Quantensystems. Beim Qubit als Zweizustandssystem kann man also durch Messung genau zwei verschiedene Zustände sicher unterscheiden. Will man demnach ein Qubit einfach als klassischen Speicher verwenden, so kann man darin genau ein klassisches Bit speichern. Allerdings liegen die Vorteile des Qubits gerade in der Existenz der anderen Zustände.
Ein Beispiel hierfür ist die Polarisation eines Photons („Lichtteilchens“). Die Polarisation von Licht gibt an, in welche Richtung Licht schwingt. Obwohl die Polarisation eigentlich eine Welleneigenschaft ist, kann sie auch für das einzelne Photon definiert werden, und alle Polarisationen (linear in beliebige Richtung, zirkular, elliptisch) sind auch für einzelne Photonen möglich. Lineare Polarisation kann beispielsweise über einen doppelbrechenden Kristall gemessen werden. Wo ein an einer bestimmten Stelle in den Kristall eintretendes Photon herauskommt, hängt davon ab, ob es parallel oder senkrecht zur optischen Achse des Kristalls polarisiert ist. Es gibt also sozusagen zwei „Ausgänge“, einen für parallel und einen für senkrecht polarisierte Photonen. Stellt man an beide Stellen einen Photon-Detektor, dann kann man so feststellen, ob das Photon parallel oder senkrecht zur optischen Achse polarisiert war.
Photonen, die eine andere Polarisation (linear in einem anderen Winkel, zirkular oder elliptisch) aufweisen, kommen aber ebenfalls an diesen „Ausgängen“ heraus. An welchem „Ausgang“ ein solches Photon herauskommt, ist in diesem Fall jedoch nicht voraussagbar; nur die Wahrscheinlichkeit kann vorhergesagt werden. Hinterher hat es jedoch die Polarisation, die zu dem entsprechenden Ausgang gehört, wie man z. B. dadurch nachweisen kann, dass anstatt des Detektors weitere Kristalle (mit parallel ausgerichteter optischer Achse) mit je zwei Detektoren an den „Ausgängen“ angebracht werden: Nur diejenigen Detektoren an den zweiten Kristallen, die zu der jeweils korrekten Polarisation für den Ausgang des ersten Kristalls gehören, registrieren Photonen.
Der Kristall zeichnet damit eine Polarisationsrichtung aus. Welche es jedoch ist, kann man dadurch bestimmen, dass man den Kristall dreht. Zwei linear polarisierte Zustände sind also zueinander orthogonal, wenn die Polarisationsrichtungen zueinander orthogonal sind. Allerdings kann diese Korrespondenz nicht direkt auf andere Polarisationszustände übertragen werden; so sind z. B. der linkszirkulär und der rechtszirkulär polarisierte Zustand ebenfalls zueinander orthogonal.
Wie bei klassischen Bits können auch mehrere Qubits zusammengefasst werden, um größere Werte zu speichern. Ein 
   klassische Bits so speichern, dass die komplette Information zuverlässig wieder ausgelesen werden kann; beispielsweise kann ein „Quantenbyte“ aus 8 Qubits 256 verschiedene zuverlässig wieder auslesbare Werte speichern.
Viel wichtiger für die Verwendung in Quantencomputern ist die Existenz verschränkter Zustände mehrerer Qubits. In solchen Zuständen hat ein einzelnes Qubit überhaupt keinen definierten Zustand, die Gesamtheit der Qubits jedoch schon. Dies führt zu nichtlokalen Korrelationen, wie sie beim Einstein-Podolsky-Rosen-Paradoxon auftreten.
Die Verschränkung der Qubits hat überraschende Folgen. Beispielsweise kann man in einem Paar verschränkter Qubits zwei klassische Bits so speichern, dass beide Bits getrennt durch Manipulation nur eines der Qubits beliebig manipuliert werden können. Jedoch benötigt man beide Qubits, um die Information auszulesen.
Auch auf der Nichtlokalität der Verschränkung beruht die Quantenteleportation, mit der sich quantenmechanische Zustände durch Übermitteln klassischer Bits übertragen lassen.
   klassischer Bits gleichzeitig dargestellt werden kann. Beispielsweise kann mit 4 Qubits ein Zustand hergestellt werden, der genau die Bitfolgen 0000, 0101, 1011 und 1110 enthält und keine anderen. Im Extremfall sind alle möglichen Bitfolgen darin enthalten, z. B. enthält ein entsprechend präpariertes „Quantenbyte“ alle Zahlen von 0 bis 255 gleichzeitig. Führt man nun mit Hilfe quantenmechanischer Operationen Berechnungen auf diesem Zustand aus, so werden diese Berechnungen effektiv auf allen diesen Bitfolgen gleichzeitig ausgeführt. Dieser sogenannte Quantenparallelismus ist der Grund dafür, dass Quantencomputer bestimmte Probleme schneller lösen können als klassische Computer. Allerdings kann man die gespeicherten Bitmuster nicht einzeln auslesen; jede Messung liefert nur einen zufällig ausgewählten der gespeicherten Werte. Um den Quantenparallelismus zu nutzen, müssen daher zusätzlich spezifisch quantenmechanische Transformationen vorgenommen werden, die keine klassischen Äquivalente haben, also Zustände, die genau einem Bitmuster entsprechen, in Überlagerungen mehrerer Bitmuster überführen können und umgekehrt.
Theoretisch kann jedes quantenmechanische Zweizustandssystem als Qubit verwendet werden. In der Praxis jedoch sind viele Systeme ungeeignet, da sie nicht in ausreichendem Maße manipulierbar sind oder zu stark von der Umgebung gestört werden. Zudem ergibt sich das Problem der Skalierbarkeit: Manche Implementierungen, wie z. B. die Verwendung der Kernspinresonanz in Molekülen, eignen sich prinzipbedingt nur für eine sehr beschränkte Zahl von Qubits.
Für die Verwendbarkeit eines Systems als Qubit hat David DiVincenzo sieben Kriterien aufgestellt. Die ersten fünf Kriterien betreffen auch die Verwendung in Quantencomputern, die letzten beiden gelten speziell für die Quantenkommunikation.
Das System muss wohldefinierte Qubits besitzen und skalierbar sein, d. h. es muss prinzipiell auf beliebig viele Qubits erweiterbar sein.
Es muss möglich sein, die Qubits in einem reinen Zustand zu präparieren (mindestens in den Zustand 
Das System muss die Implementierung eines universellen Satzes von Quantengattern erlauben. Ein Beispiel wäre z. B. alle 1-Qubit-Gatter und zusätzlich das CNOT-Gatter.
Es muss möglich sein, jedes einzelne der Qubits gezielt zu messen.Die zwei zusätzlichen Kriterien für Quantenkommunikation lauten:
Ein Austausch der beweglichen Qubits muss zwischen entfernten Orten möglich sein.In der Praxis werden unter anderem die folgenden Systeme untersucht:
Ein vielversprechender Ansatz für Quantencomputer ist die Verwendung von Ionen in Ionenfallen. Hierbei werden einzelne Ionen durch elektromagnetische Felder im Vakuum wie an einer Perlenkette aufgereiht.
Die Qubits werden dabei durch jeweils zwei langlebige interne Zustände der einzelnen Ionen, zum Beispiel zwei Hyperfeinniveaus des Grundzustands, gebildet. Die Zahl der Qubits ist identisch mit der Zahl der Ionen in der Falle. Die Manipulation der Qubits erfolgt über Laser, die mit den einzelnen Ionen wechselwirken. Über die Bewegung der Ionen in der Falle lassen sich die Qubits miteinander koppeln und so verschränken.
Ein weiterer Ansatz ist die Verwendung von Quantenpunkten. Quantenpunkte sind quasi-nulldimensionale Halbleiterstrukturen, in denen Elektronen nur diskrete Zustände einnehmen können; man spricht daher auch oft von Designer-Atomen. Ein Vorteil der Quantenpunkt-Technologie ist, dass bei der Herstellung erprobte Halbleiter-Methoden angewandt werden können. Als Basiszustände des Qubits können zwei Orientierungen des Spins bei fester Elektronenzahl ("Spinqubit") oder zwei verschiedene Ladungskonfigurationen ("Ladungsqubit"; z. B. mit ein Elektron entweder im ersten oder zweiten von zwei Quantenpunkten) oder eine Kombination dieser beiden Möglichkeiten verwendet werden. In der Praxis dominieren Spinqubits aufgrund der viel längeren Kohärenzzeiten. Bisher wurde universelle Kontrolle von bis zu zwei Spinqubits demonstriert.
Auch mit SQUIDs lassen sich Qubits implementieren. SQUIDs sind Systeme aus Supraleitern, die durch zwei parallele Josephson-Kontakte verbunden sind. Die Manipulation der Qubits erfolgt über die angelegte Spannung und das Magnetfeld. Die Basiszustände können hier über den Wert der relative Phase, Ladung oder des magnetischen Flusses durch den SQUID
bestimmt werden. Quantenprozessoren (mit voller Kontrolle) von bis zu 10 Qubits wurden bisher realisiert. Es wurden noch deutlich größere Josephson-Quanteregister (mit bis zu 72 Qubits) demonstriert, allerdings ohne volle Kontrolle über deren Zustand.
Auch die Spins der Atomkerne in Molekülen können Qubits repräsentieren. Sie können über Kernspinresonanz manipuliert und ausgelesen werden können. Dies ist eine technisch besonders einfache Methode, die jedoch nicht den oben genannten DiVincenzo-Kriterien entspricht. Insbesondere ist die Methode nicht skalierbar, da die Zahl der Spins pro Molekül beschränkt und ihre Adressierung und kontrollierte Kopplung um so schwieriger ist, je mehr Spins adressiert werden müssen. Zudem kann hierbei nicht ein einzelnes System (also ein einzelnes Molekül) gemessen werden, sondern man hat es mit vielen gleichartigen Molekülen auf einmal zu tun. 
Dagegen kann man mit Kernspins in Festkörpern im Prinzip skalierbare Architekturen realisieren. Besonders vielversprechend sind hier z. B. die Kernspins von Fremdatomen in Silizium oder von Stickstoff-Fehlstellen-Zentrum (oder anderen Farbzentren) in Diamant.
Mit Photonen lassen sich besonders gut bewegliche Qubits definieren. In der Regel werden als Basiszustände verschiedene Teilchenzahl-Eigenzustände des elektromagnetischen Felds verwendet. Eine häufige Realisierung ist das Polarisationsqubit, das durch zwei orthogonale Polarisationen eines Photons definiert ist. Eine andere wichtige Realisierung ist das Time-Bin-Qubit, das sich über ein Photon entweder im ersten oder zweiten von zwei aufeinanderfolgenden Zeitfenstern definiert. Ferner gibt es auch Qubits, die durch Vielphotonenzustände definiert sind, wie zum Beispiel durch kohärente Zustände entgegengesetzter Phase.
Photonische Qubits können per Glasfaser oder durch die Luft problemlos auch über größere Strecken übertragen werden. Experimente zur Quantenkommunikation und Quantenkryptographie verwenden daher nahezu ausschließlich Photonenzustände. Da es dagegen sehr anspruchsvoll ist, Photonen miteinander in Wechselwirkung zu bringen, sind sie für die Realisierung eines Quantencomputers weniger geeignet, auch wenn das im Prinzip möglich ist.
Ähnlich wie klassische Information lässt sich auch Quanteninformation komprimieren. Hierbei wird angenommen, dass das Signal aus zufällig ausgewählten reinen Zuständen aus einem „Alphabet“ besteht, wobei jedoch diese Zustände nicht notwendigerweise zueinander orthogonal sein müssen, d. h. es muss nicht möglich sein, die Zustände durch Messung sicher zu unterscheiden. Diese Zustände werden in ein System aus Qubits kodiert (dabei wird der Original-Zustand notwendigerweise zerstört) und diese an den Empfänger gesandt, der dann aus den gesendeten Qubits eine Näherung des originalen Zustandes rekonstruiert.
Die Genauigkeit (Fidelity) einer solchen Kodierung ist definiert durch die zu erwartenden Übereinstimmung des rekonstruierten Zustands mit dem ursprünglichen. Das heißt, angenommen der Empfänger weiß, welche Zeichen gesendet wurden, und führt an seinem rekonstruierten Zustand jeweils eine Messung aus, für die der ursprüngliche Zustand ein Eigenzustand ist, dann ist die Genauigkeit der Kodierung durch den Anteil der Messungen gegeben, die den gesendeten Zustand ergeben.
Eine ideale Kodierung ist nun analog zur klassischen Informationstheorie eine Übertragung, bei der die minimale Zahl an Qubits übertragen werden muss, um bei einer hinreichend großen Anzahl übertragener Zeichen eine beliebig hohe Übertragungswahrscheinlichkeit zu erreichen.
Es zeigt sich nun, dass die minimale Zahl von Qubits, um einen solchen Zustand zu übertragen, gerade die Von-Neumann-Entropie der durch das „Alphabet“ und die zugehörigen Wahrscheinlichkeiten definierten Dichtematrix ist. Somit kann das Qubit, analog zum klassischen Bit, als Informationseinheit der Quanteninformation betrachtet werden; die Von-Neumann-Entropie eines Quantensystems gibt dann gerade dessen Informationsgehalt in Qubits an.
Zur Beschreibung eines Qubits nimmt man eine beliebige Messgröße (z. B. im Beispiel mit den Photonen die Polarisation parallel und senkrecht zur optischen Achse eines doppelbrechenden Kristalls) und nennt die zugehörigen Eigenzustände 
   dient zur Kennzeichnung, dass es sich um einen Quantenzustand handelt, siehe auch Dirac-Notation). Das quantenmechanische Superpositionsprinzip fordert nun, dass es unendlich viele Zustände dieses Systems gibt, die sich formal als
  sind. Der Zustand lässt sich also als normierter Vektor in einem komplexen Vektorraum, genauer gesagt, einem Hilbertraum, beschreiben. (Im Fall der Photonen handelt es sich gerade um den Jones-Vektor, der die Polarisation beschreibt). Allerdings ist die Beschreibung nicht eindeutig; zwei Vektoren, die sich nur durch einen Faktor der Form 
   („Phasenfaktor“) unterscheiden, beschreiben denselben Zustand. Zu beachten ist jedoch, dass ein solcher Phasenfaktor für nur eine der Komponenten durchaus einen Unterschied macht: Die Vektoren 
Alternativ lässt sich das Qubit auch über seine Dichtematrix beschreiben. Für das Qubit im Zustand 
  Im Gegensatz zum Zustandsvektor ist die Dichtematrix eindeutig definiert. Mit Hilfe der Dichtematrix lassen sich auch Qubits beschreiben, deren Zustand nicht vollständig bekannt ist (sogenannte „gemischte Zustände“). Allgemein lässt sich die Dichtematrix für ein Qubit angeben durch
    {\displaystyle \rho ={\frac {1}{2}}\left(\mathbf {1} +\sum _{i=1}^{3}c_{i}\sigma _{i}\right),\quad c_{1}^{2}+c_{2}^{2}+c_{3}^{2}\leq 1}
Die Zustände eines einzelnen (unverschränkten) Qubits lassen sich als Punkte auf der Oberfläche einer Kugel im dreidimensionalen Raum darstellen. Diese Oberfläche nennt man nach Felix Bloch Bloch-Kugel oder -sphäre. Besonders deutlich sieht man das am Spin-1/2-Teilchen, wo der Punkt auf der Kugel angibt, in welche Richtung man mit Sicherheit Spin up messen wird. Die Äquivalenz gilt aber für alle Zweizustandssysteme. Das Bild rechts zeigt, wie die oben beschriebenen Polarisationszustände auf der Bloch-Kugel angeordnet werden können. Beispielsweise entspricht der „Nordpol“ hier der vertikalen und der „Südpol“ der horizontalen Polarisation. Allgemein entsprechen zueinander orthogonale Zustände einander gegenüberliegenden Punkten auf der Bloch-Kugel.
   die Winkel des Punktes in Kugelkoordinaten (siehe Bild links), so wird der zugehörige Zustand durch den Vektor
    {\displaystyle \left|\psi \right\rangle =\sin(\theta /2)\mathrm {e} ^{-\mathrm {i} \phi /2}\left|0\right\rangle +\cos(\theta /2)\mathrm {e} ^{\mathrm {i} \phi /2}\left|1\right\rangle }
Auch die Punkte im Inneren der Kugel lassen sich interpretieren: Man kann ihnen Qubits zuordnen, über deren Zustand man keine vollständige Information hat. Die kartesischen Koordinaten des Punktes in der Kugel sind dann gerade die Faktoren 
   vor den Pauli-Matrizen in der Gleichung (*). Der Mittelpunkt der Kugel entspricht somit einem Qubit, über das man überhaupt nichts weiß; je weiter man sich vom Mittelpunkt entfernt, desto größer wird das Wissen über den Zustand des Qubits. Diese Kugel ist in gewisser Weise das Analogon zum Wahrscheinlichkeits-Intervall [0,1] für das klassische Bit: Die Punkte am Rand geben die möglichen exakten Zustände des Bits (0 oder 1) bzw. des Qubits an (in der Quantenmechanik spricht man auch von „reinen Zuständen“), während die Punkte im Inneren unvollständiges Wissen über das Bit/Qubit repräsentieren (in der Quantenmechanik spricht man hier von „gemischten Zuständen“). Der Punkt in der Mitte repräsentiert in beiden Fällen komplettes Unwissen über das System (beim Bit: Wahrscheinlichkeit 1/2).
Auch der Vorgang des Messens lässt sich anhand der Bloch-Kugel schön darstellen: Im Bild "Kugelkoordinaten" kennzeichnet der kleine rote Punkt einen möglichen Zustand des Qubits. In diesem Fall sitzt der Punkt außen auf der Kugel, es handelt sich also um einen reinen Zustand; das Verfahren funktioniert aber auch für gemischte Zustände. Da die Eigenzustände der Messung zueinander orthogonal sind, also auf der Bloch-Kugel einander gegenüberliegen, definiert die Messung eine Gerade durch den Mittelpunkt der Kugel (im Bild durch die blaue Linie gekennzeichnet). Man betrachtet nun entlang dieser Geraden den Durchmesser (im Bild grün/weiß) durch die Kugel und projiziert den Punkt, der das aktuelle Wissen über das Qubit darstellt, senkrecht auf diese Strecke (die Projektion ist hier durch die rote Ebene und die gelbe Linie markiert; der Schnittpunkt der gelben Linie mit dem Durchmesser ist der projizierte Punkt). Diese Strecke lässt sich dann direkt als Wahrscheinlichkeitsintervall für das Messergebnis ansehen. Wenn man das Messergebnis nicht ausliest, dann gibt dieser Punkt innerhalb der Kugel in der Tat auch die neue Beschreibung des Systems an; nach Auslesen des Messergebnisses liegt der Punkt selbstverständlich (wie auch beim normalen Bit) an einem Ende der Strecke. Setzt man z. B. im Bild an den „Nordpol“ der Kugel den Zustand 
  , dann ist das Verhältnis der Länge des weißen Teils des Durchmessers (vom Südpol bis zum Schnittpunkt mit der Ebene) zum Gesamtdurchmesser gerade die Wahrscheinlichkeit, das Qubit nach der Messung im Zustand 
   zu finden, wenn der Zustand vorher durch den roten Punkt gegeben war (hinterher sitzt der Zustand in diesem Fall natürlich auf dem Nordpol).
Einige Physiker vermuten in diesem Zusammenhang zwischen Qubits und Punkten im dreidimensionalen Raum den Grund dafür, dass unser Raum dreidimensional ist. Prominenter Vertreter dieser Idee ist die Ur-Theorie von Carl Friedrich von Weizsäcker. Weizsäckers Ur ist dabei im Wesentlichen das, was heute Qubit genannt wird.
Auch die Zustände eines Systems aus mehreren Qubits bilden aufgrund des Superpositionsprinzips einen Hilbertraum. Dieser ist das Tensorprodukt der Hilberträume der einzelnen Qubits. Das bedeutet, ein System aus 
  -dimensionalen Hilbertraum beschrieben, dessen Basiszustände als direkte Produkte der Einzel-Qubit-Zustände geschrieben werden können, also z. B.
    {\displaystyle \left|0100\right\rangle =\left|0\right\rangle _{1}\otimes \left|1\right\rangle _{2}\otimes \left|0\right\rangle _{3}\otimes \left|0\right\rangle _{4}}
  wobei die Indizes angeben, zu welchem Qubit der Zustand jeweils gehört. Jedes direkte Produkt von 1-Qubit-Zuständen ergibt einen 
    {\displaystyle {\frac {1}{\sqrt {2}}}\left(\left|0\right\rangle _{1}+\left|1\right\rangle _{1}\right)\otimes {\frac {1}{\sqrt {2}}}\left(\left|0\right\rangle _{2}-\left|1\right\rangle _{2}\right)={\frac {1}{2}}\left(\left|00\right\rangle -\left|01\right\rangle +\left|10\right\rangle -\left|11\right\rangle \right)}
  -Qubit-Zustände lassen sich nicht als Produkt von Ein-Qubit-Zuständen schreiben. Ein Beispiel für so einen Zustand ist der 2-Qubit-Zustand 
    {\displaystyle {\frac {1}{\sqrt {2}}}\left(\left|00\right\rangle +\left|11\right\rangle \right)}
   (einer der Bellzustände). Solche Zustände, die sich nicht als Produkt einzelner Zustände schreiben lassen, nennt man verschränkt. Die Beschreibung eines einzelnen Qubits in einem verschränkten Zustand ist nur über eine Dichtematrix möglich, was wiederum die Unkenntnis (bzw. Nichtberücksichtigung) von Information über das Qubit anzeigt: In diesem Fall handelt es sich bei der fehlenden Information gerade um die Verschränkung mit anderen Qubits. Allerdings kann der vollständige Zustand auch nicht beschrieben werden, indem die Dichtematrizen für jedes einzelne Qubit angegeben werden. Die Verschränkung ist vielmehr eine nichtlokale Eigenschaft, die in den Korrelationen zwischen den miteinander verschränkten Qubits zum Ausdruck kommt.
Zwei Observablen sind komplementär, wenn die vollständige Kenntnis des Wertes der einen Observablen die vollständige Unkenntnis der anderen impliziert. Da völlige Unkenntnis über den Wert gleichbedeutend ist mit Projektion auf den Mittelpunkt der Bloch-Kugel in der oben angegebenen Beschreibung der Messung, ergibt sich unmittelbar, dass zueinander komplementäre Observablen durch zueinander orthogonale Richtungen in der Bloch-Kugel beschrieben werden. Dementsprechend findet man für ein einzelnes Qubit stets genau drei paarweise zueinander komplementäre Observablen, entsprechend den drei Raumrichtungen.
Hat man viele gleich präparierte Exemplare eines Qubits, so kann der Zustand durch Messen der Wahrscheinlichkeiten eines Satzes dreier paarweise komplementärer Observablen bestimmt werden (wobei jede Messung an einem neuen Exemplar gemacht werden muss, da die Messung den ursprünglichen Zustand zerstört hat). Aus den Wahrscheinlichkeiten ergeben sich dann unmittelbar die Koordinaten des den Zustand beschreibenden Punktes auf der Bloch-Kugel, und damit der Zustand.
Wie bei klassischen Bits können auch bei Qubits äußere Einflüsse nicht vollständig eliminiert werden. Deshalb benötigt man auch hier Fehlerkorrekturcodes. Im Gegensatz zu klassischen Fehlerkorrekturcodes gibt es für die Fehlerkorrektur von Qubits jedoch wichtige Einschränkungen:
Der Kollaps der Wellenfunktion sorgt dafür, dass jede Messung, die Information über den Zustand eines Qubits liefert, diesen Zustand zerstört.
Da Qubits, anders als klassische Bits, ein Kontinuum von Zuständen erlauben, können auch Fehler kontinuierlich sein.Trotz dieser Einschränkungen ist jedoch eine Fehlerkorrektur möglich. Dies ist möglich, weil man zur Korrektur eines Fehlers nicht wirklich dessen Ergebnis braucht, sondern nur wissen muss, welcher Fehler aufgetreten ist. Ist z. B. ein sogenannter Bit-Flip aufgetreten, der 
   miteinander vertauscht, so ist klar, dass das Problem behoben wird, indem ein weiterer Bit-Flip vorgenommen wird; es ist nicht nötig, den tatsächlichen Zustand des Qubits zu kennen.
Die Einschränkung des No-Cloning-Theorems ist nicht so gravierend wie es scheint, denn man kann trotzdem ein Qubit durch zwei Zustände eines Systems aus mehreren Qubits darstellen. Nur hat man dann eben im Allgemeinen keine Kopien, sondern einen Satz verschränkter Zustände.
Das Problem der kontinuierlichen Fehler wird durch das Superpositionsprinzip gelöst: Das Ergebnis einer kleinen Störung durch einen bestimmten Fehlertyp lässt sich quantenmechanisch auffassen als Überlagerung zweier Zustände: Einer, wo dieser Fehler überhaupt nicht aufgetreten ist, und einer, in dem dieser Fehler maximal aufgetreten ist. Misst man nun, ob der Fehler aufgetreten ist, so sorgt der Kollaps der Wellenfunktion dafür, dass genau einer dieser beiden Fälle vorgefunden wird; man hat es daher nur noch mit einer begrenzten Zahl von diskreten Fehlern zu tun.
  -Qubit-Fehler sind Kombinationen dieser Fehlertypen für jedes einzelne Qubit (also z. B. Qubit 1 ist ohne Fehler, aber Qubit 2 hat einen Bit-Flip gemacht). Wiederum wird ein allgemeiner Fehler durch eine Linearkombination beschrieben; dadurch lassen sich auch so komplizierte Fehlertypen beschreiben wie „Qubit 1 hatte einen Phasenfehler, sofern Qubit 2 
Ein einfaches Beispiel ist der Repetition-Code. Hierbei wird die Information einfach symmetrisch auf mehrere Qubits verteilt. Beispielsweise wird bei drei Qubits der Wert 
   codiert. Bei dieser Codierung ist es mit drei Qubits bereits möglich, Bit-Flip-Fehler sicher zu korrigieren. Nutzt man stattdessen zwei Bell-Zustände als Basis, lassen sich Phasenfehler korrigieren. Die Kombination beider Mechanismen führt zum von Peter Shor entwickelten sogenannten Shor-Code, in dem mittels 9 Qubits alle drei elementaren Fehlertypen korrigiert werden können. Fehlerkorrektur ist jedoch auch mit weniger Qubits möglich, so hat Andrew Steane einen Fehlerkorrekturcode entwickelt, der mit nur 7 Qubits pro gespeichertem Qubit auskommt.
M. Homeister: Quantum Computing verstehen. Vieweg-Verlag, Wiesbaden 2015, vierte Auflage, ISBN 3-528-05921-4.
A.J. Leggett: Quantum computing and quantum bits in mesoscopic systems. Kluwer Academic, New York 2004, ISBN 0-306-47904-4.
R.J. Lipton, K.W. Regan: Quantum Algorithms via Linear Algebra: A Primer MIT Press, Cambridge MA 2014, ISBN 978-0-2620-2839-4.
M.A. Nielsen, I.L. Chuang: Quantum Computation and Quantum Information. Cambridge University Press, Cambridge 2000, ISBN 0-521-63503-9.
O. Morsch: Quantum bits and quantum secrets – how quantum physics is revolutionizing codes and computers. Wiley-VCH, Weinheim 2008, ISBN 978-3-527-40710-1.
W. Scherer: Mathematik der Quanteninformatik Springer-Verlag, Berlin-Heidelberg 2016, ISBN 978-3-6624-9079-2.
C.P. Williams: Explorations in Quantum Computing Springer-Verlag, London 2011, zweite Auflage, ISBN 978-1-8462-8886-9.
Physics 219 Course Information – Vorlesungsskript zu Quanteninformation und Quantencomputern (englisch)

Quechua, zu deutsch Ketschua (offiziell in Bolivien Qhichwa, in Peru meist Qichwa, im östlichen Tiefland Perus und in Ecuador Kichwa), ist eine Gruppe eng miteinander verwandter indigener Sprachvarietäten, die im Andenraum Südamerikas gesprochen werden. Es existieren unterschiedliche Ansichten dazu, inwiefern man hier von einer einzigen dialektal stark differenzierten Sprache oder einer Sprachfamilie aus mehreren Sprachen, und wenn ja wie vielen, sprechen sollte; dies ist auch abhängig davon, ob man dabei von sprachimmanenten strukturellen oder von soziolinguistischen und im weitesten Sinne identitären Kriterien ausgeht.
Verschiedene Quechua-Varianten wurden neben anderen Sprachen auch in der Kultur der Inka, aber auch in vorinkaischen Kulturen gesprochen, wobei zum Ende der Inkazeit eine Variante („klassisches Quechua“) als Lingua franca in weiten Teilen des Andenraums diente.
Das Wort Quechua an sich (auf Quechua je nach Dialekt und Schreibweise: Qhichwa, Qichwa, Qiĉwa, Kichwa oder Qheswa) bezeichnet im Quechua „Tal“ oder eine Höhenlage inklusive ihrer Bewohner, die sich deshalb auch Qhichwa runa, „Menschen der Höhenzone Quechua“, nennen, woher sich auch die Sprachbezeichnung Qhichwa simi bzw. Kichwa shimi, „Sprache der Höhenzone Quechua“, ableitet. Die Quechua-Sprecher selbst nennen ihre Sprache aber traditionell in der Regel Runa Simi (Runasimi) oder Runa Shimi (von runa „Mensch“ und simi „Mund, Wort, Sprache“, also „Menschensprache“). In modernen Quechua-Texten wird dagegen meist die Bezeichnung Qhichwa simi, Qheswa simi, Qichwa simi oder Kichwa shimi („Quechua-Sprache“ also „Tal-Sprache“) verwendet.
Eine genetische Verwandtschaft der Quechua-Sprachen mit Sprachen außerhalb dieser Gruppe ist bisher nicht nachgewiesen worden.
Auf Grund des großen gemeinsamen Wortschatzes mit dem Aymara wurden Quechua und Aymara (bzw. die Aru-Sprachen) in eine gemeinsame Sprachfamilie (Quechumaran) gestellt. Neuere Untersuchungen (Sprachvergleich) weisen jedoch darauf hin, dass das gemeinsame Vokabular auf gegenseitige Entlehnungen auf Grund des jahrtausendelangen Kontakts zurückzuführen ist.
Der Sprachraum des Quechua erstreckt sich vom Süden Kolumbiens über große Teile von Ecuador, Peru und Bolivien bis in den Norden von Chile und Argentinien. Den größten Anteil an den Sprechern hat dabei Peru, gefolgt von Bolivien und Ecuador, während in den anderen Ländern nur kleine Minderheiten die Sprache beherrschen.
Heute ist Quechua mit wahrscheinlich mehr als 7 Millionen Sprechern (die Schätzungen variieren allerdings stark) die meistgesprochene indigene Sprache Südamerikas und nimmt somit in Südamerika hinsichtlich der Sprecherzahl Rang 3 hinter Spanisch und Portugiesisch ein.
Für den größten Teil des Sprachgebiets, nämlich Peru, Ecuador und Bolivien, liegen Volkszählungsangaben zur Sprecherzahl vor (Peru: 2017, Bolivien: 2012, Ecuador: 2001). Für Kolumbien, Argentinien und Chile fehlen solche Daten völlig; es gibt lediglich Schätzungen, die, wie oben erwähnt, sehr stark variieren.
Zu den Volkszählungsergebnissen ist anzumerken, dass Kinder im Vorschulalter nicht erfasst werden. Bei der Volkszählung 2017 in Peru wurde zudem nur die Muttersprache erfasst. Bei der Volkszählung 2012 in Bolivien wurden zwar pro Person bis zu fünf Sprachen erhoben, aber nur die Daten zur Erstsprache veröffentlicht. In Ecuador wurden 2001 nur rund 500.000 Quechuasprachige (Ein- und Zweisprachige) gezählt. Unklar ist, ob die bei Volkszählungen oft beobachtete Tendenz bestand, eine als minderwertig empfundenen Sprache nicht anzugeben (underreporting); umgekehrt muss auch beachtet werden, dass ein nicht unerheblicher Teil der Sprecher sich aus unterschiedlichen Gründen im Alltag des Spanischen bedient. Insbesondere die Quechuasprachigen in den großen Städten dürften die Sprache kaum benutzen, und ihre Kinder wachsen mit spanischer Sprache auf. 
Bolivien: 1,7 Millionen (Muttersprachler laut Volkszählung von 2001; Schätzungen bis über 3 Millionen)
Brasilien: unbekanntQuechua ist neben Spanisch und Aymara Amtssprache in Bolivien und in Peru, in letztgenanntem jedoch laut Verfassung nur in den „Gebieten, wo sie [Quechua und Aymara] vorherrschen“. In Ecuador ist Kichwa (wie auch die anderen indigenen Sprachen) Amtssprache „in seinen Gebieten“.
In den meisten großen Universitäten des spanischsprachigen Südamerika wird Quechua als Fremdsprache gelehrt.
Die Varietäten des Quechua bilden ein Dialektkontinuum. Sie lassen sich in zwei große Gruppen einteilen, die nach dem peruanischen Sprachforscher Alfredo Torero als Quechua I und Quechua II bezeichnet werden. Die Einteilung in die Hauptäste Quechua I (Waywash) und Quechua II (Wampuy) sowie die Unterteilung des Quechua II in drei Unteräste (Quechua II a, Quechua II b und Quechua II c) geht auf jeweils unabhängige Untersuchungen von Alfredo Torero und Gary Parker in den 1960er Jahren zurück.
Quechua I (Waywash) wird in den meisten quechuasprachigen Gebieten der zentralen und nördlichen peruanischen Anden gesprochen. Die wichtigsten Dialekte sind Ankash (Ancashino) (im Departamento Ancash), Shawsha in der Provinz Jauja (im Departamento Junín) sowie Wanka (Huanca) in den Provinzen Huancayo und Concepción (ebenfalls im Departamento Junín), außerdem gibt es diverse Dialekte in den Departamentos Huánuco (Huallaga-Quechua), Cerro de Pasco, Tarma (Nord-Junín) und im Norden des Departements Lima (Yaru-Quechua).
Quechua II (Wampuy) umfasst alle im Süden Perus sowie in Bolivien, Argentinien, Chile, Ecuador und Kolumbien gesprochenen Varietäten sowie einen kleinen Teil der Varietäten des übrigen Perus. Es gliedert sich in drei Untergruppen. Die Yunkay-Gruppe (Quechua II a) umfasst die relativ wenigen Varietäten des Quechua II im zentralen und nördlichen Peru (Untergruppe Kashamarka-Kañaris in den Departements Cajamarca und Lambayeque sowie die Dialekte von Yauyos im Departement Lima, die fast ausgestorben sind), die Chinchay-Gruppe (Quechua II b, auch als nördliches Quechua bezeichnet) die Varietäten Ecuadors und Kolumbiens (Kichwa), die eine ganz eigene Sprachentwicklung genommen haben, wie auch einige Dialekte Nordperus (Kichwa-Sprachinseln in Amazonien). Das südliche Quechua (Quechua II c) umfasst alle Varietäten des südlichen Perus (Dialekte Chanka in den Departamentos Ayacucho, Huancavelica und dem nördlichen Apurímac sowie Qusqu-Qullaw im südlichen Apurímac und den Departamentos Cusco und Puno), Boliviens, Argentiniens und Chiles.
Unterschiede zwischen den beiden Hauptgruppen bestehen in vielen Teilen der Grammatik und des Wortschatzes, so dass eine Verständigung zwischen muttersprachlichen Sprechern von Varietäten des Quechua I und des Quechua II ohne Kenntnis weiterer Varietäten nur schwierig möglich ist. Die Varietäten des Quechua I unterscheiden sich trotz ihres geringen Verbreitungsgebietes auch erheblich untereinander, während das Quechua II im Vergleich dazu verhältnismäßig einheitlich ist. Die Unterschiede zwischen den Dialekten des südlichen Quechua II in Peru und Bolivien, die zahlenmäßig die größte Gruppe der Quechuasprecher umfassen, sind relativ gering und beschränken sich vor allem auf den Bereich der Phonetik.
Die Reichssprache des Inka-Reiches, die Sprache der meisten erhaltenen älteren Schriftzeugnisse und die Sprache der Mehrzahl der modernen Publikationen auf Quechua aus Peru und Bolivien beruht auf diesen südlichen Dialekten des Quechua II. Viele andere Varietäten des Quechua sind hingegen lediglich in der modernen linguistischen Fachliteratur beschrieben worden.
SIL International listet folgende 46 Sprachen mit entsprechenden Sprachcodes auf (die dritte Gliederungsebene stammt nicht von SIL, sondern Alain Fabre [2005]):
Klassisches Quechua [qwc] (historisch in Peru; kann als historische Entwicklungsstufe an die Seite von quy oder quz gestellt werden)
Der Streit, ob es sich beim Quechua um eine Sprache mit vielen unterschiedlichen Dialekten oder um eine Sprachfamilie handelt, und wie viele Sprachen diese Sprachfamilie gegebenenfalls umfasst, ist aufgekommen, als durch die Feldforschungen des 20. Jahrhunderts deutlich wurde, dass sich die Varietäten des Quechua teilweise sehr stark voneinander unterschieden.
Die Beurteilung dieser Frage ist auch abhängig davon, ob man von sprachimmanenten strukturellen oder von soziolinguistischen und im weitesten Sinne identitären Kriterien ausgeht und welchen Standpunkt man zur Kodifizierung von Standardvarietäten des Quechua einnimmt. Es gibt dazu höchst unterschiedliche Standpunkte, die von der Position der Academia Mayor de la Lengua Quechua in Cusco, dass es nur eine Sprache Quechua gebe und alle Sprecher derselben das heutige Quechua der Stadt Cusco (also keine Ausgleichsvariante wie etwa das Südliche Quechua) mit all seinen regionalen, neu entstandenen Besonderheiten als Schriftsprache akzeptieren sollten, bis zur Position des Summer Institute of Linguistics (SIL International) reichen, das 46 eigenständige Sprachen innerhalb der Quechua-Sprachfamilie unterscheidet.
Auf Grund der Tatsache, dass sich Sprecher zum Beispiel von Wanka und Qusqu-Qullaw nicht in ihren eigenen Sprachen verständigen können, wird der Ein-Sprachen-Standpunkt nur von sehr wenigen vertreten. Mit seiner Auffassung, so nahe miteinander verwandte Dialekte wie zum Beispiel „Quechua, Cusco“ [quz] und „Quechua, Eastern Apurímac“ [qve] als eigenständige Sprachen voneinander zu trennen, steht allerdings auch SIL International praktisch allein da und wird darum von Vertretern indigener Organisationen, insbesondere der ecuadorianischen ECUARUNARI, heftig angegriffen mit dem Vorwurf, es wolle die indigenen Völker spalten.
Das peruanische Erziehungsministerium legte 1975 sechs regionale Varianten fest und ließ für diese Wörterbücher und Grammatiken anfertigen: Cusqueño bzw. Cusco-Collao (Qusqu-Qullaw), Ayacuchano (Chanka), Huanca (Wanka), Ancashino (Ankash), Cajamarca-Cañaris und San Martín (Lamas-Quechua). Allerdings sind die Abweichungen zwischen Qusqu-Qullaw und Chanka (96 % lexikalische Übereinstimmung) geringer als die Unterschiede etwa innerhalb des Ancashino, zwischen Waylla Wanka und Shawsha Wanka oder zwischen Cajamarca und Cañaris (94 % lexikalische Übereinstimmung zwischen den letztgenannten beiden). Jüngste Entwicklungen bei der Verschriftung laufen auf die Entstehung einiger weniger Schriftsprachen hinaus.
Drei Schriftsprachen bzw. orthographische Regionalstandards mit mehr als nur lokaler Bedeutung haben sich schon mehr oder weniger etabliert:
Südliches Quechua (Chanka, Qusqu-Qullaw in Peru und Bolivien, theoretisch auch Argentinien) – das gesamte Quechua II c. In der Praxis existieren derzeit drei einander sehr ähnliche Sprachvarianten: Chanka (Peru), Qusqu-Qullaw (Peru) und Quechua in Bolivien.
Ancash-Quechua (in Zentral-Peru) – gehört zu Quechua I.Alle weiteren Quechua-Schriftsprachen betreffen ausschließlich Peru. Einige kleinere, lokale schriftsprachliche Quechua-Varianten werden zumindest ansatzweise bereits im Schulbereich verwendet:
Huánuco-Quechua (in Süd-Junín) – gehört zu Quechua IEs könnten sich auch noch weitere Schriftsprachen bzw. Standards entwickeln, zum Beispiel Shawsha Wanka (Jauja; gehört zu Quechua I), Chachapoyas-Quechua (gehört zu Quechua II) oder Yauyos-Quechua (Zwischenstellung zwischen Quechua I und Quechua II), die alle drei allerdings fast ausgestorbene Varianten repräsentieren. Möglich ist aber auch die Integration dieser Varianten in die vorgenannten Standards.
Untersuchungen der Sprachwissenschaftler Torero und Parker in den 1960er Jahren zeigten, dass es im Raum Lima den größten Variationsreichtum an Dialekten gibt bzw. gab. Deshalb wird dort, im Gegensatz zu früheren Annahmen, der Ursprung der Quechua-Sprache vermutet. In mehreren Wellen im Laufe des vergangenen Jahrtausends breitete sie sich aus, in das Gebiet von Cuzco und Bolivien wohl sogar erst im 15. und 16. Jahrhundert. Dabei wurden Sprachen der Aru-Sprachfamilie (wozu Aymara gehört und die ebenfalls ihren Ursprung im Raum Lima haben) verdrängt. 
Bereits vor über tausend Jahren spaltete sich das Proto-Quechua zunächst in zwei Sprachen (s. g. zentrales und peripheres Quechua oder Quechua I und II), später in viele Varianten bzw. verwandte Sprachen auf. Quechua – genauer gesagt vermutlich eine Variante, die dem heutigen Quechua von Ayacucho stark ähnelte – war zumindest in der Zeit vor der Conquista Staatssprache im Inka-Reich (lengua general), bis ins 15. Jahrhundert war dies jedoch wahrscheinlich Aymara gewesen. 
Seine größte Verbreitung erlangte das Quechua zwischen 1500 und 1700, als es in vielen verschiedenen Varianten zwischen Zentralargentinien und dem südlichen Kolumbien mit Unterbrechungen im gesamten Andenraum gesprochen wurde. Es gab allerdings einige Gebiete der Anden, in denen es sich nie durchsetzte: so im noch heute vorhandenen Aymara-Sprachgebiet am Titicaca-See und südlich davon sowie in Teilen Nordperus (Nord-Ancash, La Libertad, Teile des Departements Cajamarca, wo bis ins 20. Jahrhundert unter anderem Culli gesprochen wurde). An der Küste wurde Quechua insbesondere im Raum Lima gesprochen, jedoch nie an der nordperuanischen Küste, wo sich Varianten des Mochica bis Anfang des 20. Jahrhunderts hielten. Auch die Missionstätigkeit begünstigte die Ausbreitung des Quechua, das in der frühen Kolonialzeit noch die Rolle der „Lengua general“ innehatte. Erst mit dem Ende der Kolonialzeit, stärker noch ab der Zeit der unabhängigen Republiken, die von Kreolen (Weißen) regiert wurden, sank das Prestige der Sprache, die seitdem durch das Spanische immer mehr aus dem öffentlichen Leben verdrängt wurde. 
Heute besteht eine Situation, in der viele Quechua-Dialekte, insbesondere im Norden Perus, vom Aussterben bedroht sind und Quechua, genauer gesagt die „großen“ Varianten Qusqu-Qullaw, Chanka (Ayakuchu) und Ankash, nur noch im ländlichen Raum der Anden südlich ab Huancavelica bis zur bolivianisch-argentinischen Grenze, in Teilen von Ancash sowie (nördliche Dialektgruppe „Kichwa“) in einigen Sprachinseln in Amazonien und den ecuadorianischen Anden allgemeine Alltagssprache ist.
Seit der Einrichtung eines breit angelegten Schulwesens durch die Staaten Lateinamerikas war das erklärte Ziel der Regierungen, die indigene Bevölkerung zu hispanisieren (castellanización). Spanisch war deshalb die einzige Unterrichtssprache. Quechua diente entweder nur als Hilfssprache zur bloßen Verständigung, oder es war in manchen Regionen sogar in der Schule verboten. Unter Juan Velasco Alvarado in Peru wurde im Zusammenhang mit der Offizialisierung der Sprache erstmals Quechua in Schulen eingeführt, dies jedoch zunächst als Zweit- bzw. Fremdsprache für Spanischsprachige in Lima, wo es auf starken Widerstand und rassistische Vorbehalte stieß. Auf Veranlassung der peruanischen Regierung wurden für sechs von ihr anerkannte regionale Quechua-Varianten – Ancash-Huaylas, Ayacucho-Chanca, Cusco-Collao, Junín-Huanca, Cajamarca-Cañaris und San Martín – Wörterbücher und Grammatiken erstellt. Nach dem Sturz Velascos endeten zunächst praktisch alle Quechua-Schulversuche. Anfang der 1980er Jahre kam es zu einem regionalen Projekt zur Verwendung von Quechua und Aymara im peruanischen Departement Puno mit Unterstützung der deutschen Gesellschaft für Technische Zusammenarbeit (GTZ), welches jedoch nur auf Druck der deutschen Seite weitergeführt wurde.
Seit den 1990er Jahren gibt es auf internationaler Ebene Vereinbarungen lateinamerikanischer Länder zur so genannten interkulturellen zweisprachigen Erziehung (Educación Intercultural Bilingüe) IZE (spanisch EIB). In Ecuador und Bolivien, seit wenigen Jahren auch in Peru, ist die IZE in der Primarstufe ein fester Bestandteil des Erziehungswesens geworden, ohne jedoch bisher auch nur die indigene Bevölkerung flächendeckend zu versorgen. Sie beschränkt sich derzeit noch – von Ausnahmen abgesehen – auf die ländliche quechua- und aymarasprachige Bevölkerung und bezieht nicht in umgekehrter Weise die Spanischsprachigen bzw. die Städte mit ein. Dies gilt auch für die weitergehende Förderung des Quechua als gleichberechtigte Sprache in der Mittel- und Oberstufe. Tatsächlich kann das Quechua auch in der Schule nur dann erfolgreich sein, wenn seine Verwendung im modernen Berufsleben notwendig und selbstverständlich ist.
Vielfach gibt es Vorbehalte von Eltern, die fürchten, dass ihre Kinder nicht gut genug Spanisch lernen (welches sie später im Beruf brauchen), wenn sie in ihrer Muttersprache alphabetisiert werden. Wissenschaftliche Untersuchungen zeigen, dass das Gegenteil der Fall ist. Dieser tatsächliche bessere Erfolg der Schüler wie auch das subjektive bessere Empfinden haben derartige Vorbehalte teilweise verstummen lassen.
Bei der EIB wird im ersten Schuljahr Lesen und Schreiben nur in der indigenen Sprache gelehrt. Im zweiten Schuljahr kommt das Erlernen der spanischen Rechtschreibung dazu. Ab dem dritten Schuljahr nimmt der Anteil des spanischsprachigen Unterrichts zu, und die Fächer werden sowohl auf Quechua als auch auf Spanisch unterrichtet. Interkulturell soll der Unterricht in der Hinsicht sein, dass er inhaltlich auf die indigene Kultur ausgerichtet ist und zusätzlich Aspekte der europäischen („weißen“) Kultur gelernt werden, es also keine einfache „Übersetzung“ des traditionellen eurozentrischen Bildungskonzepts ist.
Anfängliche Ansätze zweisprachigen Unterrichts in Lateinamerika waren in erster Linie auf eine Optimierung des Spanischlernens ausgerichtet. Die EIB soll jedoch zum Ziel haben, die Quechua-Sprache wie die indigene Kultur überhaupt aufzuwerten und so ihren Bestand zu sichern. In einigen Fällen geht es auch um das Wiedererlernen des Quechua, so etwa bei den Quechuas Lamistas in Peru oder den Saraguros in Ecuador.
In Ecuador gibt es eine Reihe zweisprachiger Schulen mit Kichwa, die in eigener Verantwortung der indigenen Gemeinden geführt werden.
In Bolivien ist der „Bildungsrat der Quechua-Nation“ (Consejo Educativo de la Nación Quechua, CENAQ) auf nationaler Ebene für den Quechua-Unterricht zuständig. Die EIB erfasst hier bisher etwa die Hälfte der Quechua-Bevölkerung. Das Gesetz zur Bildungsreform in Bolivien von 1994 (Ley 1565) legte in Artikel 9 zwei sprachliche Modalitäten fest: Einsprachig auf Spanisch mit Erlernen einer indigenen Sprache (als Fach, für Spanischsprachige) sowie zweisprachig mit der indigenen Sprache als Erstsprache und Spanisch als Zweitsprache. Das unter Evo Morales am 20. Dezember 2010 verabschiedete Gesetz Ley educativa 070 "Avelino Siñani - Elizardo Pérez" bestimmt dagegen im Artikel 7, dass in Bevölkerungsteilen und Gemeinden mit indigener Muttersprache obligatorisch die erste Sprache in der Schule die indigene und die zweite die spanische, bei spanischer Muttersprache die erste Sprache die spanische und die zweite die in der Region gesprochene indigene sein müsse. Bolivien stellt mit dem obligatorischen Lernen einer indigenen Sprache in der Schule eine Ausnahme in Lateinamerika dar. Mangels geeigneter Lehrer ist 2016 die Bestimmung in den Städten noch nicht überall durchgesetzt, doch sollen durch Ausbildung entsprechender Lehrkräfte bis 2018 alle Schulen Boliviens erreicht sein. Darüber hinaus müssen in Bolivien seit einigen Jahren alle Staatsangestellten neben dem Spanischen eine indigene Sprache beherrschen, in den quechuasprachigen Gebieten Quechua. Auch im Fernsehen werden Quechua und Aymara zunehmend verwendet. Für den Erhalt des Quechua sind dies nach Einschätzung der Linguistin Rosaleen Howard (2014) wichtige Voraussetzungen, da EIB allein noch keinen hinreichenden Anreiz zu dessen Verwendung bietet. Die Linguistin Inge Sichra aus Cochabamba beklagte allerdings im Mai 2016 auf einer Konferenz in Peru, dass die IZE als Thema in Bolivien (nicht zu verwechseln mit dem bloßen Fachunterricht in der indigenen Sprache) auf dem Rückzug sei. Am 26. November 2016 beschloss der „Erste Kongress der Quechua-Nation“ (Qhichwa Suyup Kawsayninmanta Simikamaymanta Ñawpaq Jatun Tantakuy) mit 400 quechuasprachigen Delegierten unter Vorsitz des Generalexekutivkoordinators des Instituts für Quechua-Sprache und Kultur "Tomás Katari" (“Tumas Katari Kurusa Llawi” Qhichwa Runa Simi Kawsay Jatun Wasi) in Cochabamba, Gualberto Quispe mit 90 % Zustimmung, das neue Wörterbuch Puraq Simipirwa des Linguisten Teófilo Laime Ajacopa als lexikalische und orthographische Grundlage für einen einheitlichen Quechua-Schriftstandard in Bolivien zu verwenden. Dieses verwendet das am 9. Mai 1984 unter Hernán Siles Zuazo (DS 20227) offizialisierte Quechua-Alphabet mit 3 Vokalen und 25 Konsonanten.In Peru wurden im Lauf der 2000er Jahre – neben anderen indigenen Sprachen – im Auftrag des Bildungsministeriums Schulmaterialien in der amtlichen Rechtschreibung für die drei größeren Varianten des Quechua erarbeitet – Qusqu-Qullaw, Chanka, Anqash, aber auch die beiden Varianten Inkawasi-Kañaris und Lamas-Quechua – und an einigen Schulen in der EIB verwendet. Als eine Wende in der Sprachenpolitik Perus wird die 2011 erfolgte Verabschiedung des „Gesetzes, das Gebrauch, Schutz, Entwicklung, Wiedererlangung, Förderung und Verbreitung der ursprünglichen Sprachen Perus regelt“ (Ley Nº 29735: Ley que regula el uso, preservación, desarrollo, recuperación, fomento y difusión de las lenguas originarias del Perú) gesehen, das von der quechuasprachigen Kongressabgeordneten María Sumire initiiert und maßgeblich formuliert worden war. Mit diesem Gesetz haben Indigene und somit auch die Quechua erstmals in der Geschichte Perus einen Anspruch auf interkulturelle zweisprachige Erziehung, wobei sowohl die Sprache als auch die Selbstzuschreibung als Indigene Kriterium für die Anwendung sein können. Dieses Recht auf IZE erstreckt sich auch auf Sekundarschulen und höhere Bildung. Auf dieser Grundlage sind auch Schulen für IZE zur Wiedererlangung der indigenen Sprache eingerichtet worden, so dass beispielsweise im Sprachgebiet des Lamas-Quechua und Cajamarca-Quechua sowie in größeren Städten Schüler, die bereits mit Spanisch aufgewachsen sind, das Quechua als Zweitsprache der Schüler lernen. 2013 sind in ganz Peru 15781 Schulen als Träger für interkulturelle zweisprachige Erziehung mit Quechua als Muttersprache oder Zweitsprache anerkannt. Dieses Angebot richtet sich jedoch weiterhin nur an die Indigenen und nicht auch umgekehrt an die Spanischsprachigen zum bikulturellen und zweisprachigen Lernen. Auch werden nicht alle Quechua-Sprecher erreicht; so gibt es keine IZE mit Yauyos-Quechua (Provinz Yauyos) oder Chachapoyas-Quechua (Departamento Amazonas).Auf Grundlage dieses Sprachen-Gesetzes sind nunmehr folgende Varianten des Quechua in Peru anerkannt:
Kichwa von Nordperu (Kichwa) unter Einschluss des Lamas-Quechua.Die Region Apurímac hat einen ehrgeizigen Plan zur „Generalisierung des Quechua“ (Lliwllapaq Runasimi, Quechua para todos) mit der Laufzeit von 2008 bis 2021 entworfen, der weit über die EIB hinausgeht und alle Bereiche des öffentlichen Lebens betreffen soll. Die Regionalregierung von Cusco legte 2007 in einer Regionalverordnung obligatorischen Quechua-Unterricht in allen Stufen des Bildungssystems sowie verpflichtende Grundkenntnisse in Quechua für „jede Behörde und jeden öffentlichen Bediensteten“ fest. Noch 2013 wird jedoch von einer Missachtung dieser Verordnung und einer Diskriminierung quechuasprachiger Patienten durch einsprachig spanischsprachiges Personal in Krankenhäusern der Region Cusco berichtet. Andere sehen im Zusammenhang mit seit den 2010er Jahren zunehmenden Anforderungen an Quechua-Kenntnisse für Beschäftigungsverhältnisse im südlichen Peru ein wieder steigendes Interesse junger Menschen am Quechua.In Argentinien gab es im Jahre 2005 – trotz vieler Diskussionen darum – keine EIB mit Quechua an öffentlichen Schulen – weder im Sprachgebiet von Santiago del Estero noch bei Immigranten. Obwohl IZE seit 2006 in Gegenden Argentiniens mit indigener Sprache gesetzlich vorgeschrieben ist, wird auch 2015 von einer kaum entwickelten IZE in Argentinien gesprochen. 2014 wurden Forderungen bolivianischer Immigranten in Argentinien, die ihre Kinder auf Quechua groß zogen, nach Quechua-Unterricht in der Schule laut. In einer Schule in Treorky (Gemeinde Trelew), einem von walisischen Einwanderern gegründeten Ort, in dem nunmehr 93 % der Schüler bolivianischer Abstammung waren, wurde 2015 an einem Projekt für die Einführung von Quechua gearbeitet.
Unter den Sprechern der Quechua-Sprache in Peru, Bolivien, Ecuador, Kolumbien, Chile und Argentinien gibt es bisher kaum eine gemeinsame Identität. Nicht nur mangelnde Spanischkenntnis, sondern der Gebrauch der indigenen Sprache überhaupt ist außerhalb der Dorfgemeinschaft oft noch heute durch rassistische Vorurteile Spanischsprachiger stigmatisiert. Seit den 1980er Jahren formierte sich hiergegen in Ecuador eine Bewegung der Indigenen mit der Dachorganisation der Kichwa-Völker in Ecuador, ECUARUNARI (Ecuador Runakunapak Rikcharimuy), und in ähnlicher Form auch in Bolivien, während in Peru der bewaffnete Konflikt mit seinem Klima der Angst und dem Hass auf das Indigene als potentiellen Feind (möglichen Unterstützer der maoistischen Guerilla) dies nicht zuließ. Auf Grund der politischen Gewalt, aber wie in anderen Ländern auch aus sozioökonomischen Gründen gab es eine verstärkte Landflucht. Die Migranten in die Städte gaben in der Regel ihre indigene Identität und Sprache auf. Auch in den Kleinstädten ist das Quechua durch Zuwanderung Weißer einem Assimilationsdruck ausgesetzt. So bezeichnet das „Nationale Dokument der ursprünglichen Sprachen Perus“ von 2013 die Situation des Quechua in der Region Cusco in den meisten Distrikthauptstädten als bedroht, in den Provinzhauptstädten und der Stadt Cusco gar als ernsthaft bedroht. Die Bedeutung einer positiven indigenen Identität für den Spracherhalt wird zunehmend diskutiert. Großstädte, zu denen es in Bezug auf Quechua Forschungsprojekte gegeben hat, sind in Peru Huamanga/Ayacucho und in Bolivien Cochabamba. Bei der Förderung einer positiven Quechua-Identität als Voraussetzung für eine zukünftige Weitergabe der Sprache scheinen beispielsweise in der Stadt Ayacucho die von Frauen gegründete Organisation Chirapaq („Regenbogen“ oder auch „Regen von Sternschnuppen“) und die mit ihr verbundene Jugendorganisation Ñuqanchik („Wir“) eine Schlüsselrolle zu spielen. Jugendliche im Umfeld dieser Gruppe äußerten 2014 den Willen, das Quechua an ihre Nachkommen weiterzugeben; andere, die kein Quechua von den Eltern gelernt hatten, eigneten sich dieses später an. Bei vielen Menschen ohne eine solche Identifikation sahen sie aber diesen Willen nicht. Eine Studie von 2006 über die von Indigenen der Region Cajamarca getragene Regionale Akademie der Quechua-Sprache in Cajamarca (ARIQC) kommt zu dem Ergebnis, dass die Wiederaneignung der diskriminierten und in den Hintergrund gedrängten indigenen Sprache die Zurückweisung einer aufgezwungenen herabwürdigenden Identität bei gleichzeitiger Herausbildung einer neuen, positiven indigenen Quechua-Identität bedeute, und sieht dies in der Tradition eines jahrhundertealten kulturellen Widerstands.
Unter den verschiedenen Quechua-Varianten gibt es einige bedeutende Unterschiede in der Phonologie. Zunächst werden die Verhältnisse bei der meistverbreiteten Variante Qusqu-Qullaw (gesprochen in Cusco, Puno, Teilen von Apurímac und in Bolivien) beschrieben, im Anschluss Unterschiede bei anderen Varianten.
Quechua hat nur drei Vokale im Sinne von Phonemen: [​a​], [​i​] und [​u​] (ähnlich wie klassisches Arabisch). Einsprachige sprechen diese in der Regel aus als [æ ɪ ʊ], obwohl sie auch wie die spanischen Vokale [a i u] ausgesprochen werden können. In Nachbarschaft zu den uvularen Konsonanten [​q​], [q'] und [qh] werden sie mehr wie [​ɑ​], [​ɛ​] und [​ɔ​] gesprochen. Diese nur von den Uvularen bedingten Allophone wurden in der inzwischen nicht mehr offiziellen 5-Vokal-Orthographie mit „e“ und „o“ wiedergegeben, was in Grenzfällen zu erheblicher Verwirrung führte. Heute wird darum (außer in Lehnwörtern) nur noch „a, i, u“ geschrieben.
Die Buchstaben des nunmehr in Peru offiziellen Alphabets entsprechen dem Internationalen Phonetischen Alphabet, abgesehen von den Palatalen [tʃ ɲ ʎ j], die als „ch ñ ll y“ geschrieben werden.
Die Plosive und Frikative sind immer stimmlos; Stimmhaftigkeit ist im Stammvokabular des Quechua nicht phonemisch. In der Variante Qusqu-Qullaw – anders als in allen anderen Quechua-Varianten, jedoch so wie in Aymara, von woher diese Besonderheit wahrscheinlich stammt – hat jeder Plosiv drei Formen: einfach, ejektiv und aspiriert (gehaucht), zum Beispiel:
In sämtlichen zentralen und nördlichen Varianten des Quechua (vom Departement Junín an nördlich) gibt es zudem den postalveolaren Frikativ ​[⁠ʃ⁠]​ (geschrieben „sh“, von Linguisten auch [š], entspricht deutschem sch), der in den südlichen Varianten mit [s] zusammengefallen ist. Der Laut ​[⁠ʃ⁠]​ taucht auch in manchen Gegenden des Qusqu-Qullaw wieder auf, hat sich dort jedoch aus [tʃk] („chk“) bzw. [sj] („sy“) entwickelt (so wird er auch im offiziellen Alphabet wiedergegeben) und hat somit etymologisch nichts mit dem ursprünglichen Quechua-Laut ​[⁠ʃ⁠]​ zu tun.
In den Dialekten von Junín, Cajamarca und Lambayeque gibt es außerdem noch das retroflexe [ĉ] (gesprochen ähnlich wie ein englisches „tr“), ein ursprünglicher Quechua-Laut, der in den anderen Varianten mit ​[⁠tʃ⁠]​ („ch“) zusammengefallen ist.
Etwa 30 % des modernen Quechua-Wortschatzes stammen aus dem Spanischen, und einige spanische Laute (zum Beispiel f, b, d, g) dürften phonemischen Charakter bekommen haben, auch bei einsprachigen Quechua-Sprechern. Dasselbe gilt zunehmend für die Unterscheidung der Vokale o-u und e-i bei spanischen Lehnwörtern (so zum Beispiel: karo aus dem Spanischen caro = „teuer“, Quechua karu = „weit“).
Vor der Ankunft der Europäer existierte keine Buchstabenschrift für das Quechua. Inwieweit die zur Zeit des Inkareiches gebräuchlichen Quipu (Khipu, Knotenschnüre), die in erster Linie Inventarlisten von Vorratslagern oder ähnliches darstellten, als eine Vorform einer Ganz-Wort-Schrift aufgefasst werden können, ist umstritten.
Seit der spanischen Kolonialzeit wurde das lateinische Alphabet zur Wiedergabe des Quechua verwendet, wobei sich der Lautwert der Buchstaben zunächst meist am spanischen Vorbild orientierte. 1560 schrieb Domingo de Santo Tomás die erste Grammatik für eine Quechua-Variante an der Küste bei Lima, auf deren Grundlage lange Zeit an der Universität von Lima gelehrt wurde. 1607 und 1608 veröffentlichte Diego González Holguín Wörterbuch und Grammatik für das Quechua (von ihm „Quichua“ genannt) von Cuzco, die für die nachfolgenden kolonialen Quechua-Texte maßgeblich waren.
Eine einheitliche Rechtschreibung gab es bis ins 20. Jahrhundert nicht. Geschrieben wurde nach spanischer Orthographie, die die Lautwerte des Quechua nur sehr unvollkommen wiedergab. Im Laufe des 20. Jahrhunderts wurden mehrere konkurrierende Entwürfe für eine Rechtschreibung des Quechua in lateinischen Buchstaben vorgelegt.
In Peru legte 1975 die Regierung von Juan Velasco Alvarado ein amtliches Alphabet (Alfabeto Oficial) mit sechs regionalen Varianten fest, welches als neue Buchstaben unter anderen w, k und q enthielt, so dass die Laute des Quechua [w], [k] und [q] erstmals exakt wiedergegeben werden konnten. Gehauchte Plosive wurden durch Anhängen von h, ejektive Plosive durch Anhängen von Apostroph ausgedrückt. Im Wesentlichen gilt dieses Alphabet bis heute. In Anlehnung an die spanische Schreibweise wurden jedoch noch die fünf Vokale a, e, i, o, u verwendet. Diese Form des Alphabets wird bis heute von der Academia Mayor de la Lengua Quechua (AMLQ) in Cusco propagiert. In Bolivien wurde parallel ein sehr ähnlicher offizieller Standard entwickelt, bei dem jedoch „j“ an Stelle von „h“ sowie von „k“ und „q“ am Silbenende verwendet wurde. In Argentinien etablierten sich gleich zwei verschiedene Schreibweisen: die von Tucumán / Jujuy und die von Santiago del Estero.
Die amtlichen Rechtschreibungen Perus und Boliviens wurden einander in den achtziger Jahren angeglichen, außerdem wurde offiziell in beiden Ländern die 5-Vokal-Schreibweise 1985 durch die 3-Vokal-Schreibweise abgelöst, da diese der Phonologie des Quechua näher kommt. Statt „e“ und „o“, welche immer nur in Nachbarschaft von „q“ vorkamen, wird nunmehr „i“ bzw. „u“ geschrieben. Als Unterschied zwischen beiden Ländern bleibt noch das Zeichen für den Lautwert [h], der in Peru mit „h“ und Bolivien mit „j“ wiedergegeben wird. In Ecuador wurde die am Spanischen orientierte Rechtschreibung gleichfalls durch eine an Peru und Bolivien angelehnte Schreibweise ersetzt, wobei die Phonetik des ecuadorianischen Kichwa deutlich von Peru und Bolivien abweicht. In Argentinien steht allerdings eine Übernahme des neuen Quechua-Alphabets noch aus.
Die moderne Quechua-Rechtschreibung wird bis heute von einigen Einrichtungen, darunter die AMLQ und Vertreter von SIL International, mit dem Argument kritisiert, dass das offizielle Alphabet für Personen, die auf Spanisch Lesen und Schreiben gelernt haben, schwieriger zu begreifen sei. Dem wird allerdings entgegengehalten, dass die moderne Schreibung die Quechua-Phonologie perfekt wiedergibt. Bezüglich der Kontroverse um die Vokalwiedergabe wird auf Studien verwiesen, die zeigen, dass Alphabetisierung in Quechua mit dem 5-Vokal-System später zu stärkeren Leseschwierigkeiten im Spanischen führt als bei Alphabetisierung mit dem 3-Vokal-System.
Die Regionalregierung von Cusco erkannte 2007 das Cusco-Quechua als fünfvokalige und deshalb „vollständige“ Sprache der „großen Inka-Nation“ an. Gleichzeitig werden obligatorischer Quechua-Unterricht in allen Stufen des Bildungssystems sowie verpflichtende Grundkenntnisse in Quechua für „jede Behörde und jeden öffentlichen Bediensteten“ bestimmt. Auf gesamtstaatlicher Ebene war das von der AMLQ propagierte 5-Vokal-Rechtschreibsystem übergangsweise auf experimenteller Basis zugelassen (Resolución Directoral Nr. 155-2007), doch ist seit 2013 das 3-Vokal-System des Qusqu-Qullaw vorgeschrieben (Resolución Directoral Nr. 282-2013-ED als Bestätigung der Ministerialverfügung Nr. 1218–1985-ED).Zu beachten ist, dass in der Öffentlichkeit, besonders bei geographischen Namen auch ältere, an der spanischen Orthographie orientierte Schreibweisen noch bis heute üblich sind. So werden bekanntere Namen wie Wayna Pikchu, Saksaywaman und Qurikancha auch als Huayna Picchu, Sacsayhuaman und Coricancha bzw. Qorikancha geschrieben.
Diese Schreibweisen stehen heute im Konflikt mit dem peruanischen Gesetz. Gemäß Artikel 20 des Präsidialdekrets Nr. 004-2016-MC, veröffentlicht am 22. Juli 2016 im amtlichen Verkündungsorgan El Peruano, müssen die Toponyme durch ihre entsprechenden Schreibweisen in den normalisierten Alphabeten der indigenen Sprachen ersetzt werden. Das Nationale Institut für Geographie (Instituto Geográfico Nacional) realisiert die notwendigen Änderungen in den offiziellen Landkarten von Peru.
Der peruanische Linguist Rodolfo Cerrón Palomino, der Wanka-Quechua als Muttersprache spricht, schlägt einen einheitlichen Rechtschreibstandard vor für alle südlichen Quechua-Dialekte (d. h. ganz Südperu ab Huancavelica südwärts, Bolivien und Argentinien), den er Südliches Quechua (Quechua sureño) nennt. Dieser Standard wird inzwischen von vielen Einrichtungen Perus akzeptiert. Er beinhaltet ursprüngliche Strukturen der beiden meist gebrauchten Dialekte: Chanka (Ayakuchu, Quechua ayacuchano) und Qusqu-Qullaw (gesprochen ab Cusco südlich, in Bolivien und Argentinien). Beispiele:
Im seit 1985 offiziellen Quechua-Alphabet in Peru kommen allgemein folgende 18 Buchstaben für den ererbten Quechua-Wortschatz sowie für Entlehnungen aus dem Aymara zur Anwendung:
Im Qusqu-Qullaw werden zusätzlich folgende Buchstaben (auch für Aymara-Lehnwörter) verwendet, was insgesamt 28 Zeichen ergibt:
In den Varianten von Junín, Cajamarca und Lambayeque gibt es auch noch das ĉ (inoffiziell „tr“ geschrieben), was 20 Zeichen ergibt.
Die Buchstaben e und o werden nicht für ererbte Quechua-Wörter verwendet, da es sich bei den entsprechenden Lauten um Allophone von i und u handelt, die in Nachbarschaft zu q, qh, q' auftreten.
Folgende Buchstaben werden nur in Lehnwörtern aus dem Spanischen und anderen Sprachen (nicht aus dem Aymara) verwendet:
Quechua ist eine agglutinierende Sprache wie Türkisch und Finnisch, d. h. die Bedeutung eines Wortes wird durch das Anhängen von Silben (Suffixe) an einen unveränderlichen Wortstamm angepasst, nicht durch Beugung (Veränderung des ganzen Wortes je nach Zeit, Person, Geschlecht und Fall) wie beispielsweise im Deutschen.
Die Reihenfolge der Suffixe ist streng geregelt, wie das Beispiel des Wortes chakra (Feld) illustriert:
Der Ausdruck „meine kleinen Felder“ lautet auf Quechua folglich: chakrachaykuna.Wie die meisten agglutinierenden Sprachen ist das Quechua eine Sprache vom Typ SOP (Subjekt-Objekt-Prädikat), d. h. im Normalfall gilt eine Wortfolge wie in diesem Satz:
Michiqkunaqa wayñutam takichkanku = Die Hirten [michiqkuna] singen [takiy] (gerade) [-chka-] einen Wayñu [Art Gesang/Tanz].
Das Substantiv im Quechua kennt zwei Numeri: Singular und Plural. Letzterer wird durch Anhängen von -kuna ausgedrückt. Seine Verwendung ist nicht zwingend und wird in eindeutigen Fällen oft weggelassen.
Das Substantiv wird durch eine Reihe von Suffixen „dekliniert“, die an Stelle von Präpositionen verwendet werden. Hierzu gehören -p(a) (Genitiv), -ta (Akkusativ), -nta („durch“), -man („zu“), -manta [Quechua I: -piqta] („von, aus“), -paq („für“), -pi [Quechua I: -ĉaw] („in“), -wan („mit“). Diese Suffixe sind zum Beispiel auch dadurch sehr produktiv, dass sie Adverbien bilden (chaypi, dort; kunanmanta, ab jetzt, …).
Die Funktion des bestimmten Artikels wird teilweise vom s.g. „Topic Marker“ -qa übernommen: runaqa = der Mann.
Durch Aneinanderreihen von Substantiven werden sehr einfach und häufig zusammengesetzte Wörter gebildet, wobei das vorangestellte Nomen das Attribut ist: hatun = groß, yachay = wissen, lernen, wasi = Haus, hatun yachay wasi = Hochschule, Universität.
Das Quechua kennt kein grammatisches Geschlecht, jedoch spielt in manchen Fällen das natürliche Geschlecht eine Rolle: so ist zum Beispiel churi immer das Kind eines Mannes, wawa das Kind einer Frau. Ähnlich ist es auch bei Geschwisterbezeichnungen.
In Quechua gibt es sieben Personalpronomina. Für die erste Person Plural („wir“) hat Quechua zwei unterschiedliche Pronomina (inklusives und exklusives Wir). Eines, das inklusive, wird benutzt, wenn der Sprecher den Angesprochenen mit einschließt („wir und du“, „ich und du“). Das exklusive Pronomen wird benutzt, wenn der Angesprochene nicht einbezogen wird („wir ohne dich“).
wasi = Haus; wasiy = mein Haus; wasiyki = dein Haus; wasin = sein/ihr Haus; wasinchik = unser (auch dein) Haus; wasiyku = unser (nicht dein) Haus; wasiykichik = euer Haus; wasinku = ihr Haus.
Der Genitiv -p(a) fordert im zugehörigen Nomen, das den Besitz ausdrückt, eine Possessivendung: intip churin = Sohn der Sonne.
Die wichtigsten Demonstrativpronomen (auch mit Adjektivfunktion) im Quechua sind kay (dies), chay (das) und wak (jenes).
Die Adjektive stehen im Quechua vor den Substantiven. Es gibt kein grammatisches Geschlecht, und sie werden nicht mit den Substantiven mitdekliniert.
Adverbien werden zum einen durch Anhängen von -ta, manchmal auch -lla an ein Adjektiv gebildet: allin – allinta („gut“), utqay – utqaylla („schnell, rasch“). Zum anderen bildet man sie durch Suffixe an Demonstrativpronomen: chay („das“) – chaypi („dort“), kay („dies“) – kayman („hierher“). Darüber hinaus gibt es zahlreiche selbständige Adverbien. Auffällig ist hierbei, dass das Adverb qhipa sowohl „hinten“ als auch „zukünftig“, ñawpa dagegen „vorn“ und „vergangen“ bedeutet. Räumliche und zeitliche Konzepte der Adverbien im Quechua sind somit – ähnlich wie im Aymara – genau umgekehrt verknüpft wie in den europäischen Sprachen.
Kardinalzahlen. ch'usaq (0), huk (1), iskay (2), kimsa (3), tawa (4), pichqa (5), suqta (6), qanchis (7), pusaq (8), isqun (9), chunka (10), chunka hukniyuq (11), chunka iskayniyuq (12), iskay chunka (20), pachak (100), waranqa (1000), hunu (1 000 000), lluna (1 000 000 000 000).
Ordinalzahlen werden durch Anhängen des Wortes ñiqin an die entsprechende Kardinalzahl gebildet (zum Beispiel iskay ñiqin = „zweite“). An Stelle von huk ñiqin („erste“) kann allerdings auch ñawpaq gesagt werden, was auch „vorderste, älteste“ bedeutet.
Der Infinitiv wird gebildet durch das Suffix -y (much'a= „Kuss“; much'a-y = „küssen“). Der Imperativ Singular lautet gleich; im Plural wird -ychik angehängt. Das Infix -wa- drückt „mir/mich“ aus (Much'ay! = „Küsse!“, Much'away! = „Küsse mich!“).
Wenn das Subjekt in der Mehrzahl steht, darf das Verb in der Einzahl stehen: Runakunaqa llaqtakunapim kawsan. = Die Menschen leben in Dörfern/Städten.
Verschiedene Interfixe und Suffixe dienen der Veränderung der Bedeutung, so zum Beispiel das kausative -chi- (Beispiel: wañuy = „sterben“; wañuchiy = „töten“); das reflexive -ku- (Beispiel: sipiy = „morden, schlachten“; sipikuy = „Selbstmord begehen“); das reziproke -naku- (Beispiel: marq'ay= „umarmen“; marq'anakuy= „einander umarmen“), das progressive -chka- (e.g., mikhuy = „essen“; mikhuchkay = „beim Essen sein“).
Im Quechua gibt es die objektivische Konjugation, was bedeutet, dass es nicht nur für verschiedene Subjekte, sondern auch für verschiedene Objekte unterschiedliche Verb-Endungen gibt (Transition). Beispiel:
Partikeln, also Wörter, an die niemals Suffixe angehängt werden, gibt es nur wenige. Hierzu gehören zum Beispiel das Wort arí („ja“), yaw („hallo!“, „he!“) und bestimmte Lehnwörter aus dem Spanischen, wie piru (von Spanisch pero „aber“) und sinuqa (von sino „sondern“). Das Verneinungswort mana („nein“) ist keine Partikel, da auch Suffixe angehängt werden (manam, „nein, nicht“; manas, „nein, sagen die Leute“, manapunim, keineswegs; manaraq, „noch nicht“; manaña, „nicht mehr“).
Die meisten Sätze im Quechua werden durch ein Evidentialitäts-Suffix markiert, welches anzeigt, wie sicher sich der Sprecher über seine Aussage ist bzw. woher er die Information hat. -mi drückt Wissen aus eigener Erfahrung aus (Tayta Wayllaqawaqa chufirmi, „Herr Huayllacahua ist Chauffeur, ich weiß es, ich habe es gesehen“); -si gibt Wissen vom Hörensagen wieder (Tayta Wayllaqawaqa chufirsi, „Herr Huayllacahua ist Chauffeur, hat man mir gesagt“); -cha drückt Wahrscheinlichkeit aus (Tayta Wayllaqawaqa chufircha, „Herr Huayllacahua ist wahrscheinlich – oder: vielleicht – Chauffeur“). Nach einem Vokal wird -m, -s, -ch angehängt.
Im Quechua gibt es sehr viele Fragewörter, die aus den Wurzeln ima (was), pi (wer) und may (wo) durch Anhängen von Nominalsuffixen gebildet werden. Meist wird noch ein Evidentialsuffix oder -taq angehängt:
 Runasimita qillqaytam munani. = Ich will Quechua schreiben lernen.Entscheidungsfragen werden immer mit -chu gebildet.
Als agglutinierende Sprache verwendet Quechua an Stelle von Nebensätzen mit Konjunktionen Verbalausdrücke mit entsprechenden Suffixen:
Inkaqa quri tawnanpa chayamusqanpi Qusqu llaqtatas kamasqa. = Wo sein goldener Stab auftraf, gründete der Inka die Stadt Cusco.
Runakunaqa ayninakuyta qunqachkan. = Die Menschen vergessen, einander zu helfen.An Stelle der deutschen Bindewörter wenn, als, während, weil und obwohl werden die Infixe / Suffixe -pti- (bei unterschiedlichen Subjekten) und -spa- oder -stin (bei gleichem Subjekt) verwendet, wobei zusätzliche Suffixe (zum Beispiel -qa, -m(i), -s(i) und -pas) für Bedeutungsnuancen dazukommen können:
Takistin tusurqankim. = Während du sangst, tanztest du.Einen Sonderfall hinsichtlich Nebensätzen bildet das Quechua in Bolivien, da es auch Bindewörter (gebildet aus Fragewörtern und -chus) aufweist, mit denen es neben den hier genannten Ausdrucksmöglichkeiten auch echte Nebensätze bilden kann.
Die deutsche Sprache hat eine Reihe von Lehnwörtern aus dem Quechua übernommen, in der Regel über Vermittlung durch das Spanische. Hier einige Beispiele:
Literatur auf Quechua ist seit der Kolonialzeit überliefert. Wurden in der dreisprachigen Doctrina Christiana (auf Spanisch, Quechua, Aymara, 1584), aber auch in dem um 1600 verfassten Huarochirí-Manuskript mit Mythen aus der Provinz Huarochirí noch eine dem nördlichen Kichwa und dem Chanka-Quechua ähnelnde, als „allgemeine Sprache Perus“ bezeichnete Variante verwendet, so war die Sprache später erschienener Werke, darunter das Drama Apu Ollantay, in einer früheren Sprachstufe des Cusco-Quechua verfasst.
Sowohl das moderne Cusco-Quechua als auch das Chanka-Quechua haben seit dem frühen 20. Jahrhundert eine gewisse Literaturtradition, denn in beiden Sprachen sind doktrinäre Texte der Erzdiözesen Cusco und Ayacucho, aber auch Gedichte und Theaterstücke erschienen, so etwa von dem peruanischen Hacendado Andrés Alencastre Gutiérrez. Im übrigen bestand die Quechua-Literatur in Sammlungen traditioneller Lieder und Märchen. Das 20. Jahrhundert sah auch die ersten Bibelübersetzungen, begonnen mit dem Johannesevangelium 1880 durch den protestantischen Pastor Gybbon-Spilsbury über alle vier Evangelien 1901 bis 1904 durch Clorinda Matto bis hin zu neun kompletten Bibelübersetzungen in sieben Quechua-Varianten Boliviens, Perus und Ecuadors, die zwischen 1986 und 2011 herauskamen.
Schritte hin zu einer originären fiktiven Quechua-Prosa machten die Autoren José Oregón Morales (* 1949), Porfirio Meneses Lazón (1915–2009) und Macedonio Villafán Broncano (* 1949) mit einer Reihe von Kurzgeschichten, die zwischen 1988 und 1994 erschienen, wobei die beiden ersteren auf Chanka-Quechua und letzterer auf Ancash-Quechua schrieb. 2013 folgte der kurze RomanSaqapa („Rassel“) des bolivianischen Autors Jinés Cornejo Endara (* 1952). Neuland betritt Pablo Landeo Muñoz (* 1959) aus Huancavelica mit seinem 2016 auf Chanka-Quechua veröffentlichten Roman Aqupampa, der die Migration in die Städte vor dem Hintergrund der Gewalt im bewaffneten Konflikt in Peru behandelt. Landeo sieht hier bewusst von einer zusätzlichen Übersetzung ins Spanische ab, um eine Auseinandersetzung in der Originalsprache und somit einer Festigung der Literatursprache Quechua zu fördern.
Quechuasprachige Lieder sind das Medium, durch welches man außerhalb des Quechua-Sprachgebiets die größte Chance hat, Quechua zu hören. Es gibt eine Reihe von peruanischen, bolivianischen und ecuadorianischen Musikern und Bands, die teilweise oder auch überwiegend auf Quechua singen. Zu diesen Musikern gehörte etwa die peruanische Sängerin Yma Sumac. Die bolivianische Sängerin Luzmila Carpio singt sogar fast ausschließlich auf Quechua. Kennzeichen der meisten dieser Musiker bzw. Musikgruppen ist, dass sie auf traditionelle andine Musikformen zurückgreifen. Die in Peru wohl häufigste Form des Quechua-Liedes ist der Waynu, der insbesondere durch die Migration von Musikern aus den Regionen Ayacucho und Apurimac in die Hauptstadt Lima zu einer Popularität quechuasprachiger Musik geführt hat, wobei hier das Chanka-Quechua einen erheblichen Anteil ausmacht. Zu diesen Musikern gehören der einst mit José María Arguedas befreundete und zusammenarbeitende Charangospieler und Sänger Jaime Guardia, dessen Schüler Manuelcha Prado wie auch der mit diesem oft auftretende Ranulfo Fuentes. Der Liedermacher Carlos Falconí Aramburú hat durch eine Reihe von Waynu-Texten auf Quechua zur Thematik des bewaffneten Konflikts in Peru Bedeutung erlangt. Mit eigenen Liedschöpfungen auf Chanka-Quechua hat sich auch die Sängerin und Schauspielerin Magaly Solier aus Huanta hervorgetan.
Musikalisch neue Wege beschreitet die 1991 gegründete peruanische Rock-, Blues- und Grunge-Band Uchpa, die traditionelle Elemente mit modernen Ausdrucksformen und Musikinstrumenten verbindet und auf Ayacucho-Quechua singt. Auch die peruanische Meditationsmusik- und Folklore-Band Alborada singt überwiegend auf Quechua. Die Popsängerin Damaris Mallma Porras gewann mit ihrem quechuasprachigen Titel Tukuykusun auf dem internationalen Songfestival von Viña del Mar den Folklore-Preis.
Serafin M. Coronel-Molina: Quechua Phrasebook. 2nd Edition. Lonely Planet, Footscray u. a. 2002, ISBN 1-86450-381-5.
Winfried Dunkel: Quechua für Peru-Reisende. 4. Auflage. Reise-Know-How-Verlag Rump, Bielefeld 2003, ISBN 3-89416-078-0 (Kauderwelsch 36).
Eva Gugenberger: Identitäts- und Sprachkonflikt in einer pluriethnischen Gesellschaft. Eine soziolinguistische Studie über Quechua-Sprecher und -Sprecherinnen in Peru. WUV-Universitäts-Verlag, Wien 1995, ISBN 3-85114-225-X (Dissertationen der Universität Wien 17), (Zugleich: Wien, Univ., Diss., 1994).
Roswith Hartmann (Hrsg.): „Rimaykullayki“. Unterrichtsmaterialien zum Quechua Ayacuchano – Peru. Zusammengestellt nach Clodoaldo Soto Ruiz „Quechua – manual de enseñanza“ Lima 1979 und ergänzt von Sabine Dedenbach-Salazar Sáenz. Aktualisierte, erweiterte und überarbeitete Neuauflage. 3. Auflage. Reimer, Berlin 1994, ISBN 3-496-02520-4.
Ernst Kausen: Die Sprachfamilien der Welt. Teil 2: Afrika – Indopazifik – Australien – Amerika. Buske, Hamburg 2014, ISBN 978-3-87548-656-8. (Kapitel 14)
Kendall A. King (Hrsg.): Quechua sociolinguistics. Mouton de Gruyter, Berlin u. a. 2004, (International journal of the sociology of language 167, ISSN 0165-2516).
Rosaleen Howard: Kawsay Vida: A Multimedia Quechua Course for Beginners and Beyond University of Texas Press, 2014, ISBN 978-0-292-75444-7
Online-Quechua-Kurs (auf Spanisch) von Demetrio Tupah Yupanki (Memento  vom 23. März 2007 im Internet Archive)
Detaillierte Karte der Varianten des Quechua gemäß SIL (fedepi.org) (Erklärung Quechua, Spanisch, Englisch)
AMLQ Cusco: Simi Taqe Qheswa – Español – Qheswa. Wörterbuch Quechua-Spanisch, Spanisch-Quechua. Cusco, Peru 2006.
Teofilo Laime Ajacopa: Iskay simipi yuyayk'ancha. Bolivianisches Wörterbuch Quechua-Spanisch, Spanisch-Quechua. (PDF; 5,2 MB) La Paz – Bolivia, 2007.
Clodoaldo Soto Ruiz: Runasimi-kastillanu-inlis llamkaymanaq qullqa. Wörterbuch Chanka-Quechua – Spanisch – Englisch, mit Worterklärungen/Definitionen auf Quechua. (PDF; 5,8 MB) University of Illinois, 2010.
Diccionario etnolingüístico y guía bibliográfica de los pueblos indígenas sudamericanos: Quechua. Alain Fabre, 2005 (Ausführliche Informationen auf Spanisch zur Systematisierung der Quechua-Varietäten, Sprecherzahlen sowie umfangreiche Bibliographie; PDF, 909 kB; auf Spanisch)

Die Queen-Elizabeth-Klasse, vormals als CVF-Project bezeichnet, ist eine geplante Flugzeugträgerklasse für die britische Royal Navy. Insgesamt sollen zwei Schiffe gebaut werden, um die drei Flugzeugträger der Invincible-Klasse zu ersetzen. Sie werden zu den größten außerhalb der USA gebauten Flugzeugträgern gehören.
Die Indienststellung des ersten Trägers war ursprünglich für das Jahr 2012 geplant und die des zweiten für 2015. Der erste Träger sollte dann gemäß dem Strategic Defence and Security Review (SDSR) aus dem Jahr 2010 jedoch erst 2016 in Dienst gestellt und vorläufig als Hubschrauberträger eingesetzt werden. Das zweite Schiff sollte erst 2020 in Dienst gestellt werden und mit zwölf F-35C Lightning II bestückt werden. Nach der Indienststellung des zweiten Trägers sollte der erste vorläufig der Reserve zugeteilt und anschließend entweder verkauft oder im Wechsel mit dem anderen eingesetzt werden.Im Mai 2012 wurde die Entscheidung von 2010 jedoch rückgängig gemacht. Die Schiffe sollen jetzt doch zum Betrieb der F-35B ausgelegt werden. Der Grund liegt in der exorbitanten Steigerung der veranschlagten Umbau-Kosten von 2 Mrd. Pfund (für ein Schiff). Die zu beschaffende Anzahl F-35B soll erst im Rahmen des kommenden SDSR im Jahr 2015 entschieden werden, wie auch ursprünglich der Verbleib des zweiten Schiffes. Die Entscheidung bzgl. des zweiten Schiffes wurde jedoch in Hinblick auf die Ukraine-Krise bereits 2014 dahingehend getroffen, beide Schiffe parallel zu betreiben. Hierdurch wird eine ununterbrochene Einsatzbereitschaft trotz der üblichen Werftaufenthalte gewährleistet.Auf Basis dieser Träger war unter der Projektbezeichnung Porte-Avions 2 auch ein weiteres Schiff für die französische Marine geplant. Diese Pläne entstanden im ersten Jahrzehnt des 21. Jahrhunderts und liegen seit dem Ausbruch der Finanzkrise auf Eis.
Die Planungen für den Bau neuer Flugzeugträger begannen 1994. Unter dem Projektnamen CVSG(R) (Aircraft Carrier, Support, Guided missile (Replacement)) sollte ein Ersatz für die drei Flugzeugträger der Invincible-Klasse gefunden werden. 1997 wurde der Projektname in CVF (Carrier, Vehicular, Future) geändert. Die ursprünglichen Konzepte sahen drei kleine Flugzeugträger vor, die mit einer Verdrängung zwischen 20.000 und 25.000 Tonnen etwa ihren Vorgängern entsprechen sollten. Alternativ wurde auch über den Kauf oder Neubau US-amerikanischer Trägerschiffe der Tarawa-Klasse nachgedacht.
Im Zuge der Strategic Defence Review fiel 1998 die Entscheidung, die bisherigen Träger durch wesentlich größere Schiffe zu ersetzen. Aufgrund der gesteigerten Leistungsfähigkeit sollten nun zwei Neubauten ausreichen. Das Verteidigungsministerium (MOD) fällte diese Entscheidung vor dem Hintergrund, dass die Träger der Invincible-Klasse seit dem Ende des Kalten Krieges zunehmend in offensiven Rollen eingesetzt wurden und ihre eigentliche Aufgabe, die U-Boot-Jagd und die Unterstützung US-amerikanischer Marineverbände, in den Hintergrund rückte. Für diese neuen Aufgaben waren sie jedoch aufgrund ihrer geringen Größe und einer Kapazität von maximal 20 Flugzeugen nur in begrenztem Umfang geeignet. Größere Trägerschiffe mit mehr Kampfflugzeugen schienen daher zwingend notwendig, um zukünftige Aufgaben zu erfüllen.
Im Dezember 1998 wurde schließlich die Beschaffungsmaßnahme ST(S) 7068 offiziell verabschiedet. Gefordert wurde von den in Frage kommenden Werften nun ein Konzept für einen nicht-atomaren Flugzeugträger mit einer Kapazität von 48 Flugzeugen und Hubschraubern, der komplett im Vereinigten Königreich gebaut werden sollte. Die Kosten für das gesamte Programm, inklusive des Baus von zwei Schiffen, sollten zwei Milliarden £ nicht überschreiten. Der erste Träger sollte 2012, der zweite spätestens 2015 in Dienst gestellt werden können. Anfang 1999 gab jedoch ein Parlamentsausschuss eine Studie in Auftrag, die erörtern sollte, unter welchen Bedingungen und mit welchem finanziellen Aufwand die jetzigen Flugzeugträger noch zehn Jahre länger im Einsatz bleiben und die Indienststellung neuer Träger damit auf 2022 verschoben werden könnte. Hierdurch hätte das Budget für den Bau der neuen Schiffe über einen längeren Zeitraum gestreckt werden können. Die Studie unter dem Namen Further Special Refit kam jedoch zu dem Schluss, dass der Nutzen den finanziellen Aufwand nicht rechtfertigen würde.Am 5. Mai 1999 teilte das MOD mit, dass zwei Rüstungskonzerne Konzepte eingereicht hätten. Hierbei handelte es sich um British Aerospace (später BAE Systems) und Thomson-CSF (später Thales UK). Beiden Konzernen wurden Fördergelder in Höhe von je sechs Millionen £ zu weiteren Ausarbeitung ihrer Konzepte bewilligt. Bis Mai 2000 reichte BAE Systems zwei unterschiedliche Entwürfe ein, einen für einen Flugzeugträger im CTOL-Design für konventionelle Flugzeuge sowie einen im STOVL-Design für Senkrechtstarter. Gleiches tat auch Thales UK und fügte noch einen zusätzlichen Entwurf für einen STOBAR-Träger hinzu, auf dem die Flugzeuge über einen Ski-Jump starten, aber mit Hilfe von Fangseilen landen sollten. Infolge der Entscheidung, dass die Träger mit einer Variante der F-35 Lightning II ausgestattet werden sollten, wurde das STOBAR-Konzept 2001 verworfen. Im Dezember 2002 wählte das MOD schließlich die STOVL-Variante der Lightning II, die F-35B, für ihre zukünftigen Träger aus, womit auch die CTOL-Konzepte hinfällig wurden. Gleichzeitig wurden BAE Systems und Thales UK jedoch angewiesen, die Träger zwar im STOVL-Design zu entwerfen, sie jedoch so zu konstruieren, dass sie zu einem späteren Zeitpunkt mit geringem Aufwand zu CTOL-Trägern umgebaut werden können. Hintergrund dieser Anforderung war, dass die neuen Träger eine Lebenszeit von 50 Jahren haben sollten, die F-35 jedoch bereits nach etwa 30 Jahren ausgemustert werden würde.
Am 30. Januar 2003 teilte das MOD mit, dass es die Bildung einer Allianz zwischen den beiden konkurrierenden Rüstungskonzernen für die sinnvollste Lösung halte, um die Kompetenzen beider Konzerne zu bündeln. Die Future Carrier Alliance (FCA) sollte von BAE Systems geführt werden, wobei Thales UK für weite Teile des Designs zuständig wäre. Weitere Konzerne sollten zu einem späteren Zeitpunkt als Zulieferer ausgewählt werden. Kritiker dieser Allianz behaupteten, dass ursprünglich Thales UK den Auftrag erhalten sollte, man es jedoch für politisch nicht durchsetzbar hielt, da der Mutterkonzern Thales ein französisches Unternehmen ist. Bereits im Vorfeld hatte BAE Systems gemeinsam mit Gewerkschaftern darauf hingewiesen, dass es unverantwortlich wäre, einen britischen Flugzeugträger von einem französischen Konzern bauen zu lassen. Nach der Kiellegung des ersten Schiffes wurde die Allianz in Aircraft Carrier Alliance (ACA) umbenannt.
Das Konzept, auf dem die FCA aufbauen sollte, war das Design Alpha von Thales UK. Es sah vor, dass die Schiffe eine Verdrängung von etwa 65.000 Tonnen bei einer Länge von 290 m haben sollten. Das MOD plante, bis Ende 2003 eine definitive Entscheidung über das Budget zu treffen und schließlich im April 2004 die endgültigen Aufträge zum Bau der Schiffe zu vergeben. Diese optimistische Einschätzung wurde jedoch bald verworfen, als klar wurde, dass die Probleme größer waren als angenommen. Aufgrund steigender Kosten wurde ab Mitte 2003 ein verkleinerter Entwurf für die Träger diskutiert, das Design Bravo. Es sah nun Träger mit einer Verdrängung von 55.000 Tonnen und einer Länge von 266 m vor. Die Breite des Flugdecks sollte von 74 auf 66 m reduziert werden. Als Reaktion auf Einwände der Marine gegen das Design Bravo wurde Ende 2004 das Design Delta vorgestellt, das die Verdrängung bei knapp 60.000 Tonnen und die Länge bei 280 m ansetzte. Es dauerte jedoch bis zum Oktober 2006, bis die FCA ihr endgültiges Design vorlegte. Dieser als Design Delta (II) bezeichnete Entwurf erhöhte die Verdrängung auf 64.500 Tonnen, die Länge auf 284 m und die Breite auf 73 m. Damit entsprach der Entwurf für die Träger wieder weitgehend dem ursprünglichen Design Alpha. Die späteren Dimensionen der fertiggestellten Schiffe können von diesen Eckdaten noch geringfügig abweichen.
Entsprechend der anhaltenden Diskussionen über die Größe der Träger wurde die endgültige Entscheidung über den Bau der Schiffe, die Main Gate Decision, zuerst von April 2004 auf November 2005 und später auf Mitte 2007 verschoben. Am 25. Juli 2007 teilte Verteidigungsminister Des Browne schließlich mit, dass die Träger definitiv gebaut werden und die Schwierigkeiten weitgehend ausgeräumt seien.Die offizielle Unterzeichnung der Verträge zum Bau der beiden Schiffe erfolgte am 3. Juli 2008 an Bord des Flugzeugträgers HMS Ark Royal. Gemäß der damaligen Planungen sollte die HMS Queen Elizabeth 2014 in Dienst gestellt werden, das Schwesterschiff HMS Prince of Wales sollte 2016 folgen. Im Dezember 2008 teilte das Verteidigungsministerium allerdings mit, dass sich die Indienststellung der Schiffe aufgrund des angespannten Verteidigungshaushalts um ein bis zwei Jahre verzögern könnte.
Nach dem Sieg der Conservative Party bei den Unterhauswahlen im Mai 2010 gab Premierminister David Cameron ein Strategic Defence and Security Review (Weißbuch) in Auftrag. Im Zuge dieser Maßnahme wurde auch die Beschaffung der beiden Flugzeugträger in Frage gestellt. Laut dem am 19. Oktober 2010 veröffentlichten Weißbuch wird mit dem Bau der Träger fortgefahren, es sollte jedoch erhebliche Änderungen geben. Um die kostengünstigere und aus Sicht der Regierung leistungsfähigere C-Variante der F-35 von Bord der Träger einsetzen zu können, wurde das STOVL-Konzept der Schiffe damals verworfen. Stattdessen sollte der zweite Flugzeugträger für den Einsatz von CTOL-Flugzeugen mit Fangseilen und elektromagnetischen Startkatapulten (EMALS, Electromagnetic Aircraft Launch System) analog der amerikanischen Gerald-R.-Ford-Klasse ausgestattet werden. Aufgrund der dafür notwendigen Änderungen hätte sich die Indienststellung des Trägers auf das Jahr 2020 verzögert. Das erste Schiff sollte demnach vorerst keine Katapulte und Fangseile erhalten und nur mit Helikoptern eingesetzt werden. Entgegen dieser Darstellung erklärte Premierminister Cameron am 19. Oktober 2010 im Parlament, dass er die sofortige Ausrüstung des ersten Trägers mit Katapulten und Fangseilen für sinnvoll halte. Bei Indienststellung des zweiten Schiffs sollte das erste in Reserve gesetzt, im Wechsel mit dem anderen Träger verwendet oder ins Ausland verkauft werden. Die Anzahl der zu beschaffenden Flugzeuge soll erheblich gekürzt werden, wahrscheinlich ist eine Stückzahl von rund 50 Maschinen, deshalb sollte die Prince of Wales mit lediglich zwölf F-35C bestückt werden und nur im Ernstfall die maximale Kapazität von 36 Maschinen mitführen. Das genaue Vorgehen sollte im nächsten Strategic Defence Review im Jahr 2015 festgelegt werden. Entgegen der Aussage vom Herbst 2010 wurden die Pläne im Sommer 2011 jedoch dahingehend geändert, dass nunmehr doch beide Schiffe mit Katapulten und Fangseilen ausgerüstet werden sollen. Mit der erneuten Kehrtwendung vom Mai 2012 (siehe eingangs) zurück zum ursprünglichen Konzept wurden alle Umbauplanungen ad acta gelegt.
Die Flugzeugträger entstehen in Modulbauweise an verschiedenen Standorten. Die großen Module des Rumpfes und der Aufbauten werden von BAE Systems in Govan, VT Group in Portsmouth und Babcock in Appledore gefertigt. Weitere kleinere Module entstehen bei Cammell, Laird & Company in Birkenhead, A&P Tyne in Hebburn und Babcock in Rosyth. Die Endmontage der Träger soll bei Babcock in Rosyth erfolgen. Pläne aus dem März 2007, Teile der britischen Träger in Frankreich zu bauen, wurden aufgrund der Verzögerungen auf französischer Seite verworfen. Mit dem Bau der HMS Queen Elizabeth wurde offiziell am 7. Juli 2009 begonnen.
Seit der Indienststellung des Flugzeugträgers Charles de Gaulle 2001 wurde in Frankreich die Anschaffung eines zweiten Trägers erwogen. 2004 bestätigte Präsident Jacques Chirac offiziell, dass Frankreich an einer Beteiligung am CVF-Projekt interessiert sei. Am 6. März 2006 vereinbarten das Vereinigte Königreich und Frankreich eine Kooperation bei den weiteren Planungen. Auf der Messe Euronaval teilte Frankreich im Oktober 2006 mit, dass das britische Konzept zu über 90 Prozent mit den französischen Anforderungen für den Porte-Avions 2 übereinstimmt und eine Beteiligung galt als sicher. Es waren nur geringfügige Modifikationen vorgesehen, so etwa die direkte Auslegung als CTOL-Träger, wobei es sich auszahlte, dass das Vereinigte Königreich die Träger von Beginn an so konzipieren ließ, dass sie verhältnismäßig einfach von STOVL auf CTOL umgerüstet werden können.
Die Bauentscheidung wurde jedoch aufgeschoben, bis das Projekt 2013 aufgegeben und aus dem französischen Weißbuch zu Verteidigung und Nationaler Sicherheit gestrichen wurde.
Im Juni 2003 teilte BAE Systems mit, dass die Kosten für Entwicklung und Bau voraussichtlich 3,8 anstatt der ursprünglich veranschlagten 2,0 Milliarden £ betragen würden. Man begann mit Überlegungen, die Träger deutlich zu verkleinern, um die Kosten auf 2,8 Milliarden £ zu senken. Der damalige Verteidigungsminister Geoffrey Hoon erklärte jedoch Mitte 2004, dass eine Reduzierung die Leistungsfähigkeit deutlich herabsetzen würde. Auch die Überlegungen, nur einen großen Flugzeugträger zu bauen, wurden zurückgewiesen.
Im Oktober 2006 reichte die FCA den endgültigen Entwurf für die Träger beim MOD ein, basierend auf dem Design Delta (II). Dieses entsprach wieder weitgehend den ursprünglich geplanten Dimensionen. Die Gesamtkosten wurden mit 3,9 Milliarden £ angegeben. Dieses Budget wurde am 25. Juli 2007 durch die Regierung bestätigt, wobei jedoch weiterhin Anstrengungen unternommen werden sollen, den Kostenrahmen nicht voll ausschöpfen zu müssen. Im Juni 2009 berichtete die BBC unter Berufung auf Information der am Bau der Schiffe beteiligten Unternehmen, dass die Gesamtkosten für die Träger auf rund 5 Milliarden £ gestiegen seien.
Parallel zu den Budgetproblemen ergaben sich Komplikationen bei der Anschaffung des Kampfflugzeuges Lockheed Martin F-35 Lightning II. Bereits 2002 war die Entscheidung des MOD, die F-35B und nicht CTOL-Version F-35C anzuschaffen, kritisiert worden. Die Planung sah vor, 100 Flugzeuge der Royal Air Force (RAF) und 50 der Royal Navy zu unterstellen, die sie gemeinsam auf den Trägern einsetzen sollten, wie schon im Fall des Vorgängermodells Hawker Siddeley Harrier. 2004 forderte die RAF jedoch, die F-35A zu ordern, da sie diese für leistungsfähiger hielt, womit ihre 100 Maschinen für die STOVL-Träger unbrauchbar wären. Der Royal Navy würden damit nur noch 50 Maschinen für ihre STOVL-Träger bleiben. Unter Berücksichtung der regulären Einsatzzyklen würden damit zu jedem Zeitpunkt maximal 30 Flugzeuge für beide Träger einsatzbereit sein.
Hinzu kamen Probleme bei der Entwicklung der F-35B. Der Hersteller Lockheed Martin teilte mit, dass es unter anderem Probleme mit dem Gewicht der Maschine gäbe, eine Komplikation, die bereits bei der Entwicklung des Senkrechtstarters Harrier in den 1960er Jahren aufgetreten war. Aufgrund dieser Probleme wurde im Oktober 2004 eine Überprüfung der Entscheidung für die STOVL-Variante angeordnet. Ein erstes Zwischenergebnis lautete 2005, dass die STOVL-Version weiterhin favorisiert werde, man jedoch auch das bereits verworfene CTOL-Konzept erneut in Betracht ziehe. Im April 2007 bestätigte das MOD, dass es die Entwicklung der unterschiedlichen Varianten der F-35 weiterhin genau beobachte und sich alle Optionen offen halte. Mit der Unterzeichnung der Verträge für den Bau der beiden Flugzeugträger im STOVL-Design im Juli 2008 entschied sich das MOD schließlich für die F-35B.
Im Oktober 2010 wurde diese Entscheidung jedoch verworfen. Die neue Regierung entschied, die Träger doch im CTOL-Design zu bauen und statt der F-35B die konventionelle F-35C einzusetzen.
Im Laufe des Jahres 2005 ergaben sich zudem Probleme bei der Kooperation mit den USA. Diese teilten mit, dass die F-35 nicht mit dem in England entwickelten F-136-Triebwerk, sondern ausschließlich mit der amerikanischen F-135-Turbine ausgestattet werden würde. Außerdem beklagte sich das MOD Anfang 2006 – ähnlich wie auch die Verteidigungsministerien Italiens und Australiens – über mangelnde Informationen über Fortschritt und Technik der F-35. Der damalige Premierminister Tony Blair teilte im März 2006 mit, dass die Probleme ausgeräumt worden seien. Dieser Darstellung widersprach jedoch der Verteidigungsausschuss des britischen Parlaments im Dezember 2006.
2008 bestätigte das MOD gegenüber dem Verteidigungsausschuss des Parlaments, dass bei der Indienststellung der HMS Queen Elizabeth im Jahr 2014 noch nicht genügend F-35 verfügbar sein werden. Erst 2018 wird eine ausreichende Anzahl dieser Flugzeuge bereitstehen, um die Träger mit ihnen auszustatten. Entsprechend sollten in den ersten Jahren die alten Harrier GR.9 eingesetzt werden, die bereits auf den Flugzeugträgern der Invincible-Klasse zum Einsatz kamen. Aufgrund der im Oktober 2010 beschlossenen Ausmusterung der Harrier-Flotte wird dieser Plan jedoch nicht weiter verfolgt. Gleichzeitig wurde die Indienststellung der beiden neuen Flugzeugträger auf das Jahr 2020 verschoben. Zu diesem Zeitpunkt soll bereits eine ausreichende Anzahl an F-35-Kampfflugzeugen verfügbar sein.
Am 25. Juli 2007 teilte das MOD offiziell mit, dass die Träger bei einer Länge von 284 Metern und einer maximalen Breite von 73 Metern eine Verdrängung von 65.000 Tonnen haben sollen. Bis zur Fertigstellung der Schiffe können sich noch geringfügige Abweichungen von diesen Daten ergeben. Unter dem Flugdeck befindet sich das Hangardeck, in dem bis zu 22 Flugzeuge gelagert und gewartet werden können. Der Großteil der Räumlichkeiten für die Besatzung befindet sich unter dem Hangardeck.
Eine Besonderheit der Flugzeugträger sind die geteilten Aufbauten, die sogenannten Inseln, da sämtliche aktuell im Dienst befindlichen Träger nur über eine Insel verfügen. Während eine möglichst weit vorne am Schiff positionierte Insel vorteilhaft für die Unterbringung der Kommandobrücke ist, wie bei dem französischen Flugzeugträger Charles de Gaulle, erleichtert eine weit hinten am Schiff positionierte Insel die Arbeit der Flugkontrolle, wie bei den US-amerikanischen Trägern der Nimitz-Klasse. Um die Vorteile beider Anordnungen bestmöglich auszunutzen, entschied man sich bei der Queen-Elizabeth-Klasse für zwei Inseln, von denen die vordere die Kommandobrücke und die hintere die Flugkontrolle beherbergt. Auch die Sensoren und Radarsysteme sind auf die beiden Inseln verteilt.
Das MOD entschied sich aus Kostengründen gegen einen Atomantrieb, der zu Beginn der Entwicklung aufgrund der Größe der Träger diskutiert wurde. Das MOD wählte einen elektrischen Antrieb und entschied sich für die Rolls-Royce MT30-Gasturbine als Antriebsaggregat. Jeder Träger wird zwei Gasturbinen mit einer Leistung von je 36 MW haben. Die Höchstgeschwindigkeit soll 27 Knoten betragen und die Reichweite bei einer Geschwindigkeit von 18 Knoten bei etwa 10.000 Meilen liegen.
Wie ihre Vorgänger sollen die neuen Flugzeugträger nur eine geringe bordeigene Bewaffnung erhalten, da die Träger stets im Verband – in sogenannten carrier strike groups – mit Zerstörern, U-Booten (der Astute-Klasse) und Fregatten operieren werden. Die ursprünglich geplante Installation von Tomahawk-Marschflugkörpern wird im endgültigen Design von BAE Systems und Thales UK nicht weiter verfolgt, soll aber als Nachrüstungsmöglichkeit vorgesehen werden. Gleiches gilt für die Ausstattung mit dem Luftabwehrsystem PAAMS Aster. Installiert sind lediglich vier Phalanx-CIWS-Kanonen sowie vier 30-mm-Geschütze zur Luftabwehr auf kurze Distanz. Zusätzlich werden die Träger mit Düppeln und Täuschkörpern zur Ablenkung von radar- und infrarot-gelenkten Raketen ausgestattet.
Jeder Träger ist für maximal 40 Flugzeuge und Hubschrauber ausgelegt. Im Oktober 2010 entschied die Regierung, im Regelfall jedoch nur zwölf F-35B-Kampfflugzeuge an Bord der Träger mitzuführen. Lediglich im Ernstfall würden demnach bis zu 36 Maschinen eingesetzt werden. Genau wie die frühere Invincible-Klasse sind die Flugzeugträger auch für amphibische Operationen vorbereitet, mit der Fähigkeit zum Einsatz weiterer Transport- und Kampfhubschrauber, wie Chinook, Commando Merlin, Wildcat oder Apache. Die genaue Zusammenstellung der "Tailored Air Group" (TAG), so die frühere Bezeichnung des "Carrier Air Wing" (CVQ), wird jeweils an den aktuellen Bedarf angepasst. Zur Seeüberwachung und zu Transportzwecken wird zudem permanent eine Staffel mit neun Merlin HM.2 mitgeführt; hinzu kommen vier, fünf weitere Merlins, die zusätzlich mit einem Lufraum-Überwachungsradar Rüstsatz ausgerüstet sind. 
Hieraus ergeben sich im Wesentlichen drei verschiedene Pakete des eingeschifften Geschwaders: Das Grundpaket ‘Maritime Force Protection’ (Merlin) als Standard plus alternativ die Pakete ‘Carrier Strike’ (Lightning II) oder ‘Littoral Manoeuvre’ (RAF-, RM- und Army-Hubschrauber).
Die Besatzung jedes Trägers soll aus 1450 Personen bestehen, von denen etwa 700 zum Flugzeugpersonal gehören. Der Großteil der Besatzung wird in Sechsbettkabinen untergebracht werden, von denen jede über eigene sanitäre Einrichtungen verfügt. Dies stellt ein Novum für große britische Kriegsschiffe dar und findet sich sonst nur auf den Zerstörern der Daring-Klasse.
Wie bereits ihre Vorgänger sollen die Flugzeugträger der Queen-Elizabeth-Klasse in Portsmouth in Hampshire beheimatet sein. Hierfür sind jedoch umfangreiche Umbaumaßnahmen an dem Marinestützpunkt notwendig. Die Einfahrt in das Hauptbecken muss um etwa 15 m verbreitert werden, zudem ist eine Vertiefung des Fahrwassers notwendig. Auch die Kaianlagen müssen der Größe der Schiffe angepasst werden. Aufgrund dieser Probleme wurde Ende 2006 geprüft, inwieweit ein anderer Stützpunkt für die Beheimatung der Flugzeugträger besser geeignet wäre. Im Juli 2007 wurde jedoch bekanntgegeben, dass die Träger definitiv in Portsmouth stationiert werden sollen.
Ministry of Defence (Hrsg.): The Royal Navy Handbook. Conway Maritime Press, London 2003, ISBN 0-85177-952-2.

Der Quendel-Ameisenbläuling oder Thymian-Ameisenbläuling (Phengaris arion, Syn.: Maculinea arion) ist ein Schmetterling (Tagfalter) aus der Familie der Bläulinge (Lycaenidae). Er wird auch als Schwarzgefleckter Bläuling oder Thymian-Bläuling bezeichnet. Das Artepitheton leitet sich von Arion, einem Sänger und Zitherspieler aus der griechischen Mythologie ab. Wie bei vielen Bläulingsarten leben auch die Raupen des Quendel-Ameisenbläulings während eines Teils ihrer Entwicklung myrmekophil im Nest von Ameisen. Sein deutscher Name leitet sich von seinen Raupennahrungspflanzen, den Thymianen ab, von denen manche Arten im Volksmund auch Quendel genannt werden.
Die Falter erreichen eine Flügelspannweite von 33 bis 42 Millimetern und zählen damit zu den größten in Mitteleuropa heimischen Bläulingen. Die Flügeloberseiten sind einheitlich mattblau und mit einem braunen Außenrand versehen, der bei den Weibchen kräftiger und ausgedehnter ist. Am Ende der Diskoidalzelle befinden sich ein schwarzer Fleck und eine Reihe länglicher, dunkler Postdiskalflecke. Die Unterseiten der Hinterflügel sind an der Basis stark dunkelgrün bestäubt, ferner findet man Wurzelaugen und zwei Reihen deutlicher Saumpunkte. Die Individuen in niederen Lagen sind größer und heller gefärbt als die der Gebirgslagen, die kleiner und dunkler sind. Die mit der Höhenlage zunehmend dunkler werdenden Flügeloberseiten werden als Anpassung an das insgesamt kühlere Klima betrachtet, da diese Falter Wärmestrahlung besser absorbieren können.
Die Raupen erreichen eine Länge von etwa 15 Millimetern. Der Körper ist dick und zum Kopf hin stark verjüngt. Dieser ist schwarz und wird von den ruhenden Raupen in den Körper zurückgezogen. Die jungen Raupen sind rosa, sie färben sich später weißlich bis ockerfarben um und sind dann gelegentlich lila bis rosa angehaucht.
Der Quendel-Ameisenbläuling besiedelt trockenwarme, kurzgrasige Standorte mit lückiger Vegetationsstruktur und Störstellen. Dazu zählen Magerrasen, Kalk- und Sandtrockenrasen, Halbtrockenrasen, Silbergrasfluren sowie Heiden. In Deutschland ist die Art vor allem im Süden, etwa auf der Schwäbischen Alb beheimatet; sie ist aber auch weiter nördlich, etwa im oberen Ahrtal und im Weserbergland anzutreffen. Das Verbreitungsgebiet reicht von Westeuropa (ausgestorben auf den Britischen Inseln, Wiedereinbürgerungsversuche 1983) bis nach Ostasien und umfasst hier im Wesentlichen die gemäßigte Zone. Im Norden findet man sie bis Fennoskandinavien (ohne Norwegen), während sie im Süden in Italien und auf Korsika sowie in isolierten Vorkommen auf der Iberischen Halbinsel und der Balkanhalbinsel anzutreffen ist. Den übrigen Mittelmeerraum besiedelt die Art nicht.
Die Imagines werden häufig saugend an Thymianen und Oregano angetroffen. Als weitere Nektarpflanzen werden Natternköpfe (Echium), Großblütige Braunelle (Prunella grandiflora), Gewöhnliche Kreuzblume (Polygala vulgaris), Gamander-Ehrenpreis (Veronica chamaedrys), Saat-Esparsette (Onobrychis viciifolia), Vogel-Wicke (Vicia cracca) und Zaun-Wicke (Vicia sepium) genannt.
Der Quendel-Ameisenbläuling bildet eine Generation im Jahr, die von Juni bis Juli fliegt. Die Raupen können ab August und nach der Überwinterung bis Mai angetroffen werden.
Thymianen (Thymus), insbesondere von Sand-Thymian (Thymus serpyllum) und Breitblättrigem Thymian (Thymus pulegioides), sowie Oregano (Origanum vulgare).
Die Weibchen legen die Eier einzeln an den Knospen der Raupenfraßpflanzen ab. Die Raupen schlüpfen nach etwa einer Woche und fressen in den ersten drei Stadien zunächst an den Blüten und reifen Samen ihrer Futterpflanze und verlassen diese dann, um in einem dunklen Versteck darauf zu warten, dass sie von Ameisen in deren Nest getragen werden. Den Rest ihrer Entwicklung verbringen die myrmekophilen Raupen mit Ameisen. Sie sind in der Lage, längere Zeit zu hungern, da es einige Zeit dauern kann, bis sie von Ameisen-Arbeiterinnen in ihrem Versteck entdeckt werden. Die Arbeiterinnen legen die Bläulingsraupen dann in einer Brutkammer ab. Bei den Ameisen handelt es sich vor allem um die Knotenameisenarten Myrmica sabuleti und Myrmica schencki. Die Trockenrasen-Knotenameise (Myrmica scabrinodis) wird als gelegentlicher Nebenwirt bezeichnet. Die Bläulingsraupe scheidet über die Rückendrüsen Honigtau ab, der für die Wirtsameisen eine Kohlenhydratquelle darstellt. Sie ernährt sich aber bis zur Verpuppung von den Eiern und Larven des Ameisenwirtes. Die Adoption der Raupen gelingt wohl prinzipiell bei allen Myrmica-Arten, allerdings ist die Mortalität nur bei M. sabuleti gering. Dennoch kommen viele Raupen um, weil sie entweder in Gegenwart der Königin von den Arbeiterinnen angegriffen werden, oder weil sie das Ameisennest leer plündern und sich so selbst die Nahrungsgrundlage entziehen.
Die Raupen überwintern im Nest der Wirtsameisen und setzen ihre Entwicklung im darauf folgenden Jahr fort. Die Falter schlüpfen nach einer Puppenruhe von zwei bis vier Wochen am Morgen, wenn die Aktivität der Ameisen im Nest noch gering ist. Daher gelingt es den meisten Faltern, ohne Angriffe von Ameisen das Nest zu verlassen.
Der Quendel-Ameisenbläuling ist nach der Bundesartenschutzverordnung (BArtSchV 1999) streng geschützt, die Rote Liste gefährdeter Arten Deutschlands ordnet die Art in der Kategorie 2 (stark gefährdet) ein. Sie wird von der IUCN als global gefährdet eingestuft. Bei Schutzmaßnahmen müssen die Habitatanforderungen der Wirtsameisen unbedingt einbezogen werden. Neben dem direkten Habitatverlust wird auch die Aufgabe verschiedener Nutzungsformen, wie extensive Beweidung und Mahd als Gefährdungsursache genannt. Die daraus resultierende Verbuschung gefährdet den Standort ebenso wie eine zu intensive Beweidung. Weiterhin spielt das Mikroklima des jeweiligen Standortes offenbar eine entscheidende Rolle. Ist der Erdboden durch die Vegetation zu stark beschattet, verlässt Myrmica sabuleti das Nest. Ferner spielt auch die Populationsdichte der Wirtsameisen in einem Areal eine wesentliche Rolle, da in einem Ameisennest meist nur eine parasitisch lebende Raupe zu finden ist.
Zdenék Fric, Niklas Wahlberg, Pavel Pech, Jan Zrzavý: Phylogeny and classification of the Phengaris–Maculinea clade (Lepidoptera: Lycaenidae): total evidence and phylogenetic species concepts. In: Systematic Entomology. 32, Oxford 2007, S. 558–567. doi:10.1111/j.1365-3113.2007.00387.x
Helmholtz Zentrum für Umweltforschung (Pressemitteilung) Studie an hoch entwickelten Schmetterlingen als Beispiel für den Verlust der genetischen Vielfalt.

Rabid – Der brüllende Tod (Originaltitel: Rabid, deutsche Alternativtitel: Überfall der teuflischen Bestien und Rabid – Bete, dass es dir nicht passiert) ist ein kanadischer Spielfilm von David Cronenberg aus dem Jahr 1977. In diesem Horrorfilm mit Anleihen beim Wissenschaftsthriller spielt Marilyn Chambers eine junge Frau, die Opfer eines medizinischen Experiments wird und über ein penisartiges Organ, das ihr als Folge in der Achselhöhle wächst, für eine tollwutartige Epidemie sorgt. Zusammen mit Parasiten-Mörder (1975) und Die Brut (1979) bildet Rabid Cronenbergs Beitrag zum Subgenre des Venereal Horror (zu deutsch: Geschlechtlicher Horror).
Das junge Paar Rose und Hart hat mit seinem Motorrad einen schweren Verkehrsunfall. Die beiden werden gerettet und in die nahegelegene Keloid Clinic, ein Institut für plastische Chirurgie, gebracht. Während Hart nur leicht verletzt ist, hat Rose schwere Verletzungen davongetragen. Dr. Dan Keloid, der Leiter der Klinik, entschließt sich, an Rose ein hochriskantes neues Behandlungsverfahren anzuwenden. Er entnimmt ihr gesundes Gewebe, das anschließend „morphogenetisch neutralisiert“ wird, indem die Programmierung des Zellkerns aufgehoben wird, damit es mit großer Anpassungsfähigkeit zerstörtes Gewebe andernorts ersetzen kann. Keloid nimmt das Risiko einer unkontrollierten Wucherung des unerprobten Transplantats in Kauf. Nach Wochen im Koma erwacht Rose, scheinbar wieder gesundet. Bald stellt sich heraus, dass sie nicht mehr in der Lage ist, normale Nahrung zu sich zu nehmen, sondern sich vom Blut anderer Menschen ernähren muss, das sie mittels eines neu entstandenen penisartigen Organs in ihrer Achselhöhle ihren Opfern aussaugt. Ihr erstes Opfer ist ein Mitpatient, den sie erst verführerisch anlockt, um ihn dann in ihrer Blutlust zu attackieren. Eine weitere Patientin wird ihr nächstes Angriffsziel, und schließlich Dr. Keloid selbst. Es stellt sich heraus, dass ihre Opfer an einer tollwutähnlichen Krankheit erkranken, die aus ihnen unkontrollierbare Monster macht, die die Krankheit durch Bisse weiterübertragen; die Klinik versinkt in Chaos.
Rose, verängstigt durch ihre Mutation, flieht in Richtung Montreal, um ihren Freund Hart zu finden. Ein Farmer und ein Lastwagenfahrer werden auf dem Weg dorthin ihre nächsten Opfer. Die Seuche breitet sich aus, Montreal gerät unter Ausnahmezustand und das Militär macht gnadenlos Jagd auf die Infizierten. Währenddessen ist Rose in Montreal, an Entzugserscheinungen leidend, auf der Suche nach weiteren Opfern, weigert sich aber zu glauben, dass sie für die grassierende Seuche verantwortlich ist. In einem Einkaufszentrum und in einem Pornokino wird sie fündig; die von ihr dort verführten Männer werden weitere Träger der Infektion. Hart, der zusammen mit Keloids Geschäftspartner Murray Cypher ahnt, dass Rose die Quelle der Epidemie ist, begibt sich auf die Suche nach ihr. Er findet sie, als sie gerade in ihrer Verzweiflung ihre beste Freundin Mindy angefallen hat. Hart macht ihr klar, dass sie der Ursprung der Seuche ist, doch Rose schlägt ihn nieder. In einem Selbstversuch will sie klären, ob sie wirklich die Krankheit auslöst. Sie zapft einem jungen Mann Blut ab und wartet, bis dieser erkrankt. Tatsächlich wird dieser von der Seuche befallen und stürzt sich auf Rose. Man sieht ihren leblosen Körper inmitten von Müll auf einem Hinterhof liegen. Männer in Schutzanzügen transportieren ihre Leiche in einem Müllwagen ab.
Cronenbergs Vorgängerfilm Parasiten-Mörder war in der Fachpresse und Öffentlichkeit als „pervers“ und „geschmacklos“ verrissen worden. Die Arbeit an einem neuen Filmprojekt gestaltete sich deshalb für Cronenberg schwierig. Die staatliche Filmfinanzierungsorganisation Canadian Film Development Corporation (CFDC) scheute sich trotz des Markterfolgs von Parasiten-Mörder, bei der Finanzierung eines weiteren Films behilflich zu sein. Zusammen mit der Produktionsgesellschaft Cinepix arbeitete Cronenberg ein Skript namens Mosquito aus. Eine Frau sollte darin als moderner Vampir nach einem missglückten medizinischen Versuch eine ganze Stadt mit einer Seuche infizieren. Im Zuge seiner Vorbereitungen wohnte Cronenberg auch der dreistündigen Operation eines plastischen Chirurgen bei. Cronenberg befürchtete im Fortschreiten seiner Arbeit, die von ihm erfundene Geschichte sei als Grundlage für einen Film nicht tragfähig und er würde sich damit beim Publikum lächerlich machen. Cronenbergs Produzenten John Dunning und Ivan Reitman versuchten jedoch, seine Bedenken zu zerstreuen und halfen mit, das Drehbuch in eine verfilmbare Form zu bringen. Die CFDC willigte schließlich ein, den Film über Umwege zu finanzieren. Indem sie eine Gruppe von zwei oder drei Filmen gleichzeitig aus einem einzigen Budget mit Finanzmitteln versorgte, hatte sie im Zweifelsfall immer die Möglichkeit, eine direkte Finanzierungsbeteiligung an Rabid abzustreiten.Cronenberg wünschte sich für die weibliche Hauptrolle Sissy Spacek, die gerade in Badlands – Zerschossene Träume erste Erfolge gefeiert hatte. Reitman und Dunning lehnten die Amerikanerin aber wegen ihrer Sommersprossen und ihres texanischen Akzents ab. Reitman kam auf die Idee, Marilyn Chambers zu engagieren, die bereits eine etablierte Pornodarstellerin war und Interesse zeigte, in einem nicht-pornografischen Film mitzuspielen. Sein Kalkül war, den Film mit der Zusatzattraktion eines bekannten Pornostars als Hauptdarstellerin weltweit besser vermarkten zu können. Die Dreharbeiten fanden vom 1. November 1976 bis zum 5. Dezember 1976 in Montreal und Umgebung statt. Cronenberg bezeichnet die Arbeit mit Chambers als schwierig. Durch ihr Engagement in Geschäfte in Las Vegas zusammen mit ihrem damaligen Ehemann und Manager Chuck Traynor empfand er ihre Ausstrahlung als hart und bereits vom Leben gezeichnet, während er ihre Rolle eigentlich unschuldiger und naiver anlegen wollte. Letztlich war er jedoch mit ihren schauspielerischen Qualitäten aufgrund der harten Arbeit, die sie für den Film geleistet hatte, recht zufrieden.
Uraufführung des Films war am 8. April 1977. In der Bundesrepublik Deutschland lief er am 24. Juni 1977 an. Bei einem Budget von Can$ 530.000 spielte Rabid $ 7.000.000 an den Kinokassen ein und war damit der erste Film, der für die CFDC Gewinn abwarf. Anders als Parasiten-Mörder erregte der Film kein öffentliches Aufsehen, sondern wurde von der Kritik und der Fangemeinde des Genres wohlwollend aufgenommen. Cronenberg vermutet, dass die Gründe darin lagen, „dass sich die Gesellschaft zu dieser Zeit weiterentwickelt hatte.“  Jill McGreal resümiert: „Ab dem Erscheinen von Rabid riefen Bildsprache und Inhalte der Filme Cronenbergs eher begeisterte Kritiken denn provinzielles Moralisieren hervor, und vor allem in Frankreich und Großbritannien hat der Regisseur eine stetig wachsende Fangemeinde.“In Deutschland urteilte das Lexikon des internationalen Films jedoch recht ablehnend: „Der technisch gut gemachte Horror-Schocker von Kultregisseur David Cronenberg knüpft monoton die immergleichen Effekte aneinander, wobei die Spannung leidet. Die abstruse Story ist so überzogen, daß sie schon fast parodistisch wirkt.“
Rabid gewann beim Sitges Festival Internacional de Cinema de Catalunya Preise für das Beste Drehbuch (David Cronenberg) und Beste Spezialeffekte (Al Griswold)
Edward R. O´Neill stellt fest, Rabid bilde mit Parasiten-Mörder und Die Brut „ein einheitliches Triptychon, in dem Cronenberg metaphorische Mutationen des menschlichen Körpers erschafft, die die Selbstkontrolle des Individuums zerstören, genau wie sie auch in der Folge drohen, die gesamte Gesellschaft auseinanderzureißen.“ Grünberg urteilt über den Film, er nehme „das Universum von Val Lewton für sich in Anspruch, voll von unterschwelligem Sex und Horror, nur in den Schatten sichtbar gemacht. Oder das von Herschell Gordon Lewis, eine ausgesprochen blutrünstige, aber trotzdem paradoxerweise unschuldigere Welt, erfunden, um Teenager zum Petting in Autokinos zu verführen.“ Cronenberg nutze den Film, „um einige der Hauptthemen des Undergroundfilms einzuschmuggeln: Orgien, sexuelle Befreiung, die Art von Kontamination und viraler Infektion, wie William Burroughs sie mag, Kritik an Stadtplanung usw“. Der Film sei eines der „exzentrischen, unklassifizierbaren Werke“ Cronenbergs, „als würde er in einer ihm fremden Sprache drehen.“ Beard hebt den Aspekt der Darstellung einer geschundenen Kreatur hervor: „Eines der Dinge, die Cronenbergs Arbeit (…) von ihren Genre-Artgenossen unterscheidet, ist die Tatsache, dass sie etwas anderes darstellt als nur ein Spektakel des Verlangens und des Horrors, nämlich eine (…) Perspektive menschlicher Traurigkeit und menschlichen Leidens.“
Durch zahlreichere und opulentere Schauplätze bietet Rabid einen deutlich höheren Schauwert als seine Vorgängerfilme. Beard stellt fest, dass der Mise-en-scène zugrundeliegende Locations wie die Klinik und das Einkaufszentrum „einen ironischen Kontrast zwischen dem Ausdruck einer Gesellschaft, die meint, alles unter Kontrolle zu haben und den chaotischen Anzeichen, dass diese Ordnung in Wirklichkeit sehr unsicher ist“ böten. Cronenbergs „wachsende Fähigkeiten als Filmemacher“ fänden Ausdruck in visuellen Details wie dem in einer Angriffsszene inszenierten gelb-weißem abstrakten Gemälde, das durch einen roten Schmierer aus Blut „zu einem neuen abstrakten Kunstwerk, das nun die wahren Verhältnisse besser als zuvor ausdrückt“, werde. In derselben Weise wirke auch das Kunstwerk eines gespaltenen Kopfes in einer anderen Szene, das laut Beard „die Vision des Menschen als schizoider Kreatur, deren Kopf im Kampf mit dem Körper liegt und doch unentrinnbar mit ihm verbunden ist“ verkörpere.
Beard konstatiert, Rabid biete den „flotten Spaß einer gut erzählten Geschichte.“ Die Action-Szenen seien „mit großer Finesse und Selbstbeherrschung“ inszeniert, den gezeigten spektakulären Autounfall halte er etwa für „einen perfekten Leckerbissen des Actionkinos.“ Zur Spannungslenkung setzt Cronenberg umfänglich das Mittel der Parallelmontage ein. Damit bindet der Regisseur in die laut Riepe „apokalyptische Makrostruktur“ des Films, „deren Systematik Cronenberg spannungsdramatisch geschickt entfaltet“, mehrere Nebenhandlungen ein. Die vergebliche gegenseitige Suche von Rose und Hart sei die „dramaturgische Klammer“ des Films, in die etwa die Geschichte von Murray Cypher eingebunden wird, der als zu Beginn des Films liebevoller Familienvater die gewaltsame Auflösung seiner Familienstruktur erleben muss, indem er zum Ende des Films sein totes Baby findet.Leitmotivisch funktionieren die immer wieder eingefügten Radiodurchsagen und Fernsehberichte, durch die die Handlung ironisch konterkariert wird. Cronenbergs „scharfe Ironie, sein sardonischer Sinn für Humor“ sorge im Film für eine narrative Zuspitzung, die „aus einer wahnwitzigen Übertreibung gewöhnlicher Situationen und aus dem Kontrast zwischen ‚normal‘ und ‚krank‘“ entstehe, führt Beard aus. Als Beispiel nennt er etwa die Szenen, als sich eines von Roses Opfern in einem Schnellrestaurant zuerst auf ein gebratenes Hähnchen und anschließend auf die anderen Restaurantgäste stürzt, oder als im Einkaufszentrum der Weihnachtsmann Opfer eines Militäreinsatzes wird und von Maschinengewehrsalven niedergemäht wird.Laut Rodley führt aber auch Cronenbergs „Tendenz, während des Editierprozesses exzessiv zu schneiden“, zu narrativen Auslassungen, die beim Zuschauer zu Verständnisproblemen führen können. Die Herkunft des neuen Organs in Roses Achselhöhle bleibt unerklärt, denn eine Szene, in der Kelloid einer Krankenschwester erklärt, das neue Gewebe solle wachsen, um Roses zerstörten Darm zu ersetzen, fiel Cronenbergs Schere zum Opfer. Cronenberg gab später zu: „Das war ein Fehler. Es hätte eine einfache, rationale Erklärung abgegeben, damit die Leute es verstehen. So haben sogar die Zuschauer, die den Film mochten, gefragt: ‚Was war das denn eigentlich für ein Ding?‘“ Beard rügt außerdem „die nicht komplett zufriedenstellenden Schauspielerleistungen“. Chambers und ihre Mitspieler hätten „etliche schwache und nicht überzeugende Momente.“
Sexualität, ihre Veränderung durch äußere Einflüsse und der Tausch der sexuellen Rollen spielen eine bedeutende Rolle in Rabid. Beard nennt dieses dominierende Motiv „sexuelle Grenzüberschreitung.“ Im Spiel mit der Überschreitung sexueller Konventionen kann der Film zunächst in einen voyeuristischen Zusammenhang gestellt werden, wie ihn auch der Pornofilm bedient. Kauffman führt aus: „Man könnte sagen, (…) hinter der Geschichte von Rabid (in der der Vampir eine phallische Frau ist) liegt die Geschichte von Deep Throat (in der es ebenfalls um ein Genital an falscher Stelle geht).“ Roses neues Organ transformiere sie laut Kauffman zum „sexuellen Angreifer“ und destabilisiere „alle unsere wohlbehüteten Konzepte der Stabilität von Sex und Geschlecht.“ Beard merkt an, alle Attacken durch Rose hätten „durchgängige, aber oft diegetisch nicht eingestandene sexuelle Untertöne.“ Erst die metanarrative Tatsache, dass Rose vom Pornostar Marilyn Chambers gespielt wird, sexualisiere ihre Handlungen endgültig. Beard führt aus: „Diegetisch gesehen ist Rose im Grunde unschuldig, leidend und bemitleidenswert. Über das Diegetische hinaus gesehen, richtet Marilyn Chambers mit ihrer erschreckend kraftvollen Sexualität enormes Chaos an.“ Seeßlen führt zu diesem Charakterzug der Schauspielerin Chambers aus, sie sei „wie geschaffen für die Rolle, die sexuelle Erfahrungen als essentielles Lebenselexier braucht und dabei nicht nur die eine oder andere Grenze überschreitet, sondern im Grunde nie wirklich Zufriedenheit erlangen kann. Der Einsatz ihres Körpers geschieht immer nahe am Schmerz; ein ununterdrückbares maskulines Element charakterisiert ihre Erscheinung“. Kauffman sieht Rabid als „die Parabel über die Rache einer Porno-Queen.“ Durch den verführerischen Beginn ihrer Attacken würden Erwartungshaltungen heterosexueller männlicher Zuschauer geweckt, die dann aber nicht erfüllt, sondern ins Gegenteil verkehrt werden: die verführten Opfer gehen an der Verführung zugrunde. Beard bestätigt: „Sexueller Voyeurismus und sexuelle Phantasie werden nicht nur ausgeschlossen, sondern explizit abgelehnt und bestraft.“ Kauffman führt aus, Rose erwidere „den männlichen lüsternen Blick mit dem kalten Starren der Medusa.“
Wie oft in Cronenbergs Gesamtwerk wird auch in Rabid das Thema der mutierenden Sexualität in einen medizinischen Hintergrund eingebettet. Riepe merkt an, „dass Cronenberg hier mit den Mitteln der Science-Fiction die heute so heiß diskutierte Stammzellenforschung vorwegnimmt.“ Medizin und medizinische Forschung sind für Kauffman in Rabid „eine perverse Einimpfung, die versucht, den Körper zu kontrollieren.“ Cronenberg untersuche „ihren Einfluss auf unseren psychosexuellen Grundaufbau“, fehlgeschlagene medizinische Forschung ändere „die Sexualität seiner Figuren (…) grundlegend.“Dass Keloid bei seinem Lebensrettungsversuch nicht aus finsteren Beweggründen, sondern allerhöchstens aus wissenschaftlicher Eitelkeit handelt, führt laut Riepe zu einer „Radikalisierung des Mad-Scientist-Motivs.“ Er führt aus: „Der Fehler, der Keloid bei Roses Rettung unterlaufen wird, geht nicht auf das Konto einer moralischen Schwäche, er liegt im System der Wissenschaft selbst bzw. deren Anwendung auf den Menschen“. Für Humphries ist die Figur des Keloid jedoch nicht schuldfrei; er weitet das vampirische Motiv auf ihn und seine wissenschaftliche Hybris aus: „Rose, die Heldin/das Opfer des Films, wird in einen Vampir verwandelt, der sich von neuen Opfern ernährt, genau wie sich der plastische Chirurg von ihr im Namen von Fortschritt und Wissenschaft ernährt.“
Humphries analysiert, Rabid bilde „das Schicksal einer Frau in einer arroganten Männerwelt“ ab und zieht Vergleiche zu Die Nacht der lebenden Toten, in dem der Held, als Afroamerikaner ebenfalls gesellschaftlich benachteiligt, genau wie Rose nicht nur dem grassierenden Horror, sondern auch der Gesellschaft zum Opfer fällt, die ihn nicht beschützt. Kauffman stellt fest, Rabid spiele „auf den Hauptplätzen der Konsumkultur“, unter anderem in einem Einkaufszentrum und in einem Kino. Die sexuellen Untertöne würden dadurch mit Aspekten des Konsums konnotiert. Sie erläutert: „Sex ist nur noch eine Ware, (…), die Konsumenten sind Kannibalen und die Zivilisation ist nur eine dünne Übertünchung“. Humphries ergänzt, die gezeigte Art der Verführung im Umfeld der Konsumgesellschaft spiegle „die Eigenschaft des Kapitalismus (…) wider, jede Person und jedes Objekt, sogar ein Konzept wie Sexualität in eine Ware zu verwandeln.“Unter den Voraussetzungen einer solchen dysfunktionalen Gesellschaft führt der Film unweigerlich zum Tod der Protagonistin. Rose endet wie Warenabfall, „ihr Körper steif wie ein Mannequin, ein Sexspielzeug, eine Barbiepuppe“ in einem Müllwagen. Es gibt keine gesellschaftlichen Mechanismen, die sie hätten retten können. Sie findet, so Kauffman, „keine Immunität, keine Transzendenz (…), sondern nur die totale Verneinung“. In dieser Geisteshaltung stehe Cronenberg in der Tradition von Dostojewski, Nietzsche, Bataille und Louis-Ferdinand Céline. Humphrey resümiert: „Hier gibt es keine Hoffnung, aber auch keinen einfach gestrickten Pessimismus, sondern eher den ungerührten Versuch, die gesellschaftlichen Kräfteverhältnisse, die zu der Tragödie führten, zu analysieren.“
Im März 2017 wurde bekannt, dass Jen und Sylvia Soska planen, ein Remake von Rabid drehen. Als Produzenten sollten Paul Lalonde und Michael Walker unter der Leitung von John Videttes Somerville House Releasing agieren. Geplant waren ein Spielfilm und eine TV Serien-Version des Filmes. Im Juli 2018 begann schließlich die Produktion des Remakes mit der Hauptdarstellerin Laura Vandervoort.
Linda S. Kauffman: Bad Girls and Sick Boys – Fantasies in Contemporary Art and Culture. University of California Press. Berkley, Los Angeles und London 1998, ISBN 0-520-21032-8.
Reynold Humphries: The American Horror Film – An Introduction. Edinburgh University Press 2002, ISBN 0-7486-1416-8.
Manfred Riepe: Bildgeschwüre – Körper und Fremdkörper im Kino David Cronenbergs. Transcript Verlag. Bielefeld 2002, ISBN 3-89942-104-3.
William Beard: The Artist as Monster – The Cinema of David Cronenberg. University of Toronto Press. Toronto, Buffalo und London 2006, ISBN 0-8020-3807-7.
Serge Grünberg, Claudine Paquor (Hrsg.): David Cronenberg – Interviews with Serge Grünberg. Plexus Publishing Ltd. London 2006, ISBN 0-85965-376-5.
