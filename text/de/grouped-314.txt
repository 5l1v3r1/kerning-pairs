
LibreOffice [/ˈliːbɹə ˈɒfɪs/] (Abkürzungen: LibO oder LO) ist eine freie Zusammenstellung typischer Standardsoftware für Bürotätigkeiten (Office-Paket). Zu LibreOffice gehören Programme für Textverarbeitung, Tabellenkalkulation, Präsentation und zum Erstellen von Zeichnungen. Ein Datenbankmanagementsystem und ein Formeleditor sind ebenfalls enthalten.
LibreOffice spaltete sich Ende 2010 vom Office-Paket OpenOffice.org ab, wird seither unabhängig weiterentwickelt und ist inzwischen die modernere Alternative.
Neben der Desktop-Variante für Linux und Windows gibt es eine für Android-Smartphones und -Tablets. Außerdem existiert unter der Bezeichnung LibreOffice Online eine Webapp als Online-Office.
Die Geschichte von LibreOffice und der Document Foundation begann mit der Veröffentlichung der ersten Beta-Version des Office-Pakets am 28. September 2010. Die Ursachen für die Abspaltung vom OpenOffice.org-Projekt und damit die Vorgeschichte reichen jedoch bis in das Jahr 1999 zurück. LibreOffice sieht sich als eine legitime Fortführung von OpenOffice.org. Da OpenOffice.org seinerseits eine offizielle Abspaltung von StarOffice (zwischenzeitlich Oracle Open Office) darstellt, wird auch deren Geschichte als Teil des LibreOffice-Projekts verstanden.
Nachdem 1999 das ursprüngliche Unternehmen hinter dem Projekt, Star Division, von Sun Microsystems übernommen worden war, wurden im darauf folgenden Jahr die Quellen des proprietären StarOffice freigegeben: OpenOffice.org entstand. Im Zuge dieser Freigabe regte Sun Microsystems bereits im Jahr 2000 die Gründung einer Stiftung an. Das Ziel hinter diesem Vorhaben hat sich mit der Gründung der Document Foundation nicht verändert: Die Entwicklung des Office-Pakets soll unabhängig von Firmeninteressen weitergeführt und die Freiheiten sowohl von Entwicklern als auch von Anwendern gestärkt werden.Nachdem Sun Microsystems im Januar 2010 von Oracle übernommen worden und daraus folgend auch die Entwicklung von OpenOffice.org in deren Verantwortung übergegangen war, entstand innerhalb der OpenOffice.org-Gemeinschaft Unzufriedenheit. Kritisiert wurde insbesondere, dass Oracle die Zukunft von OpenOffice.org offenließ und die Unterstützung des Projektes immer spärlicher ausfiel. Im September 2010 entschieden sich schließlich führende Mitglieder der OpenOffice.org-Gemeinde, die Document Foundation zu gründen.Kurz nach der Entstehung von LibreOffice hat sich Oracle aus dem OpenOffice.org-Projekt vollständig zurückgezogen und es an die Apache Software Foundation übergeben, die es in Apache OpenOffice umbenannte.
Die Abspaltung und unabhängige Weiterführung des Office-Pakets wurde möglich, da OpenOffice.org als freie Software entwickelt wurde. Anders als bei der Software selbst besaß Oracle am Namen „OpenOffice.org“ das ausschließliche Verwendungsrecht. Die Weiterführung des Projekts unter dem Namen OpenOffice.org erforderte die Zustimmung von Oracle. Oracle entschied, die Namensrechte nicht an die neu gegründete Document Foundation zu übergeben und strebte die Auflösung bestehender Verflechtungen beider Projekte an. Dies erzwang die Fortentwicklung unter dem neuen Namen LibreOffice. Dieser ist ein Hybridwort aus libre, dem spanischen und französischen Wort für frei und dem englischen Wort für Büro, office.
Am 16. Februar 2011 veröffentlichte die Document Foundation einen Spendenaufruf, um eine Stiftung nach deutschem Recht zu gründen. Das Ziel war es, der Document Foundation den Status einer juristischen Person zu geben. Die hierzu benötigten 50.000 Euro waren acht Tage nach Bekanntgabe des Aufrufs erreicht.Da sich LibreOffice als legitimer OpenOffice.org-Nachfolger sieht, vergibt das Projekt die Versionsnummern wie unter OpenOffice.org. Die Versionsnummer der ersten stabilen Veröffentlichung vom 25. Januar 2011 lautete daher 3.3.0. Diese Version gilt als Nachfolger von OpenOffice.org 3.2.1. Im Januar 2015 wurde mit der Version 4.4 das neunte Major-Release des Projekts veröffentlicht. Die Förderung und Koordination des Projekts wird von der gemeinnützigen Stiftung The Document Foundation getragen, die ihrerseits von ehemals führenden Mitgliedern der OpenOffice.org-Gemeinschaft gegründet wurde.
Im April 2011 erklärte Oracle, dass das OpenOffice.org-Projekt der Open-Source-Community bzw. der Apache Software Foundation übergeben werden solle. Da die Community bereits nahezu vollständig zu LibreOffice abgewandert war, kam dies einer Einstellung von OpenOffice.org gleich. Die Document Foundation betonte, dass sie einer Wiedervereinigung der Projekte positiv gegenüberstehe und jederzeit neue Mitglieder aufnehmen würde. Die Lizenzierung des OpenOffice.org-Projekts unter der Apache-Lizenz würde neue Möglichkeiten des Codeaustauschs zwischen beiden Projekten eröffnen. Trotzdem empfahl die Free Software Foundation nur wenig später, auf LibreOffice und nicht auf OpenOffice.org zu setzen.
Die Abbildung zeigt die weltweite Anzahl der LibreOffice-Nutzer von 2011 bis 2018 in Millionen. Einzelnachweise sind im Text vorhanden.
2011: LibreOffice wurde nach Angaben der Document Foundation in der ersten Woche nach Start des Projekts 350.000 Mal heruntergeladen. Sechs Monate nach Start des Projekts stieg die Zahl der Downloads auf 1,3 Millionen, Downloads über andere Webseiten und die Paketverwaltung von Linux-Distributionen nicht mitgezählt. Zum ersten „Geburtstag“ von LibreOffice im September 2011 konnte das Projekt bereits über 6 Millionen Downloads verzeichnen. Allein im September 2011 wurde das Office-Paket 900.000 Mal heruntergeladen, wofür 81 Spiegelserver zur Verfügung standen. Hinzu kommen geschätzte 1,5 Millionen Downloads über andere Webseiten. Die Document Foundation schätzte 2011, dass weltweit 10 Millionen Menschen LibreOffice aktiv verwenden und über Downloads oder CDs bezogen haben. Hinzu kommen geschätzte 15 Millionen Nutzer, die LibreOffice über die Paketverwaltung ihrer Linux-Distribution beziehen. Die Stiftung plant, bis zum Ende des Jahrzehnts diese Zahl von 25 Millionen Nutzern auf über 200 Millionen Nutzer zu steigern.2013: Im September 2013, nach zwei Jahren, belief sich die geschätzte Anzahl der LibreOffice-Nutzer auf 75 Millionen.2015: Im Jahr 2015 wurde LibreOffice von 100 Millionen Nutzern und 18 Regierungen verwendet.2016: Im August 2016 wurde die Anzahl der LibreOffice-Nutzer auf 120 Millionen geschätzt.2018: Die Document Foundation schätzte 2018, dass es weltweit 200 Millionen aktive LibreOffice-Nutzer gibt. Etwa 25 % davon seien Studenten und 10 % Linux-Nutzer (welche LibreOffice automatisch durch ihre Distribution erhalten). Im Vergleich dazu wurde Microsoft Office 2018 von 1,2 Milliarden Nutzern verwendet.
OpenOffice, der Vorläufer von LibreOffice, und Derivate davon hatten im Jahr 2010 laut einer Studie von Webmasterpro.de einen Marktanteil unter deutschsprachigen Internetnutzern von über 20 Prozent. In der EU betrug dieser Wert zwischen 9 Prozent (Großbritannien) und 22 Prozent (Polen); für die USA wurde in dieser Studie ein Marktanteil von 9 Prozent ermittelt.
Auch die ersten größeren Unternehmen erklärten ihre Absicht, künftig auf LibreOffice zu setzen. So wechselten beispielsweise die Kopenhagener Krankenhäuser und das dänische Verkehrsministerium zu LibreOffice. Die Verwaltung der französischen Region Île-de-France gab im Oktober 2011 bekannt, 800.000 USB-Sticks, welche LibreOffice und andere freie Software beinhalten sollen, an Studenten verschenken zu wollen.
Die maßgebliche Entwicklung und Pflege von LibreOffice leistet die am 28. September 2010 gegründete gemeinnützige Organisation The Document Foundation (Abkürzung: TDF). Sie fördert und koordiniert die Weiterentwicklung des Office-Pakets und beteiligt sich an der Weiterentwicklung des offenen Dateiformatstandards OpenDocument bei der OASIS. Die Document Foundation bekennt sich dabei zu freier Software und bewirbt so auch keine proprietären Zusatzmodule oder ähnliches. Sie ist meritokratisch organisiert und verpflichtet sich zur vollkommenen Transparenz. Sie möchte auf die zehnjährige Geschichte des OpenOffice.org-Projekts in der Überzeugung aufbauen, dass nur eine unabhängige Stiftung sowohl den Entwicklern die bestmöglichen Voraussetzungen als auch den Benutzern ein exzellentes Produkt bieten könne. Bis 2012 handelte es sich bei The Document Foundation nicht um eine rechtsgültig gegründete Stiftung. Vielmehr wurde die „Foundation“ bis zur Gründung einer Stiftung nach deutschem Recht von Mitgliedern des Vereins Freies Office Deutschland e. V. getragen.
Dass Oracle im April 2011 erklärte, jegliche kommerziellen Interessen am OpenOffice.org-Projekt aufzugeben, und das Projekt Anfang Juni 2011 an die Apache Software Foundation übergab, ändere nach Verlautbarungen der Stiftung nichts an ihrem Vorhaben. Sie zeigte sich im Gegenteil eher enttäuscht darüber, dass Oracle die Chance nicht genutzt habe, beide Projekte wieder zu vereinen. Sie betonte nochmals, dass die Stiftung jederzeit für neue Mitglieder offen stehe und einer Wiedervereinigung positiv gegenüberstehen würde, dennoch würden formale Differenzen zwischen der Document Foundation und der Apache Software Foundation das erschweren. Bereits kurze Zeit später, Mitte Juni 2011, besetzte die Document Foundation ihr Advisory Board (Beirat). Besonderes Augenmerk wurde dabei auf Herstellerunabhängigkeit gelegt, sind in ihm doch alle Sponsoren unabhängig von ihrem Beitrag jeweils mit einem Sitz vertreten. Zu den Mitgliedern zählen Google, SUSE, Red Hat, Novell, Intel, die Vereine Freies Office Deutschland e. V. und Software in the Public Interest (SPI) sowie die Free Software Foundation.Am 1. Februar 2012 teilte The Document Foundation mit, dass sie eine rechtsfähige Stiftung bürgerlichen Rechts in Berlin gründen wolle. Die hierdurch entstehende Rechtssicherheit stelle die langfristige Weiterentwicklung der Community und der Software sicher. Als Vorstandsvorsitzender der neuen Stiftung fungierte Florian Effenberger, als Stifter trat der Verein Freies Office Deutschland e. V. (früher OpenOffice.org Deutschland e. V.) auf. Am 17. Februar 2012 wurde die Stiftung in Berlin von der Stiftungsaufsicht anerkannt. Im Februar 2014 übernahm Thorsten Behrens den Vorsitz.
Die Stiftung arbeitet eng mit anderen Organisationen der ehemaligen OpenOffice.org-Gemeinde wie dem deutschen Freies Office Deutschland e. V. und dem brasilianischen BrOffice.org zusammen und erhält Unterstützung von Projekten wie NeoOffice, von Organisationen wie der Free Software Foundation, der Gnome Foundation und der Open Source Initiative und von Firmen wie Google. Die Linux-Distributoren Canonical (Ubuntu), SUSE (openSUSE, SUSE Linux Enterprise Desktop, zur Zeit der Verlautbarung noch unter dem Dach von Novell) und Red Hat (Fedora, Red Hat Enterprise Linux) haben neben ihrer Unterstützung auch die Aufnahme von LibreOffice in die nächsten Versionen ihrer Betriebssysteme zugesagt. Auch die Regierungen verschiedener Länder, darunter Brasilien, Indien, China und Russland, haben sich wegen ihres intensiven Einsatzes von OpenDocument für eine von Firmeninteressen unabhängige Entwicklung ausgesprochen und die Gründung der Document Foundation begrüßt. Schnell nach Gründung des Projekts haben sich große Teile der OpenOffice.org-Community neu organisiert und LibreOffice zugewandt. Die Document Foundation betonte seit ihrer Gründung wiederholt, dass sie die Beteiligung neuer Mitglieder und Partner ausdrücklich begrüße. Ende September 2011, ein Jahr nach Gründung des Projekts, zählte die Document Foundation 136 registrierte Mitglieder.Die Document Foundation ist Mitglied im Open Invention Network und SPI und präsentierte sich bereits im ersten Jahr auf Messen wie der Cebit und FOSDEM. Sie nimmt beim Google Summer of Code teil.Das deutsche Bundesamt für Sicherheit in der Informationstechnik sieht in LibreOffice (und Linux) eine Alternative zu proprietären Cloud-Produkten für Organisationen mit Interesse an Informationssicherheit und sponserte einige kryptographische Funktionen von LibreOffice. Weitere kryptographische Funktionen wurden vom niederländischen Verteidigungsministerium gesponsert.
In Anlehnung an die jährlich stattfindenden OpenOffice.org-Konferenzen veranstaltet die Document-Foundation seit 2011 jährlich eine „LibreOffice Conference“, im Web jeweils unter https://conference.libreoffice.org auffindbar.
In ihrem „Manifest fürs nächste Jahrzehnt“ definiert die Document Foundation die ihrer Arbeit zugrunde liegenden Werte und Ziele. So verpflichtet sie sich dazu, ihren Beitrag zur Überwindung der digitalen Kluft beizutragen, indem sie jedermann den Zugang zu einem kostenlosen Office-Paket ermöglicht. Das bestehende „Monopol der Anbieter von Büro-Software“, gemeint ist das Quasi-Monopol von Microsoft Office, wird abgelehnt, da dieses „de facto eine Steuer auf elektronische freie Meinungsäußerung“ bedeute. Die Vielfalt der Muttersprachen soll gefördert werden, indem sowohl LibreOffice als auch die zugehörigen Dokumentationen in möglichst vielen Sprachen angeboten werden sollen und die Übersetzung in neue Sprachen aktiv gefördert wird. „Die schleichende Dominanz der Computersysteme in einer einzigen Sprache“ soll überwunden werden und niemand gezwungen sein, vor Verwendung eines Computers eine andere Sprache zu erlernen. Die Document Foundation teilt die Ziele des OpenDocument-Projekts und lehnt „die Kontrolle über Dateiformate durch proprietäre Software-Unternehmen“ ab. LibreOffice soll in „einem offenen, transparenten und durch gegenseitige Begutachtung geprägten Softwareentwicklungsprozess, in dem hohe technische Qualität geachtet wird“, entwickelt werden. Das alles soll unter dem Dach einer demokratischen, jedermann offen stehenden Stiftung geschehen, die auch der Beteiligung von Firmen und Organisationen positiv gegenübersteht.
OpenOffice.org wurde, insbesondere in den meisten Linux-Distributionen, sukzessiv durch LibreOffice ersetzt. So setzen Ubuntu ab Version 11.04, Fedora ab Version 15, openSUSE ab Version 11.4 und Linux Mint ab Version 11 auf LibreOffice. Das Debian-Projekt geht noch einen Schritt weiter und ersetzt auch bereits bestehende OpenOffice.org-Installationen in Debian 7.0 automatisch, in Debian 6.0 mit Hilfe eines für den Benutzer verfügbaren Backports, durch LibreOffice. Mit Version 6.3 von Oracle Linux liefert auch Oracle LibreOffice als festen Bestandteil der eigenen Linux-Distribution aus.Die Document Foundation arbeitet an einer stetigen Verbesserung der Unterstützung von LibreOffice. So wurde im Februar 2012 die Plattform Ask LibreOffice eröffnet, an die sich Anwender bei Problemen wenden können, um von anderen Benutzern Hilfe zu erhalten. Das zunächst nur in englischer Sprache verfügbare Angebot ist inzwischen auch auf Deutsch verfügbar.
Da LibreOffice eine Abspaltung von OpenOffice.org darstellt, waren die Funktionen und Eigenschaften der Office-Pakete zunächst im Wesentlichen identisch. Zu Beginn des Projekts erschien unwahrscheinlich, dass Neuerungen in OpenOffice.org aufgenommen werden, die im Rahmen des LibreOffice-Projekts entwickelt wurden. Der Grund waren lizenzrechtliche Erwägungen im Zusammenhang mit der zu dieser Zeit von Oracle vertriebenen, kommerziellen Variante von OpenOffice.org, StarOffice. Im Gegenzug standen einer Aufnahme von Neuerungen in LibreOffice, die im Rahmen des OpenOffice.org-Projekts entwickelt wurden, nichts entgegen (→ siehe auch: Abschnitt „Rechtliches“). Durch die Übergabe des OpenOffice.org-Projekts an die Apache Software Foundation und die Umbenennung des Office-Pakets in Apache OpenOffice wurden diese lizenzrechtlichen Probleme nur teilweise aus dem Weg geräumt. Während eine Codeübernahme von Apache OpenOffice zu LibreOffice weiterhin kein lizenzrechtliches Problem darstellt, ist eine entgegengesetzte Übernahme nur schwer möglich. LibreOffice wurde unter der GNU GPL, einer Lizenz mit Copyleft, veröffentlicht, deren Konzept in der Apache-Lizenz keine Entsprechung findet. Da die GNU GPL aber ein Bestehenbleiben des Copyleft voraussetzt, ist eine Codeübernahme ausgeschlossen. Da LibreOffice unter der Mozilla Public License (MPL) mehrfachlizenziert ist, wäre eine Codeübernahme zumindest in Binärform theoretisch denkbar.
Nach Untersuchungen des von SUSE bezahlten LibreOffice-Entwicklers Michael Meeks sei ein gegenseitiger Codeaustausch aber immer schwieriger und werde durch die fortschreitende Entwicklung immer unwahrscheinlicher. Durch die inzwischen erheblichen Unterschiede im Quelltext beider Projekte sei eine quasi automatische Übernahme von Neuerungen nicht möglich, stattdessen sei es notwendig, jede einzelne Neuerung, die zwischen den Projekten ausgetauscht werden soll, anzupassen. Demnach sei davon auszugehen, dass mit fortschreitender Entwicklung LibreOffice andere Funktionen enthalten wird als OpenOffice.org und lediglich Änderungen, die den Aufwand rechtfertigen, zwischen den Projekten ausgetauscht werden. In geringem Umfang bestanden bereits bei Veröffentlichung der ersten stabilen Version 3.3 Unterschiede zwischen beiden Projekten (→ Abschnitt „LibreOffice 3.3“). Die OpenOffice.org-Abspaltung Go-oo ist in LibreOffice aufgegangen.
LibreOffice ist, wie auch OpenOffice.org, modular aufgebaut und besteht anwenderseitig aus sechs Einzelprogrammen.
Diese Komponenten können unabhängig voneinander installiert und verwendet werden, dem Konzept des Gesamtpakets folgend. Um unnötigen Mehraufwand zu vermeiden, werden verschiedene Funktionen, wie beispielsweise die Rechtschreibprüfung und der Thesaurus, in mehreren Komponenten verwendet.Durch diesen modularen Aufbau ist es außerdem möglich, Plug-ins und Vorlagen Dritter zu installieren und zu verwenden. Die Möglichkeiten sind dabei kaum begrenzt und reichen von simplen Dokumentvorlagen über zusätzliche Cliparts bis hin zu komplexen Erweiterungen des Funktionsumfangs. Da die Installation von Zusätzen aus nicht vertrauenswürdigen Quellen ein Sicherheits- und Stabilitätsrisiko darstellt, eröffnete die Document Foundation im September 2011 zwei zunächst als Beta-Version gekennzeichnete Plattformen, die dieses Risiko minimieren und den Benutzer bei der Suche unterstützen sollen. Entwickler können dort Zusätze als freie Software hochladen, welche nach einer Prüfung durch ein Team aus Freiwilligen in die Datenbank aufgenommen und veröffentlicht werden. Die Datenbank soll auch Erweiterungen mit einschließen, die mit OpenOffice.org kompatibel sind. Ende Oktober 2011 wurde die Plattform als final gekennzeichnet und überschritt einen Monat später die Marke von 100 aufgelisteten Erweiterungen. Erweiterungen in LibreOffice und OpenOffice teilen sich die Dateinamenserweiterung .oxt.LibreOffice wird zurzeit in 115 Sprachen angeboten, darunter auch Deutsch. Das Standard-Dateiformat ist OpenDocument.LibreOffice unterstützt von sich aus, also ohne Installation eines externen PDF-Druckertreibers, auch im aktuellen Versionszweig 6.x nur PDF Version 1.4 mit einer Verschlüsselung im RC4-Verfahren, das mit Stromchiffre und Schlüssellängen bis 128 Bit als unsicher gilt (vgl. RC4#Sicherheit).
LibreOffice steht für die Betriebssysteme Windows (ab XP), Linux (ab Kernel 2.6.18) und macOS (ab Version 10.4) zur Verfügung. Microsoft Windows 2000 wird ab der Version 4.0, XP und Vista ab der Version 6.0 von LibreOffice nicht mehr unterstützt. Zum Betrieb von LibreOffice 3.x unter Microsoft Windows 2000 wird der Windows Installer in Version 3.1 oder höher benötigt. Die Linux-Version benötigt mehrere Pakete und Bibliotheken Dritter, die bei der Mehrzahl der Distributionen ab Erscheinungsdatum Anfang 2007 enthalten sind. Anfang 2015 wurde ein LibreOffice-Viewer für Android veröffentlicht, um Open-Document-Textdokumente, Präsentationen und bedingt Tabellenkalkulationen ansehen zu können. Um diese auch verändern zu können muss dies in den Einstellungen aktiviert werden, da sich die Funktion noch in der Experimentalphase befindet.Die Systemvoraussetzungen für Windows und Linux sind ein Intel-Pentium-kompatibler Prozessor (ab Intel Pentium III), 256 MB RAM und 1,5 GB Festplattenspeicher. Die Versionen für Mac benötigen mindestens einen PowerPC- oder Intel-Pentium-Prozessor, 512 MB RAM und 800 MB Festplattenspeicher. Die Bildschirmauflösung sollte mindestens 1024×768 Bildpunkte betragen.
Da der Quelltext des Pakets verfügbar ist, ist eine Installation auch auf anderen Plattformen wie Solaris, FreeBSD und anderen Unix-Varianten möglich. Hierbei sind die Systemvoraussetzungen der Linux-Variante zu erfüllen. Auch Portierungen auf weitere, nicht genannte Plattformen sind möglich.
LibreOffice Online ist die Ausführung von LibreOffice als Online-Office. Es handelt sich dabei um eine Webapp, welche auf einem Server installiert wird und die Benutzer mit einem Webbrowser nutzen können. Sie bietet eine grundlegende gemeinsame Bearbeitung von Dokumenten unter Verwendung des „Kerns“ von LibreOffice. Der Quellcode von LibreOffice Online wurde im Februar 2017 mit der Version 5.3 von LibreOffice veröffentlicht. 2016 entstand aus LibreOffice Online Collabora Online, welches für die Cloud-Integration optimiert ist.
Unter Windows legt die Installation von LibreOffice wie auch bei anderen Programmen üblich auf der Partition C: programm- und nutzerspezifische Daten ab, was den Einsatz auf mehreren Rechnern mit Wechseldatenträgern (beispielsweise USB-Sticks oder Speicherkarten) oder auf mehreren Windows-Installationen im selben Rechner erschwert. Mit Hilfe einer portablen Ausgabe, die erstmals 2011 verfügbar wurde, ist es möglich, das Office-Paket auch ohne Installation zu verwenden. Nach dem Entpacken in die Ziel-Partition kann das portable LibreOffice mit vollem Funktionsumfang und mit den vom Nutzer gewählten Einstellungen verwendet werden; nahezu keine Daten  werden dann in die Partition C: geschrieben. Die offizielle portable Ausgabe wurde inzwischen auch in das Projekt PortableApps aufgenommen.
Im Herbst 2010 wurde erstmals die LibreOffice-Box veröffentlicht. Das von der Document Foundation offiziell unterstützte Projekt stammte vom Verein Freies Office Deutschland e. V. (vormals OpenOffice.org Deutschland e. V.), der zuvor die vergleichbare OpenOffice.org PrOOo-Box anbot. Die LibreOffice-Box ist eine DVD, die – neben der Java-Laufzeitumgebung und jeweils aktuellen Version von LibreOffice für Windows, macOS und Linux – verschiedene weitere Features rund um das Projekt beinhaltet, beispielsweise Dokumentvorlagen, Cliparts und verschiedene Plug-ins wie Anaphraseus und ein Wörterbuch. Zusätzlich sind verschiedene Programme beigelegt, die im Büroalltag häufig benötigt werden, wie beispielsweise 7-Zip, Inkscape, Mozilla Firefox und Ghostscript. Für LibreOffice-Entwickler enthält die DVD neben dem Quelltext des Programms auch das Software Development Kit und verschiedene weitere Programme für Entwickler. Ende 2013 wurde das Projekt „mangels […] Mitarbeiter und fehlender Perspektive“ vorerst eingestellt.
Mit der Version 4.0 wurde die LibreOffice Impress Remote eingeführt, mit der Präsentationen per Bluetooth oder WLAN ferngesteuert werden können. Diese Fernsteuerung muss nur in LibreOffice selbst aktiviert werden. Die LibreOffice Impress Fernsteuerung existiert als App für Android, Apple iOS und Pebbles.
Bereits zum Start der Document Foundation am 28. September 2010 erschien die erste Beta-Version 3.2.99.1. Während zu diesem Zeitpunkt lediglich 20 Personen an der Entwicklung des Office-Pakets arbeiteten, ist diese Zahl bei der Veröffentlichung der stabilen Version 3.3 bereits auf über 100 gestiegen. Zwei Monate später stieg die Zahl der Entwickler auf 150. Ein Jahr nach Gründung, im September 2011, zählte das Projekt 270 Entwickler und nochmal so viele Übersetzer. Im Jahr 2011 wuchs die Entwickleranzahl nahezu linear und stieg zum Januar 2012 auf 390 Entwickler. Die Mehrzahl dieser ist ehrenamtlich für das Projekt tätig, einige der im Projekt engagierten Unternehmen richten aber auch Mitarbeiterteams ein, die die LibreOffice-Entwicklung vorantreiben sollen.Zum ersten Geburtstag des Projekts im September 2011 wurde die Codebasis von LibreOffice analysiert. So waren zu diesem Zeitpunkt nur noch 20 Prozent des Codes direkt auf OpenOffice.org zurückzuführen, 25 Prozent entfielen auf ehrenamtliche Entwickler und die verbleibenden 55 Prozent wurden von verschiedenen Unternehmen beigesteuert. Dabei wurden 25 Prozent der Codebasis von SUSE-Mitarbeitern und weitere 20 Prozent von Red Hat entwickelt.In den 18 Monaten seit der Abspaltung von OpenOffice.org haben ungefähr 80 Entwickler pro Monat an über 30.000 Codebeiträgen gearbeitet.
Ein Vergleich von LibreOffice und OpenOffice im März 2014 ergab, dass LibreOffice sowohl bei der Häufigkeit der Updates als auch bei Zahl der Entwickler und eingereichten Changesets weit vor OpenOffice liegt.
Das Projekt hält den im Umfeld der freien Software üblichen sechsmonatigen Veröffentlichungszyklus im März und September seit Beginn ein. Da insbesondere von Linux-Distributoren Vorlaufzeiten erwünscht sind, sieht die Planung Veröffentlichungen jeweils einen Monat früher, im Februar und August eines jeden Jahres vor. Stabilitätsaktualisierungen, welche in der Regel ausschließlich Fehlerkorrekturen enthalten, sollen, sofern notwendig, in ungefähr monatlichen Abständen erscheinen. Da ein Entwicklungszweig ein Jahr lang mit derartigen Aktualisierungen versorgt werden soll, werden folglich vom Projekt der jeweils aktuelle und der vorausgehende Entwicklungszweig unterstützt.Das Qualitätsmanagement der Document Foundation hat erheblich Einfluss darauf, wem welche Veröffentlichung empfohlen wird.
Die Versionen werden in die Kategorien „Developer“, „Bleeding Edge“, „Stable“, „Very Stable“ und „Rock Solid“ eingeteilt. Selbstkompilierte Versionen, Nightly Builds, Vorschauversionen und Freigabekandidaten fallen grundsätzlich in die Kategorie „Developer“ und sollten ausschließlich von dieser Benutzergruppe (Softwareentwickler) verwendet werden.
Die jeweils erste Version eines neuen Entwicklungszweigs, beispielsweise 3.4.0, wird als „Bleeding Edge“ nur Early Adoptern empfohlen. Für die Verwendung im weniger professionellen Umfeld, beispielsweise für Privatanwender, ist ein Entwicklungszweig ab dem ersten Stabilitätsupdate, beispielsweise 3.4.1, geeignet; dieser wird in die Kategorie „Stable“ eingeteilt. Die Kategorie „Very Stable“, welche mit dem zweiten Stabilitätsupdate vergeben wird, richtet sich an konservative Anwender, beispielsweise kleine und mittlere Unternehmen. Als „Rock Solid“ werden Versionen betrachtet, die für Anwender geeignet sind, die eine hohe Stabilität benötigen – etwa Großunternehmen. Diesen Status erreicht das dritte Stabilitätsupdate. Nach den Planungen der Document Foundation wird danach ein neuer Entwicklungszweig veröffentlicht, in dem der Zyklus von neuem beginnt. In Ausnahmefällen erscheinen auch Versionen x.x.4 und höher.
Anwendern, für die im neuen Entwicklungszweig noch keine Versionen erschien, wird die jeweils letzte Version des vorherigen Entwicklungszweigs, die als „Rock Solid“ gilt, empfohlen. Für Unternehmen, die professionelle Hilfe bei einer Migration erwägen, möchte die Document Foundation künftig eine Liste zertifizierter Organisationen veröffentlichen.
LibreOffice knüpft bei der Vergabe seiner Versionsnummern direkt an OpenOffice.org an. Die erste stabile Veröffentlichung lautete daher 3.3.0 und wird vom Projekt als Nachfolger von OpenOffice.org 3.2.1 betrachtet.
Neben den Entwicklerversionen gibt es seit März 2014 und Version 4.2.2 zu jedem Zeitpunkt zwei wichtige veröffentlichte Versionen von LibreOffice: „Fresh“, die neueste Version, und „Still“, die stabile Version. Die Namen dieser zwei Versionen weisen darauf hin, dass sie jeweils für unterschiedliche Einsatzszenarien geeignet sind. Die zwei Versionen von LibreOffice werden wie folgt charakterisiert:
„Fresh“ – ist die neueste Bugfix-Veröffentlichung der aktuellen Hauptversion, in der die neuesten Verbesserungen enthalten sind, in der aber auch Programmfehler sein können, die in der „Still“-Version nicht vorhanden sind.
„Still“ (früher „Stable“ genannt) – ist die letzte Bugfix-Veröffentlichung der vorangegangenen Hauptversion. Bei dieser Version wurden bereits mehrere Monate lang Fehler behoben. Diese Version wird für Benutzer empfohlen, bei denen die Stabilität im Vordergrund steht.
LibreOffice 3.3 basiert auf OpenOffice.org 3.3, dessen Codebasis gesichtet und überarbeitet wurde. Die Import-, Öffnen- und Speicherfunktionen wurden verbessert, so ist in allen Programmen des Office-Pakets das Öffnen und Speichern von Dokumenten als einzelnes XML-Dokument möglich. Der Druckdialog wurde überarbeitet und soll dem Benutzer einen übersichtlicheren und schnelleren Zugriff auf die wesentlichen Funktionen bieten. Auf Linux-Systemen ist das Java Media Framework zum Abspielen von Musik und Filmen nicht mehr erforderlich.
Der Import von PDF-Dokumenten ist nun möglich. Diese werden in Draw geöffnet und können dort bearbeitet und wieder gespeichert werden. Eine Integration in Dokumente ist als OLE-Objekt in jedem Programm des Office-Pakets möglich. Zur Standardinstallation wurden verschiedene Plug-ins hinzugefügt, die den Funktionsumfang von LibreOffice erweitern. Die grafische Benutzeroberfläche hat Detailverbesserungen erhalten und ist in 19 zusätzlichen Sprachen verfügbar.
Mit Veröffentlichung der Version 3.3.1 erhielt das Office-Paket außerdem neue Dokumenten-Icons.Writer hat eine überarbeitete Funktion zur Autokorrektur erhalten, und Titelseiten lassen sich mit Hilfe eines neuen Dialogs leichter verwalten. Der Navigator, mit dessen Hilfe die Struktur größerer Dokumente schneller und leichter nachvollzogen werden kann, wurde überarbeitet. Die Such- und die Statistikfunktionen wurden im Detail verbessert. In Microsoft-Word-Dokumenten lassen sich Formulare einfügen und in Lotus Word Pro und Microsoft Works erstellte Dokumente können importiert werden. Der Import von in WordPerfect erstellten Dokumenten wurde wie auch der Export ins Rich Text Format deutlich verbessert.
In Calc wurde die Möglichkeit überarbeitet, Tastenkürzel zu verwenden. So ist beispielsweise die Navigation in Formeln mittels Tastenkombinationen möglich. Das Öffnen von Tabellendokumenten wurde generell beschleunigt. Das Anlegen neuer Tabellenblätter wurde vereinfacht, die zugehörigen Registerkarten lassen sich einfärben. Die Zahl der in einem Tabellenblatt nutzbaren Zeilen wurde von 65.536 auf 1.048.576 erhöht. In Diagrammen können mehrere, hierarchisch strukturierte Achsenbezeichnungen dargestellt werden. Draw kann Grafiken im SVG-Format bearbeiten; der Import in die übrigen Programme des Office-Pakets ist ebenfalls möglich. Das Öffnen von Microsoft-PowerPoint-Dokumenten in Impress wurde beschleunigt und die Handhabung von Folien-Layouts verbessert.
Zusätzlich sind alle Neuerungen von OpenOffice.org 3.3 auch in LibreOffice 3.3 enthalten. Das gilt auch für die im Rahmen von Go-oo entwickelten Verbesserungen.
Die folgenden Versionen von LibreOffice zielten nicht auf große Änderungen, sondern kleine inkrementelle Verbesserungen hinsichtlich Schnelligkeit, geringeren Verbrauchs von Systemressourcen, Stabilität, Fehlerfreiheit, Kompatibilität und neuer Funktionalität. Ein weiteres wichtiges Ziel war die Aufarbeitung des Quelltextes, um diesen für neue Programmierer einfacher verständlich und bearbeitbar zu machen. Im Zuge dieser Aufarbeitung wurde viel unbenutzter Quelltext entfernt und die Implementierung automatisierter Tests vorangetrieben. Darüber hinaus wurden Abhängigkeiten von anderen Programmen, insbesondere Java, reduziert.
Bei der Entwicklung von LibreOffice 3.4 wurde das Hauptaugenmerk auf Stabilität gelegt. So finden sich zwar auch zahlreiche Neuerungen in dieser neuen Hauptversion („major release“), die Großzahl der Änderungen entfällt aber auf Verbesserungen der Stabilität und Geschwindigkeit. So arbeitet LibreOffice unter Linux schneller und verbessert die Textdarstellung, wurde entschlackt und nimmt auf allen Plattformen weniger Arbeitsspeicher in Anspruch. Die Graphite-Engine wurde neu geschrieben, wodurch sie neben einer höheren Stabilität auch zehnmal schneller arbeiten soll. Die Unterstützung spezieller internationaler Schriftarten, unter anderem Rechts-nach-Links-Schriftarten wurde verbessert. Neben der altbekannten Suchfunktion wurde eine Suchleiste, wie auch aus modernen Webbrowsern bekannt, hinzugefügt. Die Kompatibilität zu OpenDocument und zu den OOXML-Formaten wurde verbessert.
LibreOffice 3.5 konzentriert sich, wie bereits LibreOffice 3.4, primär auf Fehlerkorrekturen und die weitere Überarbeitung des Quellcodes. In LibreOffice 3.5 neu hinzugekommen ist unter anderem eine Online-Update-Funktion, die unter Windows und Mac Aktualisierungen der Software automatisch einspielt und die Windows-Installationsdatei liegt als MSI-Paket vor. Für die Verschlüsselung wird AES statt Blowfish verwendet. Die Bedienoberfläche wurde insbesondere bei der Behandlung von Kopf- und Fußzeilen und Seitenumbrüchen sowie der Bearbeitung von Grafiken überarbeitet und vereinfacht. Die Unterstützung für die OpenFormula-Spezifikation wurde verbessert, ein nativer Treiber für PostgreSQL-Datenbanken hinzugefügt, der Import von Microsoft-Visio-Grafiken in Draw wurde erstmals ermöglicht und der Import von RTF-Textdokumenten wurde verbessert.
Wie bereits bei den Vorgängerversionen konzentrierte sich das LibreOffice-Projekt auch bei der vierten Hauptversion 3.6 eher auf zahlreiche Detailverbesserungen und Fehlerkorrekturen anstelle von einzelnen, umfangreichen Neuerungen. Neben zusätzlichen Optionen für Formatierungen in Writer und Calc können Dokumenten beim PDF-Export Wasserzeichen hinzugefügt werden. Die Oberfläche der Office-Suite wurde bereinigt und erhielt einige Detailverbesserungen, so werden Statistiken wie die Wortzahl des geöffneten Writer-Dokuments in der Statuszeile angezeigt, und das Design von Startbildschirm, „Start Center“ und Info-Bildschirm wurde vereinheitlicht. Verschiedene Dialoge wurden überarbeitet und bestehende Beschränkungen entfernt, insbesondere wurde das Kontextmenü an einigen Stellen erweitert, wodurch Funktionen schneller erreicht werden können. Der Import von „Smart Arts“ aus Microsoft-Office-Dokumenten und von Grafiken aus CorelDraw wurde erweitert, die Unterstützung von OpenFormula wurde weiter verbessert und unter Linux wurde die Unterstützung für GTK-Themes und Trinity verbessert.
LibreOffice 4.0 bringt wie die vorigen Versionen eine Reihe kleiner Neuerungen und einige Verbesserungen in der Kompatibilität mit verschiedenen Formaten, insbesondere dem DOCX-Format. Einige dieser Verbesserungen wurden aus der Lotus-Suite importiert, deren Quelltext IBM an Apache OpenOffice gespendet hat. Die bedeutendsten Änderungen, die zur Begründung des Versionssprungs angeführt werden, sind allerdings nicht für Nutzer offensichtlich. So wurde die Relizenzierung des Quelltextes abgeschlossen, der nun vollständig unter der GPL 3 und folgenden Versionen, der LGPL 3 und folgenden Versionen sowie der MPL lizenziert ist. Außerdem wurde die offizielle Unterstützung für einige sehr alte Formate eingestellt und die veraltete Funktionalität der Uno-Programmierschnittstelle entfernt. Damit besteht erstmals ein Unterschied zwischen den Programmierschnittstellen von LibreOffice und Apache OpenOffice, so dass Erweiterungen für eines der Programme in Zukunft nicht mehr automatisch mit dem jeweils anderen Programm kompatibel sind. Die Abhängigkeit von Java wurde weiter reduziert, aber nicht vollständig beseitigt.
Ab Version 4.4 bringt LibreOffice die freien Schriftarten Carlito und Caladea mit, die metrikkompatibel zu den proprietären Schriftarten Calibri und Cambria sind und diese ersetzen können, wenn sie nicht installiert sind. Microsoft Office verwendet seit 2007 die Schriftart Calibri als Standardschriftart, die freien Schriftarten verbessern daher die wahrgenommene Kompatibilität mit Microsoft-Office-Dokumenten erheblich.
Die Hauptversion 4.5 wurde im April 2015 in „Version 5.0“ umbenannt., weil LibreOffice 5 unter Android Texte nicht nur anschauen (wie seit Mai 2015 möglich), sondern auch bearbeiten kann. Die Version 5.0.0 erschien am 5. August 2015. Die Version 5.0.4 erschien am 17. Dezember 2015 und leitete ab 2016 den massenhaften Umstieg von LibreOffice 4 auf LibreOffice 5 ein.
Die für die erste Hälfte des Jahres 2018 geplante Hauptversion 5.5 wurde im Juni 2017 in „Version 6.0“ umbenannt. Gründe dafür waren wohl Veränderungen in der Markenpolitik und der Benutzeroberfläche (schließlich erschienen mehrere neue Versionen des Menübandes „LibreOffice Ribbon UI“). Die erste Version 6.0.0 wurde am 31. Januar 2018 veröffentlicht. Die Distribution Ubuntu machte in ihrer im April 2018 erschienenen Version mit langfristiger Unterstützung (18.04 LTS) den Sprung zu LibreOffice 6, was den Umstieg auf LibreOffice 6 ab April 2018 beschleunigte. Windows XP und Vista werden ab der LibreOffice-Version 6.0.0 nicht mehr unterstützt.m
LibreOffice ist, wie auch OpenOffice.org, freie Software. Vor der Übertragung des OpenOffice.org-Projekts an die Apache Software Foundation wurde OpenOffice.org unter der GNU Lesser General Public License (LGPL) in Version 3 veröffentlicht. Die dem Benutzer häufig eingeräumte Möglichkeit, eine neuere Version der Lizenz zu nutzen, bestand nicht. Das galt folglich zunächst auch für LibreOffice. Durch die Übertragung von OpenOffice.org an die Apache Software Foundation ist der Programmcode auch unter der hauseigenen Lizenz, der Apache-Lizenz, erhältlich. Diese erlaubt sowohl eine nahezu bedingungslose Weiternutzung als auch eine Wiederveröffentlichung unter vollkommen anderen Bedingungen. Da die Document Foundation freie Softwarelizenzen mit Copyleft bevorzugt und die Apache-Lizenz ein derartiges Lizenzmodell ermöglicht, ist LibreOffice unter der LGPL in Version 3 oder neuer und der Mozilla Public License (MPL) in Version 1.1 oder neuer mehrfachlizenziert.Sowohl bei LibreOffice als auch bei OpenOffice.org werden dem Anwender die aus dieser Lizenzierung entstehenden Freiheiten uneingeschränkt zugestanden. Vor der Übertragung des OpenOffice.org-Projekts an die Apache Software Foundation wurden beteiligte Entwickler, anders als bei LibreOffice, mit Bestimmungen konfrontiert, die ihre Rechte einschränken. Oracle verfolgte, wie auch schon Sun Microsystems seit Bestehen des Projekts, bei der Entwicklung von OpenOffice.org das Ziel, die urheberrechtlichen Verwertungsrechte zu erhalten. Um das zu realisieren, musste jeder Entwickler, der eine Verbesserung einbringen wollte, das Sun Microsystems Inc. Contributor Agreement unterschreiben. Die Wirkung dieser Vereinbarung war unter anderem, dass Oracle das Recht zugesprochen wurde, die Lizenz, unter der die eingebrachten Verbesserungen veröffentlicht werden, zu wählen. Erst dadurch war es Oracle möglich, in OpenOffice.org eingebrachte Änderung zurück in ihr kostenpflichtiges, proprietäres Office-Paket StarOffice fließen zu lassen.
Die Vereinbarung ging jedoch noch weiter und sprach alle urheberrechtlichen Rechte des Entwicklers auch Oracle zu – es entstand eine „gemeinsame Urheberschaft“. Die Vereinbarung wurde insbesondere auch deshalb kritisiert, da diese gemeinsame Urheberschaft erlosch, sobald eine Änderung an der eingebrachten Verbesserung vorgenommen wurde. Konkret bedeutete dies, dass eine gemeinsame Urheberschaft nur so lange bestand, wie die eingebrachte Verbesserung unverändert blieb – wurde sie verändert, verlor der ursprüngliche Entwickler seine Rechte. Die Vereinbarung war in diesem Fall mit einer Übertragung jeglicher Urheberrechte gleichbedeutend. Oracle wurde mit dieser Vereinbarung auch das Recht zugesprochen, gewerbliche Schutzrechte (Patente, Gebrauchsmuster, Geschmacksmuster, Marken usw.) anzumelden und in Anspruch zu nehmen sowie das dafür notwendige Eigentum geltend zu machen.Anders als bei OpenOffice.org brauchten Entwickler, die ihre Verbesserungen LibreOffice zur Verfügung stellen möchten, nie derartige Vereinbarungen zu unterzeichnen. Sie blieben dadurch alleinige Inhaber des Urheberrechts. Seit der Übertragung des OpenOffice.org-Projekts an die Apache Software Foundation gilt das entsprechend auch bei OpenOffice.org. Dennoch seien laut der Document Foundation gerade diese rechtlichen Aspekte ausschlaggebend dafür gewesen, dass sich schnell nach Gründung des Projekts weit mehr Entwickler LibreOffice zuwandten, als es jemals zuvor bei OpenOffice.org der Fall gewesen sei.
Dieter Brors, Ralf Nebelo: Kampf der Zwillinge: LibreOffice fordert OpenOffice heraus. In: c’t – magazin für computertechnik. Nr. 5. Hannover 2011, S. 106–109 (Der Artikel beschreibt die Unterschiede zwischen LibreOffice 3.3 und OpenOffice.org 3.3). 
Karsten Günther: LibreOffice – kurz & gut. 1. Auflage. O’Reilly, Beijing/Cambridge/Farnham/Köln/Sebastopol/Tokyo 2011, ISBN 978-3-86899-118-5. 
Thomas Krumbein: LibreOffice 3 – Einstieg und Umstieg: Kompakte Einführung in alle Module. 1. Auflage. Tintal Verlag, Wiesbaden 2012, ISBN 978-3-943771-01-5.
Florian Effenberger: Die Document Foundation – 18 Monate danach. (Memento  vom 24. April 2012 im Internet Archive) Videovortrag von der Cebit 2012

Lichtwellenleiter (LWL), oder Lichtleitkabel (LLK) sind aus Lichtleitern bestehende und teilweise mit Steckverbindern konfektionierte Kabel und Leitungen zur Übertragung von Licht. Das Licht wird dabei in Fasern aus Quarzglas oder Kunststoff (polymere optische Faser) geführt. Sie werden häufig auch als Glasfaserkabel bezeichnet, wobei in diesen typischerweise mehrere Lichtwellenleiter gebündelt werden, die zudem zum Schutz und zur Stabilisierung der einzelnen Fasern noch mechanisch verstärkt sind.
Physikalisch gesehen sind Lichtwellenleiter dielektrische Wellenleiter. Sie sind aus konzentrischen Schichten aufgebaut; im Zentrum liegt der lichtführende Kern, der umgeben ist von einem Mantel mit einem etwas niedrigeren Brechungsindex sowie von weiteren Schutzschichten aus Kunststoff. Je nach Anwendungsfall hat der Kern einen Durchmesser von einigen Mikrometern bis zu über einem Millimeter. Man unterscheidet Lichtwellenleiter nach dem Verlauf des Brechungsindexes zwischen Kern und Mantel (Stufenindex- oder Gradientenindexfasern) und der Anzahl von ausbreitungsfähigen Schwingungsmoden, die durch den Kerndurchmesser limitiert wird.
Multimodefasern, in denen sich mehrere tausend Moden ausbreiten können, haben ein stark strukturiertes Strahlprofil (siehe Bild rechts). In Monomodefasern, die einen sehr kleinen Kerndurchmesser haben, kann sich nur die sogenannte Grundmode ausbreiten, deren Intensität in radialer Richtung näherungsweise normalverteilt ist. Die Anzahl der auftretenden Moden beeinflusst die Signalübertragung, da jede Mode einen unterschiedlich langen Lichtweg nimmt. Deshalb zeigen Multimodefasern mit zunehmender Länge eine stärkere Signalverfälschung (Modendispersion) als Monomodefasern, die somit zur Signalübertragung über weite Strecken besser geeignet sind.
Lichtwellenleiter werden vor allem in der Nachrichtentechnik als Übertragungsmedium für leitungsgebundene Kommunikationssysteme bei Glasfasernetzen verwendet und haben hier, weil sie höhere Reichweiten und Übertragungsraten erreichen, die elektrische Übertragung auf Kupferkabeln in vielen Bereichen ersetzt. Lichtwellenleiter werden aber auch vielfältig in anderen Bereichen eingesetzt, wie unter anderem
zur Übertragung von Energie als Lichtleitkabel für den flexiblen Transport von Laserstrahlung zur Materialbearbeitung und in der Medizin
für Beleuchtungs- und Abbildungszwecke unter anderem in Mikroskopbeleuchtungen, Lichtleitkabeln und Bildleitern in Endoskopen sowie zur Geräte- und Gebäudebeleuchtung und zu Dekorationszwecken
in der Messtechnik als Bestandteil faseroptischer Sensoren, an Spektrometern und anderen optischen Messgeräten.
Schon 1870 versuchte John Tyndall, Licht gezielt durch einen Wasserstrahl zu leiten. In den Folgejahren beschäftigten sich Wissenschaftler und Techniker weltweit mit den Möglichkeiten, Lichtsignale durch verschiedene Medien zu übertragen. Mitte der 1950er Jahre wurden optische Leiter vor allem zur Beleuchtung innerer Organe in der Medizintechnik angewandt, für andere Anwendungen war der Lichtverlust im optischen Leiter noch zu groß. Erst mit der Entwicklung des ersten Lasers durch Theodore Maiman 1960 konnte man Licht konzentriert durch ein Medium transportieren. Die gezielte Informationsübertragung über Lichtwellenleiter trat nun aus der experimentellen Phase in die der technischen Realisierung.
Das erste optoelektronische Lichtwellenleiter-System erfand 1965 Manfred Börner. Er entwarf ein optisches Weitverkehrs-Übertragungssystem, das Laserdioden, Glasfasern und Photodioden kombinierte. 1966 meldete er das System für das Unternehmen AEG-Telefunken zum Patent an. Alle optischen Weitverkehrs-Übertragungssysteme arbeiten noch heute nach diesem von Manfred Börner vorgeschlagenen Systemprinzip. Für seine Erfindung wurde Börner 1990 mit dem Eduard-Rhein-Preis ausgezeichnet.
1966 entdeckten Charles Kuen Kao und George Hockham, dass vor allem Unreinheiten im Glas zu Verlusten bei der Übertragung führen. Für seine Pionierarbeiten im Bereich der Glasfaseroptik wurde Kao 2009 mit dem Nobelpreis für Physik geehrt. 1970 produzierte und entwickelte das amerikanische Unternehmen Corning Inc. den ersten Lichtwellenleiter, der in der Lage war, Signale auch über eine längere Strecke ohne größere Verluste zu übertragen. Die Nutzung von Lichtwellenleitern zur Übertragung von Telefonsignalen wurde von nun an stetig vorangetrieben, und bereits 1978 verband die Deutsche Bundespost die Vermittlungsstellen in der Aßmannshauser Straße und in der Uhlandstraße in Berlin-Wilmersdorf über eine ca. 4 km lange Verbindungsstrecke aus mehreren Glasfasern. In den folgenden Jahren wurden die Lichtwellenleiter immer weiter verbessert, über immer längere Strecken konnten immer höhere Datenmengen mit immer höheren Datenraten übertragen werden. 1985 zum Beispiel übertrug die British Telecom erstmals Signale ohne Zwischenverstärkung über eine Strecke von 250 km.1987 entwickelte Heraeus ein Verfahren zur Herstellung von hochreinem, synthetischem Quarzglas aus der Gasphase. Durch synthetisches Quarzglas konnten metallische Verunreinigungen und Feuchtigkeitsspuren des natürlichen Quarzglases um mehrere Größenordnungen reduziert werden. Die von Heraeus produzierten Quarzglas-Vorformen machen rund 95 % der Glasfasern für die optische Nachrichtenübertragung aus.Anfangs hatten Lichtwellenleiter im Vergleich zu elektrischen Koaxialkabeln zu hohe Dämpfung, was ihren Einsatz für längere Strecken ausschloss. Dies hat sich im Laufe der Jahre ins Gegenteil verkehrt. Lichtwellenleiter umspannen heute unseren Planeten und bilden das Rückgrat der globalen Kommunikation und Informationsübertragung. AT&T, NEC und Corning stellten im Mai 2009 einen neuen Weltrekord auf. Auf einer einzelnen Glasfaser übertrugen sie über eine Strecke von 580 km 320 Kanäle mit einer Datenübertragungsrate von jeweils 114 Gigabit pro Sekunde und erzielten so eine summierte Datenübertragungsrate von insgesamt 32 Terabit pro Sekunde.
Die als Lichtwellenleiter bezeichneten Glasfasern bestehen im Inneren aus einem Kern (1 – engl. core) und einem umgebenden Mantel (2 – engl. cladding) mit etwas niedrigerem Brechungsindex (nKern > nMantel). Durch die dadurch auftretende Totalreflexion an der Grenzschicht zum Kern wird die Führung der Strahlung bewirkt. Der Mantel besteht dazu meist aus reinem Quarzglas (SiO2) und der höhere Brechungsindex im Kern wird durch Dotierung mit Germanium oder Phosphor erreicht, wodurch im amorphen Siliziumdioxid-Gefüge des Quarzglases zusätzlich geringe Anteile an Germaniumdioxid (GeO2) bzw. Phosphorpentoxid (P2O5) entstehen. Es ist aber auch möglich, den Kern aus reinem SiO2 herzustellen und den Mantel mit Bor oder Fluor zu dotieren, was zu einer Verringerung des Brechungsindexes führt. (Reine SiO2-Kerne sind besser geeignet zur Übertragung von Wellenlängen im blauen und ultravioletten Spektralbereich.)
Der Mantel besitzt weiterhin eine Schutzbeschichtung (3 – engl. coating und/oder buffer), sowie eine äußere Schutzhülle (4 – engl. jacket). Die Mantelbeschichtung ist ein Schutz vor mechanischen Beschädigungen und besteht meist aus einer Lackierung aus speziellem Kunststoff (etwa Polyimid, Acryl oder Silikone), welche die Faser auch vor Feuchtigkeit schützt. Ohne die Beschichtung würden die auf der Faseroberfläche vorhandenen Mikrorisse zu einer erheblichen Verringerung der mechanischen Belastbarkeit führen.
Patchkabel (meist Simplex- oder Duplex-Ausführung) und mehradrige Erdkabel können als Glasfaserkabel ausgeführt sein. Die einzelnen Glasfasern werden bei Patchkabeln durch einen wenige Millimeter dicken Kunststoff- oder Metallmantel geschützt (jacket), und Erdkabel sind zusätzlich zur mechanischen Stabilisierung im Inneren mit Metalldrähten oder -kabeln versehen, sowie im äußeren Bereich eventuell mit einem Metallgeflecht zum Schutz gegen Beschädigung von außen (wie etwa Tierbiss).
Der Kern von polymeren optischen Fasern (POF) besteht meist aus Polymethylmethacrylat (PMMA) und seltener aus Polycarbonat (PC). Der Mantel wird bei diesen Fasern leicht mit Fluor dotiert, um einen geringeren Brechungsindex zu erhalten. Auf das Coating kann bei den POF verzichtet werden, da das verwendete Material unempfindlicher ist gegen mechanische Beanspruchung als Quarzglas. Weiterhin gibt es auch Fasern mit einem Quarzglaskern und einem Mantel aus fluordotiertem Kunststoff, welche als Hard-Clad Silica Fiber (HCS) oder Polymer-Clad Silica Fiber (PCS) bezeichnet werden. Sie können zur Verbesserung der mechanischen und thermischen Eigenschaften auch zusätzlich mit einem Coating (mitunter aus Ethylen-Tetrafluorethylen – ETFE) versehen sein.
Lichtwellenleiter sind physikalisch gesehen dielektrische Wellenleiter, mit welchen elektromagnetische Strahlung vom ultravioletten bis in den infraroten Spektralbereich übertragen werden kann (ca. 350–2500 nm). In diesen Wellenleitern können sich in Abhängigkeit von Geometrie und Beschaffenheit nur bestimmte Schwingungsmoden ausbreiten, die sich voneinander durch den räumlichen Verlauf der elektrischen und magnetischen Feldstärke unterscheiden. In metallischen Wellenleitern sind die Moden transversal-elektrisch (TE) und transversal-magnetisch (TM), das heißt, dass deren elektrische bzw. magnetische Feldstärke überall rein transversal zur Ausbreitungsrichtung ausgerichtet ist, die entsprechende longitudinale Feldkomponente verschwindet (TE-Moden = Ey,Hx,Hz [Ez=0] bzw. TM-Moden = Hy,Ex,Ez [Hz=0]). Im Gegensatz zu metallischen Wellenleitern treten die TE- und TM-Moden in Lichtwellenleitern im Allgemeinen nicht voneinander getrennt auf, und als Folge des rotationssymmetrischen Brechungsindexverlaufs existieren sogenannte Hybrid-Moden, bei denen immer beide Feldkomponenten in Ausbreitungsrichtung vorhanden sind. Diese werden nach den vorhandenen Hauptfeldkomponenten, als HE- (Ey,Hx,Hz) oder EH-Moden (Hy,Ex,Ez) bezeichnet.
Beim Lichtwellenleiter ist der Brechungsindexunterschied zwischen Kern und Mantel im Allgemeinen sehr gering (Δ ≈ 0,003), man spricht von einem schwach führenden Wellenleiter. Für diesen speziellen Fall sind die transversalen Feldkomponenten näherungsweise linear polarisiert und die Feldkomponenten in Ausbreitungsrichtung sind vernachlässigbar. Die so genäherten Moden heißen linear polarisiert (LP). Bei der Bezeichnung der LPl,m-Moden charakterisieren die Indizes die Struktur der Intensitätsverteilung: m Nullstellen in radialer Richtung, 2 · l Nullstellen bei 360°-Umlauf der Winkelkoordinate (l Knotenpaare). Die Moden entstehen aus den Hybrid-Moden und sind teilweise Linearkombinationen einzelner HE-/EH-Moden. (Bei den Hybrid-Moden bezeichnen die Indizes die Struktur in X- und Y-Richtung, zum Beispiel entsteht die LP01-Mode aus der HE11-Mode).In einem Lichtwellenleiter kann sich in Abhängigkeit vom Kerndurchmesser und vom Brechungsindexunterschied entweder nur die Grundmode oder zusätzlich mehrere höhere Moden ausbreiten. Die Einteilung erfolgt hiernach in Monomodefasern (engl. single-mode fiber, SMF), in denen sich für bestimmte Wellenlängenbereiche nur die LP01 Grundmode ausbreiten kann, und Multimodefasern (engl. multi-mode fiber, MMF), welche in der Regel mehr als hundert bis mehrere tausend Moden besitzen.
Bezogen auf die Faserstruktur werden weitere Unterscheidungen innerhalb der beiden Faserarten getroffen:
So wird bei den Multimodefasern zwischen Stufenindexfasern und Gradientenindexfasern unterschieden, wobei sich bei ersterer der Brechungsindex zwischen Kern- zum Mantelglas radial nach außen hin in Form einer Stufe abrupt, und bei letzterer kontinuierlich in Form einer Parabel ändert.
Monomodefasern gibt es typischerweise nur als Stufenindexfasern, aber durch Einbringung von besonderen Strukturen oder Dotierungsprofilen lassen sich bestimmte Eigenschaften gezielt manipulieren. So etwa in polarisationserhaltenden, dispersionskompensierenden oder biegeunempfindlichen Monomodefasern.
Der Kern von Multimodefasern weist einen Kerndurchmesser von 50 µm bis zu über 1500 µm auf. Die am weitesten verbreiteten Multimode-Glasfasern im Telekommunikationsbereich sind dabei 50-µm- bzw. 62,5-µm-Gradientenindexfasern (siehe: Faserkategorien und Einsatzgebiete). Der Kern wird bei diesen Fasern von einem Mantel mit einem Außendurchmesser von 125 µm sowie einem Coating mit 250 µm (diese Werte gelten typischerweise bis zu Kerndurchmessern von ca. 100 µm) umgeben. Bei größeren Kerndurchmessern ist der Mantel 30 bis 60 µm und das Coating ca. weitere 100 µm größer im Gesamtdurchmesser (siehe: Tabelle).
In Multimodefasern kann die Führung des Lichtes strahlenoptisch durch die auftretende Totalreflexion an der Grenzschicht zwischen Kern und Mantel beschrieben werden. Für den einfachen Fall einer Stufenindexfaser ergibt sich aus dem snelliusschen Brechungsgesetz mit den unterschiedlichen Brechungsindizes für den Kern (engl. core, 
    {\displaystyle \theta _{\mathrm {max} }=\arcsin \left({\frac {1}{n_{0}}}{\sqrt {n_{1}^{2}-n_{2}^{2}}}\right)}
   des umgebenden Mediums und dem Argument des Arkussinus (arcsin) wird als numerische Apertur NA der Faser bezeichnet und ergibt sich für 
  Die numerische Apertur ist ein Maß für den Brechungsindexunterschied zwischen Kern und Mantel und liegt bei Multimodefasern im Bereich von 0,2–0,3 sowie für Monomodefasern bei ca. 0,1. Aus dem maximalen Akzeptanzwinkel, unter dem eingekoppeltes Licht in der Faser noch geführt werden kann, ergibt sich ein Akzeptanzkegel (engl. acceptance cone, siehe Bild rechts), welcher auf Grund der Umkehrbarkeit des Lichtweges auch dem Austrittskegel entspricht.
Bedingt durch die Größe von Multimodefasern (Kerndurchmesser ist sehr viel größer als die Wellenlänge) können sich, wie eingangs erwähnt, mehrere Moden ausbreiten. Die typischerweise mehr als hundert bis mehrere tausend Moden können strahlenoptisch als eine Vielzahl miteinander interferierender Lichtwege betrachtet werden und erzeugen ein stark strukturiertes Strahlprofil am Faserausgang (siehe Bild). Dieses ist wiederum stark von der Art der Lichteinkopplung (Ausleuchtung der Faser in Abhängigkeit von der benutzten Lichtquelle, siehe auch: Over-Filled- bzw. Reduced-Mode-Launch) und der Biegung der Faser (Modendurchmischung) abhängig. Bedingt durch die unterschiedliche Länge der Lichtwege kommt es bei der Nachrichtenübertragung über große Distanzen zu nicht zu vernachlässigenden Laufzeitunterschieden, welche sich negativ auf die Signalqualität und Bandbreite auswirken (Modendispersion).
Zur Reduzierung der Laufzeitunterschiede bei Stufenindexfasern werden sogenannte Gradientenindexfasern (auch Gradientenfaser) verwendet, bei denen der Brechungsindex vom Faserkern nach außen hin allmählich abfällt, also einen Gradienten aufweist. Während bei der Stufenindexfaser die (Gruppen-)Laufzeitdifferenzen der Moden im einfachen geometrisch-optischen Bild mehr oder weniger den geometrischen Wegunterschieden entsprechen (langer Weg ergibt große Flugzeit), so sind die Verhältnisse bei der Gradientenindexfaser deutlich komplexer. Hier ist die Laufzeit pro Wegeinheit in den äußeren Bereichen wegen des geringeren Brechungsindex kleiner. Bei einem zum Beispiel parabolisch abfallenden Brechungsindex (Exponent = 2) verlaufen meridionale Strahlen, also Strahlen durch die Faserachse, etwa entlang einer sinusförmigen Bahn. Diese Strahlen legen zwar einen längeren Weg zurück als Strahlen entlang der Faserachse, aber durch den nach außen abnehmenden Brechungsindex holen diese im äußeren Bereich zeitlich wieder auf. Bei geeigneter Profilbildung kann die Angleichung aller Strahlen beziehungsweise aller ausbreitungsfähigen Moden um bis zu drei Größenordnungen besser sein als bei einer Stufenindexfaser.
Um solche Gradientenindexfasern mit optimalem Brechungsindexprofil richtig zu dimensionieren, muss berücksichtigt werden, dass der Brechungsindex nicht nur vom Ort, sondern zugleich auch von der Wellenlänge abhängt. Da das Profil in radialer Richtung durch eine Stoffdotierung realisiert wird, ändert sich der Stoff und somit auch die Materialdispersion. Der Brechungsindex ist also in komplexer Weise von den Variablen Ort und Wellenlänge abhängig. Je höher die Bandbreite einer Gradientenindexfaser sein soll, umso besser müssen nun die Laufzeiten der Strahlen bzw. Moden aneinander angeglichen werden. Die Berechnung der Wellenausbreitung und der Laufzeitdifferenzen in Gradientenindexfasern ist sehr komplex und kann nach der WKB-Methode erfolgen. Der optimale Exponent liegt danach nur in der Nähe von zwei und je nach Dotiermaterial zur Einstellung des Gradientenprofils ergibt sich meistens ein von zwei deutlich abweichender Exponent. Eine Angleichung der Flugzeitdifferenzen um bis zu drei Größenordnungen gegenüber der Stufenindexfaser wird nur durch eine hochpräzise Realisierung des optimalen Exponenten erreicht. Wegen des Einflusses der Materialdispersion (Wellenlängenabhängigkeit des Brechungsindex) ist bei Gradientenindexfasern weiterhin zu beachten, dass das optimale Profil, und somit die maximale Bandbreite der Faser, auch von der Betriebswellenlänge der verwendeten Lichtquelle abhängt.
Wenn der Kerndurchmesser lediglich einige Vielfache der Wellenlänge des Lichts beträgt, werden höhere transversale Moden nicht unterstützt. Jedoch kann Licht in der LP01-Grundmode übertragen werden. Fasern, die für diesen Betrieb ausgelegt sind, werden Monomodefaser, Singlemode-Faser (engl. single-mode fiber, SMF) oder Einmodenfaser genannt. Die Modenstruktur von Monomodefasern, also die transversale Abhängigkeit des elektrischen und magnetischen Feldes, lässt sich nur durch Anwendung der maxwellschen Gleichungen und der sich daraus ergebenen Wellengleichung bestimmen. Bei dieser wellenoptischen Betrachtungsweise erhält man als Lösung den Parameter normierte Frequenz bzw. V-Zahl, der sich für den Fall einer Stufenindexfaser wie folgt aus der numerischen Apertur NA (bzw. den Brechungsindizes von Kern und Mantel) und dem Kerndurchmesser 
    {\displaystyle V={\frac {d_{\text{K}}\pi }{\lambda _{0}}}{\sqrt {n_{\text{K}}^{2}-n_{M}^{2}}}={\frac {d_{\text{K}}\pi }{\lambda _{0}}}NA}
  Nur für Werte von V < 2,405 ist die entsprechende Faser einmodig und es kann sich nur die LP01-Grundmode ausbreiten. Bei größeren Werten treten höhere Transversalmoden auf und es lässt sich für jede Faser die sogenannte Cut-off-Wellenlänge angeben, bis zu der noch Einmodenbetrieb vorherrscht (
  Monomodefasern haben meistens einen Kerndurchmesser von 3 bis 9 µm, wobei der äußere Durchmesser mit dem Cladding (Brechungsindex um etwa 0,003 niedriger) auch hier 125 µm beträgt. Die Übertragung der Leistung erfolgt hauptsächlich im Kern der Faser. Die näherungsweise gaußförmige Intensitätsverteilung der LP01 Grundmode erstreckt sich aber bis in den Mantel hinein und im inneren Bereich existiert somit ein exponentiell schnell abklingendes evaneszentes Feld. Für Monomodefasern wird daher der Modenfelddurchmesser (engl. mode-field diameter, MFD) angegeben, bei dem die Amplitude der Mode in ihrem radialen Verlauf auf 1/e, bzw. am Faserausgang die Intensität (Bestrahlungsstärke) im Nahfeld auf 1/e2 abgefallen ist. Durch Approximation der Feldverteilung der Grundmode durch eine Gauß-Verteilung erhält man für eine Stufenindexfaser den folgenden Zusammenhang zwischen Kern- und Modenfelddurchmesser (siehe rechte Graphik), auch als Markuse-Formel bekannt:
  Aus der graphischen Darstellung der Gleichung wird ersichtlich, dass im Einmodenbereich für V < 2,405 der Modenfelddurchmesser stets größer ist als der Kerndurchmesser. Weiterhin nimmt der Modenfelddurchmesser für längere Wellenlängen zu, da sich mit höherem 
   die normierte Frequenz V verringert (siehe obere Gleichung für V). Dies hat zur Folge, dass Monomodefasern nur in einem Bereich bis ca. 200 bis 300 nm über der Cut-off-Wellenlänge einsetzbar sind, da die ausbreitungsfähige Grundmode bei höheren Wellenlängen immer schlechter geführt wird und die Biegeverluste, durch den erhöhten Anteil der sich in den Mantel erstreckenden Intensitätsverteilung, steigen.
Beispielsweise beträgt bei der Monomodefaser Corning SMF-28e der Kerndurchmesser 8,2 µm, der MFD dagegen 9,2 µm bei 1310 nm bzw. 10,4 µm bei 1550 nm. Der MFD kann aus dem Brechungsindexverlauf, der vor dem Ausziehen des Halbzeugs zur Faser einer Messung zugänglich ist, berechnet oder grob im Nahfeld gemessen werden. Genauer ist allerdings das Ausmessen des Fernfelds mit anschließender Rücktransformation mittels Hankel-Transformation. Bei Corning wird beispielsweise als Referenzmethode die Variable-Aperture Method in the Far Field (VAMFF) benutzt nach TIA/EIA-Standard FOTP-191.
Die folgende Tabelle gibt die Größenverhältnisse von Kern, Mantel und Schutzhülle (coating) für einige übliche Monomode- und Multimodefasern wieder. Für Monomodefasern wird üblicherweise der Modenfelddurchmesser anstatt des Kerndurchmessers angegeben. Weiterhin sind für die Monomodefasern die entsprechenden Cut-off-Wellenlängen angegeben, unterhalb derer auch höhere Moden existieren. Typischerweise sind die Monomodefasern für Wellenlängen bis zu 200–300 nm über der Cut-off-Wellenlänge geeignet. Mit größer werdenden Wellenlängen steigt der Anteil der im Fasermantel transportierten Leistung (MFD ~ λ) und die entsprechende Faser wird dadurch stark biegeempfindlich und die Koppeleffizienz sinkt. (Bei den kursiv angegebenen Fasertypen handelt es sich nicht um reine Glasfasern, siehe dazu unter POF und PCS.)
Sowohl die Länge der Übertragungsstrecke als auch die Übertragungsrate werden durch Eigenschaften des Lichtwellenleiters begrenzt. Die maximale Übertragungsstrecke ist bei digitalen Signalen erreicht, wenn der Empfänger nicht mehr sicher die Flanken des Signals erkennen kann. Dies ist der Fall, wenn das Signal zu schwach oder wenn die Signalform zu stark verzerrt ist. Je geringer die Verluste pro Kilometer ausfallen, desto weiter kann ein Signal übertragen werden, bevor es zu schwach wird. Die Dispersion beeinflusst, wie sehr sich Wellenzüge bei der Übertragung verformen. Das Spektrum eines Signals ist umso breiter, je höher seine Taktrate ist. Bei gegebener Dispersion nehmen daher die Verformungen mit der Taktrate zu.
Die während der Lichtleitung in Glasfasern entstehenden intrinsischen Verluste sind auf fundamentale Materialeigenschaften und unerwünschte Verunreinigungen des verwendeten Glases zurückzuführen.
Es gibt materialspezifische Absorptionsbanden im ultravioletten und infraroten Spektralbereich. Die Ausläufer erstrecken sich jeweils bis in den dazwischen liegenden Bereich der optischen Datenübertragung (nahes Infrarot, NIR), und würden unter Vernachlässigung der weiter unten beschriebenen zusätzlich auftretenden Verlustmechanismen ein theoretisches Dämpfungsminimum bei ca. 1500 nm ergeben.
Die UV-Absorption beruht auf elektronischen Übergängen in der komplexen Bandstruktur des Glases, welche durch die variierenden Bindungslängen und Bindungswinkel im unregelmäßigen Siliziumdioxid-Gefüge (SiO2) gegeben ist. Die Bandübergänge werden durch die Anregung von Phononen und Exzitonen, und deren mögliche Interaktion untereinander verursacht. Die UV-Absorption amorpher Materialien wie Glas zeigt ein typisches exponentielles Abklingverhalten mit zunehmender Wellenlänge, welche als Urbach-Ausläufer (engl. Urbach tail) bezeichnet werden.
Im infraroten Spektralbereich kommt es durch Materialresonanzen zu Absorptionsbanden, die hauptsächlich auf Molekülschwingungen der Si-O-, Ge-O- und P-O-Bindungen zurückzuführen sind.Die UV-Absorption wird zusätzlich noch überlagert von der Rayleigh-Streuung, welche durch die statistische amorphe Struktur des Glases verursacht wird und mit 1/λ4 zu längeren Wellenlängen hin abnimmt. Sie überwiegt im nahen infraroten Spektralbereich bis ca. 1500 nm und trägt entscheidender zur Gesamtdämpfung bei als die Ausläufer der UV-Absorption. Die in Glasfasern ebenfalls auftretende Brillouin- und Raman-Streuung kann bei den meisten Anwendungen typischerweise vernachlässigt werden, da deren Beitrag zur Dämpfung sehr gering ist. Mögliche Beeinflussungen durch nichtlineare Effekte treten bei diesen Streuprozessen erst beim Einsatz hoher optischer Leistungen auf (stimulierte Brillouin- bzw. Raman-Streuung).
Weitere Ursachen sind Verunreinigungen des Fasermaterials, hauptsächlich während des Herstellungsprozesses absorbiertes Wasser, oder des Ausgangsmaterials. Höhere Harmonische der Molekülschwingungen der O-H-Bindungen (Fundamentale um etwa 2800 nm) erzeugen zusätzliche Absorptionsmaxima bei 950 nm, 1240 nm und 1380 nm, Wasserbanden welche auch als water peaks (engl.) bezeichnet werden. Die einzelnen Beiträge zum Energieverlust ergeben einen wellenlängenabhängigen Gesamtverlust, wie er im Bild rechts dargestellt ist. Einfache Fasern werden deshalb in den um die Minima liegenden Spektralbereichen um 850 nm, 1310 nm (O-Band) oder 1550 nm (C-Band) betrieben.
Eine Weiterentwicklung der Standard-Singlemode-Faser (SSMF) sind die sogenannten Low-Water-Peak-Fasern (ITU-T G.652.C und G.652.D) und Zero-Water-Peak-Fasern. Im Gegensatz zur SSMF werden diese Fasern durch verbesserte Herstellungsprozesse und Ausgangsmaterialien (nahezu) wasserfrei hergestellt, wodurch die Dämpfung im Wellenlängenbereich zwischen 1260 nm und 1625 nm stark reduziert werden kann.
Mit diesen Fasern wird das sogenannte E-Band (engl. extended band) für die Datenübertragung geöffnet. Dieser Bereich wird überwiegend mit der CWDM-Technologie (engl. coarse wavelength division multiplex, dt. ‚grobes Wellenlängenmultiplexing‘) erschlossen, die es ermöglicht aufgrund der großen Kanalabstände auf sehr kostengünstige, ungekühlte Laser für die Übertragung zurückzugreifen.
Bei Biegeradien der Glasfasern von einigen Zentimetern entstehen Verluste durch Abstrahlung von Leistung aus dem Kern in den Mantel. Für Multimodefasern kann dies strahlenoptisch dadurch erklärt werden, dass der Grenzwinkel für die Totalreflexion an der gebogenen Stelle unterschritten wird und dadurch ein Teil des Lichtes aus dem Glasfaserkern entweicht. Für Monomodefasern gilt die wellenoptische Betrachtungsweise, die aussagt, dass immer ein Teil der transportierten Leistung sich auch auf den Mantel erstreckt. Der Modenfelddurchmesser ist immer größer als der Kerndurchmesser, und nimmt mit der Wellenlänge zu. Im äußeren Bereich der Biegestelle kommt es mit zunehmendem Abstand vom Kern zu einer Wegverlängerung, die ein Zurückbleiben der Phasenfronten verursacht, da die maximale Ausbreitungsgeschwindigkeit im Mantel nicht überschritten werden kann. Durch die nicht mehr ebene Wellenfront kommt es zu einer radialen Komponente des Poyntingvektors, welche eine Abstrahlung von Energie zur Folge hat.
Die beschriebenen Effekte machen sich in Form einer Dämpfungserhöhung bemerkbar, welche je nach Leistungsbudget, Streckenlänge und Biegung zum Totalausfall der Übertragung führen kann.
Speziell für den Bereich Fiber to the Home (FTTH) und den damit verbundenen schlechteren Installationsbedingungen in Wohnhäusern, entwickelten die Glasfaserhersteller in der jüngsten Zeit neue Glasfasern mit reduzierten Biegeverlusten (engl. bending loss). Ziel ist es bei diesen Low-Bending-Loss-Singlemode- und -Multimode-Fasern, den Brechungsindex im Mantel durch geeignete Maßnahmen zu verringern bzw. so zu modifizieren, dass der Modenfelddurchmesser reduziert und somit weniger Leistung in den Mantel abgestrahlt wird. Vorgeschlagene Methoden sind dabei die Einbringung einer ringförmigen mit Fluorid dotierten Schicht im Mantel, in der der Brechungsindex grabenförmig um den Kern verringert wird (engl. trench-assisted), die Einbringung einer ringförmigen Nano- oder Mikrostruktur (photonische-Kristall-Struktur) aus Hohlräumen im Cladding (engl. photonic-crystal fiber, kurz PCF), welche auch zu einer Reduzierung des effektiven Brechungsindexes (siehe Wellenleiterdispersion) in den entsprechenden Bereichen führt.Durch solche biegeunempfindlicheren Fasern ist es möglich, auch bei Biegeradien im Bereich von unter 10 mm eine nahezu verlustlose Übertragung sicherzustellen. Im Singlemode-Bereich sind sie spezifiziert nach ITU-T G.657, Kategorie A und B, wobei die Kategorie A die Anforderungen für Standard-Singlemode-Fasern nach ITU-T G.652 erfüllt.
Beim Einkoppeln des Lichtes in die Faser, sowie beim Verbinden von Fasern mittels Steck- und Spleißverbindungen können Einfüge- bzw. Koppelverluste durch mehrere Faktoren auftreten:
falsche Anpassung der numerischen Apertur und Fokusgröße zwischen Einkoppeloptik und Faser.Bei Verbindungen von Lichtwellenleitern ist es wichtig, dass die Lage des Faserkerns mittig ist (Kernexzentrizität), sowie die Abmessungen und Rundheit der Fasern genau eingehalten werden und zueinander kompatibel sind. Die Exzentrizität des Faserkerns (Versatz zwischen Mittelpunkt des Faserkerns und Mittelpunkt des Fasermantels) bei heutigen Monomodefasern liegt bei kleiner 0,5 µm. Weitere transversale Versätze können durch Toleranzen bei der Steckermontage entstehen, wo die Faser in eine Aufnahmehülse (engl. ferrule) mit einer Bohrung von zum Beispiel 
   µm (Multimodefasern) eingeklebt wird, sowie durch Toleranzen der Führungshülsen der Steckeraufnahmen, welche im Bereich von 1 bis 2 µm liegen. Da das Signal bei Monomodefasern durch einen wenige Mikrometer dicken Kern transportiert wird, führt jede Fehlanpassung zu einer Teilüberlappung und somit zu einem Leistungsverlust.
Der größere Kerndurchmesser von Multimodefasern gestattet größere Toleranzen am Übergang zwischen zwei Fasern. Applikationen wie 10-Gigabit-Ethernet und speziell 40- und 100-Gigabit-Ethernet haben jedoch nur geringe Reserven für Dämpfung und Verluste und zu hohe Toleranzen und Abweichungen können daher auch hier schnell die Grenzen erreichen.
Verschiedene Dispersionseffekte tragen dazu bei, dass es während der Übertragung zu einer Verformung der dem Licht aufmodulierten Signalform kommt, was auf unterschiedliche Ausbreitungsgeschwindigkeiten verschiedener Signalanteile zurückzuführen ist: Das zur Informationsübertragung genutzte Licht hat eine spektrale Breite, die mindestens so groß ist wie die Bandbreite des aufmodulierten Nutzsignales. Erreichen nun unterschiedliche Wellenlängen den Empfänger mit unterschiedlicher Verzögerung, so verschleift beispielsweise die Signalform eines Rechteckes. Die Verformung ist umso größer, je länger die Faserstrecke ist und je größer ihre Dispersion bei den benutzten Wellenlängen ist.
Modendispersion: In Stufenindex-Multimodefasern können sich verschiedene Moden verschieden schnell ausbreiten. Dies hängt vom radialen Verlauf des Brechungsindex ab. Ein parabelförmig nach außen absinkender Brechungsindex senkt die Modendispersion im Idealfall bis auf Null. Für Einmodenfasern entfällt diese Art der Dispersion; es dominiert die
chromatische Dispersion: Sie ist die wellenlängenabhängig unterschiedliche Ausbreitungsgeschwindigkeit. Eine praktische Maßeinheit der chromatischen Dispersion für die Übertragbarkeit digitaler Signale ist Pikosekunden pro Kilometer Faserlänge und Nanometer Wellenlängenunterschied, ps/(km·nm). Die chromatische Dispersion ist die Summe zweier Mechanismen:
Materialdispersion: Anders als im Vakuum ist die Ausbreitungsgeschwindigkeit von Licht in Glas abhängig von der Frequenz des Lichts, was auch bei Glaslinsen als chromatischer Fehler zu beobachten ist (chromatische Aberration). Die Materialdispersion wechselt ihr Vorzeichen abhängig von der Glassorte im nahen Infrarot. Das bedeutet, dass bei einer bestimmten Wellenlänge die Materialdispersion verschwindet beziehungsweise dass Materialien gewählt werden können, für die die Materialdispersion bei der gewünschten Wellenlänge null ist.
Wellenleiterdispersion: Der effektive Brechungsindex liegt zwischen dem des Faserkerns und dem geringeren Index des Mantels. Die Gewichtung selbst hängt von der Wellenlänge ab: Je langwelliger, desto tiefer dringt die Mode in den Mantel ein und desto geringer ist der effektive Brechungsindex und somit umso höher die Ausbreitungsgeschwindigkeit. Die Wahl des Mantelmaterials nimmt damit Einfluss auf die Dispersion des Lichtwellenleiters – je geringer der Brechzahlunterschied, desto geringer die Wellenleiterdispersion. Aber auch die Numerische Apertur nimmt ab.
Polarisationsmodendispersion (PMD): Licht breitet sich in einem doppelbrechenden Medium je nach Polarisation unterschiedlich schnell aus. Eine Glasfaser ist entweder auf Grund ihrer Bauform doppelbrechend oder auf Grund äußerer Einflüsse wie etwa Biegung oder Temperaturschwankungen. Die PMD kann durch polarisationserhaltende Glasfasern (engl. polarization-maintaining optical fiber, PMF) unterdrückt werden, wobei die Lichtquelle dann nur eine Polarisationsmode anregen darf. Dieser Fasertyp kommt aber auf Grund der höheren Dämpfung und höherer Herstellungskosten nur auf kurzen Übertragungsstrecken und in der Messtechnik zum Einsatz. Solche Fasern sind radial gezielt inhomogen, z. B. durch eine geometrische Asymmetrie des Kerns oder durch Stress-Elemente im Fasermantel, die zu Spannungsdoppelbrechung im symmetrischen Kernen führen.Als Singlemode-Fasern für Weitverkehrsnetze werden heutzutage Non-Zero-Dispersion-Fasern (ITU-T G.655.C) verwendet. Sie verbinden eine sehr geringe Dämpfung mit einer geringen Dispersion im C-Band (engl. conventional band), wodurch im Gegensatz zu Standard-Singlemodefasern (SSMF) Übertragungen über längere Strecken ohne externe Dispersionskompensation möglich sind.
Der Brechungsindex von Glas hängt nicht nur von der Frequenz, sondern auch von der Amplitude des hindurch geleiteten Lichts ab. Für bestimmte, Soliton genannte Signalformen hebt dessen Einfluss die Verformungen durch von der Frequenz abhängige Dispersion auf. Seit über drei Jahrzehnten wird darauf hingewiesen, dass es dadurch im Prinzip möglich ist, eine Faserstrecke über tausende Kilometer ohne Repeater zu betreiben. Eine Signalverstärkung ist jedoch nötig. Praktische Hürden verhindern jedoch bisher einen breiten Einsatz in der Faserkommunikation.
Die Herstellung von Glasfasern erfolgt in zwei Schritten. Zuerst wird eine sogenannte Preform mittels chemischer Gasphasenabscheidung (englisch chemical vapor deposition, CVD) erzeugt, bei der es sich um einen Glasstab von typischerweise 1 m Länge und 10–50 mm Durchmesser handelt. Die Preform besitzt schon das Brechungsindexprofil der späteren Faser, welche später durch Aufschmelzen aus dieser gezogen wird.
Um eine möglichst geringe Dämpfung in Glasfasern zu erzielen, bedarf es einer besonders hohen chemischen Reinheit des erzeugten Quarzglases. Um dies zu erreichen, bedient man sich verschiedener CVD-Prozesse, bei denen sich hochreines Siliziumdioxid (SiO2) aus der Gasphase an der Preform abscheidet. Die eingesetzten Verfahren unterscheiden sich hauptsächlich darin, ob der Abscheidungsprozess im Inneren oder auf der Außenseite der Preform stattfindet. Bei allen Verfahren wird zur Erzeugung des Glases eine chemische Reaktion von Tetrachlorsilan (SiCl4) und Sauerstoff (O2) zu Siliziumdioxid und Chlor (Cl2) eingesetzt:
  .Für chemische Reaktion bei den gezielt eingebrachten Dotierungen (zur Realisierung des gewünschten Brechungsindexprofils, siehe Aufbau) gilt je nach Wertigkeit ähnliches (Germanium) bzw. in leichten Abwandlungen, z. B. Bor (B) oder Phosphor (P):
Das OVD-Verfahren (engl. outside vapor deposition, dt. ‚außenseitige Gasphasenabscheidung‘) ist die älteste Herstellungsmethode. Sie wurde von Corning entwickelt und wird dort immer noch verwendet. Bei diesem Verfahren wird das Glas auf der Außenseite eines massiven Rundstabes aus Aluminiumoxid oder Graphit aufgebracht, indem die gasförmigen Halogenide und Reaktionsgase kontrolliert in eine Brennerflamme geblasen werden und sich dann die entstehenden Glaspartikel am Glasstab abscheiden. Eine gleichmäßige Schicht wird durch entsprechende Rotation und Vortrieb des Stabes erreicht. Mehrere tausend Schichten können so aufgebracht werden um den gewünschten Brechungsindexverlauf zu erzielen. Durch einen anschließenden Sinterprozess wird die noch poröse Struktur verdichtet und noch vorhandene Gase und Wasserreste entfernt. Der innere Rundstab wird dann entfernt und durch weiteres Erhitzen des entstandenen Hohlstabes wird dieser zur Preform geschrumpft (kollabiert). Während des Kollabierens kommt es typischerweise zu einem Brechungsindexeinbruch in der Mitte des späteren Faserkerns, da es durch die Erhitzung im Innenbereich zu einer Ausgasung des Dotiermaterials Germanium (Ge), in Form von Germanium(II)-oxid (GeO) kommt.
Beim VAD-Verfahren (engl. vapor phase axial deposition, dt. ‚axiale Gasphasenabscheidung‘) wird das Glas an der Stirnseite eines rotierenden massiven Stabes abgeschieden, wobei das Brechungsindexprofil durch variable geometrische Anordnung der Gasbrenner bzw. -düsen erreicht wird. Auch hier wird die noch poröse Struktur später durch Sintern verdichtet, aber es ist kein Kollabieren des Rundstabes mehr nötig, und der bei der OVD entstehende radiale Brechungsindexeinbruch im Kern wird vermieden. Mit diesem Verfahren kann gewissermaßen eine endlose Preform erzeugt werden, was die Herstellung besonders langer Fasern ermöglicht.
Im Gegensatz zu den ersten beiden Verfahren findet beim MCVD-Verfahren (engl. modified chemical vapor deposition, dt. ‚modifizierte chemische Gasphasenabscheidung‘) der Abscheidungsprozess im Inneren eines Glasrohres statt, aus dem später der äußere Bereich des Mantels wird. Die gasförmigen Halogenide werden dazu, mit einer geeigneten Mischung aus Reaktionsgas (Sauerstoff) und inerten Transportgasen (Argon oder Helium), kontrolliert in das Glasrohr eingeblasen. Von außen wird das Rohr mittels Gasbrenner erhitzt und es kommt an den heißen Zonen zur Abscheidung der Glaspartikel. Durch Rotation des Rohres oder der Brenner bzw. geeignete Positionierung mehrerer Brennerflammen wird die Abscheidung rotationssymmetrisch erreicht. Durch Führung der Brenner entlang des Rohres werden dann gleichmäßige Schichten an der Innenseite erzeugt. Da sich zwischen den Brennerflammen und der Reaktionszonen die Glasrohrwand befindet, wird bei diesem Verfahren der Einschluss von Restgasen und Wasserdampf vermieden. Auch hier schließt sich vor dem Kollabieren ein Sintervorgang an. Ähnlich wie bei dem OVD-Verfahren kommt es auch hier zu einem Brechungsindexeinbruch, da das für den Kern typischerweise benutzte Germanium (Ge) in Form von Germanium(II)-oxid (GeO) während des Kollabierens an der Innenseite entweicht, welche später den Faserkernmittelpunkt bildet.
Beim PCVD-Verfahren (engl. plasma(-assisted) chemical vapor deposition, dt. ‚plasmaunterstützte chemische Gasphasenabscheidung‘) handelt es sich um eine Abwandlung der MCVD, bei dem die Gasbrenner durch Mikrowellengeneratoren (2,5–3 GHz) ersetzt werden, welche ein Plasma im Inneren des Rohres erzeugen. Hierbei wird direkt auf einen Quarzglas Kernstab aufgebaut der meist nicht dotiert ist. Eine zusätzlich elektrische Aufheizung des Rohres auf ca. 1000 °C verhindert mechanische Spannungen zwischen den aufgebrachten Schichten und dem Trägerglas. Bei diesem Verfahren schlägt sich das Glas gleich porenarm nieder und es kann auf den Sinterschritt verzichtet werden. Ein weiterer Vorteil ist die relativ hohe Geschwindigkeit und die erzielbare Schichtdicke von unter 1 µm, was die Realisierung von sehr präzisen Brechungsindexverläufen erlaubt.
Ähnliche Verfahren, die synonym als PCVD-Verfahren zu betrachten sind, ist das PECVD-Verfahren (engl. plasma-enhanced CVD), das PICVD-Verfahren (engl. plasma impulsed CVD) und das SPCVD-Verfahren (engl. surface plasma CVD), welche sich weitestgehend nur in der Art der Erzeugung des Plasmas und des verwendeten Druckes im Rohrinneren unterscheiden.
In Faserziehtürmen wird ein Bereich des Rohlings auf Temperaturen von ca. 2000 °C erhitzt. Bei dieser Temperatur wird das Glas so weich, dass es zu einer Faser gezogen werden kann. Die damit verbundene Verringerung des Durchmessers im Verhältnis von etwa 200:1 führt zu einer Längenänderung von ca. 1:40000. Damit können aus einem Meter des Rohlings 40 km Faser erzeugt werden. Das Profil des Brechungsindex bleibt während des Ziehvorganges erhalten.
Während des Ausziehens der Faser wird der Faserdurchmesser ständig überprüft und der Vortrieb der Faser entsprechend geregelt. Die blanke Glasfaser wird nach dem Ausziehen gleich mit einer Beschichtung aus Kunststoff wie beispielsweise Polyimid, Acryl oder Silikon versehen. Dazu wird die Faser durch einen Extruder geführt und anschließend der Kunststoff durch UV-Bestrahlung ausgehärtet. Eine Aushärtung durch Heizen ist auch möglich, aber langsamer. Die Faserziehgeschwindigkeiten liegen im Bereich von einigen hundert bis 2000 m pro Minute und bestimmen in Verbindung mit der Aushärtezeit maßgeblich die Höhe des Faserziehturms.  Vor dem Aufwickeln der fertigen Faser wird noch eine Zugfestigkeitsprüfung durchgeführt.
Lichtwellenleiter werden mit Steckverbindungen oder Spleißverbindungen miteinander oder mit anderen Komponenten verbunden. In der Nachrichtentechnik sind dies Sender, Empfänger oder Verstärker und in der Messtechnik, Spektroskopie oder Medizintechnik beispielsweise Laser, Sensoren oder Detektoren.
Zur Verbindung von rotierenden Teilen kommen sogenannte optische Schleifringe oder Drehübertrager zum Einsatz, welche die kontinuierliche Datenübertragung (analog oder digital) von stehenden auf rotierende Bauteile, wie etwa in Computertomographen oder Industrierobotern, ermöglichen.
Die Mehrheit der Steckverbindungen sind Stecker-Stecker-Verbindungen. Die verwendeten Stecker müssen dabei eine möglichst geringe Signaldämpfung (auch Einfügedämpfung, engl. insertion loss) und eine hohe Rückflussdämpfung (engl. return loss, Kehrwert des Reflexionsgrad), sowie eine hohe Reproduzierbarkeit bzw. Aufrechterhaltung dieser Parameter über mehrere hundert Verbindungszyklen besitzen.
Erzielt wird dieses durch die Verwendung von federnd gelagerten sehr präzisen zylindrischen Hülsen zur Faseraufnahme (sogenannte Ferrulen), welche in den Steckeraufnahmen in direkten Kontakt gebracht werden, womit eine Einfügedämpfung von 0,1–0,5 dB erreicht wird. Die hauptsächlich aus Metall oder Keramik bestehenden Ferrulen werden mit der eingeklebten Faser speziell angeschliffen bzw. poliert. Heute werden nur noch die sogenannten PC-Stecker verwendet (engl. physical contact), mit einer abgerundeten Endfläche (Radius ca. 10–15 mm), welche beim Stecken einen physischen Kontakt der Faserkerne herstellen.
Immer höhere Anforderungen an die Rückflussdämpfung der installierten Steckverbindungen führten schließlich zu immer besseren Polierqualitäten der PC-Stecker, wozu die Grade SPC (engl. super physical contact) und UPC (engl. ultra physical contact) gehören. Eine weitere Erhöhung konnte dann nur noch durch die sogenannten HRL-Stecker (engl. high return loss) bzw. APC-Stecker (engl. angled physical contact) erreicht werden (Werte für die Rückflussdämpfung siehe Tabelle). Bei dieser Steckerart ist die Steckerendfläche nicht nur ballig ausgeführt, sondern sie ist zusätzlich noch um einige Grad (Standard ist 8°) verkippt zum typischerweise rechten Winkel zur Faserachse. Durch diesen Aufbau wird von der Steckerendfläche reflektiertes Licht aus dem Kern über das Mantelglas in die Luft hinaus gebrochen und kann somit die Datenübertragung nicht mehr stören. Stecker dieser Bauart führen ein APC als Ergänzung in ihrer Bezeichnung (ST/APC, SC/APC, FC/APC, LC/APC, E2000/APC usw.). UPC- und APC-Steckertypen kommen speziell bei Monomodefasern zum Einsatz.
Die am häufigsten verwendeten Steckerarten sind heute LC (engl. local connector) und SC (engl. subscriber connector). Aus älteren Installationen sind auch noch ST (engl. straight tip) und E-2000 weit verbreitet. Der LC-Stecker gehört wie der MU-, LX.5- und der FV-45-Stecker zu den sogenannten small-form-factor-Steckern (SFF-Stecker). Diese besitzen 1,25 mm Ferrulen und ermöglichen durch ihre kleinere Bauform eine höhere Bestückungsdichte als ältere Stecker, wie beispielsweise der SC-, ST- und E-2000-Stecker mit 2,5 mm Ferrulen. Eine weitere Erhöhung der Portdichte kann mit Mehrfasersteckern mit MT-Ferrulen (engl. mechanical transfer) erreicht werden, wie etwa dem MTRJ-, MPO- bzw. MTP-Stecker. In MT-Ferrulen sind typischerweise 2 (MTRJ) bis 16 (MPO/MTP) Fasern pro Reihe (Faserabstand 250-750 µm) untergebracht und die Ausrichtung der Mehrfaser-Ferrule erfolgt durch zwei seitlich angebrachte hochpräzise Führungsstifte.
Das thermische Verspleißen von Glasfasern ist eine sichere und verlustarme Verbindungsmethode, erfordert jedoch eine spezielle Ausrüstung (Spleißmaschine) und Erfahrung. Die Enden müssen vor dem Verspleißen von Coating befreit (mit einem Abisolierer), plan zugerichtet (mit einem Trennwerkzeug zur Erzeugung qualitativ hochwertiger Faserbrüche) und genau zueinander positioniert werden (erfolgt typischerweise in der Spleißmaschine). Dann folgt eine Aufschmelzung der Faserenden durch einen kurzzeitigen Lichtbogen. Während des Aufschmelzens werden die Glasfaserenden ohne zusätzliches Fügemittel aneinandergeschoben. Danach wird die bruchempfindliche Spleißstelle mit einem Spleißschutz mechanisch und vor Feuchtigkeit geschützt. Die Erstellung einer lösbaren Verbindung, um zum Beispiel innerhalb eines Verteilerfeldes Rangiermöglichkeiten zwischen verschiedenen Strecken zu ermöglichen, erfolgt durch das Verspleißen eines Pigtails mit der Verlegefaser. Ein Pigtail ist ein Lichtwellenleiter, der auf der einen Seite einen konfektionierten Stecker besitzt.
Glasfasermuffen enthalten mehrere Spleißverbindungen und verbinden zwei oder mehr Kabel mit jeweils mehreren Fasern bzw. LWL miteinander. Hierfür müssen die Glasfaserkabel einzeln gestrippt, verspleißt und in Kassetten eingelegt werden. Diese dienen dazu, dass bei evtl. Störungen einer Faser die restlichen Fasern unbeeinflusst bleiben. Eine Muffe kann über 200 einzelne Fasern aufnehmen, was mehrere Tage Installationszeit beanspruchen kann.
Daneben gibt es Spleißverbindungen sogenannter Ribbon- oder Bändchenkabel. Bei diesen Kabeln sind als Einzelelement bis zu zwölf Glasfasern in einer Klebematrix bandförmig nebeneinander untergebracht. Die zugehörigen Kabel beinhalten bis zu 100 solcher Bändchen, d. h. bis zu 1200 Glasfasern. Die entsprechende Spleißtechnik verspleißt immer die gesamten Bändchen miteinander, d. h. vier, sechs oder zwölf Glasfasern gleichzeitig mittels Lichtbogen.
In optischen Bauelementen finden sich auch Abzweige und Zusammenführungen von Fasern (Weichen). Zum Pumpen von starken Faserlasern müssen mehrere Fasern der Pumplaser an die aktive Faser angeschlossen werden. Dazu dienen sogenannte fiber combiner und WDMs. Zur Verbindung von Lichtwellenleitern mit unterschiedlichen Kerndurchmessern dienen sogenannte Taper. Weiterhin gibt es Umschalter für mehrere Fasern, sogenannte Faserschalter (engl. fiber switch). Diese können mechanisch oder optisch, d. h. berührungslos, arbeiten.
Glasfaserkabel werden in der Nachrichtentechnik zur Informationsübertragung über kurze und weite Strecken mit hoher Bandbreite verwendet. Kostengünstige Multimodefasern kommen dabei auf kurzen Strecken zum Einsatz, und mit Monomodefasern können Strecken von einigen 10 bis über 100 km ohne Zwischenverstärkung mittels Repeatern überbrückt werden. Im Vergleich zu Kupferkabeln ist bei Glasfaserkabeln das Produkt aus Bandbreite und möglicher Entfernung wesentlich höher, das heißt es können höhere Datenraten erreicht oder größere Entfernungen überbrückt werden.
In lokalen Datenübertragungsnetzen (Local Area Network und Storage Area Network) kommen Glasfaserkabel heute fast bei jedem Netzwerkstandard, wie etwa Ethernet, Fibre Channel oder Infiniband zum Einsatz, früher populär war auch Fiber Distributed Data Interface (FDDI). Eine Erweiterung von bestehenden, auf Kupferkabeln beruhenden Netzen ist mit sogenannten Medienkonvertern möglich, die Netzwerksegmente unterschiedlicher Übertragungsmedien wie Twisted-Pair-Kabel, Koaxialkabel oder Lichtwellenleiter miteinander verbinden können. Vorrangig haben sich modulare Schnittstellen etabliert, bei denen die leitungsspezifischen Transceiver auswechselbar sind und die für diverse Geschwindigkeiten, Wellenlängen und LWL-Steckertypen verfügbar sind. In den verschiedenen Generationen und mit unterschiedlichen Portdichten gibt es Gigabit Interface Converter (GBIC), Small-Form-factor-Pluggable- (SFP bzw. Mini-GBIC), XENPAK-, X2-, XFP-, SFP+-, QSFP- und CFP-Module.
Im globalen Weitverkehrsbereich (Global Area Network) werden Lichtwellenleiter seit Ende der 1980er Jahre insbesondere für interkontinentale Seekabel bzw. transatlantisches Telefonkabel verwendet, um den mit der rasanten Entwicklung des Internets steigenden Anforderungen an Bandbreite und Übertragungsrate gerecht zu werden. Aber auch im Weitverkehrsbereich von landesweiten Netzen (Wide Area Network und Metropolitan Area Network) werden Glasfaserkabel verstärkt eingesetzt. Die verwendeten Lichtwellenleiter werden dabei im DWDM-Verfahren betrieben, das enorme Übertragungskapazitäten ermöglicht. Hierbei werden mittels mehrerer Laser Signale verschiedener Wellenlänge eingekoppelt und gleichzeitig auf einer Faser übertragen. Man hat somit verschiedene Kanäle auf einer Faser. Mit Hilfe der breitbandig verstärkenden EDFAs ist ein Bandbreitenlängenprodukt von mehr als 10.000 (Tbit/s)·km möglich. Diese Systeme der 4. Generation wurden verstärkt Mitte der 1990er Jahre verbaut und sind bis heute Stand der Technik.
An den Endpunkten von Glasfaserkabeln werden die optischen Signale meist noch in elektrische gewandelt, die dann zum Beispiel über Koaxialkabel in die einzelnen Haushalte geführt werden. Anwendung findet hierbei u. a. die HFC.Technologie (Hybrid Fiber Coax) für Kabelfernsehen (Video-on-Demand). In den letzten Jahren wird vor allem in Japan, den USA und Europa der Ausbau von Glasfasernetzen im Anschlussbereich vorangetrieben. So werden dort die einzelnen Häuser direkt mit Glasfasern angeschlossen. Diese Vorgehensweise wird unter dem Begriff Fiber To The Home (FTTH) zusammengefasst. Bei diesem Ausbau werden pro Gebäude ein bis zwei Fasern verlegt. Eine Faser wird dabei für den Download und die andere für den Upload benutzt, und wird nur eine Faser verlegt, so läuft der Download über die Wellenlänge 1310 nm, während der Upload bei 1550 nm realisiert wird.
Es gibt erste erfolgreiche Experimente in denen Informationen parallel über verschiedene Moden übertragen wurden. Mit einem solchen Raummultiplexverfahren könnte die Datenübertragungsrate in der Theorie auf das hundert bzw. tausendfache gesteigert werden. In der Praxis regt man allerdings durch ein "unsauberes" Einkoppeln des Laserpulses in eine Multimodefaser eine Vielzahl verschiedener Moden an, die sich durch Krümmungen der Faser vermischen und durch unterschiedliche Übertragungsgeschwindigkeiten das zu übertragende Signal verzerren. Somit stellt eine höhere Anzahl an anregbaren Moden momentan noch einen gewissen Nachteil dar.
Die heutigen Kommunikationsnetze bestehen im Kernbereich fast ausschließlich aus Glasfasernetzen, wobei wie oben erwähnt der direkte Anschluss der Endverbraucher über Lichtwellenleiter weiter vorangetrieben wird. Eine auf Lichtwellenleitern basierende Übertragungsstrecke besteht dabei aus folgenden Komponenten:
Empfänger (Umwandlung der optischen in elektrische Signale)Als optische Sender werden LEDs bei Übertragungsraten bis zu 622 Mbit/s eingesetzt, mit einer Sendeleistung von ca. −24 bis −3 dBm (Leistungspegel in dBm mit der Bezugsgröße 1 mW). Für höhere Übertragungsraten (> 622 MBit/s) werden Laserdioden verwendet, wie beispielsweise Oberflächenemitter-Dioden bei der Übertragung mittels Multimodefasern und DFB- (Distributed Feedback Laser) oder Fabry-Pérot-Laser bei der Übertragung mittels Monomodefasern (typische Sendeleistungen liegen hier im Bereich von −10 bis 13 dBm).
Die als Übertragungsmedium verwendeten Lichtwellenleiter müssen eine möglichst kleine Dämpfung und Dispersion besitzen. Monomodefasern (geringe Dispersion) werden vorwiegend im Fernnetzbereich eingesetzt und Multimodefasern (größere Dispersion) finden dagegen im Ortsbereich oder in kleinen Netzen Anwendung (siehe Faserkategorien und Einsatzgebiete).
Als sogenannte Repeater zur Signalregeneration, sowie auch als Aus- und Eingangsverstärker, werden hauptsächlich mit Diodenlasern gepumpte Erbium-Faser-Verstärker (EDFA, engl. erbium-doped fibre amplifier) verwendet. Die Verstärkung erfolgt dabei wie bei einem Laser durch stimulierte Emission, jedoch wird durch Isolatoren am Verstärkerausgang der Aufbau eines optischen Resonators verhindert. Weiterhin kommen speziell für DWDM-Anwendungen Raman-Verstärker zum Einsatz, welche gegenüber einem EDFA die gleichzeitige Abdeckung des C- und L-Bandes erlauben, sowie einen einstellbaren Verstärkungsbereich besitzen. Die Verstärkung erfolgt hierbei in der eigentlichen Übertragungsfaser. Da sich die Verstärkung über die gesamte Faser verteilt, erzielt man ein deutlich besseres Signal-Rausch-Verhältnis.
Der optische Empfänger am Ende eines Lichtwellenleiters muss eine möglichst große Empfindlichkeit besitzen (ca. −30 bis −53 dBm) und sehr breitbandig sein. Verwendung finden hauptsächlich pin-Dioden, aber auch Avalanche-Photodioden (APD), welche auf Grund ihrer internen Verstärkung eine höhere Empfindlichkeit als pin-Dioden besitzen.
Ähnlich wie in der Kupfertechnik wurden zur Kenntlichmachung der Übertragungsbandbreiten und des Leistungsvermögens von Multimode- und Monomodefasern optische Klassen und Kategorien eingeführt. Durch den zunehmenden Bandbreitenbedarf und immer höhere Datenraten beim Übergang vom MBit- zum GBit-Bereich, sowie der Einführung von (Multi-)GBit-Protokollen wie zum Beispiel Ethernet, Fibre Channel oder Infiniband, wurden so seit Mitte der 1980er Jahre bisher die Kategorien OM1, OM2, OM3, OM4 und OM5 für Multimodefasern, sowie die Kategorien OS1 und OS2 für Monomodefasern eingeführt. Die Faserkategorien sind nach ISO/IEC 11801 und 24702 international spezifiziert, und die steigende Anzahl der aufgenommenen Kategorien trägt dabei den gewachsenen Anforderungen Rechnung.
Historisch bedingt gibt es bei den Methoden zur Klassifizierung der Faserkategorien für Multimodefasern (OM1–OM4) wesentliche Unterschiede. Frühere Übertragungsverfahren nutzten primär kostengünstige LEDs zu Sendezwecken. LEDs sind jedoch nur bis zu einer Datenrate von 622 MBit/s geeignet, da sie bauartbedingt eine geringe Fokussierung aufweisen und somit sowohl in den Faserkern als auch in einen Teil des Claddings Licht einkoppeln. Man spricht hier vom sogenannten over-filled launch (OFL). Ab Gbit-Ethernet kommen Oberflächenemitter (VCSEL, engl. vertical-cavity surface-emitting laser) bei Wellenlängen von 850 und 1310 nm zum Einsatz, die eine recht starke Fokussierung aufweisen und nur noch in einen Bruchteil des Faserkerns einkoppeln. Man spricht in diesem Fall vom reduced mode launch (RML). Die Bestimmung und Spezifizierung der Bandbreite wurde früher (OM1-2) mit der OFL- und RML-Methode im Frequenzbereich durchgeführt, welche sich aber zur Bestimmung der Übertragungslängen für Gbit-Anwendungen als unzureichend erwiesen. Die Messmethodik musste insofern abgeändert werden und für hochwertige laseroptimierte Multimodefasern wird heute (anstelle der RML-Methode) die effektive modale Bandbreite (EMB) im Zeitbereich bestimmt, mit der DMD-Messmethode (engl. differential mode delay) oder der minEMBc-Messmethode (engl. minimum effective modal bandwidth calculated).Die Faserkategorien OM1 und OM2 sind typischerweise für LED-basierte Anwendungen konzipiert, wobei durch die Reduzierung des Kerndurchmessers auf 50 µm die Modendispersion verringert und die Bandbreite somit erhöht werden konnte. Die Faserkategorien OM3 und OM4 sind nur noch mit 50 µm Kerndurchmesser erhältlich (G50/125) und für Hochgeschwindigkeits-Applikationen wie (10/40/100-)Gigabit-Ethernet oder Fibre Channel bei 850 nm vorgesehen. Sie besitzen ein verbessertes Brechungsindexprofil als OM1/2-Fasern, welche herstellungsbedingt einen leichten Brechungsindexeinbruch in der Faserkernmitte besitzen (beispielsweise bei dem OVD-, MCVD- oder PCVD-Verfahren mit Abscheidungsprozessen im Inneren der Preform), was die hochbitratige RML-Übertragung mit Oberflächenemittern beeinträchtigt.
Die maximale spezifizierte Übertragungsreichweite der eingesetzten Faserkategorie (siehe Tabelle) richtet sich nach der Datenrate und der genutzten Wellenlänge (850 nm oder 1300 nm). Während bei 10 MBit/s bis 1 GBit/s ohne weiteres 300 m bei 850 nm auch mit OM1- und OM2-Fasern möglich sind, ist die erreichbare Länge bei Übertragungsraten von mehr als 4 GBit/s auf unter 100 m bei dieser Wellenlänge begrenzt (für die unterschiedlichen Hochgeschwindigkeits-Anwendungen sind leicht abweichende minimale Übertragungslängen spezifiziert, siehe Tabelle). Fasern der Kategorie OM3 und OM4 erlauben hingegen auch Längen von ca. 300 m bei 850 nm. Mit OM4-Fasern kann zusätzlich bei Anwendungen mit mehr als 10 GBit/s die Übertragungslänge um einige 10 Meter gegenüber OM3-Fasern erhöht werden. OM5 wurde eingeführt, um per Wellenlängenmultiplexverfahren Datenraten von 100 bis 400 Gbit/s auch mit wenigen Fasern erreichen zu können.
In Monomodefasern tritt im Gegensatz zu Multimodefasern keine Modendispersion auf und es sind mit ihnen wesentlich größere Übertragungsdistanzen und Bandbreiten möglich. Da Monomodefasern aber einen deutlich kleineren Kern als Multimodefasern aufweisen, was die praktische Handhabung bei der Lichteinkopplung und Faserverbindung erschwert, werden für kürzere Distanzen weiterhin Multimodefasern verwendet.
Die bisher gebräuchlichsten Monomodefasern im Telekommunikationsbereich sind für den Einsatz im O- und C-Band der optischen Datenkommunikation um λ = 1310 nm bzw. λ = 1550 nm bestimmt. Bei diesen Wellenlängen liegt das Dämpfungsminimum des Fasermaterials und weiterhin werden in diesem Bereich die Erbium-dotierten Faserverstärker (engl. erbium-doped fiber amplifier, EDFA) betrieben. Zwar ist die Dispersion bei diesen Wellenlängen ungleich null, deren Effekt kann aber durch dispersionskompensierende Fasern reduziert werden. Es ist sogar von Vorteil, dass die Dispersion ungleich null ist, da sonst nichtlineare Effekte wie etwa die Vier-Wellen-Mischung auftreten würden, die das Signal erheblich stören. Zu beachten ist allerdings, dass dispersionskompensierende Fasern, die in sogenannten Dispersionskompensationsmodulen Anwendung finden, mit ihrer hohen Dämpfung das Leistungsbudget stark belasten können.
Definiert sind für Monomodefasern (Singlemode-Fasern) die Klassen OS1 (seit 1995) und OS2 (seit 2006), welche sich nur in ihrer maximalen Dämpfung unterscheiden. Speziell bei 1383 nm besitzen die sogenannten Low-Water-Peak-Fasern der OS2-Kategorie eine geringe Dämpfung mit spezifizierten maximalem Wert von 0,4 dB/km und sind damit für den Einsatz von CWDM-Übertragungen geeignet. Weiterhin hängt die Faserkategorie von der Verlegungsart ab, da die Dämpfungswerte davon beeinflusst werden. Die Spezifizierung nach ITU-T G.652 ist nicht eindeutig übertragbar auf die OS-Kategorisierung. Im Allgemeinen kann aber die OS1-Kategorie den Fasern nach ITU-T G.652A und B, und die OS2-Kategorie den Low-Water-Peak-Fasern nach ITU-T G.652.C und D zugeordnet werden.
Die Verlegung erfolgt oft unterirdisch. Die Kabel werden in bereits bestehenden Schächten, Rohren oder Abwasserkanälen untergebracht und anschließend an den gewünschten Stellen mittels Verteilern zu den einzelnen Gebäuden verlegt. Dies ist kostengünstig, da keine Bauarbeiten nötig sind und durch die Ein- und Ausgangsschächte die jeweiligen Verbindungen schnell und einfach installiert werden können. Bei FTTH (Fibre to the Home) werden die Kabel mit einem Durchmesser von 2 mm in den schon vorhandenen Telefonanschlusskanälen (Elektrokanälen) verlegt.
Dark Fibre (dt. „dunkle Faser“) ist eine LWL-Leitung, die unbeschaltet verkauft oder vermietet wird. Der Lichtwellenleiter ist dabei zwischen zwei Standorten Punkt zu Punkt durchgespleißt. Für die Übertragung und die Übertragungsgeräte ist der Käufer oder Mieter verantwortlich. Er bestimmt auch die Verwendung. Dieses Geschäftsmodell wird auch mit carriers carrier oder wholesale business bezeichnet. Da es sich um eine reine Infrastrukturleistung handelt, unterliegt dieser Vertrag nicht dem Telekommunikationsgesetz. Neben den regionalen Stromversorgern werden in Deutschland LWL-Leitungen dieser Art von Schinenwegebetreibern, Stadtwerken, kommunalen Zweckverbänden und Carrier-Anbietern wie etwa Colt, Versatel oder Telekom zur Verfügung gestellt. Die entstehenden Kosten hängen neben der Vertragslaufzeit und Strecke der Leitung vor allem davon ab, welche Baumaßnahmen zur erfolgreichen Anbindung erforderlich sind und ob es für die gleiche Strecke weitere Interessenten/Wettbewerber gibt. Die gewünschte Bandbreite definiert nicht den Preis einer solchen Leitung.
Um Störungen bei Erdarbeiten oder Erweiterungen möglichst zu umgehen, sind in den Kabeln redundante Fasern enthalten. Auch nicht genutzte Glasfaserkapazitäten bezeichnet man als Dark Fibre, da bei unbenutzten Glasfasern keine Lichtsignale übertragen werden und die Faser somit „dunkel“ ist. Bei Bedarf können durch die vorhandene Redundanz weitere Fasern in Betrieb genommen werden.
Wie andere Übertragungsmedien sind auch Lichtwellenleiter nicht sicher gegen „Abhören“. Dabei gibt es zwei wesentliche Punkte, an denen Informationen aus dem Lichtwellenleiter abgehört werden können.
Die erste Methode setzt am Spleiß an, bei dem trotz der geringen Übertragungsverluste guter Spleiße von unter 0,02 dB Strahlung austritt, die ausgewertet werden kann. Die zweite Methode nutzt Strahlungsverluste an Biegekopplern aus (Coupler-Methode). Denn wird eine Glasfaser gebogen, folgt das durchströmende Licht größtenteils der Biegung – ein Teil des Lichtes strahlt jedoch aus der Faser heraus. Schon wenige Prozent des Lichtsignals genügen, um alle übertragenen Informationen zu erhalten. Aufgrund der sich dadurch ändernden Dämpfung ist das Verfahren grundsätzlich nachweisbar. Eine Methode zur Erhöhung der Abhörsicherheit stellt die Verschlüsselung dar.
Der Siegeszug der Lichtwellenleitertechnik basiert auf entscheidenden Vorteilen der optischen Übertragung gegenüber der älteren auf Kupferkabeln basierenden elektrischen Übertragung. Hauptvorteile sind dabei die erheblich höheren möglichen Übertragungsraten (Giga- bis Terabit pro Sekunde), bei gleichzeitig sehr großen möglichen Reichweiten (bis zu mehreren hundert Kilometern ohne Zwischenverstärker). Dieses wiederum bedingt leichtere Kabel und weniger Platzbedarf, sowie weniger Zwischenverstärker, was die Installations- und Wartungskosten erheblich reduziert.
keine Beeinflussung durch elektromagnetische Störfelder, was u. a. die Kombination mit Hochspannungs-Gleichstrom-Übertragungskomponenten möglich macht
keine Brandauslösung durch Blitzeinwirkung oder Kurzschluss und geringere Brandlast, sowie verwendbar auch in explosionsgefährdetem Umfeld (Einschränkungen gibt es bei der Verwendung von höheren optischen Leistungen, die an Koppelstellen oder bei Faserbruch entweichen und in ungünstigen Fällen auch brand- bzw. explosionsauslösend wirken können.)
hoher AbhöraufwandNachteile sind der höhere Konfektionierungsaufwand und die höhere erforderliche Präzision und Sorgfalt bei der Verlegung und Installation, was eine teure Gerätetechnik, sowie eine aufwendige und komplexe Messtechnik erfordert, weshalb Fiber to the Desk wenig verbreitet ist.
empfindlich gegenüber mechanischer Belastung und Einschränkungen bei der Verlegung, da keine starken Krümmungen möglich sind (stark typenabhängig, z. B. für die Medizintechnik gibt es spezielle Typen für die Videoendoskopie)
Anfang der 1990er-Jahre wurden D/A-Umsetzer und CD-Player angeboten, die mit einer ST-Verbindung kommuniziert haben. Gerätebeispiele sind Parasound DAC 2000, WADIA DAC, Madrigal Proceed PDP 3 mit CD-Transport PDT 3. Diese Art der Verbindung konnte sich allerdings gegen TOSLINK, einer Verbindungstechnik mit POF, nicht durchsetzen und fand deshalb recht selten Verwendung.
Optische Verbindungen in der Audiotechnik vermeiden Signalstörungen durch elektrische und magnetische Felder sowie durch Masseschleifen, da sie eine Potentialtrennung bilden.
bei Leistungselektronik- und Hochspannungsanlagen, um Steuersignale zum Beispiel zu den auf Hochspannungspotential befindlichen Thyristoren zu übertragen. Es ist sogar möglich, die Stromrichterthyristoren direkt über die in der Glasfaser übertragenen Lichtpulse zu zünden (siehe Optothyristor).
zur galvanisch getrennten Netzwerkanbindung von medizinischen Geräten (beispielsweise digitales Röntgengerät) an lokale Netzwerke.
Durch die Übertragung oder auch gleichzeitige Erfassung von Messsignalen mittels Lichtwellenleiter ist es möglich an schwer zugänglichen Stellen wie in Staumauern oder unter Extrembedingungen wie in Stahlwerken eine Vielzahl von physikalischen Größen wie etwa Druck oder Temperatur zu messen. Auch Spektrometer besitzen häufig LWL-Anschlüsse. Es lassen sich weiterhin miniaturisierte Glasfaserspektrometer herstellen, da sich mit dem Lichtaustrittskegel einer Glasfaser das optische Gitter direkt beleuchten lässt, und somit auf zusätzliche Abbildungsoptiken verzichtet werden kann.
Bei faseroptischen Sensoren wird die Messgröße nicht durch eine elektrische Größe repräsentiert bzw. übertragen, sondern durch eine optische. Dies macht die Übertragung unanfällig gegenüber äußeren Einflüssen wie zum Beispiel elektromagnetischen Feldern, und erlaubt auch die Verwendung in explosionsgefährdeter Umgebung. Man unterscheidet zwei Klassen faseroptischer Sensoren: intrinsische und extrinsische faseroptische Sensoren.
Bei intrinsischen faseroptischen Sensoren dient die Glasfaser direkt als Messaufnehmer, das heißt, die optischen Signale werden direkt durch äußere Parameter beeinflusst, beispielsweise Biegeverluste. Die Lichtwellenleiter sind hierbei zugleich Sensor und Leitung.
Bei extrinsischen faseroptischen Sensoren sind die LWL hingegen meist nur ein Teil eines Sensorsystems, sie dienen hier vor allem als Überträger der vom Sensor erfassten Messgröße, die jener als optisches Signal zur Verfügung stellen muss. Ihr Vorteil gegenüber elektrischen Leitungen ist ihre weitgehende Robustheit gegenüber äußeren Einflüssen wie elektromagnetischen Felder.
Die Strahlung von Hochleistungs-Lasern im nahen Infrarot (Einsatz u. a. zur Materialbearbeitung) wird oft in Lichtleitkabeln (LLK) geführt, um sie besser an den Wirkungsort heranführen zu können. Die verwendeten Lichtwellenleiter sind Multimodefasern (hier muss die Leistungsdichte im Kern der Faser reduziert werden, da dieser sonst zerschmolzen oder zerrissen würde) und es können Leistungen bis zu mehreren Kilowatt in Fasern mit 0,02–1,5 mm Kerndurchmesser nahezu verlustfrei über kurze Distanzen übertragen werden.
Steckverbindungen derartiger Fasern sind prinzipiell anders aufgebaut als diejenigen der Nachrichtenübertragung:
Sie müssen hohe thermische Verlustleistungen aufgrund der Streustrahlung und ggf. Rückreflexionen vertragen. Die Faserendflächen sind plan und ragen frei ohne Einbettung heraus. Teilweise werden sie an einen Kieselglasblock gepresst, um Verunreinigungen der Endflächen zu vermeiden. Aufgrund der hohen Leistungsflussdichten führen kleinste Verunreinigungen zur Zerstörung. Antireflexbeschichtung der Endflächen ist aus diesem Grund ebenfalls selten möglich.
Bis etwa 500 Watt Laserstrahlleistung sind bei SMA-Steckverbindungen möglich, wobei die Faser jedoch nicht bis zum Ende eingebettet ist.
Dotierte Fasern (zum Beispiel mit Erbium) können selbst als Laser- oder Licht-Verstärker arbeiten (siehe Faserlaser). Hierzu werden sie optisch mittels Hochleistungs-Diodenlasern gepumpt. Diese Technik findet sowohl in der Nachrichtentechnik als auch im Hochleistungsbereich Verwendung.
In der Lasershowtechnik wird Laserlicht von einer zentralen Quelle über Lichtleitkabel zu verschiedenen im Raum verteilte Projektoren geleitet. Die Leistungen betragen hier einige hundert Milliwatt bis zu zweistelligen Wattbeträgen.
Auch zu Beleuchtungs-, Abbildungs- und Dekorationszwecken werden Fasern und Faserbündel eingesetzt. So etwa in Mikroskop- oder Endoskoplichtquellen um das Licht einer Halogenglühlampe zum Untersuchungsobjekt zu leiten, oder als Bildleiter in flexiblen Endoskopen.
Kunststoff- und Glasfasern werden auch in einer Vielzahl von Lampen und Beleuchtungsinstallationen verwendet, wobei die Fasern nicht nur zum Lichttransport, sondern selbst auch als abstrahlende Elemente benutzt werden. In ihrer klassischen Anwendung dienen sie als sogenannte Endlichtfasern (Beispielhaft: „Sternenhimmel“, wo mehrere Fasern eines Bündels vor der Verteilung mit einer Halogenglühlampe und einem Filterrad beleuchtet werden) und bei Beleuchtungsinstallationen in und an Gebäuden werden sogenannte Seitenlichtfasern verwendet. Dabei handelt es sich um spezielle Polymere optische Fasern mit gezielt eingebrachten Störungen in der Kern-Mantel-Grenzfläche, was zu einer seitlichen Abstrahlung führt.Für die genannten Anwendungen kommen ausschließlich Multimodefasern zum Einsatz, da hier ein Singlemode-Betrieb auf Grund der vielen unterschiedlichen und meist gleichzeitig übertragenen Wellenlängen nicht möglich ist.
Lichtwellenleiter bzw. Glasfaserkabel sind nach ITU-T G.651 bis G.657, ISO/IEC 11801 und 24702 und IEC 60793 international genormt, sowie nach DIN VDE 0888 national genormt (die Normen DIN VDE 0899 Teil 1–5 wurden zurückgezogen).
Govind P. Agrawal: Nonlinear Fiber Optics (Optics and Photonics). Academic Press, ISBN 0-12-045143-3.
Fedor Mitschke: Glasfasern : Physik und Technologie. Elsevier, Spektrum, Akad. Verlag, Heidelberg 2005, ISBN 3-8274-1629-9.
C. R. Pollock, Clifford Pollock, Michal Lipson: Integrated Photonics. Springer Netherlands, 2003, ISBN 1-4020-7635-5. 
Bishnu P. Pal: Fundamentals of fibre optics in telecommunication and sensor systems. New Age International, New Delhi, 1992, ISBN 978-81-224-0469-2. 
Edgar Voges, Klaus Petermann: Optische Kommunikationstechnik: Handbuch für Wissenschaft und Industrie. Springer, 2002, ISBN 3-540-67213-3. Technik:
Volkmar Brückner: Elemente optischer Netze: Grundlagen und Praxis der optischen Datenübertragung. 2. Auflage. Vieweg+Teubner, 2011, ISBN 3-8348-1034-7. 
D. Gustedt, W. Wiesner: Fiber Optik Übertragungstechnik: Franzis' Verlag GmbH, Poing 1998, ISBN 978-3-7723-5634-6.
Rongqing Hui, Maurice S. O'Sullivan: Fiber optic measurement techniques. Elsevier Academic Press, 2009, ISBN 978-0-12-373865-3. 
Christoph P. Wrobel: Optische Übertragungstechnik in der Praxis: Komponenten, Installation, Anwendungen. Hüthig, Bonn 2004, ISBN 3-8266-5040-9.
O. Ziemann,J. Krauser,P. E. Zamzow,W. Daum: POF-Handbuch: Optische Kurzstrecken-Übertragungssysteme. 2. Auflage. Springer, 2007, ISBN 978-3-540-49093-7.
Faseroptik Schäfter + Kirchhoff GmbH (incl. Erklärungen zu Eigenschaften von MM-, SM-, PM-SM- und photonischen Kristallfasern, PDF, englisch).
Interaktive Darstellung der chromatischen Dispersion Institut für Nachrichtenübertragung der Universität Stuttgart

Liebe (über mhd. liep, „Gutes, Angenehmes, Wertes“ von idg. *leubh- gern, lieb haben, begehren) ist eine Bezeichnung für stärkste Zuneigung und Wertschätzung.
Nach engerem und verbreitetem Verständnis ist Liebe ein starkes Gefühl, mit der Haltung inniger und tiefer Verbundenheit zu einer Person (oder Personengruppe), die den Zweck oder den Nutzen einer zwischenmenschlichen Beziehung übersteigt und sich in der Regel durch eine entgegenkommende tätige Zuwendung zum anderen ausdrückt. Das Gefühl der Liebe kann unabhängig davon entstehen, ob es erwidert wird oder nicht. Hierbei wird zunächst nicht unterschieden, ob es sich um eine tiefe Zuneigung innerhalb eines Familienverbundes (Elternliebe, Geschwisterliebe) oder um eine Geistesverwandtschaft handelt (Freundesliebe, Partnerschaft) oder aber um ein körperliches Begehren gegenüber einem anderen Menschen (Eros). Dieses Begehren ist als körperliche Liebe eng mit der Sexualität verbunden, die jedoch nicht unbedingt auch ausgelebt zu werden braucht (vgl. platonische Liebe).
Ausgehend von dieser ersten Bedeutung wurde der Begriff in der Umgangssprache und in der Tradition schon immer auch im übertragenen Sinne verwendet und steht dann allgemein für die stärkste Form der Hinwendung zu anderen Lebewesen, Dingen, Tätigkeiten oder Ideen. Diese allgemeine Interpretation versteht Liebe also zugleich als Metapher für den Ausdruck tiefer Wertschätzung.
Kulturgeschichtlich und historisch ist „Liebe“ ein schillernder Begriff, der nicht nur in der deutschen Sprache in vielfältigen Kontexten und in den unterschiedlichsten Konnotationen verwendet wird. Das Phänomen wurde in den verschiedenen Epochen, Kulturen und Gesellschaften unterschiedlich aufgefasst und erlebt. Jede Zeit und jeder soziale Verband setzt je eigene Verhaltensregeln für den Umgang mit der Liebe. Daher können die Bedeutungsebenen zwischen der sinnlichen Empfindung, dem Gefühl und der ethischen Grundhaltung Liebe wechseln.
Ebenso vielschichtig wie die Bedeutungen der Liebe sind die Bedeutungen der Antonyme. Im Hinblick auf die emotionale Anziehung zwischen Personen ist es der Hass. Im Sinne der Abwesenheit von Liebe kann aber auch die Gleichgültigkeit als Antagonismus angesehen werden. Absoluter Mangel an Liebe führt beim Kind zu Hospitalismus. Fehlentwicklungen der Liebesfähigkeit sind im Sinne des „reinen“ Liebesbegriffes das Besitzdenken (Eifersucht) oder verschiedene Formen der freiwilligen Abhängigkeit bzw. Aufgabe der Autonomie bis hin zur Hörigkeit.
Altgriechisch unterscheidet mehrere verschiedene Sinne, in denen das Wort „Liebe“ verwendet wird. Die alten Griechen identifizierten vier Formen der Liebe: Verwandtschaft oder Vertrautheit (auf Griechisch, Storge), Freundschaft und/oder platonisches Verlangen (Philia), sexuelles und/oder romantisches Verlangen (Eros) und selbstentleerende oder göttliche Liebe (Agape). Moderne Autoren haben weitere Varianten der romantischen Liebe dargelegt. Beim Griechischen (wie bei vielen anderen Sprachen) war es jedoch historisch schwierig, die Bedeutungen dieser Wörter vollständig zu trennen. Gleichzeitig hat der altgriechische Text der Bibel Beispiele dafür, dass das Verb agapo die gleiche Bedeutung wie phileo hat.
Die lateinische Sprache hat verschiedene Wörter, die dem deutschen Wort „Liebe“ entsprechen. Amō ist das grundlegende Verb, das „ich liebe“, mit dem Infinitiv Amare („lieben“), wie es heute noch auf Italienisch verwendet wird. Die Römer benutzten es sowohl in einem liebevollen Sinne als auch in einem romantischen oder sexuellen Sinn. Das entsprechende Substantiv ist amor (die Bedeutung dieses Begriffs für die Römer wird dadurch deutlich, dass der Name der Stadt, Rom – lateinisch: Roma – als Anagramm für amor angesehen werden kann, das in der Antike in weiten Kreisen als geheimer Name der Stadt verwendet wurde), das auch in der Pluralform verwendet wird, um Liebesaffären oder sexuelle Abenteuer anzuzeigen. Dieselbe Wurzel produziert auch Amicus- „Freund“ und amicitia, „Freundschaft“ (oft zum gegenseitigen Vorteil, und entspricht manchmal eher „Verschuldung“ oder „Einfluss“). Cicero schrieb eine Abhandlung mit dem Titel de Amicitia, die den Begriff ausführlich diskutiert. Ovid schrieb einen Leitfaden zur Datierung mit dem Namen Ars Amatoria (Die Kunst der Liebe), der alles von außerehelichen Angelegenheiten bis hin zu überfürsorglichen Eltern behandelt.
Liebe wird häufig als eine auf den freien Willen gegründete Beziehung zwischen zwei Personen gesehen, die ihren Wert nicht im Besitz des adressierten Objekts findet, sondern sich im dialogischen Raum zwischen den Liebenden entfaltet. Die Liebenden erkennen einander in ihrer Existenz wechselseitig an und fördern sich „zueinander strebend“ gegenseitig.
Liebe wird teilweise als anomisches und entgrenzendes Gegenmodell zu den Beschränkungen, Anforderungen, Funktionalisierungen und Ökonomisierungen der menschlichen Alltags- und Arbeitswelt aufgefasst. Liebe ist kein bewusster oder rationaler Entschluss der Liebenden; gleichwohl ist sie nicht irrational.
Im Sinne des Diskurses der Anerkennung (zum Beispiel John Rawls, Axel Honneth) enthält Liebe die von Hegel betonte „Idee der wechselseitigen Anerkennung“, was ihr ein moralisches Fundament verleiht. Liebe ist daher für Honneth neben dem Recht und der Solidarität eines der drei „Muster intersubjektiver Anerkennung“.
Liebe zeichnet sich demnach in Freundschaften und in auf Liebe gegründeten Beziehungen durch emotionale Zugewandtheit und Wohlwollen aus. Das praktische Erleben von Liebe ermöglicht Selbstvertrauen und ist identitätsstiftend. Das heißt, dass im Sozialisationsprozess Forderungen an Identitätsbildung geltend gemacht werden. Zu deren Erfüllung leistet Liebe einen Beitrag, indem einer Person positive Eigenschaften zugeordnet werden und die Persönlichkeit Zustimmung erfährt. Somit bestärkt die durch Liebe erfahrene soziale Anerkennung die Herausbildung einer intakten Identität.
Darüber hinaus unterscheidet beziehungsweise erweitert die moralische Grundierung Liebe auch vom reinen Trieb.
Die abendländische Auffassung von Liebe wird von der Dreiteilung der antiken Terminologie geprägt. In der Antike wurden drei Begriffe verwendet, die unterschiedliche Formen von Liebe bezeichneten:
Éros – bezeichnet die sinnlich-erotische Liebe, das Begehren des geliebten Objekts, den Wunsch nach Geliebt-Werden, die Leidenschaft;
Philía – bezeichnet die Freundesliebe, Liebe auf Gegenseitigkeit, die gegenseitige Anerkennung und das gegenseitige Verstehen;
Agápe – bezeichnet die selbstlose und fördernde Liebe, auch die Nächstenliebe und die Feindesliebe, die das Wohl des Anderen im Blick hat.Die genauen Bedeutungen und Schwerpunkte der Begriffe haben sich im Laufe der Zeit verändert, sodass – im Gegensatz zum ursprünglich Gemeinten – unter „platonischer Liebe“ heute ein rein geistig-seelisches Prinzip ohne körperliche Beteiligung und Besitzwunsch verstanden wird, dem das leiblich-erotische Modell von geschlechtlicher Liebe schroff gegenübergestellt wird.
Im Laufe der Zeiten wurden diese Grundformen der Liebe immer wieder differenziert. So bezeichnet man manchmal die spielerisch-sexuelle Liebe als „ludus“, die besitzergreifende Liebe als „mania“ und die auf Vernunftgründen basierende Liebe als „pragma“. Ein besonderes Liebesverhältnis stellt in theistischen Religionen auch jenes zwischen der erbarmenden Liebe Gottes zu den Menschen und der verehrenden Liebe der Menschen zu Gott dar (Oberbegriff für beides ist Gottesliebe).
In Anlehnung an diese Dreiteilung kann man die Ausprägungen des Phänomens der Liebe in Empfindung, Gefühl und Haltung unterscheiden:
Unter Liebesempfindungen versteht man die primär sinnlichen Liebesgefühle, insbesondere die Verliebtheit und die sexuelle Anziehung. Sie stehen in der Regel in Verbindung mit den beiden anderen Formen der Liebe, können aber auch durch die Wahrnehmung eines fremden Körpers, das heißt durch visuelle, olfaktorische oder taktile Reize ausgelöst werden oder ganz einfach durch den empfundenen Mangel an einem geliebten Gegenüber. Die Liebesempfindung steht in enger Verbindung mit der Sexualität, das heißt sexuellen Wünschen, Bedürfnissen und Handlungen (zum Beispiel dem Geschlechtsverkehr, auch als „Liebe machen“ bezeichnet).
Unter Liebesgefühlen versteht man ein komplexes, vielfältiges Spektrum unterschiedlicher Empfindungen und Haltungen gegenüber verschiedenen Arten möglicher Liebesobjekte, in denen die sinnlich-erotische Komponente nur sekundär von Bedeutung ist. Sie führen zu einer Hinwendung und Zuwendung zum Anderen, dem Wertschätzung, Aufmerksamkeit und Zärtlichkeit geschenkt werden.
Sympathie, Freundschaft, Fürsorge und emotionale Liebe sind Erscheinungen, in denen Liebesgefühle eine große Rolle spielen. Ebenso können die kontemplative Liebe (zum Beispiel zur Natur), die aktive sorgende Liebe um den Nächsten (Karitas), die religiöse bzw. mystische Liebe und das Mitleid hierzu gerechnet werden.
Dies bezeichnet die innere Haltung gegenüber der geliebten Person, um ihrer selbst willen zu handeln und durch das eigene Verhalten deren Wohlergehen und Glück zu befördern.
Selbstliebe: Selbstliebe wird in der Regel als immer vorhanden angesehen; von einigen auch als die Voraussetzung zur Fähigkeit zum Lieben und zur Nächstenliebe angesehen, wobei nach Auffassung Erich Fromms (1900–1980) Selbstsucht Selbsthass bedeute. Selbstsucht äußere sich in der Liebe durch besitzgieriges Interesse. Fromm behauptete weiterhin, dass zu starke Selbstlosigkeit keine Tugend sei, sondern ein Symptom, durch das unbeabsichtigter Schaden entstehen könne. Körperliche Selbstliebe wird auch als Masturbation ausgelebt, welche die sexuelle Entwicklung fördere. Überhöhte Eigenliebe oder pathologische Eigenliebe wird als Narzissmus bezeichnet.
Partnerliebe: Die geschlechtliche Liebe kann in gegengeschlechtliche (Heterosexualität) und gleichgeschlechtliche Liebe (Homosexualität) unterschieden werden und findet oft in Liebesbeziehungen Ausdruck, für die in heutigen europäischen Kulturen das Ideal der Partnerschaft, vermischt mit dem ehemals höfischen Ideal der romantischen Liebe, betont wird. In der Gesellschaft hat die eheliche Liebe häufig eine institutionell bedeutsame Rolle und nimmt oftmals Exklusivität für sich in Anspruch (siehe Monogamie). Nicht auf exklusiven Zweierbeziehungen beruhende Liebesmodelle (Polygamie) spielen in außereuropäischen Kulturen und in den letzten Jahrzehnten in seltenen Fällen auch im Westen („Polyamorie“), z. B. in subkulturellen Lebensformen, eine Rolle.
Familiäre Liebe: Neben der partnerschaftlichen Liebe sind insbesondere die Liebe zwischen (engen) Verwandten (Vaterliebe, Mutterliebe, Kindesliebe) und die Freundesliebe in menschlichen Gemeinschaften von größter Bedeutung.
Nächstenliebe: Die Nächstenliebe gilt im Sinne von Religion und Ethik primär den Bedürftigen, während die Philanthropie sie zur allgemeinen Menschenliebe ausdehnt (vgl. Menschlichkeit). Die Feindesliebe ist eine im Neuen Testament auf Feinde bezogene Nächstenliebe, die oft als christliche Besonderheit gilt, aber auch in anderen Religionen vorkommt – so zum Beispiel im hawaiischen Hoʻoponopono, bei dem sie sich als „den anderen loslassen“ ausprägt. Ein weiteres Konzept ist das Konzept der Fernstenliebe.
Objekt- und Ideenliebe: Insbesondere in jüngerer Zeit ins Zentrum gesellschaftlicher Begriffe gerückt sind in westlichen Kulturen auch Tierliebe oder die Liebe zur Natur. In der weitesten sprachlichen Auslegung „liebt“ man darüber hinaus seine Hobbys oder Leidenschaften und kann diese dann auch als Liebhaberei oder Vorlieben bezeichnen. Auch Ideale können demnach geliebt werden, etwa durch den Begriff „Freiheitsliebe“ dargestellt, aber auch Zugehörigkeiten wie Vaterlandsliebe (Patriotismus).
Gottesliebe: Eine besondere Rolle nimmt die Gottesliebe ein, in ihrer allgemeinen Form die in verschiedenen (keinesfalls allen) Religionen vorausgesetzte Liebe Gottes zu seiner Schöpfung und insbesondere dem Menschen. Der gleiche Begriff bezeichnet auch die Liebe zu einem Gott.
„Objektlose Liebe“: Liebe als Grundhaltung benötigt für christliche Mystiker wie Meister Eckhart kein Objekt. Liebe wird hier als bedingungsloses Öffnen verstanden. Der Philosoph und Metaphysiker Jean Émile Charon bezeichnet diese „universale“ Liebe als „Finalität der Evolution“ und „Selbsttranszendenz des Universums“.
Liebe, insbesondere Verliebtheit („Verliebtsein“) kann sich nonverbal, etwa durch Blicke, Mimik, Unruhe oder Körperhaltung ausdrücken. Beruht die Liebe auf Gegenseitigkeit, drückt der Mensch sie durch Zärtlichkeiten, insbesondere Küssen und Berührungen wie das  Händchenhalten aus. Die körperliche Vereinigung kann dabei als intimste Ausdrucksform der Liebe dienen. Verbale Ausdrucksformen sind in erster Linie Bezeichnungen der oder des Geliebten, meistens in Form von Komplimenten und Koseworten bzw. Kosenamen wie „Liebling“ oder „Schatz“.
Besondere, konventionelle Formen sind die „Liebeserklärung“ oder der Liebesbrief, die auch in der Literatur eine besondere Würdigung erfuhren. Auch Rituale wie die Verlobung oder Symbole wie der Verlobungsring gehören hierzu.
Das Ideal einer „Liebe als Verehrung“ unter Ausschluss einer konkreten körperlichen Beziehung gehört eher in die (Literatur-) Geschichte und fand dort eine besondere Form in der sogenannten „hohen Minne“, ein Begriff, den Walther von der Vogelweide als Gegenbegriff zur „nideren minne“, also der körperlich erfüllten Minne, verwendet. In dieser poetischen Form der Liebe bleibt die „frouwe“ unerreichbar.
Die Herzform (♥) ist ein Ideogramm, das die Idee des „Herzens“ in seinem metaphorischen oder symbolischen Sinn als Zentrum der Emotionen, einschließlich Zuneigung und Liebe, besonders der romantischen Liebe, ausdrückt. Das „verwundete Herz“, das die Liebeskrankheit anzeigt, wurde als Herzsymbol dargestellt, das mit einem Pfeil durchbohrt wurde (Amors), oder als Herzsymbol, das in zwei oder mehr Teilen „gebrochen“ wurde.
Die Kombination der Herzform und ihrer Verwendung innerhalb der Herzmetapher entwickelte sich am Ende des Mittelalters, obwohl die Form in vielen epigraphischen Denkmälern und Texten bereits in der Antike verwendet wurde. Mit möglichen frühen Beispielen oder direkten Vorgängern im 13. bis 14. Jahrhundert entwickelte sich im 15. Jahrhundert das bekannte Symbol des Herzens, das die Liebe darstellt, und wurde in Europa im Laufe des 16. Jahrhunderts populär. Seit dem 19. Jahrhundert wird das Symbol oft auf Valentinstags-Karten, Süßigkeitenschachteln und ähnlichen populären Kulturartefakten als Symbol für romantische Liebe verwendet.
Der Claddagh-Ring (Gaeilge : fáinne Chladaigh) ist ein traditioneller irischer Ring, der für Liebe, Loyalität und Freundschaft steht (die Hände stehen für Freundschaft, das Herz für Liebe und die Krone für Loyalität).
Die Gestaltung und die damit verbundenen Bräuche stammen aus dem gleichnamigen irischen Fischerdorf in Galway. Der Ring, wie heute bekannt, wurde erstmals im 17. Jahrhundert hergestellt. Dieses Symbol wird mit der Legende des Claddagh in Verbindung gebracht, einem Fischerdorf außerhalb der Stadt Galway. Der Legende nach fischte ein junger Mann namens Richard mit anderen Männern aus seiner Familie auf See, als sie von Piraten gefangen und als Sklaven nach Afrika gebracht wurden. Jahre vergingen, viele der irischen Fischer starben und Richard war unglücklich, weil er nur zu seiner Geliebten zurückkehren wollte, die in Irland war. Um seinen Geist aufrechtzuerhalten und die Hoffnung in seinem Herzen zu erhalten, stahl Richard jeden Tag einen winzigen Fleck Gold von seinen Sklavenmeistern in der Goldschmiede, wo er die Feuer pflegte. Jahre vergingen, und mit seinen kleinen Goldstücken konnte er endlich einen Ring anfertigen. Es war seine Hoffnung, dass er trotz allem, was fast unmöglich schien, in sein Dorf zurückkehren und den Ring seiner wahren Liebe schenken würde.
Amor ist eines der berühmtesten der Liebessymbole. Er wird oft dargestellt als neckisches, geflügeltes Kind mit Pfeil und Bogen. Die Pfeile bedeuten Wünsche und Emotionen der Liebe, und Amor zielt diese Pfeile auf Götter und Menschen, wodurch sie sich tief verlieben. Amor hat immer eine Rolle bei den Feierlichkeiten der Liebe und der Liebenden gespielt. Im alten Griechenland war er bekannt als Eros, der junge Sohn der Aphrodite, der Göttin der Liebe und Schönheit.
Die alten Griechen behandelten die Rose als heiliges Wahrzeichen der Schönheit der Aphrodite. Nach einer alten Legende wuchs die rote Rose aus dem Blut des Gottes Adonis, die Rose wird auch mit römischen Gottheiten wie Hecate, Bacchus und den Drei Grazien in Verbindung gebracht.
Schwäne haben viele verschiedene Bedeutungen in Mythos und Folklore. Sie symbolisieren neben Freiheit, Gemeinschaft, und Loyalität insbesondere (monogame) Liebe und Treue. Der Schwan ist ein weiteres Symbol der Jungfrau Maria und steht für ihre Reinheit und Liebe. Nach englischer und keltischer Tradition ist der Schwan ein mächtiges und uraltes Krafttier. Hier wird der Schwan mit Göttinnen des Heilwassers in Verbindung gebracht.
In ausgebildeten polytheistischen Religionen, die sich von einer Umwandlung vieler Lokalgottheiten zu einem ‚arbeitsteiligen‘ Pantheon fortentwickelt hatten, wurden oft besondere (meist weibliche) Gottheiten der Liebe verehrt. So gab es im antiken Griechenland die Göttin Aphrodite und ihren Sohn Eros, bei den Römern die Göttin Venus und ihren Sohn Amor.
Die monotheistischen Religionen haben ausgehend von der Gottesliebe ausgefeilte Theologien der Liebe entwickelt (Siehe auch die davon abgeleiteten Vorstellungen von Nächstenliebe und Karitas). Die Allliebe Gottes ist eine seiner Eigenschaften; da aber auch Zorn oder Eifersucht zu seinen Eigenschaften zählen, hat die Theologie hier ein komplexes Arbeitsfeld. Im griechischen Neuen Testament ist Liebe die erste Frucht des Heiligen Geistes. Die Kirche unterscheidet nach Platon zwischen Agape und Eros (siehe oben). Papst Benedikt XVI. widmete seine erste Enzyklika Deus caritas est, dem Thema Liebe.
Selbst in der negativen Theologie, wie auch in der Mystik, wird als einzige Aussage über das Unsagbare in der Regel dennoch die Feststellung Gott ist die Liebe anerkannt; vgl. dazu auch die natürliche Theologie.
Im Hebräischen ist אהבה (ahava) der am häufigsten verwendete Begriff für zwischenmenschliche Liebe und Liebe zwischen Gott und Gottes Schöpfungen. Chesed, oft übersetzt als liebende Güte, wird verwendet, um viele Formen der Liebe zwischen Menschen zu beschreiben.
Das Gebot, andere Menschen zu lieben, ist in der Thora gegeben, die besagt: „Liebe deinen Nächsten wie dich selbst“ (Levitikus 19:18). Das Gebot der Thora, Gott zu lieben „mit ganzem Herzen, mit ganzer Seele und mit ganzer Kraft“ (Deuteronomium 6:5) wird von der Mischna (einem zentralen Text des jüdischen mündlichen Gesetzes) als Hinweis auf gute Taten, die Bereitschaft, sein Leben zu opfern, anstatt bestimmte schwere Übertretungen zu begehen, die Bereitschaft, seinen gesamten Besitz zu opfern, und die Dankbarkeit gegenüber dem Herrn trotz Not genommen (Traktat Berachoth 9:5). Die rabbinische Literatur unterscheidet sich darin, wie diese Liebe entwickelt werden kann, z. B. durch die Betrachtung göttlicher Taten oder das Zeugnis der Wunder der Natur. Was die Liebe zwischen den Ehepartnern betrifft, so wird sie als ein wesentlicher Bestandteil des Lebens angesehen: „Seht das Leben mit der Frau, die ihr liebt“ (Prediger 9,9). Das biblische Buch Hohelied Salomos gilt als romantisch formulierte Metapher der Liebe zwischen Gott und seinem Volk, liest sich aber in seiner einfachen Lesart wie ein Liebeslied. Der Rabbiner Eliyahu Eliezer Dessler aus dem 20. Jahrhundert definiert in seinem Michtav me-Eliyahu die Liebe als „Geben ohne zu erwarten“.
Christen glauben, dass die Liebe zu Gott mit ganzem Herzen, Verstand und Kraft und die Liebe zum Nächsten wie zu sich selbst die beiden wichtigsten Gebote im Leben sind. Der heilige Augustinus fasste dies zusammen, als er schrieb: „Liebe Gott und tu, was du willst.“ 
Der Apostel Paulus verherrlichte die Liebe als die wichtigste Tugend von allen. In der berühmten poetischen Interpretation in 1. Korinther schrieb er: „Liebe ist geduldig, Liebe ist gütig. Sie beneidet nicht, sie prahlt nicht, sie ist nicht stolz. Es ist nicht unhöflich, es ist nicht selbstsüchtig, es ist nicht leicht verärgert, es hält keine Aufzeichnungen über Unrecht. Die Liebe erfreut sich nicht am Bösen, sondern freut sich an der Wahrheit. Sie beschützt immer, vertraut immer, hofft immer und hält immer durch.“ (1 Kor 13,4–7, NIV) 
Der Apostel Johannes schrieb: „Denn Gott hat die Welt so sehr geliebt, dass er seinen einzigen Sohn gab, damit jeder, der an ihn glaubt, nicht verloren geht, sondern das ewige Leben hat. Denn Gott hat seinen Sohn nicht in die Welt gesandt, um die Welt zu verdammen, sondern um die Welt durch ihn zu retten.“ (Johannes 3:16–17, NIV). Johannes schrieb auch: „Liebe Freunde, lasst uns einander lieben, denn die Liebe kommt von Gott. Jeder, der liebt, ist von Gott geboren und kennt Gott. Wer nicht liebt, kennt Gott nicht, denn Gott ist Liebe.“ (1. Johannes 4:7–8, NIV) 
Augustinus sagt, dass man in der Lage sein muss, den Unterschied zwischen Liebe und Lust zu entschlüsseln. Lust, so der heilige Augustinus, ist ein übermäßiger Genuss, aber lieben und geliebt werden ist das, was er sein ganzes Leben lang gesucht hat. Er sagt sogar: „Ich war in die Liebe verliebt.“ Der Einzige, der dich wirklich und vollkommen lieben kann, ist Gott, weil die Liebe zu einem Menschen nur Fehler wie „Eifersucht, Misstrauen, Angst, Wut und Streit“ zulässt. Nach dem heiligen Augustinus bedeutet die Liebe zu Gott „den Frieden zu erlangen, der dir gehört“. Augustinus betrachtet das Doppelgebot der Liebe in Matthäus 22 als das Herz des christlichen Glaubens und die Interpretation der Bibel. 
Christliche Theologen sehen Gott als Quelle der Liebe, die sich in den Menschen und ihren eigenen Liebesbeziehungen widerspiegelt. Der einflussreiche christliche Theologe C.S. Lewis schrieb ein Buch mit dem Titel The Four Loves. Benedikt XVI. schrieb seine erste Enzyklika über „Gott ist Liebe“. Er sagte, dass ein Mensch, geschaffen nach dem Bild Gottes, der Liebe ist, fähig ist, Liebe zu praktizieren, sich Gott und anderen zu schenken (agape) und die Liebe Gottes in der Kontemplation (eros) zu empfangen und zu erfahren. Dieses Leben der Liebe ist für ihn das Leben der Heiligen wie Teresa von Kalkutta und der Seligen Jungfrau Maria und ist die Richtung, die Christen einschlagen, wenn sie glauben, dass Gott sie liebt.
Und so lehrte Papst Franziskus: „Wahre Liebe ist Liebe und sich selbst lieben zu lassen.... was in der Liebe wichtig ist, ist nicht unsere Liebe, sondern sich von Gott lieben zu lassen.“ Für ihn ist der Schlüssel zur Liebe „....nicht unsere Tätigkeit. Es ist die Aktivität der größten und der Quelle aller Kräfte im Universum: der Gottes.“
Im Christentum wird die praktische Definition der Liebe am besten vom heiligen Thomas von Aquin zusammengefasst, der die Liebe als „zum Wohl des Anderen“ oder zum Wunsch nach Erfolg des Anderen definiert hat. Dies ist die Erklärung für das christliche Bedürfnis, andere zu lieben, einschließlich ihrer Feinde. Wie Thomas Aquinas erklärt, ist die christliche Liebe durch das Bedürfnis motiviert, andere im Leben erfolgreich zu sehen, gute Menschen zu sein.
Tertullian schrieb über die Liebe zu den Feinden: „Unsere individuelle, außergewöhnliche und vollkommene Güte besteht darin, unsere Feinde zu lieben. Seine Freunde zu lieben ist gängige Praxis, seine Feinde nur unter Christen zu lieben.“
Die Liebe umfasst die islamische Sicht des Lebens als universelle Bruderschaft, die für alle Gläubigen gilt. Unter den 99 Namen Gottes (Allah) gibt es den Namen Al-Wadud, oder „der Liebende“, der sowohl in Sure 11:90 als auch in Sure 85:14 zu finden ist. Gott wird auch am Anfang jedes Kapitels im Qur'an als Ar-Rahman und Ar-Rahim bezeichnet, oder als der „Barmherzigste“, was bedeutet, dass niemand liebevoller, mitfühlender und gütiger ist als Gott. Der Qur'an spricht von Gott als „voller liebender Güte“.
Der Qur'an ermahnt muslimische Gläubige, alle Menschen, die sie nicht verfolgt haben, mit Birr oder „tiefer Güte“ zu behandeln, wie es in Sure 6:8-9 heißt. Birr wird auch vom Qur'an verwendet, um die Liebe und Freundlichkeit zu beschreiben, die Kinder ihren Eltern zeigen müssen.
Ishq, oder „göttliche Liebe“, ist auch der Schwerpunkt des Sufismus in der islamischen Tradition. Praktizierende des Sufismus glauben, dass die Liebe eine Projektion der Essenz Gottes auf das Universum ist. Gott will die Schönheit erkennen, und als ob man in einen Spiegel schaut, um sich selbst zu sehen, schaut Gott sich selbst in der Dynamik der Natur an. Da alles ein Spiegelbild Gottes ist, praktiziert die Schule des Sufismus, die Schönheit im scheinbar Hässlichen zu sehen. Der Sufismus wird oft als die Religion der Liebe bezeichnet. Gott im Sufismus wird in den Hauptbegriffen „der Liebende“ und „Geliebte“ genannt, wobei der letzte dieser Begriffe oft in der Sufi-Poesie zu finden ist. Ein gemeinsamer Standpunkt des Sufismus ist, dass die Menschheit durch die Liebe zu ihrer innewohnenden Reinheit und Gnade zurückkehren kann. Die Heiligen des Sufismus sind berüchtigt dafür, wegen ihrer Liebe zu Gott „betrunken“ zu sein.
Der Begriff Liebe ist in der Biologie nicht definiert und damit keine biologische Kategorie. Allgemein ist es schwierig, emotionale Prozesse mit naturwissenschaftlicher Methodik zu bearbeiten, zumal die zu Grunde liegende Biochemie noch nicht ausreichend bekannt ist. Gesichert sind beim Menschen lediglich folgende Erkenntnisse:
Neueren Untersuchungen des Gehirnstroms und Studien zufolge bewirkt Verliebtheit in Bereichen des menschlichen Gehirns, die auch für Triebe zuständig sind, die höchste Aktivität, was darauf schließen lässt, dass „das Gefühl“, das gemeinhin als Liebe (im Sinne von Verliebtheit) bezeichnet wird, in seinem biochemischen Korrelat einen starken Zusammenhang mit dem biologischen Trieb aufweist.
Die mitunter sehr lange anhaltenden Wirkungen der Verliebtheit (Limerenz) deuten aber auch auf neuroendokrine Prozesse hin, die dem Phänomen zugrunde liegen. Das würde sich auch in das Entstehungsfeld einfügen, das in der Sexualität zu suchen ist, die ihrerseits maßgeblich der neuroendokrinen Steuerung des Zwischenhirns unterliegt. Dabei spielen nicht zuletzt die endogenen Opiate des Hypophysenzwischenlappens eine Rolle.
Verliebt sich ein Mensch, so sorgen verschiedene Botenstoffe für Euphorie (Dopamin), Aufregung (Adrenalin), rauschartige Glücksgefühle und tiefes Wohlbefinden (Endorphin und Cortisol) sowie erhöhte sexuelle Lust (Testosteron sinkt bei Männern, steigt bei Frauen). Umgekehrt können Momente, in denen man nicht mit der geliebten Person zusammen ist, als sehr schmerzlich bis hin zur Verzweiflung empfunden werden. Auch Sexualduftstoffe (Pheromone) werden vermehrt abgegeben. Hingegen sinkt der Serotoninspiegel stark ab, wodurch der Zustand der Verliebtheit in diesem Punkt eine Ähnlichkeit mit vielen psychischen Krankheiten aufweist. Das trägt dazu bei, dass Verliebte sich zeitweise in einem Zustand der „Unzurechnungsfähigkeit“ befinden können, sich dabei zu irrationalen Handlungen hinreißen lassen und Hemmschwellen abbauen. Nach einiger Zeit (wenige Monate) gewöhnt sich der Körper an diese Dosen und ganz allmählich (laut WHO maximal nach 24 bis 36 Monaten) beendet das Gehirn diesen sensorischen „Rauschzustand“.
Das vertiefte Gefühl der Liebe ist aus evolutionsbiologischer Sicht möglicherweise im Zusammenhang mit der Sexualität entstanden, wobei die Liebe es ist, die die Partnerwahl und damit die Paarbeziehung über längere Zeiträume stabilisiert. Es sind zwar bei vielen Tierarten monogame Paarbeziehungen bekannt (zum Beispiel auch bei den Graugänsen von Konrad Lorenz), aber ob diese Tiere dabei so etwas wie Liebe empfinden, ist eine unbeantwortete Frage.
Im Rahmen des Konzepts der biologischen Determiniertheit entsteht Liebe zwingend aus bestimmten körperlichen Reaktionen.
Nach Auffassung der Evolutionspsychologen werden Frauen und Männer bei der Partnerwahl von Vorlieben regiert, die sich über Millionen von Jahren von unseren Vorfahren auf uns weitervererbt haben. Diese „Steinzeit-Psyche“ soll Frauen auf starke oder statushohe Beschützer-Typen reagieren lassen; Männer dagegen auf junge, hübsche Frauen. Schönheit gelte bei beiden Geschlechtern offenbar als Indiz für „gesunde Gene“, wie auch Humanethologen bestätigen. In diesem Zusammenhang wurde auch vielfach untersucht, was „Schönheit“ in diesem Zusammenhang bedeutet, welche körperlichen Merkmale für beide Geschlechter als attraktiv gelten („Durchschnittlichkeit“ als Ideal).
Die Psychiatrie befasst sich unter dem medizinischen Aspekt mit dem Phänomen. So wird zum Beispiel die Psychopathologie des „Liebeswahns“ im Zusammenhang mit paranoischen Vorstellungen diagnostiziert (vgl. Wahnsinn).
In soziologischen Forschungen werden häufig mit dem Thema Liebe verwandte Phänomene behandelt. Unter Anderem wird auf Bedingungen der Partnerwahl oder die sozialpsychologische Bedeutung der Paartherapie eingegangen. Zudem wird Liebe innerhalb der Familiensoziologie aufgegriffen.
Es liegen in der Soziologie mindestens vier substantielle, thematisch einander eher ergänzende Ansätze zur Liebe vor. Sie betonen mehr oder weniger die auf Liebe bezogenen Aspekte von Kommunikation Interaktion und Semantik. Demnach wird Liebe (1) als „Emotion“ (zum Beispiel bei Jürgen Gerhards), (2) als „Kulturmuster“ (zum Beispiel bei Niklas Luhmann), (3) als „Intimsystem“ (bei Frank Becker/Elke Reinhardt-Becker, Jürgen Fuchs) und (4) „nicht-kognitive Form kommunikativer Praxis“ (bei Günter Burkart, Cornelia Koppetsch) definiert.
Liebe wird unter anderem als ein gesellschaftlich wirkendes Symbol für Interaktionen betrachtet (vgl. Symbolischer Interaktionismus) und auf seine soziale Funktion hin untersucht. Die Soziologie untersucht zahlreiche Einzelformen der Liebe, etwa die „romantische Liebe“, die „Liebe“ im Bürgertum, die „Mutterliebe“, die „Vaterlandsliebe“ (oft als Ideologie), die Bezüge zwischen Liebe, Gewalt und Macht und andere mehr. Unter den zeitgenössischen Soziologen behandelt zum Beispiel Bálint Balla eingehend die Liebe in seiner Soziologie der Knappheit; Horst Herrmann untersucht die (geschlechtsspezifischen) Zusammenhänge von Liebe und Gewalt sowie die gesellschaftlich wirkenden Modelle heutiger Liebesbeziehungen. Auch hat die Soziologie angrenzende soziale Bräuche wie die Koketterie (Georg Simmel) oder den Flirt untersucht.
Die Systemtheorie nahm eine einschneidende Begriffsverengung vor, indem sie Liebe neu als eine „gesellschaftliche Semantik“ bzw. als Code des Miteinander-Umgehens definierte. So formulierte Niklas Luhmann in Liebe als Passion (1982) romantische Liebe als ein Phänomen der Moderne, welches seine Grundlegung vor allem im Bürgertum des 18. Jahrhunderts erfährt.
Liebe fungiert – nach Luhmann – in der heutigen funktional ausdifferenzierten Gesellschaft in erster Linie als „symbolisch generalisiertes Kommunikationsmedium“, das unwahrscheinliche Kommunikation wahrscheinlich machen soll. Die Gesellschaft differenziert sich immer stärker in einzelne Teilbereiche. Jedes Individuum ist nicht mehr nur in einem Bereich, zum Beispiel der Familie verwurzelt, sondern in vielen Teilbereichen, etwa Freizeit oder Beruf. Auch ist es immer auch nur zu einem Teil verortet und bewegt sich ständig zwischen verschiedenen Bereichen hin und her. Aufgrund dieser kommunikativen „Polykontextualität“ erschwere sich die identitätsbildende Interaktion.
Dem Einzelnen fällt es vor diesem Hintergrund zunehmend schwerer, sich selbst zu bestimmen. Hinzu kommt, dass diese Individualität und Identität im kommunikativen Austausch mit anderen bestätigt werden muss. Diese „höchstpersönliche“ Kommunikation nimmt in einer derart ausdifferenzierten Gesellschaft aber ständig ab, denn zum einen wird durch die Vielzahl an Rollen in den beschriebenen Teilbereichen (zum Beispiel als Tochter, Sekretärin, Freizeitseglerin etc.) dort auch nur unpersönliche Kommunikation erfahren und zum anderen begreift sich der Mensch als Individuum, also etwas Besonderes, Einzigartiges, anders als die Anderen. Angesichts dieser Entwicklung ist es nicht nur schwierig, miteinander in Kontakt zu treten, es wird auch schwierig, einander überhaupt noch zu verstehen bzw. die Motivation zu finden, sich auf einen doch so Besonderen, Anderen einzulassen. Genau dieses Problem zu bewältigen ist – in dieser Theorie – Aufgabe der Liebe. Der Systemtheoretiker Peter Fuchs definiert Liebe daher als „wechselseitige Komplettannahme im Modus der Höchstrelevanz“. Liebe als Kommunikationsmedium motiviert dazu, sich dem Anderen unter Ausblendung von Idiosynkrasien in seiner „Ganzheit“ zu nähern und nicht unter der verengenden Perspektive des jeweiligen Sozialsystems (zum Beispiel als Freizeitsegler). Durch diese Komplettannahme entsteht eine wechselseitige Bestätigung des „Selbst-Seins“ und des jeweiligen „Weltbezugs“.
Liebe, bzw. genauer das Intimsystem, das im Medium Liebe operiert, ist eine Vorform des Sozialsystems Familie, dem grundlegende gesellschaftliche Funktionen zukommen (nämlich Reproduktion und Sozialisation). Des Mediums Liebe bedarf es, da unwahrscheinliche Ereignisse (zwei Menschen begegnen sich unter Millionen anderen und begründen und stabilisieren ein Zusammenleben) erwartbar gemacht werden müssen. Liebe ist also wie Geld oder Macht ein sogenanntes Steuerungsmedium, das die Chance auf das Eintreffen unwahrscheinlicher Sinnzumutungen steigert. Überraschend ist dabei jedoch, dass Intimsysteme auf dem paradoxen, komplexen und sehr täuschungsempfindlichen Medium Liebe basieren.
Wanders Deutsches Sprichwörter-Lexikon bietet zu Liebe, Liebeli, Liebeln, Lieben, Liebende usw. ca. tausend Sprichwörter. Auch in der medizinischen Fachliteratur erscheinen die Liebe und deren Folgen (etwa der „Liebeskummer“) seit der Antike.
Wesentlich ist im sozialen Kontext die Unterscheidung zwischen der einseitigen und der gegenseitigen Liebe. Erstere hat ihren Spezialfall in der im Volksmund sogenannten unglücklichen Liebe (vgl. Liebeskummer).
Viele Bezeichnungen für Fachgebiete sind, ebenso wie eine Reihe anderer Begriffe, auf dem Präfix phil- aufgebaut, das für Liebe steht. Hierzu zählen insbesondere die Philosophie (ursprünglich: Liebe zur Weisheit) und die Philologie (ursprünglich: Liebe zu Sprachen). Die Philatelie sei stellvertretend für andere Sammelleidenschaften genannt, der Name Philipp („Philhippos“, verschiedene Schreibweisen) bedeutet „Pferdeliebhaber“.
Einen christlichen Standpunkt innerhalb der Existenzphilosophie vertritt Gabriel Marcel in Sein und Haben: Der Mensch existiere ursprünglich nicht in der Abgrenzung, sondern in der Teilhabe am Mitmenschen und göttlichen Sein. In dieser Seinsteilhabe verwirkliche sich die Liebe, die sich vorbehaltlos öffne, wenn der Mensch dessen in innerlicher Hingabe gewahr werde.
Seit Harry Harlow (siehe Literatur: Psychologie) seinen Bericht über die Untersuchungen an Affenjungen veröffentlicht hat ("Das Wesen der Liebe"), lässt sich diskutieren, ob Liebe auch von Tieren empfunden und erlebt werden kann. Sein Konzept von der Liebe ist allerdings eher positivistisch orientiert, denn Liebe sei ein Gefühl der Haut, indem sie sich eben vorwiegend über die Berührung zweier Lebewesen realisiere.
Minnesang bzw. Minnedichtung reflektierten programmatisch unerfüllte Liebe, priesen die Angebetete oder schilderten erotische Erlebnisse (ab Mitte des 13. Jahrhunderts).
Die Schäferdichtung war eine beliebte Literaturgattung der europäischen Renaissance und des Barock in der Tradition der Bukolik. Sie entwickelte sich ursprünglich aus der Schäferei, einer höfischen Rollendichtung, die durch die Verschmelzung von lyrisch-musikalischen Elementen, Prosa, Dialogen und kunstvollen Versen gekennzeichnet war und das Hirtenleben idealisierte. Ein beliebtes Thema ist der Bericht von einer spröden, abweisenden Geliebten, der ein Liebender gegenübersteht, der ihr völlig ausgeliefert ist, sich nach ihr verzehrt und, unter Umständen sogar krankhaft leidet. Gefühle wie unerfüllte Liebe, Lobgesänge auf eine Schäferin, Wehmut in Anbetracht einer schöneren Vergangenheit oder einer verlorenen Heimat zählen zu den typischen Gegenständen der künstlerischen Darstellung.
Marsilio Ficino: Über die Liebe oder Platons Gastmahl. 3. Auflage. Meiner, Hamburg 1994, ISBN 3-7873-1189-0.
Werner Schüßler / Marc Röbel (Hrsg.): LIEBE – mehr als ein Gefühl. Philosophie – Theologie – Einzelwissenschaften. Schöningh, Paderborn 2016, ISBN 978-3-506-78513-8.
C.S. Lewis: Was man Liebe nennt: Zuneigung, Freundschaft, Eros, Agape. 7. Auflage. Brunnen, Basel 2004, ISBN 3-7655-3266-5.
Roland Barthes: Fragmente einer Sprache der Liebe. Suhrkamp, Frankfurt am Main 2005, ISBN 3-518-38086-9. Zitate von Roland Barthes.
Kai Buchholz (Hrsg.): Liebe. Ein philosophisches Lesebuch. Goldmann Verlag, München 2007, ISBN 978-3-442-07756-4.
Axel Honneth: Liebe und Moral. Zum moralischen Gehalt affektiver Bindungen. In: Axel Honneth: Das Andere der Gerechtigkeit. Aufsätze zur praktischen Philosophie. Suhrkamp, Frankfurt am Main 2000, ISBN 3-518-29091-6, S. 216 ff.
Maik Hosang: Liebe in Zeiten des Klimawandels. Phänomen Verlag, Hamburg 2008, ISBN 978-3-933321-72-5.
Yudit Kornberg Greenberg: Encyclopedia of Love in World Religions. ABC-Clio, 2007, ISBN 978-1-85109-980-1.
Martha Nussbaum: Konstruktion der Liebe, des Begehrens und der Fürsorge. Drei philosophische Aufsätze. Reclam, Stuttgart 2002.
Wilhelm Schmid: Die Liebe neu erfinden. Suhrkamp, Frankfurt am Main 2010, ISBN 978-3-518-42203-8. (Rezension: dradio.de, Deutschlandfunk, Andruck, 1. November 2010, Annette Brüggemann: Zwischen Endlichkeit und Unendlichkeit.)
Stascha Rohmer, Liebe – Zukunft einer Emotion, Freiburg/München: Karl Alber 2008. ISBN 978-3-495-48317-6
Robert A. Johnson: Traumvorstellung der Liebe. Der Irrtum des Abendlandes. 3. Auflage. Walter, Olten u. a. 1988, ISBN 3-530-40391-1.
Peter Lauster: Die Liebe. Psychologie eines Phänomens. 35. Auflage. Rowohlt, Reinbek bei Hamburg 2004, ISBN 3-499-17677-7.
Harry Harlow: Das Wesen der Liebe. In: Otto M. Ewert (Hrsg.): Entwicklungspsychologie. Band 1. Kiepenheuer & Witsch, Köln 1972, S. 129–135.
Nadine Bauers: Psychologische Aspekte der Liebe. Neuere Befunde. jmb Verlag, Hannover 2016, ISBN 978-3-944342-86-3.
Ulrich Beck, Elisabeth Beck-Gernsheim: Das ganz normale Chaos der Liebe. Suhrkamp, Frankfurt am Main 1990
Ulrich Beck, Elisabeth Beck-Gernsheim: Fernliebe. Lebensformen im globalen Zeitalter. Suhrkamp, Frankfurt am Main 2011.
Christoph Egen: Zur Sozio- und Psychogenese der romantischen Liebesvorstellung in westeuropäischen Gesellschaften. Cuvillier, Göttingen 2009, ISBN 978-3-86955-199-9.
Werner Faulstich (Hrsg.): Liebe 2000. Konzepte von Liebe in der populären Kultur heute. Wissenschaftler-Verlag, Bardowick 2002, ISBN 3-89153-034-X.
Peter Fuchs: Liebe, Sex und solche Sachen. Zur Konstruktion moderner Intimsysteme. UVK, Konstanz 1999, ISBN 3-87940-663-4.
Anthony Giddens: The transformation of intimacy. Sexuality, love, and eroticism in modern societies. Polity Press, Cambridge 2001, ISBN 0-7456-1012-9, ISBN 0-7456-1239-3. (Nachdruck, 1. Auflage. 1992)
Doris Guth, Heide Hammer (Hrsg.): Love me or leave me. Liebeskonstrukte in der Populärkultur. Campus Verlag, Frankfurt am Main/ New York 2009.
Kornelia Hahn, Günter Burkart (Hrsg.): Liebe am Ende des 20. Jahrhunderts. Studien zur Soziologie intimer Beziehungen. Leske + Budrich, Opladen 1998.
Eva Illouz: Der Konsum der Romantik. Liebe und die kulturellen Widersprüche des Kapitalismus. Suhrkamp, Frankfurt am Main 2007, ISBN 978-3-518-29458-1. (orig. 1997)
Jean-Claude Kaufmann: Der Morgen danach. Wie eine Liebesgeschichte beginnt. UVK, Konstanz 2004. (im frz. Orig. 2002)
Henk J. Koning: Die Freundesliebe in Holteis Kriminalroman Schwarzwaldau (1856). In: Leszek Dziemianko, Marek Halub (Hrsg.): Karl von Holtei (1798–1880). Leben und Werk. Fragestellungen – Differenzierungen – Auswertungen. Leipzig 2011 (= Schlesische Grenzgänger. Band 3), S. 100–121.
Annemarie Leibbrand-Wettley, Werner Leibbrand: Formen des Eros. Kultur- und Geistesgeschichte der Liebe. (= Orbis academicus. Sonderbände 3/1-2). 2 Bände. Alber, Freiburg/ München 1972, ISBN 3-495-47256-8.
Karl Lenz: Soziologie der Zweierbeziehung. Eine Einführung. 2. Auflage. Westdeutscher Verlag, Wiesbaden 2003, ISBN 3-531-33348-8.
Niklas Luhmann: Liebe als Passion. Zur Codierung von Intimität. 7. Auflage. Suhrkamp, Frankfurt am Main 2003, ISBN 3-518-28724-9.
Regina Mahlmann: Was verstehst du unter Liebe? Ideale und Konflikte von der Frühromantik bis heute. Wiss. Buchgesellschaft/ Primus Verlag, Darmstadt 2003, ISBN 3-89678-468-4.
Yvonne Niekrenz, Dirk Villányi (Hrsg.): LiebesErklärungen. Intimbeziehungen aus soziologischer Perspektive. VS Verlag, Wiesbaden 2008.
Paul Ridder: Sonette gegen Liebesschmerz. Bibliotherapie in der Medizingeschichte. Verlag für Gesundheitswissenschaften, Greven 2008, ISBN 978-3-9807065-6-8.
Christian Schuldt: Der Code des Herzens. Liebe und Sex in den Zeiten maximaler Möglichkeiten. Eichborn, Frankfurt am Main 2004, ISBN 3-8218-5592-4.
Kurt Starke: Nichts als die reine Liebe. Beziehungsbiographien und Sexualität im sozialen und psychologischen Wandel. Pabst, Lengerich u. a. 2005
Bennett Helm: Eintrag in Edward N. Zalta (Hrsg.): Stanford Encyclopedia of Philosophy.Vorlage:SEP/Wartung/Parameter 1 und Parameter 3 und nicht Parameter 2

Der Liebherr T 282 ist ein zweiachsiger Großmuldenkipper für den Einsatz im großen Tagebau. Der T 282 hat einen der stärksten Antriebe bei kommerziellen Nutzfahrzeugen. Er wird in Newport News, Virginia, gefertigt.
Der T 282 ist ein Tagebauspezialfahrzeug zum Abtransport von Erzen, Kohle und Abraum mit einer maximalen Ladekapazität von 364 Tonnen. Er wird im Kohle-, Gold-, Kupfer- und Diamanttagebau, aber auch zum Transport von Ölsand eingesetzt.
Mit dem Vorgängermodell T 262 mit einer maximalen Ladekapazität von 218 Tonnen hatte Liebherr bereits seit 1982 einen Großmuldenkipper für den Tagebau im Programm. Wegen der gestiegenen Anforderungen, insbesondere seitens sehr großer Tagebaue, wurde ein Modell mit deutlich höherer Ladekapazität entwickelt.
Die Größe der Fahrzeugflotte eines Tagebaus liegt meist nur im unteren zweistelligen Bereich. Die Fahrzeuge laufen in der Regel 24 Stunden täglich. Ein Ausfall eines oder mehrerer Fahrzeuge hat daher große Auswirkungen auf die Förderleistung eines Bergwerks. Deshalb wurde bei der Entwicklung des T 282 großes Gewicht auf hohe Verfügbarkeit und Zuverlässigkeit bei gleichzeitiger Reduktion der Betriebskosten gelegt. Alle Elemente des Fahrzeugs wurden auf weitgehende Wartungsfreiheit hin gestaltet. 1999 wurde der T 282 der Öffentlichkeit vorgestellt.Auf der Fachmesse Bauma in München präsentierte Liebherr fünf Jahre später das neue Modell T 282 B, das im Rahmen von rund 200 Veränderungen insbesondere eine gesteigerte Ladekapazität mit stärkerer Motorleistung verband und im August des Jahres erstmals ausgeliefert wurde.
Ein T 282 B kostet rund 3,5 Millionen US-Dollar. In seinem Marktsegment konkurriert der T 282 vor allem mit dem Modell 797 des Marktführers Caterpillar sowie dem Komatsu 930 und dem 2007 vorgestellten Terex Unit Rig MT 6300AC. Weitere Modelle ähnlicher Größe wurden unter anderem von Hitachi angekündigt. Der weißrussische Hersteller Belaz hat 2013 den BelAZ-75710 vorgestellt. Wie alle Großmuldenkipper von Liebherr wird auch der T 282 in einem Spezialwerk in Newport News gebaut. Ende 2005 waren unter den weltweit 711 Großmuldenkippern der Ultraklasse mit mehr als 290 Tonnen 110 T 282, Ende Juni 2006 lagen für weitere 37 Fahrzeuge Bestellungen vor. Hauptkunden sind Codelco (Chile), Mt Arthur Coal (Australien), Syncrude (Kanada) und die Cortez Goldmine (Vereinigte Staaten).
Die folgenden Daten beziehen sich auf den T 282 B (Stand 2007), aktuell ist jedoch das Modell T 284 (Stand 2017).Der T 282 B ist 14,5 Meter lang, 7,4 Meter hoch und 8,8 Meter breit, kann, abhängig von der verwendeten Mulde, bis zu 364 Tonnen zuladen und hat ein Gesamtgewicht von maximal 592 Tonnen. Das Fahrzeug erreicht eine Höchstgeschwindigkeit von 64 km/h. Die Räder haben einen Durchmesser von 4,11 Meter, ihr Gewicht beträgt 7,8 Tonnen. Der Durchschnittsverbrauch beträgt 174 Liter Diesel pro Betriebsstunde, das sind 4176 Liter pro Tag, der Tankinhalt von 4732 Liter reicht damit für etwas über 24 Stunden.
Die aus einem etwas über 20 Millimeter dicken Stahlblech bestehende Mulde ist 30 Tonnen schwer und kann innerhalb von 28 Sekunden bis zu einem Winkel von 45 Grad angehoben werden. Der geschweißte Kastenrahmen ist in seinen Hohlräumen durch zusätzliche Elemente versteift. Kontrollzentrum, Gebläse, Schaltkästen und Fahrerkabine befinden sich auf einem Deck oben an der Fahrzeugfront, die Kontrolle des Verkehrsraumes durch den Fahrer wird durch Videokameras um das Fahrzeug gewährleistet. Unterhalb des Decks befinden sich der Dieselmotor und der Generator.
Der T 282 B verwendet zwei einander ergänzende Bremssysteme. Als primäres Bremssystem kommt eine weitgehend verschleißfreie elektrodynamische Dauerbremse mit rund 4480 kW (6030 PS) zum Einsatz. Wird sie aktiviert, fungieren die Antriebsmotoren als elektrische Generatoren, die die Bewegungsenergie in elektrische Energie umwandeln, die wiederum von luftgekühlten Widerständen in Wärme umgewandelt wird.
Der Liebherr T 282 B hat einen dieselelektrischen Antrieb, der Kupplung und Schaltgetriebe überflüssig macht, wodurch Gewicht und Wartungsaufwand stark reduziert werden konnten. Der Dieselmotor treibt einen Generator an, der über elektronische Frequenzumrichter zwei Drehstrommotoren mit Strom versorgt. Von diesen Motoren ist je einer direkt hinter jeder Radnabe an der Hinterachse angebracht und treibt über ein Planetengetriebe je ein Radpaar der Hinterachse an.
Dieses Verfahren ermöglicht eine vollständig unabhängige Versorgung und Steuerung der Hinterradpaare voneinander, wodurch die Drehzahl jedes Paares getrennt regelbar ist. Wenn das Fahrzeug Kurven fährt, wird die Geschwindigkeit der äußeren Räder automatisch erhöht beziehungsweise die der inneren gesenkt, um die äußerst teuren Reifen zu schonen; eine Slip/Slide-Control gewährleistet darüber hinaus Spurtreue durch die Bereitstellung der jeweils notwendigen Antriebskraft für die einzelnen Hinterradpaare.
Als Dieselmotoren stehen zwei Modelle zur Verfügung. Die stärkere Ausführung ist ein bei MTU Friedrichshafen gebauter MTU DD 20V4000. Der 10,5 Tonnen schwere Motor hat 20 Zylinder und 90 Liter Hubraum und erzeugt nach Werksangaben 2722 kW (3650 PS). Damit ist sie die stärkste in einem nicht spurgebundenen Landfahrzeug verwendete Antriebsmaschine. Alternativ kann ein Cummins-QSK-78-Dieselmotor eingesetzt werden, der mit 18 Zylindern und 76 Liter Hubraum 2610 kW (3500 PS) leistet und 11,3 Tonnen wiegt. Als Starterbatterie dienen sechs 12-Volt-Batterien, die 1475 Ampere bei 0 °C liefern.
Dawn M. Geske: Bigger and stronger: Liebherr redesigns T 282 haul truck; now includes 3650 hp engine option, in: Diesel Progress North American Edition, Sept. 2004, FindArticles.com. Zugriff am 14 Jul. 2007.

Der Liederkreis Op. 39 ist ein Zyklus von Robert Schumann (1810–1856) aus zwölf Vertonungen von Gedichten Joseph von Eichendorffs (1788–1857) für Singstimme und Klavier, der im Jahr 1840 entstand und 1842 in einer ersten Druckfassung veröffentlicht wurde. Schon bald danach erfreuten sich einzelne Lieder des Werkes großer Beliebtheit. Den vollständigen Zyklus in seiner endgültigen, zweiten Fassung führte Julius Stockhausen erstmals am 21. Oktober 1862 im Kölner Gürzenich-Saal auf. Heute bildet der Liederkreis einen wichtigen Teil des romantischen Kunstliedrepertoires.
Exzerpte Eichendorff’scher Gedichte, die Robert Schumann in seiner Neuen Zeitschrift für Musik einigen Besprechungen von Liedkompositionen und geistlicher Musik sowie allgemein geistlichen Themen vorausstellte, zeigen, dass er von Eichendorff weitgehend das traditionelle Bild des volksliedhaften, frommen Dichters hatte.Insgesamt 21 Eichendorff-Gedichte sind der Nachwelt in Vertonungen Robert Schumanns erhalten, 12 davon im Liederkreis op. 39, den er in einem Brief vom 22. Mai 1840 an Clara Wieck, seine spätere Ehefrau, als sein „aller Romantischstes“ bezeichnete (→ Romantik). Eichendorff hörte einige Lieder des Liederkreises im Januar 1847 bei einem Zusammentreffen in Wien. Clara gegenüber versicherte er, Schumanns Musik habe seinen „Liedern erst Leben gegeben.“ In brieflichen Berichten über seinen damaligen Wiener Aufenthalt erwähnte er allerdings Schumanns Vertonungen nicht, sondern die von Josef Dessauer, die er als „unglaublich schön komponirt“ bezeichnete.
Das Jahr 1840 ging als sogenanntes „Liederjahr“ in die Biographie von Robert Schumann ein, in diesem Jahr komponierte er etwa die Hälfte seines gesamten Liedschaffens. Davor hatte er fast ausschließlich Klaviermusik und nur wenige Lieder verfasst: Als moderner „deutscher Geist“ hatte Schumann im musikästhetischen Konflikt des 19. Jahrhunderts der absoluten Musik zunächst den Vorzug gegeben und die Vokalmusik „unter die Instrumentalmusik gesetzt und nie für eine große Kunst gehalten.“, wie er 1839 an den Geiger und Komponisten Hermann Hirschbach schrieb. Der Zusatz „Doch sagen Sie niemand davon.“ im selben Brief gibt jedoch Zeugnis von seinem Sinneswandel zu dieser Zeit. Zugleich aber konnte der Komponist nur durch die zuvor erfolgte Entwicklung seines Klavierstils zu einem gleichermaßen persönlichen Liedstil finden, der den schon erprobten ausdrucksstarken Klaviersatz mit klaren Gesangsmelodien verbindet.
Alle Liedertexte des Zyklus wurden der 1837 in Berlin erschienenen Erstausgabe von Joseph von Eichendorffs Gedichten entnommen, in die auch Gedichte aufgenommen wurden, die ursprünglich in Prosatexten Eichendorffs ihren Platz hatten. Im April 1840 schickte Clara Wieck Abschriften einiger Eichendorff-Gedichte an Robert Schumann, der die Kopien von ihr erbeten hatte. Diese Gedichte finden sich in Claras Handschrift in den Abschriften von Gedichten zur Composition. Gesammelt von Robert und Clara Schumann vom Jahre 1839 an. Fast alle Themenbereiche der Eichendorff’schen Lyrik sind dabei vertreten, lediglich Studentenlieder und solche mit stark religiösem Inhalt fehlen. Ob die Auswahl der Gedichte von Clara oder Robert Schumann oder von beiden stammt, lässt sich nicht ergründen. Die Reihenfolge in den Abschriften von Gedichten zur Composition folgt der Reihenfolge in Eichendorffs Ausgabe von 1837. Diese Gedichte bilden bei Eichendorff keinen Zyklus.An einigen Stellen finden sich Änderungen in den von Schumann vertonten Texten, die vielleicht auf Abschreibfehler zurückgehen, zum Teil aber auch absichtliche Retuschen sein könnten, um die Aussage eines Liedes in eine bestimmte Richtung zu verstärken.
Die ursprüngliche Fassung des Zyklus wies noch eine andere Anordnung der Lieder auf, in der der düstere Charakter vorherrschte: Waldesgespräch – In der Fremde („Aus der Heimat“) – Mondnacht – Intermezzo – Schöne Fremde – In der Fremde („Ich hör’“) – Wehmut – Frühlingsnacht – Die Stille – Zwielicht – Im Walde – Auf einer Burg. Diese Auswahl könnte durch Schumanns damalige Lebenssituation begründet sein, in der seine Beziehung mit Clara von ihrem Vater missbilligt wurde. Der alte Ritter im Lied Auf einer Burg kann hier auch als Porträt des unerbittlichen Friedrich Wieck verstanden werden. Nachdem aber am 1. August 1840 ein Gerichtsentscheid die Ehe der beiden für rechtens erklärt hatte, reihte der Komponist den Zyklus um: Er teilte die zwölf Lieder in zwei mal sechs, wobei er in die erste Hälfte hauptsächlich die positiven und in die zweite die dunkleren Inhalte stellte. An die beiden Enden der Teile positionierte er aber die optimistischen Lieder Schöne Fremde und Frühlingsnacht. Die letzte Zeile des Liederkreises in dieser Anordnung, der triumphale Ausruf „Sie ist Deine, sie ist dein!“ macht den autobiographischen Zusammenhang deutlich.
Im Allgemeinen wird der Zyklus in zwei Teile gegliedert. Diese ergeben sich aus der oben erwähnten Anordnung von Inhalten und Tonarten, wurden jedoch nicht ausdrücklich von Schumann festgesetzt.
Der Zyklus wird mit dem Lied In der Fremde („Aus der Heimat“) eröffnet, das in Eichendorffs Erzählung Viel Lärmen um nichts zur Gitarre gesungen wird. Dieses Detail findet sich in Schumanns arpeggierter Klavierbegleitung wieder. Im zweiten Vers des Liedes, nach den Worten „Wie bald“ tritt im Klavier zum ersten Mal die aufsteigende Quinte auf, die ein wichtiges wiederkehrendes Motiv des Liederkreises ist und stets positive Inhalte transportiert. Hier symbolisiert sie Hoffnung, wenngleich es die Hoffnung auf den Tod ist.
Doch schon im nächsten Stück, dem Intermezzo, das in der Liebestonart A-Dur steht, folgt das Quintmotiv dem Wort „wunderselig“. Dieses zweite Lied basiert auf einem Gedicht, das Eichendorff 1810, wohl für seine Verlobte Luise von Larisch, verfasst hatte. Den wunderbaren, „auf Metaphern schwebenden“ Text setzt der Komponist mit ebenso schwebenden Synkopen in der Begleitung um.
Das darauf folgende Waldesgespräch steht in Dialogform, im Roman Ahnung und Gegenwart wird es auch als Wechselgespräch vorgetragen. Die beiden Personen werden von Schumann in zwei verschiedenen musikalischen Sphären dargestellt: Während die Worte des Reiters mit forschen punktierten Rhythmen und Hornquinten unterlegt sind, wird die Hexe Loreley zunächst mit harfenartigen Akkorden auf einem Orgelpunkt, später mit dramatischen alterierten Akkorden begleitet. Besonders in diesem Lied zeigt sich auch die Musikalität der Eichendorffschen Vorlage, wenn der Mann kurze, konsonantenreiche Worte, die Hexe aber solche mit langen und dunklen Vokale bevorzugt.
Das nächste Lied, Die Stille, ist zwar aus demselben Roman wie das Waldesgespräch, führt jedoch in eine ganz andere Stimmung: Es handelt von unterdrückter, aber innerlich explodierender Liebe. So vertont Schumann es als glücklich wiegenden 6/8-Takt in G-Dur, dämpft den Ausdruck aber durch die Vortragsbezeichnung Nicht schnell, immer sehr leise und durch die anfangs kurzen, gehauchten Noten der Singstimme. Erst bei den Worten „Ich wünscht’, ich wäre ein Vöglein“ beginnt ein melodisch ausdrucksstarker Bogen, der jedoch kurz darauf wieder unterdrückt wird, indem Schumann die Vorlage verändert und die erste Strophe des Gedichts zum Schluss wieder aufnimmt.
Die Mondnacht ist wohl das bekannteste Lied des Zyklus und eine der populärsten Schöpfungen Schumanns überhaupt, weil Musik und Dichtung sich hier auf eine einzigartige Weise ergänzen: Der Text besticht durch betont schlichten, aber tiefe Assoziationen weckenden Ausdruck und führt gleich zu Beginn mit einem Konjunktiv in eine Art Traumsituation. Musikalisch wird diese Idee durch dominantische Akkorde, wie dem Nonenakkord gleich zu Beginn und die pulsierende Wiederholung einzelner Töne oder Intervalle, die harmonisch umgedeutet werden, umgesetzt. Während die ersten vier Halbverse stets mit der gleichen Melodie gesungen werden, wird bei der Zeile „Und meine Seele spannte …“ harmonisch und melodisch ein neuer Weg eingeschlagen, was den Abflug der Seele hör- und spürbar macht.
Abgeschlossen wird der erste Teil des Liederkreises durch das Lied Schöne Fremde. Auch in diesem Gedicht treffen wir auf eine typische Eichendorffsche Nachtsituation, die zunächst unsicher und gefährlich erscheint, schließlich aber in einer Voraussage von „großem Glück“ gipfelt. Diese Klimax vertont Schumann, indem er die Harmonik zu Beginn eher diffus gestaltet und noch keine klare Tonika etabliert – erst am Ende des Liedes wird H-Dur als Grundtonart klar erkennbar. In dieser letzten Strophe wird auch die Quint, die zuvor eine wichtige Rolle gespielt hat, zur Sexte erweitert, die auch im Klaviernachspiel das wichtigste Intervall darstellt.
Gleich zu Beginn von Auf einer Burg wird eine absteigende Quinte vorgestellt, die, symmetrisch zum aufsteigenden „Leitmotiv“ des ersten Teils, das prägende Gestaltungsmerkmal der zweiten Hälfte darstellt. Das Lied schildert die erstarrte Situation rund um einen toten, versteinerten Ritter, was durch statische, träge voranschreitende und altertümlich kirchentonale Harmonien illustriert wird. Auch mit der einzigen italienischen Tempobezeichnung (Adagio) und dem Halbschluss, mit dem das Lied endet, spielt Schumann bewusst auf ältere Musikstile an.
Obwohl das darauffolgende Lied In der Fremde („Ich hör’“) auf den ersten Blick einen völlig anderen Inhalt hat, ist es eng mit Auf einer Burg verbunden. Zum einen, weil es mit seinem a-Moll den vorhergegangenen E-Dur-Akkord auflöst, zum anderen, weil die beiden Lieder einander motivisch sehr ähneln, wie die obenstehende Abbildung zeigt. Die fallende Quint ist hier zudem in den immer wiederkehrenden Sechzehntel-Girlanden des Klaviers zu finden (in der Abbildung blau markiert).
Im Lied Wehmut übernimmt der Dichter selbst das Wort und vergleicht seine Lieder mit jenen der Nachtigall, der alle mit Vergnügen lauschen, ohne ihr inneres Leid herauszuhören. Die Begleitung des Liedes gestaltet Schumann sehr einfach und homophon, um den Text und seine Melodie in den Vordergrund zu stellen und durch zusätzliche musikalische Einfälle davon abzulenken. Möglicherweise will er damit zeigen, dass es ihm hier um mehr geht, als nur um die romantische Darstellung eines lyrischen Ichs, dass er vielmehr wirklich etwas über sich selbst erzählen will. Erst in der letzten Strophe wird die Klavierstimme etwas selbständiger und drückt im Nachspiel durch einen chromatischen Abgang im Bass das „tiefe Leid“ aus.
Eine der seltsamsten und dunkelsten Schöpfungen Eichendorffs ist das Gedicht Zwielicht (wieder aus Ahnung und Gegenwart), in dem vor Gefahr aus jeder Richtung, selbst von dem besten Freund, gewarnt wird. Auch hier wird das Material des Vorspiels aus der absteigenden, diesmal verminderten, Quint gewonnen: Im ersten Takt wird das Intervall g-cis verwendet, im zweiten f-h. Dieses Vorspiel bildet das harmonische Grundgerüst, auf dem alle Strophen basieren.
Robert Schuman komponiert außerdem in Zwielicht verschiedenste musikalische Figuren ein, um die zwiespältige Atmosphäre akustisch zu untermalen.
So finden sich in der Vokalstimme die Figuren: Kyklosis, Seufzer und Exclamatio wieder. Weiterhin wird an mehreren Stellen vom Rezitativ gebrauch gemacht. In der Begleitung durch die Instrumente kommen folgende musikalischen Figuren vor: Tritonus, Dubitatio, Extensio und Anabasis.
Auch die Rhythmik wird teilweise verschleiert, um den Aspekt der drohenden Gefahr akustisch zu untermalen.
Das Lied Im Walde beginnt als fröhliches Jagd-Scherzo im ziemlich lebendigen 6/8-Takt, wo von einer lustigen Hochzeit die Rede ist, verändert seine Stimmung aber zum Negativen, als der Erzähler alleine zurückbleibt. Die galoppierende Bewegung im Wechsel von Achtel- und Viertelwerten setzt aus. Das Gedicht endet mit der Zeile „Und mich schauderts im Herzensgrunde“, bei der Schumann die Singstimme bis zum kleinen a hinunterführt, dem tiefsten Ton des Zyklus. Kurz vor dem triumphalen Ende findet also ein letzter emotionaler Tiefpunkt statt.
Dieses ekstatische Ende wird im letzten Lied Frühlingsnacht erreicht, in dem die Musik von Anfang an eine freudig erregte Stimmung erzeugt: Die Harmonien bilden eine mitreißende Kette aus Septakkorden, hoch liegende Akkorde pulsieren in Sechzehntel-Triolen, die immer nur am Ende jeder der drei Strophen zum Stillstand kommen. Während der zweiten Strophe wird ein neues sich aufwärts schwingendes Motiv eingeführt, das am Schluss des Liedes, bei den Worten „Sie ist deine, sie ist dein!“ zur Septime erweitert wird (Notenbeispiel links). Wo also am Ende des ersten Teils die aufsteigende Quinte zur Sext vergrößert wurde, kulminiert der Zyklus nun in der noch weiter ausholenden Septim. Im Klaviernachspiel aber dominiert wieder die Quinte, was den motivischen Kreis des Gesamtwerks wieder schließt.
Für viele Musikfreunde legendär ist die Interpretation des Liederkreises durch Dietrich Fischer-Dieskau und Gerald Moore, wie sie zum Beispiel 1959 bei den Salzburger Festspielen live mitgeschnitten wurde (Orfeo C 140301 B). Ebenso eindrucksvoll ist die Aufnahme Fischer-Dieskaus mit Christoph Eschenbach (mit Dichterliebe und Myrthen) aus dem Jahr 1975. Die DDR-Plattenfirma Eterna brachte 1975 eine 1972 entstandene Aufnahme des Zyklus mit Peter Schreier und Norman Shetler heraus (Eterna 8 26 498). Hervorragende Kritiken erhielt auch die Einspielung von Thomas Quasthoff (RCA/Sony BMG, 1993). Interpretationen mit Frauenstimme gibt es von Sena Jurinac begleitet von Franz Holletschek (1952, Westminster), Dame Janet Baker begleitet von Daniel Barenboim (EMI 1968/1975), Jessye Norman (Philips/Universal, 1987), Dame Margaret Price begleitet von Graham Johnson (1991, Hyperion), Soile Isokoski begleitet von Marita Viitasalo (1993, Finlandia Records 1995) und von Anne Schwanewilms begleitet von Manuel Lange (Capriccio, 2013).
Theodor W. Adorno: Schumanns Lieder-Kreis nach Eichendorff-Gedichten op. 39. In: Akzente. Zeitschrift für Dichtung. München 1958.
Eckart Busse: Die Eichendorff-Rezeption im Kunstlied. Versuch einer Typologie anhand von Kompositionen Schumanns, Wolfs und Pfitzners. Eichendorff-Gesellschaft, Würzburg 1975
Reinhold Brinkmann: Schumann und Eichendorff: Studien zum Liederkreis opus 39. Ed. Text+Kritik, München 1997 ISBN 3883775223
Christiane Tewinkel: Vom Rauschen singen. Robert Schumanns ‚Liederkreis‘ op. 39 nach Gedichten von Joseph von Eichendorff (= Epistemata 482), Würzburg 2003 ISBN 3-8260-2652-7
Liederkreis op. 39 (Schumann): Noten und Audiodateien im International Music Score Library Project.
Liederkreis. Haslinger, Wien 1842. Digitalisierte Ausgabe der Universitäts- und Landesbibliothek Düsseldorf

Die Liesenstraße liegt an der Grenze zwischen den Berliner Ortsteilen Mitte und Gesundbrunnen im Bezirk Mitte. An ihrer südöstlichen Seite verlief die Berliner Mauer. Gesundbrunnen gehörte zu dieser Zeit zum West-Berliner Bezirk Wedding, der 2001 in den vormalig nur in Ost-Berlin liegenden Bezirk Mitte einbezogen wurde.
An der rund 500 Meter langen Liesenstraße befindet sich so gut wie keine Wohnbebauung. Geprägt ist sie stattdessen durch die sie kreuzenden, denkmalgeschützten Liesenbrücken und vier der bekanntesten Berliner Friedhöfe. Auf den Grundstücken südöstlich der Straße sind außerdem Reste der Grenzanlagen an der Berliner Mauer erhalten geblieben.
Die Liesenstraße verbindet die Chausseestraße mit der Gartenstraße und führt nach der Kreuzung mit dieser als Scheringstraße weiter. Sie führt dabei südlich vom Humboldthain über das ehemalige Grundstück des Berliner Gastwirts Carl Adolf Friedrich Liesen und wurde 1826 angelegt. 1833 wurde sie nach dem ehemaligen Besitzer benannt. Die Freiflächen an der Liesenstraße boten sich den Berliner Kirchengemeinden als Alternative zu innerstädtischen Begräbnisstätten an, die inzwischen gefüllt waren (siehe: Berliner Bestattungswesen).
Ab 1867 betrieb Louis Schwartzkopff nördlich der Liesenstraße den Erweiterungsbau seiner Eisengießerei und Maschinenfabrik Schwartzkopf. Deren Hauptsitz lag in der Chausseestraße; aus ihr ging die Berliner Maschinenbau AG hervor.
Die vier Friedhöfe, der zunehmende Zugverkehr von der benachbarten Stettiner Bahn und das Umfeld an metallverarbeitenden Betrieben, die der Gegend den Namen Feuerland einbrachte, machten verbleibende Grundstücke entlang der Liesenstraße für eine Wohnbebauung unattraktiv.
Nach dem Mauerbau am 13. August 1961 konnte die Liesenstraße nur noch vom West-Berliner Bezirk Wedding aus betreten werden. Auf den drei südlich der Straße gelegenen Friedhöfen und den bis zur Chausseestraße anschließenden Grundstücken wurde ein Grenzstreifen angelegt und in den folgenden Jahrzehnten immer weiter ausgebaut. Auf dem ehemaligen Friedhofsgelände war dieser Grenzstreifen beim Fall der Mauer 1989 rund 40 Meter, nahe der Chausseestraße wegen des dort gelegenen Grenzübergangs sogar bis zu 120 Meter tief. Inzwischen ist die Liesenstraße als Teil des zwischen 2002 und 2006 unter Verantwortung der Senatsverwaltung angelegten Berliner Mauerwegs ausgeschildert.
Von den Begräbnisstätten an der Liesenstraße war ab 1961 nur noch der nördlich gelegene Dorotheenstädtische Friedhof frei zugänglich. Dieser war jedoch durch die Mauer von seiner Gemeinde getrennt und wurde von Kreuzberger Gemeinden verwaltet. Die früheren Eingänge zu den anderen Friedhöfen waren durch die Grenzanlagen der DDR geschlossen. Die im Grenzstreifen liegenden Gräber wurden vollständig abgeräumt. Auf dem Gelände wurde – zum Teil mit abgebauten Grabsteinen – ein Kolonnenweg angelegt, der für die Fahrzeuge der Grenzpatrouillen genutzt wurde. Der Kolonnenweg unterquerte in einem eigens angelegten Tunnel die angrenzende S-Bahn-Trasse und setzte sich auf dem Grenzstreifen auf dem Gelände des Nordbahnhofs fort. Damit die hier die Sektorengrenze überquerende S-Bahn besser kontrolliert werden konnte, stand an der Tunneleinfahrt ein Wachturm.
Der Zugang zu den Friedhöfen südlich der Liesenstraße war nur noch über einen kleinen, gemeinsamen Eingang in der Wöhlertstraße möglich und auch nur direkten Angehörigen der hier beerdigten Personen unter strengen Auflagen gestattet. Es gab sogar Pläne, die Friedhöfe vollständig zu beseitigen, diese wurden aber nicht realisiert. Trotzdem wurden die Begräbnisstätten durch die Abräumung im Grenzteil, durch Zerstörungen im Grenzbetrieb und nicht zuletzt durch Vandalismus und Souvenirjäger nach der Öffnung der Berliner Mauer teilweise sehr stark beschädigt.
Der ehemalige Grenzstreifen gehört inzwischen wieder zum Gelände der drei Friedhöfe. Außer der Neuerrichtung der Friedhofsmauern an der Liesenstraße und der Wiederherstellung der Hauptwege im entleerten Gelände hat man allerdings auf rekonstruierende Maßnahmen weitgehend verzichtet. Die Abmessungen des Grenzstreifens und die in der Mauerzeit entstandenen Zerstörungen sind dadurch vor Ort noch erfassbar.
In der Grünanlage an der nördlichen Ecke von Liesenstraße und Chausseestraße erinnert eine 2,40 Meter hohe Skulptur aus Muschelkalkstein an die Zeit der Teilung. Das von Hildegard Leest 1962 entworfene Kunstwerk trägt den Titel Wiedervereinigung. Es zeigt zwei stilisierte Menschen, die sich über eine Kluft hinweg die Hände reichen. Der Standort wurde so gewählt, dass zur Zeit der Errichtung in südwestlicher Blickachse der Händedruck über den Grenzübergang Chausseestraße hinweg zu erfolgen schien.
Entlang der gesamten Liesenstraße, besonders auf dem Friedhof der St.-Hedwigs-Gemeinde, sind Reste der Grenzanlagen erhalten geblieben. Einige davon stehen heute unter Denkmalschutz.
Ein 15 Meter langer, denkmalgeschützter Abschnitt der „Grenzmauer 75“ in Originalhöhe mit oberem Betonrohr befindet sich in der nördlichen Spitze des Friedhofs der St.-Hedwigs-Gemeinde, direkt an die Liesenbrücken anschließend. Es handelt sich um den kürzesten der drei noch erhaltenen Abschnitte der eigentlichen Berliner Grenzmauer („Vorderes Sperrelement“). Die anderen finden sich in der Bernauer Straße und in der Niederkirchnerstraße. Der Mauerabschnitt an der Liesenstraße sitzt etwas hinter der alten Friedhofsmauer auf. Zur Straßenseite hin ist er stark von „Mauerspechten“ bearbeitet worden.
Im westlichen Teil des Friedhofs der St.-Hedwigs-Gemeinde steht ein kurzer Abschnitt der Hinterlandmauer des Grenzstreifens an der Liesenstraße; auch dieser steht unter Denkmalschutz.
Eine ebenfalls denkmalgeschützte Plattenwand begrenzt den Friedhof der St.-Hedwigs-Gemeinde im Osten. Sie ist etwa 200 Meter lang und besteht aus zwischen Stahlträgern aufgehängten Betonplatten. Sie verlief als „Vorfeldsicherung“ parallel zu einem (nicht erhaltenen) Abschnitt der Hinterlandmauer des Grenzstreifens auf dem Gelände des Nordbahnhofs. Diese auch an einigen anderen Grenzabschnitten zu findende doppelte Staffelung der Sicherungsmauern auf Ost-Berliner Seite wurde gewählt, weil dazwischen die Trasse der nur an West-Berliner Bahnhöfen haltenden S-Bahn verlief. Auf der Friedhofsseite der Plattenwand findet sich noch in den frischen Beton eingeritzte Graffiti, darunter eine Reihe von Daten aus den Monaten Oktober bis Dezember 1974. Auch die eingeritzte Zeichnung eines Grenzwachturms des Typs BT 11 („dritte Generation“) ist zu erkennen. Dies zeigt, dass die Betonteile vor Ort erstellt wurden.In der südwestlichen Ecke des Domfriedhofs I befindet sich der (nicht denkmalgeschützte) Rest einer ähnlichen Plattenwand, die den Friedhof vom angrenzenden Gebiet jenseits der Hinterlandmauer abtrennen sollte. Entlang des Gräberfeldes ist nur die Pfostenreihe dieser „Vorfeldsicherung“ erhalten geblieben. Sie entspricht dem Verlauf einer älteren Version der Hinterlandmauer; deren Fundamentreste sind im Brachland südlich der Friedhofsmauer noch zu entdecken.
Der den Grenzstreifen einst durchlaufende Kolonnenweg ist im Friedhofsbereich nicht mehr zu erkennen. Der Tunnel, mit dem der Kolonnenweg die S-Bahn-Trasse unterquerte, ist zugemauert worden. Lediglich im Gelände, das sich westlich des Friedhofsgeländes bis zur Chausseestraße erstreckt und ganz zum Grenzgebiet gehörte, findet sich noch ein Teilstück des Kolonnenwegs vom Grenzabschnitt Liesenstraße. Der Kolonnenweg biegt hier in südlicher Richtung ab, sodass die Zufahrt hinter dem Grenzübergang Chausseestraße erfolgen konnte.
Alle Überreste der Grenzanlagen im Geländeeck an Liesenstraße und Chausseestraße sind nicht denkmalgeschützt, sodass sie wahrscheinlich im Zuge der sich abzeichnenden baulichen Erschließung des Areals verschwinden werden. Für den Neubau einer Tankstelle auf dem Gelände wurden im Frühjahr 2008 bereits Mauerreste aus verschiedener Epochen entfernt, die sich in einem Buschwerk direkt am Gehweg an der Liesenstraße befunden hatten. Es handelte sich um vermauerte Hohlblocksteine der ursprünglichen Grenzmauer von 1961 („erste Generation“). Diese war später mit Beton vergossen worden und hatte schließlich als Fundament der „Grenzmauer 75“ („vierte Generation“) gedient, deren Betonbett an dieser Stelle noch in Umrissen zu erkennen war. Inzwischen mussten diese Mauerspuren einem Bauprojekt an der Ecke Liesen-/Chausseestraße weichen.
Die Friedhöfe an der Liesenstraße entstanden in den 1830er und 1840er Jahren, zu einem Zeitpunkt, als das Gelände am nördlichen Stadtrand Berlins lag. Als ältester Friedhof wurde ab 1830 der evangelische Domfriedhof I der Oberpfarr- und Domkirche genutzt. 1834 folgte der katholische alte Domfriedhof der St.-Hedwigs-Gemeinde und ein Jahr später wurde der Friedhof der Französisch-reformierten Gemeinde eingeweiht. Diese drei Friedhöfe liegen nebeneinander an der Südseite der Liesenstraße im Bezirk Mitte. 1842 folgte der Bau des Dorotheenstädtischen Friedhofs auf der Nordseite der Straße.
Der Domfriedhof I wurde 1830 auf einer 10.000 m² großen Fläche an der Liesenstraße angelegt. Er sollte den heute nicht mehr vorhandenen Begräbnisplatz in der Elisabethstraße nahe dem Alexanderplatz ablösen, wo auch das ehemalige Domhospital stand. Er ist etwas unter einem Hektar groß und damit der kleinste der Friedhöfe an der Liesenstraße, durch den Mauerbau wurde er weiter verkleinert. Auf Grund des begrenzten Platzes legte die Gemeinde bereits 1870 an der Müllerstraße den Domfriedhof II an.
An den Mauern, die den Friedhof an drei Seiten eingrenzen, befinden sich historische Wandgrabstellen. Die Friedhofskapelle aus dunkelrotem Backstein im neogotischen Stil wurde Mitte der 1990er Jahre saniert und steht heute wieder für Trauerfeiern zur Verfügung.
Der Dornröschenschlaf, den der Friedhof durch den Verlauf der Mauer jahrzehntelang führte, hat der Atmosphäre des Friedhofs mit seiner parkartigen Anlage keinen Abbruch getan. Er atmet Ruhe, Stille und Geborgenheit.
Im Eingangsbereich begrüßt ein 15 Meter hohes strahlend goldenes Kreuz den Besucher. Es ist das alte Kuppelkreuz, das wegen Rostschäden im Dezember 2006 von der Kuppel des Berliner Doms entfernt werden musste.
Zu den bekanntesten Personen, die hier beerdigt sind, gehören der Ratsmaurermeister Johann Christoph Bendler (1789–1873) und der Begründer eines Kurzschriftsystems Wilhelm Stolze (1798–1867). Auch Max Bäckler (1856–1924) gehörte zu den Förderern der Stenografie. Der Stallmeister Kaiser Wilhelms I., Rudolf Rieck (1831–1892), ist gemeinsam mit seiner Frau Valeska (1840–1892) nördlich der Kapelle beigesetzt. Die Grabstätte des Hof- und Domorganisten Bernhard Heinrich Irrgang (1869–1916) kennzeichnet eine – zurzeit noch umgestützte – Stele mit Porträtrelieftondo (deponiert). Das Grab des Oberhof- und Dompredigers Wilhelm Hoffmann ist durch ein hohes Kreuz aus Marmor gekennzeichnet. Der Schriftsteller Gunther Tietz wurde 1993 auf dem Domfriedhof beigesetzt.
Zu den heute nicht mehr auffindbaren Gräbern mit architektonischer und historischer Bedeutung gehören die Grabstätten folgender Personen:
Der knapp über ein Hektar große Friedhof II der Französisch-Reformierten Gemeinde wurde seit 1835 benutzt und löste damit den alten Friedhof der Gemeinde an der Chausseestraße ab. Eine Kapelle befindet sich heute auf dem Gelände nicht mehr, die zuletzt vorhandene wurde ebenso wie das Haus des Friedhofswärters 1961 mit dem Bau der Berliner Grenzanlagen abgerissen. Der Friedhof besitzt eine zentrale Hauptallee, in deren Zentrum ein Ehrenmal an die gefallenen Mitglieder der Gemeinde in den Kriegen von 1864, 1866 und 1870/1871 erinnert, eine Gedenkplatte erinnert zudem an die Toten aus dem Ersten Weltkrieg.
Dieser Friedhof ist unter anderem die letzte Ruhestätte des märkischen Schriftstellers Theodor Fontane (1819–1898) sowie seiner Frau Emilie (1824–1902). Dieses Grab wurde im Zweiten Weltkrieg zerstört und später wieder neu angelegt, wobei statt der ehemals vorhandenen schlichten Fußsteine ein Grabstein aus schwarzem Granit aufgestellt wurde. Nach 1990 wurde die Grabstätte zweimal neu gestaltet, zuletzt 2012 nach historischen Fotografien wieder mit zwei kleinen rundbogigen Granitstelen und einer Eisenpfosten-Ketten-Einfassung. Wie alle anderen Grabstätten der Friedhöfe konnte auch das Grab Fontanes bis 1989 nur mit Passierschein besichtigt werden.
Außerdem liegt hier der Erfinder eines Stenografiesystems Leopold Alexander Friedrich Arends (1817–1882), auf dessen Grab eine hohe Granitstele mit Bildnisbüste von Alexander Calandrelli steht. Die Büste wurde nach der Maueröffnung gestohlen, konnte jedoch kurze Zeit später auf einem Trödelmarkt sichergestellt werden und wurde, nachdem sie der Französischen Gemeinde zurückgegeben wurde, für einige Zeit im Berliner Hugenottenmuseum ausgestellt. Der Bildhauer Martin Schauß (1867–1927), der vor allem für Bildnisbüsten bekannt war, liegt in einem Erbbegräbnis bestattet. An der Rückwand liegt außerdem in der Grabstätte der Familie Michelet der Pelzwarenhändler, Kommunalpolitiker und Berliner Ehrenbürger Paul Michelet (1835–1926). Ob in dieser Grabstätte auch der Philosophieprofessor Charles Louis Michelet (1801–1893) bestattet wurde, ist heute nicht mehr nachzuvollziehen. Auch die Gräber des Journalisten John Peet (1915–1988), des Fotografen Will McBride (1931–2015), des Grafikers und Plakatkünstlers Herrmann Abeking (1882–1939), des Autors Heinz Bergschicker (1930–1989) sowie des Dramatikers und Schriftstellers Peter Hacks (1928–2003) befinden sich auf diesem Friedhof. Das unten als „Modernes Grab“ in einer Abbildung gezeigte Grabmal aus Stahl mit blauem Aufsatz wurde für den Berliner Bildhauer Manfred Hodapp (1951–1999), Mitglied der Bildhauergruppe „Die Glyptiker“, errichtet. Des Weiteren befindet sich dort das Grab des US-amerikanischen Fotografen und Künstlers Will McBride (1931–2015).
Wie bei den anderen Friedhöfen an der Liesenstraße gingen durch den Bau der Grenzanlagen und teilweise bereits vorher eine Reihe von architektonisch und historisch bedeutsamen Grabstätten verloren. Darunter befanden sich die Gräber von
Der alte Domfriedhof der St.-Hedwigs-Gemeinde wurde 1834 geweiht und löste den ersten katholischen Friedhof am Oranienburger Tor ab, der heute nicht mehr vorhanden und mit Mietshäusern überbaut ist. Damit ist dieser Friedhof heute der älteste noch bestehende katholische Friedhof Berlins. Er ist etwa über zwei Hektar groß. 1833 wurde das gesamte Gelände umzäunt und ein Totengräberhaus sowie ein Schuppen erbaut. 1849 wurden hier 429 Opfer der Choleraepidemie begraben, 1866 nochmals 1111 Opfer derselben Krankheit.
Die Kapelle des Friedhofs wurde 1866/1867 nach dem Vorbild italienischer Renaissancebauten mit Terrakottaformsteinen und einem Kupferdach errichtet. Diese Kapelle wurde 1987 originalgetreu wieder aufgebaut, nachdem sie wegen Baufälligkeit mehrere Jahrzehnte lang nicht mehr benutzbar war. Auf der östlichen Seite der Kapelle befindet sich die Grabstätte der Barmherzigen Schwestern vom Heiligen Karl Borromäus, auf der westlichen die der Schwestern des St.-Hedwigs-Krankenhauses, die beide mit einfachen Marmortafeln bedeckt sind. Ohne Namen befindet sich hier außerdem die Grabstätte der Schwestern von der Heiligen Elisabeth.
Am Eingang des Friedhofs von der Liesenstraße befinden sich zwei kniende Engel aus Marmor, die von Josef Limburg (1874–1955) geschaffen wurden und gemeinsam mit der Friedhofsgrenze um etwa 40 Meter von der Liesenstraße entfernt wurden. Durch die Einebnung des Mauerstreifens 1961 sowie den Bau der zweiten Mauer 1967 gingen eine Reihe von architektonisch und historisch bedeutsamen Grabstätten verloren, an die heute ein Gedenkstein auf der freien Rasenfläche sowie ein stehengebliebener Mauerrest vor dem Friedhof erinnern.
Eine Reihe von bedeutenden Berlinern wurden auf dem Friedhof beerdigt, deren Grabmäler heute nicht mehr vorhanden sind. Die folgenden Grabmäler sind teilweise verloren gegangen oder wurden verändert:
Carl Joseph Begas (1794–1854), Maler (das Grab wurde mit einer neuen Stele bestückt, weil das Grabmal aus Granit mit Marmorbildnis, geschaffen von Reinhold Begas, zerstört wurde)
Franz von Forckenbeck (1796–1840), Vizepräsident des Berliner Oberlandesgerichts (Eisenkreuz verschwunden)
Ernst Formes (1841–1898), Sänger (von Johannes Boese geschaffenes, bronzenes Porträttondo verschwunden)
Ceccardo Gilli (1798–1862), Bildhauer (Gedenkstein mit Medaillon, von seinem Sohn Alexander Gilli stammend, aufgrund des Mauerbaus versetzt)
Ernst Eberhard von Ihne (1848–1917), Architekt (1956 aus der St.-Hedwigs-Kathedrale hierher umgebettet, die Grabstätte später eingeebnet, da sie im Grenzstreifen lag)
Wilhelmine von Lichtenau (1752–1820), bekannt als „die schöne Wilhelmine“, Mätresse von König Friedrich Wilhelm II. (der Sarg wurde aus der St.-Hedwigs-Kathedrale hierher umgebettet, die kleine Grabplatte im ehemaligen Todesstreifen wurde erneuert)
Henriette (Maria) Mendelssohn (1775–1831), Erzieherin, Tochter des Philosophen Moses Mendelssohn (ihr Grab wurde 1955 „wegen Ablaufs der Liegefrist“ eingeebnet)
Athanasius von Raczynski (1788–1874), Adeliger und Kunstsammler (erneuerte Grabplatte im ehemaligen Todesstreifen)
Marianne Schadow (1758–1815), Ehefrau des Bildhauers Johann Gottfried Schadow (das von ihrem Ehemann geschaffene Grabmal wurde 1925 ins Märkische Museum überführt)
Joseph Sucher (1843–1908), Komponist, und Rosa Sucher (1849–1927), Opernsängerin (Grabmal verschwunden)
Karl Weierstraß (1815–1897), Mathematiker (Grabstein aufgrund des Mauerbaus versetzt; Grab war von 1994 bis 2014 Ehrengrab des Landes Berlin)
Wilhelm Weskamm (1891–1956), Bischof von Berlin (1968 umgebettet in die Unterkirche der St.-Hedwigs-Kathedrale, Grabplatte jedoch noch vorhanden)
Anton Eduard Wollheim da Fonseca (1810–1884), Schriftsteller, ÜbersetzerNeben diesen Verlusten gibt es auf dem heute nur noch etwa 1,4 Hektar großen Gelände eine Reihe weiterer Gräber historisch mehr oder weniger bedeutsamer Personen, darunter:
Bruno Binnebesel (1902–1944), Theologe, Opfer des Nationalsozialismus (Urne 1947 von Brandenburg an der Havel hierhin überführt)
Herrmann Cohen, Komponist und Pianist, Kleriker (1943 hierher umgebettet aus der zerstörten St.-Hedwigs-Kathedrale; später nach Frankreich umgebettet, Grabstätte mit nach 2001 neu geschaffenem Gedenkstein)
Enrique Gil y Carrasco (1815–1846), Dichter und Botschaftssekretär (die sterblichen Überreste wurden nach Spanien überführt)
Josef Limburg (1874–1955), Bildhauer (sein schlichter, vermutlich erneuerter Grabstein wurde neben den Engeln aufgestellt)
Matthias Carl Schilling (1851–1909), Königlicher Hofsteinmetzmeister, Vater von Carl Schilling (Grabstätte 2011 durch Ausbrechen der Bronzegitter stark beschädigt)
Carl Sonnenschein (1876–1929), Theologe (das Holzkreuz mit Bronzekruzifix auf seinem Grab gilt als eines der bedeutendsten Grabmäler des Friedhofs)
Der Dorotheenstädtische Friedhof II wurde 1842 geweiht und sollte den Friedhof der Dorotheenstädtischen und Friedrichswerderschen Gemeinden an der Chausseestraße ablösen. Anders als bei diesem sollten hier jedoch nur Mitglieder der Dorotheenstädtischen Gemeinde beerdigt werden. Durch den Mauerbau wurde der Friedhof von der Gemeinde im Bezirk Mitte getrennt, die Pflege und Weiterführung übernahmen mehrere Kirchengemeinden in Kreuzberg.
1912/1913 wurde das dreiteilige Tor nach Entwurf von Friedrich und Wilhelm Hennings erbaut. Die Kapelle entstand 1950/1951 nach Plänen von Otto Bartning, um einen Ersatz für die Kirche zu schaffen.
Zu den wichtigsten Grabstätten des Friedhofs gehört das unter Denkmalschutz stehende Mausoleum für den Zirkusdirektor Paul Busch (1850–1927) und seine Frau Barbara Sidonie Busch (1849–1898), das 1898 von Herrmann Paulick und Felix Voss erbaut wurde. Auch das Grabmal des Unternehmensgründers Rudolph Hertzog (1815–1894) steht unter Denkmalschutz. Außerdem finden sich auf dem Gelände die Ehrengräber für den Physiker August Kundt (1839–1894), Otto Nicolai (1810–1849), Julius Carl Raschdorff (1823–1914), Ernst Jacob Renz, Albert Schumann (1858–1939) und Eduard Fürstenberg (1827–1885).
Die heute als Liesenbrücken bekannten Eisenbahnbrücken überqueren die Liesenstraße bei der Kreuzung mit der Gartenstraße. Der gesamte Komplex steht unter Denkmalschutz.Erbaut wurden die Brücken 1890–1896 von den Ingenieuren B. Hildebrandt und K. Bathmann, um die Trasse der bereits seit 1843 existierenden Stettiner Bahn, die bis dahin die Straße niveaugleich auf einem Bahnübergang kreuzte, höher zu legen und damit eine störungsfreie Kreuzung von Bahn- und Straßenverkehr zu ermöglichen.Für den Bau der Brücken wurden die Gleise auf Dammaufschüttungen verlegt. Die eigentlichen Brücken sind eiserne Fachwerkkonstruktionen, die halbparabolische Obergurte besaßen. Die Endstücke bilden Portale. Auf den Brücken wurden die Gleise in einer leichten Kiesschüttung verlegt, und das Gleisbett wurde mit Platten abgedeckt, die nicht mehr vorhanden sind.
Die westliche Brücke wurde in den Jahren 1956/1957 erneuert. Die Widerlager wurden für diesen Zweck vollständig abgetragen und neu aufgebaut.
Heute sind nur noch die renovierten westlichen Brücken in Betrieb. Die Initiative „Grünzüge für Berlin“ setzt sich dafür ein, eine Grünverbindung zwischen der Parkanlage am Nordbahnhof und dem Volkspark Humboldthain über die nicht mehr genutzten Liesenbrücken zu realisieren.Koordinaten der Brücke: 52° 32′ 25,2″ N, 13° 22′ 47″ O
Kathrin Chod, Herbert Schwenk, Hainer Weisspflug, Hans J. Mende: Berliner Bezirkslexikon Mitte. 2 Bände. Bd. 1: A bis N. Bd. 2: N bis Z. Edition Luisenstadt, Berlin 2001, ISBN 3-89542-111-1.
Alfred Etzold, Wolfgang Türk: Der Dorotheenstädtische Friedhof. Die Begräbnisstätten an der Berliner Chausseestraße. Aktualisierte Neuauflage. Links, Berlin 2002, ISBN 3-86153-261-1.
Klaus Hammer: Historische Friedhöfe & Grabmäler in Berlin. Stattbuch Verlag, Berlin 1994, ISBN 3-922778-32-1.
Gartendenkmale in Berlin: Friedhöfe, hrsgg. von Jörg Haspel und Klaus-Henning von Krosigk, bearbeitet von Katrin Lesser, Jörg Kuhn, Detlev Pietzsch u. a., Michael Imhof Verlag, Petersberg 2008

Das Lift-off-Verfahren (englisch lift-off technique) ist in der Halbleiter- und Mikrosystemtechnik eine Prozessfolge zur Herstellung einer meist metallischen Mikrostruktur. In einem ersten Prozessschritt werden dabei strukturierte dünne Schichten auf der Oberfläche von Substraten wie beispielsweise Wafern erzeugt. Auf diese strukturierte Opferschicht wird dann das Zielmaterial ganzflächig abgeschieden. Jene Bereiche, in denen sich das Zielmaterial auf der Opferschicht befindet, werden anschließend durch einen weiteren Prozessschritt entfernt, und die verbliebenen Strukturen bilden die gewünschte Mikrostruktur. Die Größe der mit dem Lift-off-Verfahren herstellbaren Strukturen reicht von einigen zehn Nanometern bis zu Zentimetern, wobei die typischen Strukturgrößen im Mikrometerbereich liegen. Eingesetzt wird das Verfahren unter anderem zur Herstellung von Leiterbahnebenen oder Kontaktflächen bei der Fertigung von integrierten Schaltungen (ICs) und Mikrosystemen. Im Gegensatz zu diesem additiven bzw. aufbauenden Verfahren stehen die subtraktiven Verfahren, bei denen zuerst ganzflächig eine homogene Schicht des Zielmaterials auf dem Substrat abgeschieden wird und die spätere Struktur durch Ätzen dieser Schicht entsteht.
Das Lift-off-Verfahren ist eine relativ einfache und effiziente Folge aus verschiedenen Grundverfahren der Halbleitertechnik. Eine typische Prozessfolge besteht zum Beispiel aus der fotolithografischen Strukturierung, dem Schichtabscheiden und dem Entfernen der Fotolackschicht. Im Laufe der Zeit haben sich jedoch verschiedene Varianten entwickelt, wobei deren Möglichkeiten sehr von den verwendeten Prozessbedingungen bzw. -einstellungen abhängen. Im Folgenden werden daher nur die grundlegenden Prozessschritte beschrieben.
Der Lift-off-Prozess beginnt mit dem ganzflächigen Abscheiden der späteren Opferschicht (häufig Fotolack) auf einem vorbehandelten Substrat. Die Vorbehandlung umfasst in der Regel das Reinigen des Substrates und falls notwendig eine Planarisierung der Oberfläche (beispielsweise durch chemisch-mechanisches Polieren oder den Auftrag einer Haftvermittlerschicht). Anschließend erfolgt die fotolithografische Strukturierung der Opferschicht mit einem inversen Muster der späteren Struktur. Die Parameter der Opferschichtstrukturierung sollten dabei so eingestellt werden, dass sich hochgradig vertikale Seitenwände oder Seitenwände mit leichtem Unterschnitt (negativer Seitenwandwinkel) ergeben.
Nach der Strukturierung der Opferschicht folgt das ganzflächige Abscheiden des Zielmaterials, beispielsweise Aluminium, durch thermisches Verdampfen. Dabei sollte es zu keiner Verbindung zwischen dem abgeschiedenen Zielmaterial auf dem Substrat und dem Zielmaterial auf der Opferschicht kommen, damit zum einen diese Verbindung später nicht nachträglich aufgetrennt werden muss, zum anderen die Seitenflächen der Opferschicht weiterhin unbedeckt sind und somit die Entfernung der Opferschicht im letzten Prozessschritt nicht behindert wird. Eine Verbindung der beiden Bereiche kann durch zwei Rahmenbedingungen vermieden werden:
Die Schichtdicke des Opfermaterials sollte mindestens dreimal so groß sein wie die des Zielmaterials, denn andernfalls wird die Opferschichtstruktur zu sehr gefüllt und blockiert spätere Prozessschritte.
Der Beschichtungsprozess sollte eine schlechte Kanten- bzw. Seitenwandbedeckung aufweisen, so dass an der Seitenwand der Opferschicht möglichst kein Material abgeschieden wird. Hierfür eignen sich  beispielsweise das thermische Verdampfen oder bestimmte Varianten der Sputterdeposition.Beim Abscheiden des Zielmaterials ist weiterhin darauf zu achten, dass die Opferschicht diesen Arbeitsschritt unbeschadet übersteht. Für den Einsatz von Fotolack als Opferschicht bedeutet dies, dass die Prozesstemperatur die Glastemperatur des Fotolacks nicht überschreiten darf. Aus diesem Grund wird die Zielschicht meist bei Raumtemperatur abgeschieden und ist daher häufig amorph oder polykristallin.
Im letzten Prozessschritt wird die Opferschicht nasschemisch entfernt. Dazu kann beispielsweise der Fotolack in einem Lösungsmittel (z. B. Aceton) aufgelöst werden, gegebenenfalls mit Ultraschallunterstützung. Die Opferschicht wird dabei von den Seitenwänden (Flanken) her aufgelöst. Das Zielmaterial auf der Oberseite der Opferschicht wird mit abgehoben (englisch lift off) und weggewaschen. Danach verbleibt das Zielmaterial nur mehr in solchen Regionen, wo es einen direkten Kontakt mit dem Substrat hatte.
Die fertige Struktur kann nach dem Auflösen der Opferschicht drei prozessbedingte typische Fehler aufweisen:
Zurückbleibendes Material: Dabei handelt es sich in der Regel um nicht aufgelöstes Opferschichtmaterial und das darauf befindliche Zielmaterial. Dieses Problem tritt auf, wenn das Opfermaterial von Zielmaterial umschlossen wurde und vom Lösungsmittel daher nur schlecht oder gar nicht angreifbar war.
Wiederabscheidung: Darunter versteht man die Anlagerung von abgelöstem Material, welches sich wieder an der Oberfläche anlagert. Diese Partikel sind nach dem Prozess nur schwer bis überhaupt nicht entfernbar, vor allem nach dem Trocknen des Wafers.
Grate: Dabei handelt es sich um Zielmaterial, das an den Seitenwänden der Opferschicht abgeschieden wurde und nach dem Entfernen der Opferschicht stehen bleibt. Solche Grate haben negative Auswirkungen auf nachfolgende Herstellungsschritte, denn sie erzeugen zum einen eine unerwünschte Topografie auf dem Wafer, die beispielsweise eine gleichmäßige Abscheidung einer weiteren Schicht behindert. Zum anderen können sie auch „umfallen“ und so einen elektrischen Kurzschluss erzeugen.Die drei Fehlerbilder sind mehr oder weniger stark die Folge einer Seitenwandbedeckung der strukturierten Opferschicht durch Zielmaterial. Daraus folgt, dass eine gute Strukturqualität entscheidend vom Profil der Opferschicht und der „Kantenbedeckung“ des Beschichtungsverfahrens abhängt. Günstig sind hierbei eine Kombination aus unterschnittenen (negativen) Flanken oder Opferschichtsystemen, bei denen die unterste Schicht zurückgeätzt wurde, und einem Beschichtungsverfahren mit schlechter Kantenbedeckung.
Das Lift-off-Prinzip zur Herstellung von Metallstrukturen wurde bereits in den 1940er-Jahren vor den Anfängen der Mikroelektronik beschrieben. Seitdem wurden in der Literatur zahlreiche Varianten entwickelt, die sich im Hinblick auf die abgeschiedene Schicht, die Opferschicht und die eingesetzte Chemie sowie zahlreiche Prozessparameter und die Anwendungsbereiche voneinander unterscheiden.
Wie bereits erwähnt, nutzen einfache Verfahren eine Schicht aus Fotolack oder einem Polymer wie Polymethylmethacrylat (PMMA), wobei diese Opferschicht mittels konventioneller Fotolithografie strukturiert werden kann – erstmals 1969 durch Hatzakis für die Herstellung von Leiterbahnen und Source/Drain-Kontakten aus Aluminium vorgestellt. Es wurden aber auch Prozessvarianten beschrieben, die Elektronenstrahl- oder Imprintlithografie nutzen.
Weiter entwickelte Varianten nutzen mehr oder weniger komplexe Schichtstapel als Opferschicht, beispielsweise Fotolack/Aluminium/Fotolack-, Polyimid/Molybdän- oder Polyimid/Polysulfon/Siliziumdioxid-Stapel etc. Der Einsatz eines zusätzlichen Ätzschritts, mit dem die Seitenkante der untersten Opferschicht zurückgeätzt wird, wirkt hierbei wie ein unterschnittenes Kantenprofil und verhindert die Ausbildung einer geschlossenen Schicht an den Seitenwänden.
Neben Prozessvarianten mit unterschiedlichen Opferschichten bzw. -schichtsystemen wurden auch Verfahren beschrieben, bei denen der Lift-off-Effekt nicht durch chemische Auflösung erfolgt, sondern durch Abheben der beschichteten Opferschicht mithilfe eines Klebebandes (tape-assisted lift off) oder durch eingebrachte Spannungen im Material (vgl. Carbon-Dioxide-Snow-Technik).Im Folgenden werden einige Prozessvarianten beispielhaft beschrieben.
Bei der Verwendung einer Fotolackopferschicht wird häufig ein sogenannter Umkehrschritt (engl. image reversal process) genutzt, mit dessen Hilfe ein unterschnittenes Kantenprofil erzeugt werden kann. Dies ist mit einer einfachen fotolithografischen Strukturierung nur schwer möglich, da in den oberen Bereichen der Lackschicht Licht stets stärker absorbiert wird und sich so nach der Entwicklung ein Profil mit steilen Flanken oder Überschnitt ergibt, vgl. Schritt 2. in der nebenstehenden Abbildung. Umkehrfotolacke (engl. image-reversal resists) bieten die Möglichkeit der Bildumkehr (engl. image reversal) der Maske. Je nach eingesetztem Umkehrfotolack wird zwischen einem direkten (sauer-katalytischen) und einem indirekten (basischen) Umkehrprozess unterschieden, sie ergeben je nach Prozessführung ein negatives oder ein positives Abbild der Maske.
Bei sauer-katalytischen Umkehrfotolacken, beispielsweise einem Diazonaphtoquinon (DNQ)/Novolak-Fotolack in Kombination mit einem beigemischten säureaktivierbaren Polymerisator (z. B. Hexamethoxymethylmelamin, HMMM), entspricht die Prozessführung bis zur Belichtung weitgehend einer normalen fotolithografischen Strukturierung (Lackauftrag, soft bake etc.). Eine Entwicklung des Fotolacks (Positivlack) unmittelbar nach dieser Belichtung würde daher ein positives Abbild der Maskenstruktur ergeben. Durch einen zusätzlichen Umkehrprozess vor der Entwicklung werden die Löslichkeitsverhältnisse jedoch umgekehrt. Das heißt, nach der Belichtung werden unlösliche Bereiche löslich und umgekehrt. Erreicht wird dies im Wesentlichen durch zwei Teilschritte. Zunächst folgt auf die Belichtung ein sogenanntes Umkehrausheizen (engl. image reversal bake). Durch die Temperatureinwirkung werden in den belichteten Bereichen des Fotolacks Quervernetzungsreaktionen bewirkt und nach einer Ruhephase, in der eine ausreichende Rehydrierung sichergestellt wird (eine längere Lagerung an Luft ist dafür ausreichend), folgt der zweite Zusatzschritt. Eine Flutbelichtung des gesamten Wafers bewirkt in den noch unbelichteten Bereichen die Bildung von 3-Indencarbonsäure und macht diese Bereiche löslich gegenüber dem alkalischen Entwickler. Nach der Entwicklung entsteht so ein Negativbild der Maskenstruktur mit einem unterschnittenen Flankenprofil.Bei basischen Umkehrfotolacken wird die Fotolackschicht nach der Belichtung zunächst einem Amin-Dampf oder einer Ammoniak-Lösung ausgesetzt. Dabei diffundiert ein basischer Katalysator in die Schicht, der beim anschließenden Umkehrheizen zur Zersetzung der in den belichteten Bereichen gebildeten 3-Indencarbonsäure führt. Die so entstandenen Inden-Derivate sind sehr wirksame Löslichkeitshemmer. Vor der Entwicklung des Lacks erfolgt wie beim direkten Prozess eine Flutbelichtung, bei der die zunächst unbelichteten Bereiche belichtet und somit löslich werden.Bei beiden Prozessvarianten kann bei der ersten Belichtung durch die Variation der Belichtungszeit der Flankenwinkel des Fotolackprofils beeinflusst werden. Wie bereits erwähnt, geschieht dies aufgrund des tiefenabhängigen Absorptionsverhaltens des Lichts und des daraus resultierenden Vernetzungsgrades nach dem Umkehrheizen. Eine hohe Belichtungsdosis führt hierbei zu steilen Flanken und eine geringe Belichtungsdosis zu stark unterschnittenen Flanken. Mit einem solchen Profil kann beim Lift-off-Prozess das Risiko von undefinierten Abrisskanten an den Seitenwänden vermieden werden. Darüber hinaus erhält die entstehende Opferschicht eine erhöhte thermische Stabilität bis zu 200 °C.
Eine weitere Möglichkeit, ein unterschnittenes Fotolackprofil herzustellen, ist die Strukturierung der Opferschicht mittels Elektronenstrahllithografie.  Bei dieser Prozessvariante wird das Streuverhalten von Elektronen bzw. deren Energieverteilung in der Fotolackschicht ausgenutzt. Je nach Ausgangsenergie ergibt sich eine mehr oder weniger langgezogene, birnenförmige Energieverteilung. Für die Herstellung eines hinterschnittenen Profils kann der obere Bereich dieser Verteilung genutzt werden. Hier nimmt der Verteilungsquerschnitt mit zunehmender Tiefe ebenfalls zu. Um dieses Profil in einer nur wenige hundert Nanometer dicken Opferschicht zu erreichen, sind jedoch relativ geringe Energien notwendig, was wiederum das Auflösungsvermögen negativ beeinflusst. Dies gilt vor allem bei dichten Strukturen. Für die Fertigung von dichten Strukturen mit Linienbreiten im Bereich unterhalb 100 nm werden daher dennoch hohe Energien eingesetzt, auch wenn es damit schwieriger wird, die notwendige Profilhinterschneidung zu erreichen.
Abhilfe kann die Verwendung eines Doppelschicht-Fotolacksystems schaffen. Hierbei hat die untere Schicht eine wesentlich höhere Empfindlichkeit (z. B. 50-mal höher) als die obere Schicht, und es kann eine Profilhinterschneidung auch bei hoher Strahlungsenergie erreicht werden. Häufig verwendete Materialkombination für ein solches System ist ein Schichtstapel aus einer Deckschicht aus Polymethylmethacrylat (PMMA; mit hoher molarer Masse und geringerer Elektronenempfindlichkeit) und einer darunterliegenden Schicht aus einem seiner Copolymere (z. B. P(MMA-MAA) mit niedriger molarer Masse und höherer Elektronenempfindlichkeit). Vorteilhaft an einer solchen Materialkombination ist, dass beide Schichten mit derselben Lösung entwickelt werden können.
Eine neuere Lift-off-Variante nutzt gefrorene Kohlendioxid-Partikel (engl. carbon dioxide snow) zur Entfernung einer Metallschicht (Zielmaterial) auf der Fotolack-Opferschicht. Festes Kohlendioxid bildet sich bei Temperaturen unterhalb von −60 °C und damit deutlich unterhalb typischer Prozesstemperaturen bei der Beschichtung. Beim Abkühlen der Metall- und der Fotolackschicht kommt es aufgrund unterschiedlicher thermischer Ausdehnungskoeffizienten zu mechanischen Spannungen an der Grenzfläche der beiden Materialien, die zum Bruch bzw. zur Ablösung des Metallfilms führen. Das abgelöste Metall wird anschließend durch einen Kohlendioxid-Strahl mit Geschwindigkeiten von bis zu 40 m/s abgenommen. Üblicherweise wird dabei die Rückseite des Substrats auf bis zu +60 °C erwärmt, was die mechanische Spannung an den Grenzflächen nochmals erhöht und gleichzeitig die mechanischen Spannungen zwischen den gewünschten Metallstrukturen auf dem Substrat reduziert. Nach dem Ablösen der Metallschicht von der Opferschicht kann diese leicht nasschemisch oder durch Plasmaveraschung entfernt werden. Der Vorteil dieser Methode ist, dass die metallischen Partikel sofort abgeführt werden und die Probenoberfläche somit besser gegen eine wiederholte Abscheidung geschützt ist.
Das Lift-off-Verfahren ist eine sehr allgemeine Herstellungsmethode, mit der sich prinzipiell alle Metalle und ihre Legierung sowie Mehrfachschichten strukturieren lassen. Da hierbei, anders als bei einer subtraktiven Strukturierung durch Ätzen, keine auf die zuätzenden Materialien abgestimmten Ätzprozesse bzw. Ätzchemie benötigt werden, kann hierbei immer annähernd der gleiche Lift-off-Prozess genutzt werden. Es muss nur eine ähnliche Abscheidung des Zielmaterials gewährleistet sein.
In Fällen, bei denen kein direktes Ätzen mit genügend hoher Selektivität für die bereits abgeschiedenen Materialien zur Verfügung steht.
Bei Mehrfachschichten, bei denen zunächst die Einzelschichten nacheinander aufgebracht und anschließend gemeinsam strukturiert werden.
Bei Materialien wie Aluminium-Kupfer-Legierungen, die schwer entfernbare Rückstände bilden, wenn sie durch das Trockenätzen strukturiert werden.Des Weiteren bieten die geneigten Seitenwände Vorteile bei der Abscheidung von Schichten in nachfolgenden Prozessebenen, etwa eine höhere Leiterbahnebene bei der Metallisierung des ICs. Denn anders als bei den steilen Seitenwänden, wie sie typischerweise beim Trockenätzen entstehen, können die Seitenflächen des mit Lift-off strukturierten Materials mit einer breiteren Auswahl von Verfahren beschichtet werden. Auch zeigt die Oberfläche deutlich weniger Sprünge in der Topografie, so dass unter anderem mit Rotationsbeschichtung hergestellte Schichten (z. B. Fotolackschichten für nachfolgende Prozesse) homogener aufgetragen werden können. Angewendet wird das Verfahren daher vor allem für die Strukturierung von metallischen Schichten. Es ermöglicht die Herstellung von Leiterbahnen mit Strukturen im Mikrometerbereich für die Fertigung von diskreten Bauelementen und auch von (für heutige Verhältnisse) relativ einfachen integrierten Schaltkreisen mit bis zu vier Leiterbahnebenen.
Trotz der genannten Vorteile hat sich das Lift-off-Verfahren für die Herstellung der Verdrahtungsebenen von integrierten Schaltkreisen nicht gegenüber der Strukturierung mittels Trockenätzen durchsetzen können; aktuell wird in diesem Bereich der Damascene- und Dual-Damascene-Prozess in größerem Umfang eingesetzt. Gründe hierfür sind unter anderem die verhältnismäßig geringe Auflösung sowie die aufwendigere Herstellung der strukturierten Opferschicht, da hier auf die Kompatibilität der chemischen und physikalischen Eigenschaften der Opfer- und Zielschicht beim Abscheidungs- und Auflösungsprozess geachtet werden muss.Analog zur Herstellung von Leiterbahnen kann Lift-off auch zur Herstellung von Bumps eingesetzt werden. Dabei handelt es sich um metallische (meist eine Blei-Zinn-Legierung) Kontaktelemente zur Direktmontage der Chips, beispielsweise für das Tape-Automated Bonding oder die Flip-Chip-Montage. Auch hier gibt es mehrere Prozessvarianten, die auch mit dem Lift-off-Verfahren hergestellte Kupferkontaktflächen inklusive Kupferdiffusionsbarrieren umfassen können. Für weitere Informationen sei hier auf die Literatur verwiesen.
Heutzutage wird Lift-off als ein gängiges Verfahren bei der Fertigung von Bauelementen im Nanometerbereich (z. B. Einzelelektronentransistoren oder Mikro-SQUIDs) eingesetzt. Hierbei wird meistens eine Elektronenstrahllithografie in Kombination mit einem Positivfotolack, in der Regel Polymethylmethacrylat (PMMA), verwendet. Begrenzt wird das Verfahren vor allem durch die begrenzte Auflösung der lithografischen Strukturierung und durch die Korngröße des abgeschiedenen Materials. So kann eine Linienstruktur unterbrochen sein, wenn die Korngröße im Bereich der Linienbreite liegt.
Zheng Cui: Nanofabrication: principles, capabilities and limits. Springer, 2008, ISBN 978-0-387-75576-2, S. 218–225 (eingeschränkte Vorschau in der Google-Buchsuche – Hauptquelle für die Prozessvarianten). 
Friedemann Völklein, Thomas Zetterer: Praxiswissen Mikrosystemtechnik. Grundlagen – Technologien – Anwendungen; mit 55 Tabellen. 2., vollst. überarb. u. erw. Auflage. Vieweg+Teubner, 2006, ISBN 3-528-13891-2. 
Kenneth A. Jackson, Wolfgang Schröter (Hrsg.): Handbook of Semiconductor Technology: Electronic structures and properties of semiconductors. Wiley-VCH, 2000, ISBN 3-527-29834-7, S. 587–590.
Gary E. McGuire: Development of liftoff processes for patterning of magnetic and other materials. Abgerufen am 5. Februar 2012. 
Liftoff Processes. Stanford Nanofabrication Facility, 29. August 2003, archiviert vom Original am 25. Februar 2001; abgerufen am 5. Februar 2012. 
