
Die Ethan-Allen-Klasse war eine Klasse von atomgetriebenen Raketen-U-Booten (SSBN) der United States Navy. Zwischen 1959 und 1962 wurden fünf Boote der Klasse gebaut. Anfang der 1980er-Jahre wurden die Boote auf Grund der Beschränkungen des SALT-I-Vertrages zu Jagd-U-Booten umklassifiziert und schließlich bis 1992 alle außer Dienst gestellt.
Die Planungen zur Ethan-Allen-Klasse begannen Mitte der 1950er-Jahre. Sie wurde als erste Klasse von Raketen-U-Booten der US Navy von Reißbrett weg für die Aufnahme von Interkontinentalraketen geplant und gefertigt, die fünf Boote der Vorgängerklasse George Washington besaßen noch den Rumpf von Jagd-U-Booten der Skipjack-Klasse mit einer zusätzlich eingefügten Raketenabteilung.
Vier der fünf Boote wurden nach Staatsmännern aus der Geschichte der Vereinigten Staaten benannt, dies waren Ethan Allen, ein Freiheitskämpfer aus dem Amerikanischen Unabhängigkeitskrieg (SSBN-608), Sam Houston, Präsident der Republic of Texas (SSBN-609), John Marshall, Chief Justice of the United States (SSBN-611) sowie Thomas Jefferson, dritter Präsident der Vereinigten Staaten (SSBN-618). Aus der Rolle fällt SSBN-610, die nach dem Erfinder Thomas Alva Edison benannt ist. Edison war während des Ersten Weltkrieges Vorsitzender des zivilen Naval Consulting Board, über das Edison zur Gründung des United States Naval Research Laboratory beitrug.
Der Bau der Boote begann 1959 und fand auf den Werften von Electric Boat (Ethan Allen und Edison) und Newport News Shipbuilding (Houston, Marshall und Jefferson) statt. Im Trockendock verbrachten die Boote jeweils etwas über ein Jahr, danach lagen die Ethan-Allens noch jeweils etwas unter einem Jahr an der Ausrüstungspier und befanden sich auf den Werfterprobungsfahrten. Damit lagen zwischen Kiellegung und Indienststellung rund zwei Jahre. Über die Baukosten ist nichts bekannt geworden.
Die fünf Boote der Klasse kamen 1961, 1962 und 1963 in die Flotte, dort ergänzten sie die fünf Boote der Washington-Klasse. Bis 1967 kamen außerdem noch 31 Boote der Lafayette-Klasse (inklusive der Unterklassen James Madison und Benjamin Franklin) hinzu. Diese insgesamt 41 Raketen-U-Boote machten die sogenannten 41 for Freedom aus, die Flotte von Unterseebooten, welche die Abschreckung auf Seiten der USA auf die Weltmeere trug.
Als Anfang der 1980er-Jahre die modernen SSBN der Ohio-Klasse in Dienst gestellt wurden, war in den Regularien der Strategic Arms Limitation Talks kein Raum mehr für die Raketen an Bord der Ethan Allens, weshalb die Raketenabteilung versiegelt und die Startrohre mit Beton unbrauchbar gemacht wurden. Zusätzlich wurde das Raketenkontrollsystem entfernt. Danach wurden die Boote zu Jagd-U-Booten umklassifiziert und mit der Kennung SSN versehen. Als solche wurden die Boote größtenteils für U-Jagd-Übungen eingesetzt. Drei der Boote wurden bis 1985 außer Dienst gestellt, die anderen zwei (Houston und Marshall) blieben bis 1991 beziehungsweise 1992 aktiv, da sie für den Transport von zwei Dry Deck Sheltern (DDS) ausgerüstet worden waren. Dabei wurden nicht nur die Befestigungen für das DDS installiert, es wurde außerdem zusätzlicher Platz für die Ausrüstungsgegenstände der zu transportierenden Spezialeinheiten geschaffen.
Alle fünf Boote wurden in den 1990er-Jahren in der Puget Sound Naval Shipyard abgebrochen. Sie wurden nach den Richtlinien des Ship-Submarine Recycling Program zuerst von allen radioaktiven Teilen befreit und dann zerlegt.
Der Rumpf eines Bootes der Klasse war 125,1 Meter lang und 10,1 Meter breit. Der Tiefgang lag bei 9,1 Metern. Damit verdrängte jedes Boot getaucht rund 7.900 ts Wasser. Im Gegensatz zu ihren Vorgängern wurde hier allerdings die Raketensektion bereits beim Bau eingeplant, was an der Linienführung auch erkennbar ist. Die Startschächte befinden sich, wie bei amerikanischen SSBN üblich, hinter dem Turm. Um die maximale Tauchtiefe auf etwa 400 Meter zu erhöhen, wurde die Klasse aus HY-80-Stahl gefertigt. Das bedeutet, dass der Stahl eine garantierte „Yield strength“ (Streckgrenze) von 80.000 psi – das entspricht ca. 5.516 Bar – aufweist.
Im Inneren waren die Boote ähnlich aufgeteilt wie Jagd-U-Boote. Vor und unter dem Turm befanden sich Torpedo-, Mannschafts- und Kommandoräume, dahinter folgte zuerst der Raketensiloraum – wegen der wie Baumstämme aussehenden Startschächte auch „Sherwood Forest“ genannt –, weiter achtern dann die Reaktorkammer und zuletzt der Maschinenraum.
Der Antrieb der Ethan Allens bestand aus einem Druckwasserreaktor vom Typ S5W. Das S steht dabei für die Verwendung auf einem bestimmten Schiffstyp, hier Submarine, Unterseeboot, die 5 stand für die Generation, das W repräsentierte den Hersteller, bei diesem Typ Westinghouse Electric. Dieser Reaktor lieferte rund 15.000 PS, die auf einer Welle wirkten. Die Höchstgeschwindigkeit lag getaucht bei höchstens 20 Knoten, einige Knoten langsamer als Jagd-U-Boote. Allerdings zählte für die SSBN ein leises Antriebssystem mehr als Geschwindigkeit, da ihr Hauptanliegen heimliches Patrouillefahren war. Die Rumpfform war auf Unterwasserfahrt optimiert, weshalb die Boote nur zum Transit aus und in die Basis auftauchten. Hierbei lag die theoretische Höchstgeschwindigkeit bei rund 15 Knoten.
Die Hauptbewaffnung der Boote der Ethan-Allen-Klasse bestand aus 16 U-Boot-gestützten Interkontinentalraketen (SLBM) von Typ UGM-27B Polaris A2. Diese hatten eine Reichweite von bis zu 1500 nautischen Meilen und dabei eine Zielgenauigkeit (angegeben in Circular Error Probable, kurz CEP) von rund 900 Metern. Bis 1974 wurden alle fünf Boote umgerüstet, um die wesentlich verbesserte UGM-27C Polaris A3 zu tragen. Nicht nur die Reichweite wurde um 1000 Meilen vergrößert, während die CEP um ein Drittel verringert wurde, zusätzlich konnte die A3 in einem Multiple Reentry vehicle nun drei statt nur einen Sprengkopf tragen. Ende der 1970er-Jahre wurde die U-Boot-Flotte auf die neue UGM-73 Poseidon umgerüstet; da diese jedoch über zehn Meter lang war, war eine Verwendung auf den Ethan Allens nicht möglich, ohne teure strukturelle Änderungen am Boot vorzunehmen. Auch daher war die bevorstehende Ausmusterung aus dem Dienst kein großer Verlust für das Abschreckungskonzept der USA.
Zur Selbstverteidigung besaß jedes Boot vier Torpedorohre im Bug. Der Durchmesser von 533 mm erlaubte ab der Einführung der Waffe 1972 den Abschuss des Mark-48-Schwergewichtstorpedos. Es gab Nachladekapazität für vier der großen Mark-48- oder acht der kleineren Mark-46-Leichtgewichtstorpedos.
Die Boote der Ethan-Allen-Klasse besaßen mehrere Sonaranlagen, mit denen sie feindliche U-Boote und Überwasserschiffe erfassen konnten. Das Hauptsystem war das AN/BQS-4, das sowohl aktiv als auch passiv arbeiten konnte und sich zusammen mit dem reinen Passivsonar AN/BQR-7 im Bug befindet. Da die Bugspitze der Boote von den Torpedorohren belegt wurde, erhielt die Sonaranlage zum Auffangen von Geräuschen einen kleinen „Flügel“ auf dem Rumpf. Zusätzlich wurde jedes Boot mit einem aktiven AN/BQR-19 ausgerüstet, das zur Navigation eingesetzt wurde. Als Schleppsonar zum Lauschen auf große Entfernungen fungierte ein AN/BQR-15.
Die Ethan Allens waren ein Teil der Abschreckungspolitik der Vereinigten Staaten auf den Weltmeeren. Mit seinen 16 Interkontinentalraketen verließ ein Boot seinen Heimathafen unter strengen Sicherheitsvorkehrungen. Jagd-U-Boote und Zerstörer stellten sicher, dass die Gewässer um den Hafen frei von feindlichen U-Booten waren, so dass das SSBN sicher auslaufen konnte. Daraufhin blieb das Boot rund drei Monate ununterbrochen getaucht und befuhr ein bestimmtes Patrouillengebiet, dessen Koordinaten höchster Geheimhaltung unterlagen. Nach der Patrouille kehrte das Boot in den Hafen zurück, wechselte die Besatzung – jedes Boot besaß zwei komplette Mannschaften, die in Blue und Gold unterteilt waren –, und brach nach einer Instandsetzungsperiode mit der zweiten Besatzung zur nächsten Patrouille auf.
Als zu dieser Zeit modernste Klasse von Boomern (so der Spitzname der SSBN in der US Navy), wurden die fünf Boote nach ihrer Indienststellung in Europa stationiert, da sie von dort die polnahen Gewässer schnell erreichen konnten. Der Grund für die Auswahl dieses Patrouillengebiet liegt in der Tatsache begründet, dass von dort abgefeuerte Atomraketen die sowjetische Hauptstadt Moskau schnell erreichen konnten, ergo die sowjetische Vorwarnzeit sehr kurz war. Stützpunkt der fünf Boote war zuerst Holy Loch in Schottland, ab Mitte/Ende der 1960er-Jahre dann Rota in Spanien. Ab Mitte der 1970er-Jahre wurden die inzwischen relativ alten Boote dann hauptsächlich im Pazifik eingesetzt, Heimathäfen waren Pearl Harbor auf Hawaii und Apra Harbor auf Guam.
Das Typschiff der Klasse, die Ethan Allen, ist das einzige Unterseeboot in der Geschichte der Vereinigten Staaten, das jemals eine scharfe, mit Atomsprengkopf versehene Interkontinentalrakete gestartet hat. 1962 nahm das Boot an Operation Dominic I teil, im Rahmen der Teilübung Frigate Bird startete vom Boot eine Polaris A2, die in der Region der Insel Kiritimati im Zentralpazifik detonierte.
Nach ihrer Konvertierung konnten Sam Houston und John Marshall mittels des Dry Deck Shelters Spezialeinheiten, wie zum Beispiel United States Navy SEALs transportieren und unbemerkt zu Zielen innerhalb feindlicher Hoheitsgewässer bringen. Dort konnten die Kommandos durch eine Druckschleuse das Boot verlassen und nach Erfüllung eines Einsatzes wieder betreten.

Das Ethanol oder der Ethylalkohol, auch Äthanol oder Äthylalkohol, gemeinsprachlich auch (gewöhnlicher) Alkohol genannt, ist ein aliphatischer, einwertiger Alkohol mit der Summenformel C2H6O.
Die reine Substanz ist eine bei Raumtemperatur farblose, leicht entzündliche Flüssigkeit mit einem brennenden Geschmack und einem charakteristischen, würzigen (süßlichen) Geruch. Die als Lebergift eingestufte Droge wird bei der Herstellung von Genussmitteln und alkoholischen Getränken wie Wein, Bier und Spirituosen aus kohlenhydrathaltigem Material durch eine von Hefen ausgelöste Gärung in großem Maßstab produziert.
Die Vergärung von Zucker zu Ethanol ist eine der ältesten bekannten biochemischen Reaktionen. Seit dem 19. Jahrhundert wird Ethanol für industrielle Zwecke aus Ethen hergestellt. Ethanol hat eine weite Verbreitung als Lösungsmittel für Stoffe, die für medizinische oder kosmetische Zwecke eingesetzt werden, wie Duftstoffe, Aromen, Farbstoffe oder Medikamente sowie als Desinfektionsmittel. Die chemische Industrie verwendet es sowohl als Lösungsmittel als auch als Ausgangsstoff für die Synthese weiterer Produkte wie Carbonsäureethylester.
Ethanol wird energetisch als Biokraftstoff, etwa als sogenanntes Bioethanol verwendet. Beispielsweise enthält der Ethanol-Kraftstoff E85 einen Ethanolanteil von 85 Volumenprozent.
Ethanol (C2H5OH) gehört zu den linearen n-Alkanolen. Ethanol leitet sich von dem Alkan (gesättigten Kohlenwasserstoff) Ethan (C2H6) ab, in dem formal ein Wasserstoffatom durch die funktionelle Hydroxygruppe (-OH) ersetzt wurde. Zur Benennung wird dem Namen Ethan das Suffix -ol angehängt. Die Summenformel für Ethanol nach dem Hill-System ist C2H6O, die häufig verwendete Schreibweise C2H5OH ist keine Summen-, sondern eine Halbstrukturformel.
„Alkohol“ ist das umgangssprachliche Wort für „Ethanol“; die Fachbezeichnung „Alkohole“ hingegen steht für eine Gruppe organisch-chemischer Verbindungen, die neben dem Kohlenwasserstoffgerüst als zusätzliche funktionelle Gruppe mindestens eine Hydroxygruppe besitzen, wobei sich an dem Kohlenstoffatom mit der Hydroxygruppe kein höherwertiger Substituent befindet.
Ethanol entsteht auf natürlichem Weg vor allem bei der Vergärung zuckerhaltiger Früchte. Dem Menschen ist diese natürlich vorkommende Substanz seit langem zur Berauschung bekannt. So finden sich in ägyptischen Schriftrollen der 3. Dynastie sowie auf alt-mesopotamischen Keilschrifttafeln Hinweise auf die Herstellung ethanolhaltiger Getränke.
Biere, später Weine wurden zunächst mit Hilfe von Wildhefen erzeugt. Die Ethanolgehalte solcher Getränke waren geringer als heute, da die Wildhefen ab einer bestimmten Ethanolkonzentration die Umwandlung von Zucker in Ethanol einstellen. Durch jahrhundertelange Züchtung tolerieren heutige Kulturhefen wie Saccharomyces cerevisiae höhere Ethanolgehalte. Ethanol konzentriert zu gewinnen, gelang um 900 dem persischen Arzt, Naturwissenschaftler, Philosophen und Schriftsteller Abu Bakr Mohammad ibn Zakariya ar-Razi durch die Destillation von Wein; auf ein Wort der arabischen Sprache (arabisch الكحل, siehe dazu Kohl) geht die im 18. Jahrhundert nachweisbare Bezeichnung Alkohol für ‚Geist des Weines‘ zurück. Eine solche destillatorische Trennung wurde im Frühmittelalter wohl auch schon in China durchgeführt, war – wahrscheinlich über arabische Vermittlung – gegen 1100 in Salerno bekannt und wurde von Taddeo Alderotti vor 1288 einer breiteren Öffentlichkeit in Europa bekanntgemacht.Im Jahr 1796 erhielt Johann Tobias Lowitz erstmals reines Ethanol durch Filtrierung von destilliertem Alkohol über Aktivkohle. Damals war auch der heute noch verwendete Begriff Weingeist für den reinen Alkohol üblich. Antoine Lavoisier beschrieb Ethanol erstmals als eine Verbindung aus Kohlenstoff, Wasserstoff und Sauerstoff. Im Jahr 1808 bestimmte Nicolas-Théodore de Saussure die chemische Zusammensetzung von Ethanol. Fünfzig Jahre später veröffentlichte Archibald Scott Couper die Strukturformel von Ethanol. Es war eine der ersten Strukturformeln, die bestimmt wurden.
Ethanol wurde zum ersten Mal synthetisch im Jahr 1826 durch Henry Hennel und Georges Simon Serullas hergestellt. Im Jahr 1828 stellte Michael Faraday Ethanol durch säurekatalysierte Hydratisierung von Ethylen her, einen Prozess, welcher der industriellen Ethanolsynthese ähnelt.
Heute wird Ethanol hauptsächlich durch Gärung aus Biomasse gewonnen. Im Kontext der Erzeugung von Biokraftstoff wird es Bioethanol genannt. Agraralkohol ist Ethanol aus Agrarrohstoffen, in Deutschland wird Agraralkohol unter staatlicher Aufsicht in landwirtschaftlichen Brennereien erzeugt.
Ethanol ist ein in reifen Früchten und Säften natürlich vorkommendes Produkt der alkoholischen Gärung. Viele Lebensmittel enthalten natürlicherweise geringe Mengen Ethanol. So enthält alkoholfreies Bier noch bis 0,5 Volumenprozent Ethanol. Laut dem Deutschen Lebensmittelbuch dürfen Fruchtsäfte einen Ethanolgehalt von etwa 0,38 Volumenprozent aufweisen. So enthält Apfelsaft bis 0,016, Traubensaft bis 0,059 Volumenprozent Ethanol. Eine reife Banane kann bis zu 1 Volumenprozent, Brot bis 0,3 Volumenprozent enthalten. Reifer Kefir kann bis 1 Volumenprozent Ethanol enthalten, Sauerkraut bis zu 0,5 Volumenprozent. Der physiologische Ethanolgehalt des menschlichen Bluts beträgt etwa 0,02 bis 0,03 ‰.Ethanol wurde neben anderen organischen Molekülen wie Vinylalkohol in interstellaren Molekülwolken nachgewiesen, wobei deren Bildungsmechanismus ungeklärt ist.
Ethanol wird durch Gärung aus Biomasse, meist aus zucker- oder stärkehaltigen Feldfrüchten oder traditionell aus Produkten des Gartenbaus gewonnen. Dieser Prozess wird mit einer Reihe von Nahrungsmitteln kontrolliert durchgeführt, wodurch zum Beispiel Wein aus Weintrauben oder Bier aus Malz und Hopfen entstehen. Holzzucker kann als Nebenprodukt des Sulfitverfahrens zu Sulfitsprit fermentiert werden. Dieser kann aufgrund zahlreicher Verunreinigungen jedoch nur energetisch genutzt werden.
Vor der eigentlichen Gärung wird meist zuerst Stärke in Disaccharide gespalten, deren glycosidische Bindung durch Hydrolasen gelöst wird; anschließend werden die entstehenden Monosaccharide durch Hefe oder Bakterien vergoren. Bei einer Ethanolkonzentration nahe 15 % beginnen Hefezellen und Bakterien abzusterben, weshalb durch Gärung keine höhere Konzentration erreicht werden kann. Die Bruttogleichung der alkoholischen Gärung ist:
Ethanol kann durch Destillation für technische und Genusszwecke konzentriert werden, da es bereits bei 78 °C verdampft.
Zum Verzehr geeigneter Trinkalkohol wird durch Destillation – das sogenannte Brennen – einer alkoholhaltigen Maische aus landwirtschaftlichen Ausgangsprodukten gewonnen. Je nach Brennverfahren sind im Destillat, dem sogenannten Brand, neben Ethanol noch Aromen, Fuselöle, weitere organische Verbindungen und Wasser enthalten, die den Charakter und den Geschmack des Endproduktes wie zum Beispiel Weinbrand, Whisky oder Rum bestimmen. Für die Herstellung von Wodka wird hingegen fast reines Ethanol verwendet und nur noch mit Wasser verdünnt. Unverdünnt dient reines Ethanol mit der Verkehrsbezeichnung Ethylalkohol landwirtschaftlichen Ursprungs als Ausgangsprodukt für weitere alkoholische Getränke, zum Beispiel für die meisten Liköre. Alkoholische Getränke, die destilliertes Ethanol enthalten, heißen Spirituosen (umgangssprachlich auch Branntwein oder Schnaps) – im Gegensatz zu Wein und Bier, deren Ethanol ausschließlich durch alkoholische Gärung entstanden ist.
Großtechnisch erfolgt die Herstellung reinen Ethanols für technische Anwendungen durch azeotrope Rektifikation (Schleppmittelrektifikation). Die Anlage besteht aus zwei Rektifikationskolonnen. In der Haupttrennsäule erfolgt die Rektifikation des Ethanol-Wasser-Gemisches bis in die Nähe des azeotropen Punktes. Das Sumpfprodukt ist Wasser.Dem Kopfprodukt, das aus 95,6 % Ethanol und 4,4 % Wasser besteht, wird der Hilfsstoff Cyclohexan beigemischt. Früher übliche Schleppmittel wie Benzol im Young-Verfahren oder wie Trichlorethen im Drawinol-Verfahren werden heutzutage nicht mehr verwendet. Dieses Dreistoffgemisch aus Ethanol, Wasser und Schleppmittel gelangt in die Hilfsstoff-Trennsäule. Dort erfolgt eine Auftrennung in den im Sumpf anfallenden reinen Alkohol sowie in ein Cyclohexan-Wasser-Gemisch als Kopfprodukt. Cyclohexan und Wasser sind im flüssigen Zustand nicht mischbar und trennen sich nach der Kondensation in einem Abscheider (Dekanter). Der Hilfsstoff Cyclohexan wird am Einlauf der Hilfsstoff-Trennsäule wieder dem zuströmenden, azeotropen Ethanol-Wasser-Gemisch beigefügt. Er läuft im Kreislauf im oberen Bereich der Hilfsstoff-Trennsäule und wird deshalb als „kopflaufender Hilfsstoff“ bezeichnet. Wasserfreies Ethanol wird im Labormaßstab durch Destillation über wasserentziehenden Chemikalien wie Calciumoxid, wasserfreiem Calciumsulfat oder Molsieben gewonnen. Der Prozess der Herstellung von absolutem Alkohol wird als Absolutierung bezeichnet.
Ethanol wird durch chemische Synthese aus Wasser und Ethen im sogenannten indirekten Prozess homogenkatalytisch unter Zugabe von Schwefelsäure hergestellt. Auf diese Weise hergestellter Alkohol wird auch als Industriealkohol bezeichnet.
Der Prozess läuft zweistufig ab unter Bildung von Schwefelsäureestern, die in einem zweiten Schritt hydrolysiert werden müssen. Die Schwefelsäure muss nach erfolgter Hydrolyse wieder aufkonzentriert werden. Im direkten Prozess dient auf Silika aufgebrachte Phosphorsäure als heterogener Katalysator. Bei Temperaturen bis zu 300 °C und Drücken von 70 bar wird Ethanol direkt aus Ethen und Wasser in der Gasphase hergestellt. Der Umsatz beträgt pro Reaktordurchgang allerdings nur 5 % bezogen auf Ethen. Wegen der Abwasserproblematik und Korrosionsproblemen durch die anfallende Schwefelsäure beim indirekten Prozess wird Ethanol industriell heutzutage mittels Phosphorsäurekatalyse hergestellt. Die Bruttogleichung für beide Prozesse lautet:
Prinzipiell ist die Gewinnung von Ethanol durch katalytische Hydrierung von Acetaldehyd möglich. Bei hohen Wasserstoffdrücken wird Acetaldehyd dabei an nickelhaltigen Kontakten umgesetzt:
Ethanol fällt im Synol-Verfahren durch Reaktion von Kohlenstoffmonoxid mit Wasserstoff an und kann destillativ von den anderen entstehenden Alkoholen getrennt werden. Durch Kernspinresonanzspektroskopie lässt sich anhand der Wasserstoff- und Kohlenstoffisotopenverhältnisse synthetisches Ethanol aus fossilen Rohstoffen von Ethanol aus nachwachsenden Rohstoffen unterscheiden. Dieser Umstand lässt sich zum Nachweis des Panschens von Wein oder Spirituosen mit industriellem Ethanol nutzen. Bei durch Gärprozesse hergestelltem Ethanol lässt sich über die Deuteriumverteilung die pflanzliche Herkunft feststellen.
Weltweit erzeugten die USA und Brasilien 2005 zusammen über 90 % der Jahresproduktion von 29 Millionen Tonnen. Die größten europäischen Erzeuger sind Russland und Frankreich. Deutschland erzeugt jährlich fast 4 Mio. hl zu gleichen Teilen als Getränkealkohol und als Alkohol für chemisch-technische Zwecke, was einer Eigenbedarfsdeckung von etwa 62 % entspricht.
Neben der Produktion von Neutralalkohol für Getränke, Lebensmittel und technische Zwecke entfallen weltweit etwa 65 % auf die Herstellung von Kraftstoffethanol. In den USA wird der Aufbau neuer Produktionsanlagen für Ethanol besonders forciert, vor allem durch das Gesetz „Energy Policy Act“ (EPACT) von 2005, das den Ausbau von erneuerbaren flüssigen Energieträgern fördern soll.
Ethanol unterliegt in Deutschland der Alkoholbesteuerung (bis 2018 Branntweinsteuer). Sie wird von der Zollverwaltung beim Inverkehrbringer (Spirituosenhersteller, berechtigter Empfänger, Branntweinlagerinhaber) zum Zeitpunkt des Lagerabganges erhoben. Ein Versand unter Steueraussetzung ist per BVD oder EVD möglich – beispielsweise zwischen Hersteller und Großhändler mit offenem Branntweinlager sowie bei Exportgeschäften.
Für technische Zwecke, etwa in Druckereien, bei der Lackherstellung, Reinigungsmittelproduktion, für Kosmetik und ähnliche Einsatzgebiete und als Brennspiritus ist die Verwendung von Ethanol steuerfrei möglich. Um zu verhindern, dass dieses Ethanol ohne Entrichtung der Steuer als Genussmittel getrunken oder solchen beigefügt wird, wird unversteuerter Alkohol unter Zollaufsicht vergällt. Vergällung bedeutet, dass Ethanol mit anderen Chemikalien, wie beispielsweise Methylethylketon (MEK) und zwei weiteren branntweinsteuerrechtlich vorgeschriebenen Markierungskomponenten, Petrolether, Cyclohexan, Diethylphthalat, Bitrex oder Ähnlichem versetzt wird, um es für den menschlichen Genuss unbrauchbar zu machen. Dies wird in Deutschland über die Branntweinsteuerverordnung (BrStV) und in Österreich über die Verordnung des Bundesministers für Finanzen über die Vergällung von Alkohol (VO-Vergällung) geregelt.Bioethanol für die Beimischung zu Kraftstoff wird bei der Produktion mit ETBE oder Benzin vergällt. Die oben genannten, für Spiritus oder kosmetische Zwecke gängigen Vergällungsmittel, beispielsweise Methylethylketon (MEK), dürfen in Kraftstoffen nach EN 228 nicht verwendet werden.
Bei dem in Form von Brennspiritus als Brennstoff verwendeten Ethanol, beispielsweise für Rechauds sowie Camping- und Expeditionskocher, wird dem Ethanol zusätzlich zum MEK noch das extrem bittere Denatoniumbenzoat (1 Gramm/100 Liter) beigemischt. Das früher als Vergällungsmittel für Brennspiritus verwendete Pyridin wird wegen seiner gesundheitlichen Bedenklichkeit seit 1993 von deutschen Herstellern nicht mehr eingesetzt und ist seit dem 1. Juli 2013 nicht mehr zulässig. Im Gegensatz zu Pyridin, dessen Siedepunkt bei 115 °C liegt, ist Denatoniumbenzoat ein Feststoff, der erst bei 163 bis 170 °C schmilzt. Es verdampft daher bei der Verwendung von Brennspiritus nicht, sondern reichert sich in den Dochten von Spiritusgeräten an, was zum Beispiel bei Spiritusglühlichtern und Spiritus-Vergaserkochern zu Betriebsstörungen führt.
Die Vergällungsmittel haben meist ähnliche Siedepunkte wie Ethanol, sodass sie sich durch Destillieren nur schwer entfernen lassen.
Herausragendes Merkmal des Ethanols ist seine Hydroxygruppe. Da ein Sauerstoffatom Elektronen stärker anzieht als Wasserstoff und Kohlenstoff, resultiert eine asymmetrische Verteilung der Elektronendichte entlang dieser Bindung: Es bildet sich ein molekularer Dipol. Er verleiht Ethanol seine typischen Eigenschaften. Zum einen ziehen sich die Dipole auf molekularer Ebene gegenseitig an, sodass daraus eine vergleichsweise hohe Siedetemperatur von 78 °C resultiert (Sp, Ethan = −88,6 °C), zum anderen ist Ethanol mit Flüssigkeiten mischbar, die ähnliche Dipoleigenschaften aufweisen, zum Beispiel mit Wasser und Methanol. Diese Eigenschaft wird als Hydrophilie bezeichnet. Gleichzeitig besitzt das Molekül einen organischen Rest, der ihm eine begrenzte Mischbarkeit mit rein lipophilen Substanzen verleiht. Aus diesem Grund ist Ethanol in der Chemie und Pharmazie ein wichtiges Lösungsmittel. Pflanzenauszüge oder andere Medikamente werden als alkoholische Lösungen, sogenannte „Tinkturen“, angeboten.
Ethanol bildet am Gefrierpunkt ausreichend große Einkristalle für die Bestimmung mittels Kristallstrukturanalyse. Es kristallisiert im monoklinen Kristallsystem mit der Raumgruppe Pc (Raumgruppen-Nr. 7)Vorlage:Raumgruppe/7 und weist bei 87 K die Gitterparameter a = 537,7 pm, b = 688,2 pm, c = 825,5 pm und β = 102,2° auf sowie 4 Formeleinheiten pro Elementarzelle. Die Moleküle bilden über Wasserstoffbrückenbindungen mit einem Sauerstoff-Sauerstoff-Abstand von 271,6 pm und 273,0 pm lange Ketten. Die Konformation um die Kohlenstoff-Kohlenstoff-Bindung ist in beiden Molekülen versetzt. Während die Hydroxygruppe in einem Molekül entlang der C-C-OH-Achse eine gauche-Konformation besitzt, weist das andere Molekül eine trans-Konformation auf.
Ethanol ist in jedem Verhältnis mit Wasser mischbar. Dabei kommt es beim Vermischen unter Wärmeentwicklung zu einer Volumenkontraktion. Das Gesamtvolumen einer Wasser/Ethanol-Mischung ist kleiner als die Summe der Einzelvolumina. So entstehen durch Mischen von 50 ml Ethanol mit 50 ml Wasser 97 ml Ethanol-Wasser-Gemisch (vgl. Rechnung sowie weitere Beispiele und Fazit im Hauptartikel Alkoholgehalt).
Der Schmelzpunkt wässriger Ethanollösungen sinkt mit steigendem Ethanolgehalt, bis bei einem Gehalt von 93,5 Massenprozent ein Eutektikum mit einer Schmelztemperatur von −118 °C erreicht wird. Bei Temperaturen um −20 °C verdunstet Ethanol (96 %) kaum noch und nimmt eher zähflüssige Eigenschaften an. Bei −70 °C wird es noch zähflüssiger (Kühlol).
In organischen Lösungsmitteln wie Tetrachlormethan bildet Ethanol in Abhängigkeit von der Konzentration über Wasserstoffbrückenbildung Dimere, Trimere und Tetramere. Die Bildungsenthalpie ist über Infrarotspektroskopieuntersuchungen ermittelbar. Sie liegt für das Tetramer bei 92 kJ mol−1, bei 42 kJ mol−1 für das Trimer und bei 21 kJ mol−1 für das Dimer.
Die OH-Gruppe des Ethanols ist mit einem pKs-Wert von 16 sehr schwach sauer, wodurch sie in der Lage ist, mit starken Basen (wie etwa den Alkalimetallen Natrium und Kalium) ein Proton (H+) abzuspalten. Durch Umsetzen mit Alkalimetallen wird Ethanol quantitativ in seine deprotonierte Form, das Ethanolat-Ion (CH3CH2O−), überführt. Die Reaktion läuft unter Entwicklung von Wasserstoff ab:
Ethanol löst sich in allen Verhältnissen mit Wasser und vielen anderen organischen Lösungsmitteln wie Diethylether, Chloroform und Benzol.
Ethanol kann sowohl als Brønsted-Säure als auch als Brønsted-Base reagieren und ist damit ein Ampholyt:
In aprotischen Lösungsmitteln reagiert Ethanol mit Halogenwasserstoffen über eine nukleophile Substitution zu Ethylhalogeniden. Ethanol und Chlorwasserstoff reagieren zu Ethylchlorid und Wasser:
Ethylhalogenide können spezifischer durch Halogenierungsreagenzien wie Thionylchlorid oder Phosphortribromid gebildet werden.
Da das gebildete Wasser jedoch höher siedet als Ethanol, wird Ethylester besser durch Reaktion mit Säureanhydriden hergestellt. Ethylester finden Verwendung als Zusätze für Kosmetika sowie Geruchs- und Geschmacksstoffe.
Sehr starke Säuren, wie Schwefelsäure, katalysieren die Dehydratation des Ethanols. Es bilden sich Diethylether oder Ethen:
Welches Produkt sich bildet, hängt von den Reaktionsbedingungen wie Temperatur, Konzentrationen usw. ab. Bei der Dehydratation kann unter bestimmten Reaktionsbedingungen das hochgiftige Diethylsulfat gebildet werden.
Ethanol kann bereits von Luftsauerstoff bei Raumtemperatur über Acetaldehyd bis hin zur Essigsäure oxidiert werden. Derartige Reaktionen werden beispielsweise in biologischen Systemen von Enzymen katalysiert. Im Labor dienen kräftige anorganische Oxidationsmittel wie Chromsäure oder Kaliumpermanganat zur Oxidation zu Essigsäure. Die teilweise Oxidation bis zum Acetaldehyd gelingt mit schwächeren Oxidationsmitteln, etwa mit Pyridiniumchlorochromat (PCC).
Die Oxidation des Ethanols muss nicht auf der Stufe der Essigsäure stehenbleiben. An Luft verbrennt Ethanol mit blauer Flamme (siehe Bild) mit einem Heizwert von 26,8 MJ/kg zu Kohlendioxid und Wasser:
Mit Chlor oder Brom reagiert Ethanol langsam zu Acetaldehyd und anderen halogenhaltigen Oxidationsprodukten. Acetaldehyd bildet mit überschüssigem Ethanol Halbacetale. Es überwiegt aber die Halogen-Addition an die Enolform des Acetaldehyds und es bildet sich dadurch (tränenreizendes) α-Halogenacetaldehyd. Die weitere Oxidation mit Chlor führt letztlich zu Halbacetalen des Chlorals.
Entsprechend der Denaturierung durch Säuren oder Laugen kann Ethanol die in Biopolymeren zur Aufrechterhaltung der Struktur erforderlichen Wasserstoffbrücken stören, indem es als polares Lösungsmittel interferiert. Daraus resultieren Konformationsänderungen. 50- bis 70-prozentiges Ethanol denaturiert die meisten Proteine und Nukleinsäuren. Da durch Zerstörung der Raumstruktur Membranproteine ihre Funktion einbüßen und aufgrund der Membrandefekte die betreffenden Zellen luftballonartig platzen, kann mit höherprozentigem Ethanol desinfiziert werden: Bakterien- und Pilzzellen werden über die Denaturierung ihrer Membranproteine irreversibel inaktiviert, entsprechend werden behüllte Viren ihrer proteinhaltigen Hülle beraubt.
Ethanol findet Verwendung in den drei Hauptmärkten alkoholische Getränke, als Rohstoff für die chemische Industrie und als Energieträger. Ethanol, das aus der Vergärung von zucker- und stärkehaltigen Lebensmitteln stammt, wird in allen Bereichen eingesetzt, während synthetisches Ethanol nur als Chemierohstoff und Energieträger verwendet wird. Die konkurrierende Verwendung von Ethanol aus der Nahrungsmittelproduktion als Chemie- und Energierohstoff wird kontrovers diskutiert.
Die Hauptmenge des produzierten Ethanols wird in Form von alkoholischen Getränken für Genusszwecke verbraucht. Es dient weiterhin als Lösungsmittel sowohl für Konsumprodukte unter anderem im Haushalt (Parfüm, Deodorant) als auch für medizinische Anwendungen (Lösungsmittel für Medikamente, Desinfektionsmittel) sowie in der Industrie selbst ebenfalls als Lösungsmittel und allgemein als Brennstoff.
Ethanol findet als hervorragendes Lösungsmittel überall im Haushalt Verwendung, so als Träger für Geruchsstoffe wie Parfüm, Deodorant und Duftspray. Auch als Reinigungsmittel beispielsweise für Glas (Fensterreinigungsmittel), Chrom, Kunststoff, in Kfz-Scheibenwaschlösungen und als Fleckenentferner findet Ethanol Verwendung. Als Zusatz zum Wasser dient es als Frostschutzmittel.
Ethanol findet verbreiteten Einsatz als Lebensmittelzusatz. So wird Portweinen, Sherry und anderen Südweinen Ethanol zugegeben, die sogenannte Aufspritung, um zum gewünschten Zeitpunkt den Fermentationsprozess zu beenden. Durch die vorzeitig beendete Gärung haben diese Liköre und Weine - bis auf einige Ausnahmen - einen hohen Restzuckergehalt und sind dadurch sehr süß.Ethanol kann zur Haltbarmachung anderer Lebensmittel zugesetzt werden.
Als Brennstoff für Campingkocher als sogenannter Brennspiritus findet Ethanol im Haushalt eine energetische Verwendung. Durch Zugabe von Celluloseacetat oder Seife kann Brennspiritus in ein Gel, den sogenannten Hartspiritus überführt werden.Einfache Kapillarthermometer mit blau oder rot sichtbarer Flüssigkeitssäule sind mit gefärbtem Ethanol gefüllt. Bei ausreichend langem, graduiertem Rohr können Temperaturen vom Schmelzpunkt bis nahe dem Siedepunkt gemessen werden, womit Außentemperaturen gut abgedeckt werden.
Die Wirksamkeit als Desinfektionsmittel oder Antiseptikum (etwa zur Händedesinfektion) hängt von der Konzentration des Ethanol-Wasser-Gemisches ab. Bei einem optimalen Alkoholgehalt zwischen 50 und 80 % wird die Bakterienhülle zerstört und Ethanol wirkt damit tödlich. Alle Bakterien einschließlich der Tuberkelbakterien werden innerhalb einer Minute durch Denaturierung der Bakterienzellwand abgetötet (Bakterizidie). Daneben wirken Ethanol-Wasser-Mischungen durch ihren hohen osmotischen Druck; 70-prozentiges Ethanol hat mit 250·106 Pascal den höchsten osmotischen Druck aller Mischungen mit Wasser. Eingeschränkt wirksam ist das Gemisch gegen Viren, nicht wirksam gegen Bakterien-Endosporen. Bei offenen Wunden sollte es nicht eingesetzt werden: Neben einem unangenehmen Brennen wirkt Ethanol (vorwiegend kutan) vasodilatierend, was im Allgemeinen zwar förderlich für die Reinigung von Wunden ist, aber besonders bei größeren Verletzungen Blutungen drastisch verschlimmern kann. Lösungen mit über 80 % Alkoholgehalt zeigen eine noch stärkere Wirkung, werden aber aufgrund mangelnder Hautverträglichkeit nicht regelmäßig eingesetzt. Wasserfreies Ethanol härtet die Bakterienhülle, die Bakterien bleiben dadurch am Leben. Das Trinken von Ethanol oder alkoholischen Getränken wirkt nicht antiseptisch. Getränke mit einem Ethanolgehalt von weniger als 20 % töten praktisch keine Keime ab. Durch Kombination mit Alkalien (etwa 1 %) oder Peroxycarbonsäuren (0,2 bis 0,5 %) wird die Wirksamkeit unter anderem gegen Viren und Sporen stark verbessert. Ethanol dient als Lösungsmittel zur Herstellung der Iodtinktur, einer Mischung aus Iod in Ethanol zur Wunddesinfektion, der zu Vermeidung der Bildung von Iodwasserstoff Kaliumiodid zugegeben wird.
95-prozentiges beziehungsweise reines Ethanol kann als PEI-Therapie zur Verödung „heißer“ Schilddrüsenknoten (Perkutane Ethanol-Injektionstherapie) und anderer umschriebener Tumoren wie des Leberzellkarzinoms (ebenfalls Perkutane Ethanol-Injektionstherapie) benutzt werden.
Flüssige Medikamente können Ethanol als Lösungsmittel, Cosolvens oder Lösungsvermittler enthalten, wenn der oder die Arzneistoffe in Wasser schlecht löslich oder unlöslich sind. Ethanol selbst ist mit Wasser beliebig mischbar. Es hat eine wichtige Funktion in der Konservierung und Stabilisierung flüssiger pflanzlicher Medikamente (Phytotherapeutika). Die Medikamente sind entsprechend der Arzneimittel-Warnhinweisverordnung (AMWarnV) zu kennzeichnen.
Durch Einreiben der Haut mit hochprozentiger Ethanollösung (beispielsweise Franzbranntwein) wird die Durchblutung gefördert. Zur Wundreinigung wurde „gebrannter Wein“ von deutschsprachigen Wundärzten seit dem 12. Jahrhundert regelmäßig verwendet. Volksmedizinisch werden verdünnte ethanolische Lösungen heute noch zur Behandlung von Insektenstichen verwendet. Ein alkoholgetränktes Tuch wird dazu einige Zeit auf den frischen Stich gelegt. Die Schmerzlinderung geschieht aufgrund der kühlenden Wirkung der Ethanollösung; der Juckreiz wird unterdrückt. Eine chemische Veränderung oder Inaktivierung der Gifte bewirkt Ethanol jedoch nicht. Als schmerzstillende und Empfindungslosigkeit hervorrufende Narkosemittel wurden alkoholhaltige Tränke bereits im Altertum verwendet.
Bei einer Vergiftung mit Methanol wird als erste Maßnahme Ethanol intravenös gegeben, was die Umwandlung von Methanol über das Enzym Alkoholdehydrogenase in das giftige Methanal hemmt. Ethanol bindet etwa 25-mal stärker an Alkoholdehydrogenase als Methanol. Bei einer schweren Alkoholsucht kann ein Alkoholprädelir mit Ethanol unterbrochen werden, um eine akute Zweiterkrankung ohne die sonst auftretenden Symptome behandeln zu können.
Ethanol findet als Ethanol-Kraftstoff in Form des biogenen Bioethanols Verwendung als Kraftstoff für Ottomotoren, wobei vor allem Mischungen mit Benzin vorliegen. Dafür kann sowohl fossiles als auch aus regenerativer Biomasse hergestelltes Bioethanol verwendet werden, da es chemisch gesehen keinen Unterschied zwischen beiden Arten gibt. Aufgrund der Verfügbarkeit, der Herstellungskosten und politischer Fördermaßnahmen wird heute vor allem Bioethanol verwendet, das auf der Basis von fermentierbarem Zucker (Zuckerrohr und Zuckerrübe) und Stärke (vor allem Mais- und Weizenstärke) erzeugt wird. Es wird untersucht, ob zukünftig die Nutzung von Cellulose-Ethanol aus Holz möglich ist.
Ethanol wird vor allem als Beimischung zu herkömmlichem Kraftstoff genutzt, beispielsweise in einer Konzentration von 5 % Ethanol (E5 als Beimischung in gewöhnlichem Fahrzeugbenzin) oder 85 % Ethanol (als E85 für dafür geeignete Fahrzeuge). Im Zusammenhang mit dem Kyoto-Protokoll wird heute häufig über die Herstellung und den Einsatz biogener Treibstoffe (Biokraftstoffe) und die Reduzierung von Kohlenstoffdioxid-Emissionen pro gefahrenem Kilometer debattiert. In der Europäischen Union stieg die Produktionsmenge von Ethanol für den Kraftstoffsektor von 525 Millionen Liter im Jahr 2004 auf 3,7 Milliarden Liter im Jahr 2009. Seit 2011 bleibt die Ethanolproduktion sowohl für die Verwendung als Kraftstoff wie auch für andere Zwecke gleich.Ethanol wurde nach einer Entwicklung von Wernher von Braun zudem bis in die 1950er Jahre als Treibstoff für die Raketen der Typen A1, A2, A3, A4, A4b und A5 verwendet. Im Unterschied zu Benzin kann durch Verdünnen mit Wasser für Testzwecke leicht der Heizwert heruntergesetzt werden, um bei Probeläufen von Triebwerken Explosionen zu verhindern, zum anderen war Ethanol während des Zweiten Weltkriegs leicht aus landwirtschaftlichen Produkten gewinnbar, im Gegensatz zum knappen Benzin.
Neben reinem Ethanol finden seine Derivate Einsatz im Kraftstoffbereich. So wird Ethyl-tert-butylether (ETBE) analog zum Methyl-tert-butylether zur Erhöhung der Oktanzahl von Ottokraftstoffen eingesetzt. ETBE wird durch säurekatalysierte Addition von Ethanol an Isobuten hergestellt:
Ethanol ist ein wichtiges Lösungsmittel und Zwischenprodukt in der chemischen Industrie. Ein wichtiges Folgeprodukt ist Ethylchlorid, das aus Ethanol durch Umsetzung mit Chlorwasserstoff hergestellt wird. Die Oxidation liefert weitere Folgeprodukte wie Acetaldehyd und Essigsäure.Ethanol wird in einer Vielzahl von Veresterungsreaktionen eingesetzt. Die erhaltenen Ester haben vielfältige Verwendungsmöglichkeiten als Lösungsmittel und als Zwischenprodukt für Folgesynthesen. Ein wichtiges Folgeprodukt ist Ethylacrylat, ein Monomer, das als Co-Monomer in verschiedenen Polymerisationsprozessen eingesetzt wird. Essigsäureethylester wird als Lösungsmittel für Klebstoffe und Nagellack und zur Extraktion von Antibiotika eingesetzt. Glycolether wie 2-Ethoxyethanol sind als Lösungsmittel für Öle, Harze, Fette, Wachse, Nitrozellulose und Lacke weit verbreitet.
In Umkehrung der petrochemischen Herstellungsreaktion entsteht aus Ethanol wieder Ethen, das zum Beispiel von Braskem als Rohstoff für die Polyethylenherstellung genutzt wird. In einer Anlage in Rio Grande, Brasilien, produziert Braskem bereits auf Zuckerrohr basierendes Polyethylen in einer Anlage mit einem Ausstoß von 200.000 t pro Jahr.Flüssigkeitspräparate aus Biologie und Humanmedizin werden vielfach mit Ethanol-Wasser-Mischungen oder Formalin fixiert und konserviert.
Ethanol wird im gesamten Verdauungstrakt aufgenommen. Dies beginnt in geringem Umfang bereits in der Mundschleimhaut. Das dort resorbierte Ethanol geht direkt in das Blut über und wird damit über den gesamten Körper einschließlich des Gehirns verteilt. Etwa 20 % werden im Magen resorbiert, der Rest im Dünndarm. Das in Magen und Darm aufgenommene Ethanol gelangt zunächst mit dem Blut in die Leber, wo es teilweise abgebaut wird. Die Ethanolaufnahme wird durch Faktoren, welche die Durchblutung steigern, erhöht, beispielsweise Wärme (Irish Coffee, Grog), Zucker (Likör) und Kohlenstoffdioxid (Sekt). Dagegen verlangsamt Fett die Aufnahme. Dies führt nicht zu einer niedrigeren Resorption des Alkohols insgesamt, sondern nur zu einer zeitlichen Streckung.Etwa 2 bis 10 % des aufgenommenen Ethanols werden unverändert über Urin, Schweiß und Atemluft wieder abgegeben. Ein Teilabbau findet schon im Magen statt; eine dort gefundene sigma-Alkoholdehydrogenase zeigt eine etwa um den Faktor 200 höhere Aktivität als die in der Leber lokalisierten Isoenzyme. Der Anteil am gesamten Ethanolabbau beträgt lediglich ungefähr 5 %.In der Leber wird der Hauptteil des Ethanols – wie andere wasserlösliche Gifte – durch die Enzyme Alkoholdehydrogenase (ADH) und Katalase sowie das MEOS-System zu Ethanal (Acetaldehyd, H3C-CHO) abgebaut, um weiter durch Acetaldehyddehydrogenase zu Essigsäure oxidiert zu werden. Die Essigsäure wird über den Citratzyklus und die Atmungskette in allen Zellen des Körpers unter Energiegewinnung zu CO2 veratmet. Die Leber kann bei erheblich gesteigertem, regelmäßigem Konsum ihre Abbauaktivität in geringem Maße anpassen. Das Zwischenprodukt Ethanal ist für die sogenannten „Kater“-Symptome wie Kopfschmerzen, Übelkeit und Erbrechen mitverantwortlich. Der Abbau des Ethanals wird durch Zucker gehemmt, daher ist der Kater bei süßen alkoholischen Getränken, insbesondere Likör, Bowlen, Fruchtweinen und manchen Sektsorten besonders intensiv.
Die Abbaurate durch die Alkoholdehydrogenase ist innerhalb gewisser Grenzen konstant. Sie beträgt bei Männern etwa 0,1 und bei Frauen 0,085 Gramm pro Stunde und Kilogramm Körpergewicht. Die exakt gemessenen Abbauraten für Männer lagen dabei zwischen 0,088 und 0,146 Gramm pro Stunde und Kilogramm Körpergewicht. Bei Männern findet sich eine leicht erhöhte Aktivität der gastrischen Alkoholdehydrogenase im Magen, mit der Folge einer geringfügigen Beschleunigung des Alkoholabbaus. Hochdosierte Aufnahme von Fructose kann bei manchen Menschen durch Unterstützung des Katalase-Ethanolabbaus zu einer schnelleren Metabolisierung führen. Bei höherer Alkoholkonzentration – ab etwa 50 g Ethanolaufnahme pro Tag – oder bei chronischen Trinkern wird der Alkohol zusätzlich über das mikrosomale Ethanol oxidierende System (MEOS) abgebaut. Dabei wird Ethanol im glatten ER der Leberzellen durch Cytochrom P450 (CYP2E1) unter Sauerstoffverbrauch ebenfalls zu Ethanal oxidiert. Ethanol bewirkt situativ eine Betäubung, eine Stimulation oder einen Stimmungswandel. Es führt zu einer Erweiterung insbesondere der peripheren Blutgefäße.
Ethanol wird von Pathologen zu den „obligat hepatotoxischen Stoffen“, also zu den Lebergiften, gezählt. Es gilt ein „direkter toxischer Effekt des Alkohols auf die Erythropoiese“, die Bildung roter Blutzellen, als gesichert. Pädiater nennen ihn eine „teratogene Noxe“, also ein die Leibesfrucht schädigendes Gift, und die Pharmakologen und Toxikologen sprechen von „akuter Vergiftung“ ab einer bestimmten Schwellendosis sowie von einer „chronischen Vergiftung“ beim Alkoholismus. So zeigen verdünnte Lösungen von Ethanol in Wasser schon bei Konzentrationen von wenigen Volumenprozent physiologische Effekte. Die Aufnahme führt – ab etwa 0,5–1 Promille Ethanolkonzentration im Blut – zu typischen akuten Trunkenheitssymptomen wie Schwindel, Übelkeit, Orientierungsstörung, Redseligkeit und gesteigerter Aggressivität. Die letale Dosis (LD) liegt etwa bei 3,0 bis 4,0 Promille für ungeübte Trinker. Es wurden jedoch schon Werte über 7 Promille gemessen. Die LD50 beträgt für die Ratte 7060 mg/kg bei oraler Applikation. Bei einer akuten Ethanolvergiftung kann der noch im Magen befindliche Alkohol durch Herbeiführen von Erbrechen oder durch Auspumpen des Mageninhalts teilweise entfernt werden.
Ethanol kann durch Veresterung als p-Nitrobenzoesäureester oder 3,5-Dinitrobenzoesäureester nachgewiesen werden. Die Reaktion erfolgt durch Umsetzung mit dem entsprechenden Säurechlorid. Unspezifisch kann Ethanol durch die Iodoformprobe nachgewiesen werden. Durch chromatografische Methoden wie der Gaschromatografie kann Ethanol quantitativ bestimmt werden. Nasschemisch-quantitativ ist der Nachweis durch Oxidation mit einem Überschuss von Kaliumdichromat möglich, wobei das nicht umgesetzte Kaliumdichromat jodometrisch ermittelt werden kann.
In der Lebensmittelanalytik macht man sich den Dichteunterschied zwischen Wasser und Ethanol zunutze. Der Ethanolgehalt wird in einer (Wasserdampf-) Destillation abgetrennt und pyknometrisch bestimmt. Alternativ kann die Dichte auch im Biegeschwinger gemessen werden. Bei beiden Verfahren wird anhand von Tabellenwerten ausgewertet.
Im Protonenresonanzspektrum weist Ethanol bei Raumtemperatur eine Triplettstruktur durch Kopplung der Protonen der Hydroxygruppe mit den Methylenprotonen auf. Dies weist auf eine Fixierung der Hydroxygruppe gegenüber den Methylenprotonen hin. Mit steigenden Temperaturen wird die Aufspaltung kleiner und verschwindet durch die steigende Rotation der Hydroxygruppe schließlich ganz.Die Ethanolkonzentration während des Herstellungsprozesses, etwa in Brauereien, kann infrarotspektroskopisch durch die Messung der Intensität der Schwingungsfrequenz der C-H-Bande bei 2900 cm−1 überwacht werden. Das Infrarotspektrum für Ethanol weist eine C-H-, eine O-H- und eine C-O-Streckschwingung sowie verschiedene Biegeschwingungen auf. Die O-H-Streckschwingung erscheint als eine breite Bande bei etwa 3300–3500 cm−1, die C-H-Streckschwingung bei etwa 3000 cm−1.
Allinger, Cava, de Jongh, Johnson, Lebel, Stevens: Organische Chemie. 1. Auflage, Walter de Gruyter, Berlin 1980, ISBN 3-11-004594-X, S. 125–127.
Beyer, Walter: Lehrbuch der Organischen Chemie. 19. Auflage, S. Hirzel Verlag, Stuttgart 1981, ISBN 3-7776-0356-2, S. 115–117.
Morrison, Boyd: Lehrbuch der Organischen Chemie. 3. Auflage, VCH, Weinheim 1986, ISBN 3-527-26067-6, S. 526–527.

Ethernet ([ˈeːtɐˌnɛt] oder englisch [ˈiːθərˌnɛt]) ist eine Technik, die Software (Protokolle usw.) und Hardware (Kabel, Verteiler, Netzwerkkarten usw.) für kabelgebundene Datennetze spezifiziert, welche ursprünglich für lokale Datennetze (LANs) gedacht war und daher auch als LAN-Technik bezeichnet wird. Sie ermöglicht den Datenaustausch in Form von Datenframes zwischen den in einem lokalen Netz (LAN) angeschlossenen Geräten (Computer, Drucker und dergleichen). Derzeit sind Übertragungsraten von 1, 10, 100 Megabit/s (Fast Ethernet), 1000 Megabit/s (Gigabit-Ethernet), 2,5, 5, 10, 40, 50, 100, 200 und 400 Gigabit/s spezifiziert. In seiner ursprünglichen Form erstreckt sich das LAN dabei nur über ein Gebäude; Ethernet-Varianten über Glasfaser haben eine Reichweite von bis zu 70 km.
Die Ethernet-Protokolle umfassen Festlegungen für Kabeltypen und Stecker sowie für Übertragungsformen (Signale auf der Bitübertragungsschicht, Paketformate). Im OSI-Modell ist mit Ethernet sowohl die physische Schicht (OSI Layer 1) als auch die Data-Link-Schicht (OSI Layer 2) festgelegt.
Ethernet entspricht weitestgehend der IEEE-Norm 802.3. Es wurde ab den 1990ern zur meistverwendeten LAN-Technik und hat andere LAN-Standards wie Token Ring verdrängt oder, wie im Falle von ARCNET in Industrie- und Fertigungsnetzen oder FDDI in hoch verfügbaren Netzwerken, zu Nischenprodukten für Spezialgebiete gemacht. Ethernet kann die Basis für Netzwerkprotokolle, wie z. B. AppleTalk, DECnet, IPX/SPX und TCP/IP, bilden.
Für Anwendungen, in denen hohe Anforderungen an die Zuverlässigkeit der Kommunikation gestellt werden, kommt Echtzeit-Ethernet zum Einsatz.
Ethernet wurde ursprünglich am Xerox Palo Alto Research Center (PARC) entwickelt. Metcalfe sagt, dass er das Ethernet erstmals 1973 in einem Memo über das Potenzial von Ethernet an seine Vorgesetzten skizzierte. Er leitete das Protokoll von dem an der Universität von Hawaii entwickelten funkbasierten ALOHAnet ab. Daher auch der Name Ethernet (englisch für „Äther“, der nach historischen Annahmen das Medium zur Ausbreitung von (Funk-)Wellen wäre). Metcalfe selbst sagt, dass Ethernet über mehrere Jahre entwickelt worden sei und sich daher kein Anfangszeitpunkt festmachen ließe.
Ursprünglich war es also ein firmenspezifisches und nicht standardisiertes Produkt. Diese erste Version des Ethernet arbeitete noch mit 3 Mbit/s. 1976 veröffentlichten Metcalfe und sein Assistent David Boggs einen Artikel mit dem Titel Ethernet: Distributed Packet-Switching For Local Computer Networks.
Robert Metcalfe verließ Xerox 1979, um die Nutzung von Personal Computern und LANs zu fördern, und gründete die Firma 3Com. Er überzeugte DEC, Intel und Xerox, mit ihm zusammenzuarbeiten, um Ethernet zum Standard zu machen. Ihre erste Ethernet-Version 1 wurde ab 1980 vom IEEE (Institute of Electrical and Electronics Engineers) in der Arbeitsgruppe 802 weiterentwickelt. Ursprünglich war nur ein LAN-Standard für Übertragungsraten zwischen 1 und 20 Mbit/s geplant. Ebenfalls 1980 kam noch eine sogenannte „Token-Access-Methode“ hinzu. Ab 1981 verfolgte das IEEE drei verschiedene Techniken: CSMA/CD (802.3), Token Bus (802.4) und Token Ring (802.5), wovon die letzten beiden bald in einer wahren Flut von Ethernet-Produkten untergingen. 3Com wurde dabei ein großes Unternehmen.
Die Arbeiten am Cheapernet-Standard (10BASE2) wurden im Juni 1983 veröffentlicht. Zur gleichen Zeit begann die Arbeit an den Spezifikationen für Ethernet-on-Broadband (10BROAD36) und für das StarLAN (1BASE5). Als 1985 der Ethernet-Standard auch als internationaler Standard ISO/DIS 8802/3 veröffentlicht wurde, wurde er binnen kurzer Zeit von über 100 Herstellerfirmen unterstützt. 1986 begannen einige kleinere Firmen mit der Übertragung von Daten im Ethernet-Format auf Vierdrahtleitungen aus dem Telefonbereich (CAT-3). Danach verstärkte das IEEE seine Aktivitäten in den Gebieten Ethernet-on-Twisted Pair, was 1991 zum Standard für 10BASE-T wurde, sowie Ethernet auf Glasfaserkabeln, was 1992 zu den 10BASE-F-Standards (F für Fibre-Optics) führte. Mitte der 1990er Jahre kam es zu einem Tauziehen um den Nachfolge-Standard; auf der einen Seite standen AT&T und HP, die eine technisch elegantere Lösung nach IEEE 802.12 (100BASE-VG) anstrebten, auf der anderen Seite standen die Hersteller der Fast Ethernet Alliance, bestehend aus ca. 35 namhaften Firmen wie Bay Networks, 3Com, Intel, SUN, Novell usw., die 100 Mbit/s nach dem altbewährten IEEE-802.3-Standard propagierten.
Letztendlich wurde 1995 der 100-Mbit/s-Standard für Ethernet auf Bestreben der Fast Ethernet Alliance gemäß IEEE 802.3u verabschiedet, etwa gleichzeitig mit dem Standard für ein Wireless-LAN mit der Bezeichnung 802.11. Inzwischen nehmen die Arbeiten am 10-Gbit/s-Ethernet und am Ethernet in the First Mile (EFM) statt des rein lokalen Betriebs bereits Universitäts- und Stadtnetze ins Visier.
In der Form des Industrial Ethernet findet der Ethernet-Verkabelungsstandard heutzutage immer mehr auch in industriellen Fertigungsanlagen Anwendung. Die weltweite Vernetzung und die dadurch wachsenden Anforderungen an die Datenübertragung – nicht nur für berufliche, sondern auch für private Zwecke – hat dazu geführt, dass auch in Privatgebäuden und sogar Kreuzfahrtschiffen leistungsfähige Netzwerke installiert werden.
Robert Metcalfe wurde für seine Verdienste um die Entwicklung des Ethernets im Jahr 2003 die „National Medal of Technology“ verliehen.
Ethernet basiert auf der Idee, dass die Teilnehmer eines LANs Nachrichten durch Hochfrequenz übertragen, allerdings nur innerhalb eines gemeinsamen Leitungsnetzes. Jede Netzwerkschnittstelle hat einen global eindeutigen 48-Bit-Schlüssel, der als MAC-Adresse bezeichnet wird. Das stellt sicher, dass alle Systeme in einem Ethernet unterschiedliche Adressen haben. Ethernet überträgt die Daten auf dem Übertragungsmedium dabei im sogenannten Basisbandverfahren und in digitalem Zeitmultiplex.
Ein Algorithmus mit dem Namen „Carrier Sense Multiple Access with Collision Detection“ (CSMA/CD) regelt den Zugriff der Systeme auf das gemeinsame Medium. Es ist eine Weiterentwicklung des ALOHAnet-Protokolls, das in den 1970er-Jahren auf Hawaii zum Einsatz kam.
In der Praxis funktioniert dieser Algorithmus bildlich wie eine Diskussionsrunde ohne Moderator, auf der alle Gäste ein gemeinsames Medium (die Luft) benutzen, um miteinander zu sprechen. Bevor sie zu sprechen beginnen, warten sie höflich darauf, dass der andere Gast zu reden aufgehört hat. Wenn zwei Gäste zur gleichen Zeit zu sprechen beginnen, stoppen beide und warten für eine kurze, zufällige Zeitspanne, bevor sie einen neuen Anlauf wagen.
Die Stelle, die Daten senden möchte, lauscht also auf dem Medium (Carrier Sense), ob es bereits belegt ist und sendet erst, wenn die Leitung frei ist. Da zwei Stellen gleichzeitig zu senden anfangen können, kann es trotzdem zu Kollisionen kommen, die dann festgestellt werden (Collision Detection), woraufhin beide Stellen noch kurz ein „Störung-Erkannt“-Signalmuster erzeugen, dann mit dem Senden aufhören und eine zufällige Zeit warten, bis sie einen erneuten Sendeversuch starten. Hierzu muss ein Sender während des Sendens zugleich auf dem Medium lauschen, ob ein anderer Sender mit ihm kollidiert. Daher sind Medien ungeeignet für CSMA/CD, wenn eine hohe Sendeleistung notwendig ist und einem sehr schwachen Empfangssignal gegenübersteht, das dann „untergeht“.
Damit die Kollision festgestellt und eine Sendewiederholung initiiert werden kann, müssen die Datenframes abhängig von der Leitungslänge eine bestimmte Mindestlänge haben – das Störsignal des zweiten Senders muss den ersten erreichen, bevor dieser sein Datenpaket beendet hat (und als „kollisionsfrei gesendet“ betrachtet). Diese Mindestlänge ergibt sich aus der Signalgeschwindigkeit und der Übertragungsrate. Bei einer Übertragungsrate von 10 Mbit/s und einer maximalen Entfernung von 2,5 km zwischen zwei Stationen ist eine Mindestlänge von 64 Byte (14 Byte Header, 46 Byte Nutzdaten, 4 Byte CRC) vorgeschrieben. Kleinere Datenframes müssen entsprechend aufgefüllt werden. Für eine Übertragungsrate mit 10 Mbit/s (Standard-Ethernet) sind eine maximale Segmentlänge von 100 m sowie vier Repeater erlaubt. Damit können zwei Stationen bis zu einer Distanz von 500 m direkt verbunden werden. Bei höheren Übertragungsraten und maximaler Segmentlänge reduziert sich die Anzahl der Repeater aufgrund der physikalischen Abhängigkeiten. So sind bei Fast-Ethernet (100 Mbit/s) nur zwei Repeater und bei Gigabit-Ethernet (1000 Mbit/s) ein Repeater erlaubt. Bei 1-Gbit/s-Ethernet (1000 Mbit/s) im (allerdings eher hypothetischen) Halbduplex-Betrieb werden kleine Frames im Ethernet-Paket auf 520 Byte verlängert, um noch eine sichere Kollisionserkennung bei sinnvoller physischer Netzwerkgröße zu erlauben.Auch wenn die Norm IEEE 802.3 den Namen „CSMA/CD“ im Titel hat, spielt die Kollisionsauflösung heute nur mehr in geringem Maße eine Rolle. Die meisten Netzwerke werden heute im Vollduplexmodus betrieben, bei dem Teilnehmer (Router, Switches, Endgeräte, etc.) mittels Punkt-zu-Punkt-Verbindung die Sende- und Empfangsrichtung unabhängig voneinander nutzen können und somit keine Kollisionen mehr entstehen. Trotzdem blieb das Frame-Format, insbesondere der Frame-Header und die für die Kollisionserkennung vorgeschriebene minimale Frame-Länge, bis hinauf zu 400-Gbit/s-Ethernet, unverändert.
In den ersten Ethernetimplementierungen wurde die gesamte Kommunikation über einen gemeinsamen Bus, der in Form eines Koaxialkabels realisiert war, abgewickelt. An diesen wurden alle Arbeitsstationen abhängig vom Kabeltyp entweder per T-Stück oder „Invasivstecker“ (auch Vampirklemme, Vampirabzweige oder Vampire Tap genannt) angeschlossen. Jede Information, die von einem Computer gesendet wurde, wurde auch von allen empfangen. Die über Ethernet verbundenen Geräte müssen ständig Informationen ausfiltern, die nicht für sie bestimmt sind.
Diese Tatsache kann genutzt werden, um Broadcast- (deutsch: Rundruf)-Nachrichten an alle angeschlossenen Systeme zu senden. Bei TCP/IP beispielsweise verwendet das ARP einen derartigen Mechanismus für die Auflösung der Schicht-2-Adressen. Diese Tatsache ist auch ein Sicherheitsproblem von Ethernet, da ein Teilnehmer mit bösen Absichten den gesamten Datenverkehr auf der Leitung mitprotokollieren kann. Eine mögliche Abhilfe ist der Einsatz von Kryptographie (Verschlüsselung) auf höheren Protokollebenen. Die Vertraulichkeit der Verkehrsbeziehungen (wer tauscht mit wem in welchem Umfang wann Daten aus?) ist aber so nicht zu schützen.
Der Einsatz von (Repeater) Hubs zur Bildung von Multi-Segment-Ethernet-Netzen ändert hier nichts, weil alle Datenpakete in alle Segmente repliziert werden.
In moderneren Ethernetnetzen wurden zur Aufteilung der Kollisions-Domänen zunächst Bridges, heute Switches eingesetzt. Durch diese wird ein Ethernet in Segmente zerlegt, in denen jeweils nur eine Untermenge an Endgeräten zu finden ist. Werden ausschließlich Switches verwendet, so kann netzweit im Full-Duplex-Modus kommuniziert werden, das ermöglicht das gleichzeitige Senden und Empfangen für jedes Endgerät. Über Switches werden Datenpakete in der Regel direkt vom Sender zum Empfänger befördert – unbeteiligten Teilnehmern wird das Paket nicht zugestellt. Broadcast- (deutsch: Rundruf-) und Multicast-Nachrichten hingegen werden an alle angeschlossenen Systeme gesendet.
Das erschwert das Ausspionieren und Mithören, der Sicherheitsmangel wird durch die Einrichtung einer „geswitchten“ Umgebung allerdings nur verringert und nicht behoben. Zusätzlich zu den Broadcast-Meldungen werden auch die jeweils ersten Pakete nach einer Sendepause – dann, wenn der Switch die Ziel-MAC-Adresse (noch) nicht kennt – an alle angeschlossenen Systeme gesendet. Dieser Zustand kann auch böswillig durch MAC-Flooding herbeigeführt werden. Pakete können auch böswillig durch MAC-Spoofing umgeleitet werden.
Die Sicherheit des Betriebs im Sinne der störungsfreien Verfügbarkeit von Daten und Diensten beruht auf dem Wohlverhalten aller angeschlossenen Systeme. Beabsichtigter oder versehentlicher Missbrauch muss in einer Ethernetumgebung durch Analyse des Datenverkehrs aufgedeckt werden (LAN-Analyse). Switches stellen vielfach statistische Angaben und Meldungen bereit, die Störungen frühzeitig erkennbar werden lassen bzw. Anlass geben zu einer detaillierteren Analyse.
Ethernet in seinen frühen Ausprägungen (z. B. 10BASE5, 10BASE2), mit einem von mehreren Geräten gemeinsam als Übertragungsmedium genutzten Kabel (collision domain/shared medium – im Unterschied zu dem späteren geswitchten Ethernet), funktioniert gut, solange das Verkehrsaufkommen relativ zur nominalen Bandbreite niedrig ist. Da die Chance für Kollisionen proportional mit der Anzahl der Sender (englisch „transmitter“) und der zu sendenden Datenmenge ansteigt, tritt oberhalb einer Auslastung von 50 % (und höher) vermehrt ein als Congestion (Stau) bekanntes Phänomen auf, wobei Kapazitätsüberlastungen entstehen und somit eine gute Effizienz der Übertragungsleistung innerhalb des Netzwerks verhindert wird.
Um dieses Problem zu lösen und die verfügbare Übertragungskapazität zu maximieren, wurden Switches entwickelt (manchmal auch als Switching Hubs, Bridging Hubs oder MAC bridges bezeichnet), man spricht auch von Switched Ethernet. Switches speichern Pakete/Frames zwischen und beschränken damit die Reichweite der Kollisionen (die Kollisionsdomäne) auf die an dem entsprechenden Switchport angeschlossenen Geräte. Bei Twisted-Pair- oder Glasfaser-Verkabelung können Verbindungen zwischen zwei Geräten (Link) außerdem im Vollduplex-Modus (FDX) betrieben werden, wenn beide Geräte dies unterstützen (dies ist dann die Regel).
Wenn (alle) Hubs/Repeater aus einem Netzwerk entfernt und durch vollduplex-fähige Komponenten ersetzt werden, spricht man von einem (pure) switched Ethernet, bei dem es keine Halbduplex-Links und damit auch keine Kollisionen mehr gibt. Die Verwendung von Switches ermöglicht also eine kollisionsfreie Kommunikation im FDX-Modus, d. h., Daten können gleichzeitig gesendet und empfangen werden, ohne dass es zu Kollisionen kommt. Trotz kollisionsfreier Bearbeitung kann es jedoch zu Paketverlusten kommen, etwa wenn zwei Sender jeweils die Bandbreite beanspruchen, um zu einem gemeinsamen Empfänger Datenpakete zu senden. Der Switch kann zwar Pakete kurzzeitig puffern, wenn der Empfänger aber nicht über die doppelte Bandbreite verfügt oder der Datenfluss nicht verlangsamt werden kann, muss er bei Überlauf des Puffers Daten verwerfen, so dass sie nicht zugestellt werden können.
Ethernet flow control (Flusskontrolle) ist ein Mechanismus, der die Datenübertragung bei Ethernet temporär stoppt. In CSMA/CD-Netzen konnte auf diese spezielle Signalisierung verzichtet werden, denn hier ist die Signalisierung einer Kollision praktisch gleichbedeutend mit einem Stopp- oder Pausensignal (Back Pressure).
Seit Fast-Ethernet und der Einführung von Switches findet die Datenübertragung praktisch nur noch kollisionsfrei im Vollduplex-Modus statt. Da damit auf CSMA/CD verzichtet wird, ist eine zusätzliche Flusskontrolle erforderlich, die es einer Station beispielsweise bei Überlastung ermöglicht, ein Signal zu geben, dass sie zurzeit keine weiteren Pakete zugesandt haben möchte – anders als mit CSMA/CD gibt es keine Möglichkeit, einen Verlust und damit die Notwendigkeit einer erneuten Sendung anzuzeigen. Hierzu wurde Flow Control eingeführt. Damit kann eine Station die Gegenstellen auffordern, eine Sendepause einzulegen und vermeidet so, dass Pakete (zumindest teilweise) verworfen werden müssen. Die Station schickt hierzu einer anderen Station (einer MAC-Adresse) oder an alle Stationen (Broadcast) ein PAUSE-Paket mit einer gewünschten Wartezeit. Die Pause beträgt 0 bis 65535 Einheiten; eine Einheit entspricht der Zeit, die für die Übertragung von 512 Bit benötigt wird.
Ethernet Flow Control verbessert die Zuverlässigkeit der Zustellung – da die angeforderten Pausen direkt auf den sendenden Knoten wirken, kann es aber zu Leistungseinbußen kommen. Wenn zum Beispiel ein Zielknoten die zu empfangenden Daten nur langsamer als mit der Übertragungsrate aufnehmen kann und deshalb Pause-Frames verschickt, bremst es den sendenden Knoten insgesamt, und dieser versorgt auch andere Zielknoten langsamer mit Daten als eigentlich möglich wäre (head-of-line blocking).
Flow Control ist optional und wird häufig nicht eingesetzt, um Head-of-Line-Blocking zu vermeiden. In den meisten Netzwerken werden für wichtige Daten in den höheren Netzwerkschichten Protokolle verwendet, die leichte Übertragungsverluste ausgleichen können, insbesondere Transmission Control Protocol. Wenn dies nicht möglich ist, muss durch die Netzwerkarchitektur oder andere Mechanismen sichergestellt werden, dass wichtige Pakete nicht verloren gehen können, zum Beispiel mit Quality of Service oder bei Fibre Channel over Ethernet.
Der Ethernet-Version-2- oder Ethernet-II-Datenblock (englisch ethernet II frame), der sogenannte DIX-Frame (Definition 1982 durch das Konsortium DEC, Intel und Xerox).Seit 1983 entsteht der Standard IEEE 802.3. Ethernet ist quasi ein Synonym für diesen Standard. IEEE 802.3 definiert zwei Frame-Formate:
IEEE 802.3 3.1.b Tagged MAC frameDer ursprüngliche Xerox-Version-1-Ethernet-Datenblock hatte ein 16-bit-Feld, in dem die Länge des Datenblocks hinterlegt war. Da diese Länge für die Übertragung der Frames nicht wichtig ist, wurde es vom späteren Ethernet-II-Standard als Ethertype-Feld verwendet. Das Format von Ethernet I mit dem Längenfeld ist jetzt Teil des Standards 802.3.
Das Ethernet-II-Format verwendet die Bytes 13 und 14 im Frame als Ethertype. Auf ein Längenfeld wie im Ethernet-I-Frame wird verzichtet. Die Länge eines Frames wird nicht durch einen Zahlenwert, sondern durch die bitgenaue Signalisierung des Übertragungsendes übermittelt. Die Länge des Datenfeldes bleibt wie bei Ethernet I auf 1500 Bytes beschränkt. Auch das Ethernet-II-Format ist jetzt Teil des Standards 802.3, nur die Ethertypen mit Zahlenwerten kleiner als 1500 sind weggefallen, weil jetzt die Zahlenwerte kleiner gleich 1500 in diesem Feld als Länge interpretiert werden und gegen die tatsächliche Länge geprüft werden.
IEEE 802.3 definiert das 16-bit-Feld nach den MAC-Adressen als Type/Length-Feld. Mit der Konvention, dass Werte zwischen 0 und 1500 auf das originale Ethernet-Format hindeuteten und höhere Werte den EtherType angeben, wurde die Koexistenz der Standards auf demselben physischen Medium ermöglicht. Die zulässigen Werte für Ethertype werden von IEEE administriert. Diese Verwaltung beschränkt sich auf die Vergabe neuer Ethertype-Werte. IEEE nimmt bei der Neuvergabe Rücksicht auf bereits für Ethernet II vergebene Ethertype-Werte, dokumentiert diese aber nicht. So kommt es vor, dass zum Beispiel der Wert 0x0800 für IP-Daten in der IEEE-Dokumentation der Ethertype-Werte fehlt. Ethertype beschreibt das Format bzw. das Protokoll zur Interpretation des Datenblocks. Das LLC-Feld und ein eventuelles SNAP-Feld sind bereits Teil des MAC-Frame-Datenfeldes.
Im Tagged-MAC-Frame werden vier Bytes mit dem QTAG-Präfix nach der Quell-MAC-Adresse eingeschoben. Dieses Feld wird durch den Standard 802.1Q definiert und ermöglicht bis zu 4096 virtuelle lokale Netzwerke (VLANs) auf einem physischen Medium. Die erlaubte Gesamtlänge des Mac-Frames wird auf 1522 Bytes verlängert, die Länge des Datenfeldes bleibt auf 1500 Bytes beschränkt.
Ethernet überträgt die Daten seriell, beginnend jeweils mit dem untersten, niederwertigsten Bit (der „Einerstelle“) eines Bytes. Das bedeutet, dass beispielsweise das Byte 0xD5 als Bitsequenz (links nach rechts) „10101011“ auf die Reise geht. Die Bytes der breiteren Felder werden als BigEndians übertragen, d. h. mit dem Byte mit der höheren Wertigkeit zuerst. Beispielsweise wird die MAC-Adresse im Bild 0x0040F6112233 in dieser Reihenfolge als „00 40 F6 11 22 33“ übertragen. Da das erste Bit eines Frames das Multicast-Bit ist, haben Multicastadressen ein erstes Byte mit einer ungeraden Zahl, z. B. 01-1B-19-00-00-00 für IEEE 1588.
Eine Abweichung betrifft die FCS (Frame Check Sequence, CRC): Da sämtliche übertragenen Bits durch den CRC-Generator vom LSB zum MSB geschoben werden, muss das höchstwertige Bit des höchstwertigen Bytes der CRC an vorderster Stelle übertragen werden. Ein errechneter CRC-Wert von 0x8242C222 wird somit als „41 42 43 44“ an die übertragenen Datenbytes als FCS-Prüfsumme zur Übertragung angehängt.
Im Gegensatz zum Ethernet-Frame befindet sich bei manchen anderen LAN-Typen (beispielsweise Token Ring oder FDDI) in einem Frame das höchstwertige Bit eines Bytes an erster Stelle. Das bedeutet, dass beim Bridging zwischen einem Ethernet-LAN und einem anderen LAN-Typ die Reihenfolge der Bits eines jeden Bytes der MAC-Adressen umgekehrt werden muss.
Die Präambel besteht aus einer sieben Byte langen, alternierenden Bitfolge „101010…1010“, auf diese folgt der Start Frame Delimiter (SFD) mit der Bitfolge „10101011“. Diese Sequenz diente einst der Bit-Synchronisation der Netzwerkgeräte. Sie war für all jene Geräteverbindungen notwendig, die die Bit-Synchronisation nicht durch die Übertragung einer kontinuierlichen Trägerwelle auch in Ruhezeiten aufrechterhalten konnten, sondern diese mit jedem gesendeten Frame wieder neu aufbauen mussten. Das alternierende Bitmuster erlaubte jedem Empfänger eine korrekte Synchronisation auf die Bit-Abstände. Da bei einer Weiterleitung über Repeater (Hubs) jeweils ein gewisser Teil der Präambel verloren geht, wurde sie in der Spezifikation groß genug gewählt, dass bei maximaler Ausdehnung des Netzwerkes für den Empfänger noch eine minimale Einschwingphase übrig bleibt.
Die Bus-Netzwerkarchitekturen, die auf derartige Einschwingvorgänge angewiesen sind, werden heute kaum mehr verwendet, wodurch sich die Präambel, genauso wie das Zugriffsmuster CSMA/CD, die minimale und maximale Frame-Länge und der minimale Paketabstand (IFG, auch IPG) nur aus Kompatibilitätsgründen in der Spezifikation befinden. Genau genommen sind Präambel und SFD Paketelemente, die auf einer Ebene unterhalb des Frames und damit auch des MACs definiert sein sollten, damit ihre Verwendung vom konkreten physischen Medium abhinge. Moderne drahtgebundene Netzwerkarchitekturen sind stern- oder ringförmig und verwenden dauerhaft eingeschwungene (synchrone) Punkt-zu-Punkt-Verbindungen zwischen Endteilnehmern und Netzwerkverteilern (Bridges bzw. Switches), die Paketgrenzen in anderer Form signalisieren und daher Präambel und SFD eigentlich unnötig machen. Andererseits ergeben sich durch IFGs und minimale Frame-Längen für Netzwerkverteiler auch gewisse maximale zu verarbeitende Paketraten, was deren Design vereinfacht.
Die Zieladresse identifiziert die Netzwerkstation, die die Daten empfangen soll. Diese Adresse kann auch eine Multicast- oder Broadcast-Adresse sein.
Die Quelladresse identifiziert den Sender. Jede MAC-Adresse der beiden Felder hat eine Länge von sechs Bytes bzw. 48 Bit.
Zwei Bit der MAC-Adresse werden zu ihrer Klassifizierung verwendet. Das erste übertragene Bit und damit Bit 0 des ersten Bytes entscheidet, ob es sich um eine Unicast- (0) oder Broadcast-/Multicast-Adresse (1) handelt. Das zweite übertragene Bit und damit Bit 1 des ersten Bytes entscheidet, ob die restlichen 46 Bit der MAC-Adresse global (0) oder lokal (1) administriert werden. Gekaufte Netzwerkkarten haben eine weltweit eindeutige MAC-Adresse, die global von einem Konsortium und der Herstellerfirma verwaltet wird. Man kann aber jederzeit individuelle MAC-Adressen wählen und den meisten Netzwerkkarten über die Treiberkonfiguration zuweisen, in denen man für das Bit 1 den Wert (1) wählt und eben spezifikationsgemäß die restlichen 46 Bit lokal verwaltet und in der Broadcast Domain eindeutig hält.
MAC-Adressen werden traditionell als Abfolge von sechs zweistelligen Hex-Zahlen dargestellt, die mit Doppelpunkten getrennt sind, z. B. als „08:00:01:EA:DE:21“, was der Übertragungsreihenfolge am Medium entspricht. Die einzelnen Bytes werden beginnend mit dem LSB gesendet.
Im Tagged-MAC-Frame nach IEEE 802.1Q folgen zusätzlich vier Bytes als VLAN-Tag. Die ersten beiden Bytes enthalten die Konstante 0x8100 (=802.1qTagType), die einen Tagged-MAC-Frame als solchen kenntlich machen. Von der Position her würde hier im Basic-MAC-Frame das Feld Ethertype stehen. Den Wert 0x8100 kann man damit auch als Ethertype für VLAN-Daten ansehen, allerdings folgt nach dem Tag noch der eigentliche Ethertype (s. u.). In den nächsten beiden Bytes (TCI Tag Control Information) stehen dann drei Bit für die Priorität (Class of Service, 0 niedrigste, 7 höchste Priorität), ein Bit Canonical Format Indicator (CFI), das für die Kompatibilität zwischen Ethernet und Token Ring sorgt (dieses 1-bit-Datenfeld zeigt an, ob die MAC-Adresse in einem anerkannten oder nicht anerkannten Format ist. Hat das gesetzte Bit eine 0, dann ist es nicht vorschriftsmäßig, bei einer 1 ist es vorschriftsmäßig. Für Ethernet-Switches ist es immer 0. Empfängt ein Ethernet-Port als CFI-Information eine 1, dann verbindet der Ethernet-Switch das Tagging-Frame nicht zu einem nicht-getaggten Port.), sowie 12 Bit für die VLAN-ID. An diesen VLAN-Tag schließt das ursprünglich an der Position des VLAN-Tags stehende Typ-Feld (EtherType) des eigentlichen Frames mit einem Wert ungleich 0x8100 (im Bild beispielsweise 0x0800 für ein IPv4-Paket) an.
Der VLAN-Tag wird als Folge von zwei Bytes „81 00“ übertragen. Die 16 Bit des TCI werden in gleicher Weise Big-Endian mit dem höheren Byte voran verschickt.
Das Typ-Feld gibt Auskunft über das verwendete Protokoll der nächsthöheren Schicht innerhalb der Nutzdaten. Die Werte sind größer als 0x0600 (ansonsten ist das ein Ethernet-I-frame mit Längenfeld in dieser Position). Der spezielle Wert 0x8100 zur Kennzeichnung eines VLAN-Tags ist im Wertevorrat von Type reserviert. Ist ein VLAN-Tag vorhanden, darf das daran anschließende Typ-Feld nicht 0x8100 sein.
In Ethernet-802.3-Frames kann zur Kompatibilität mit Ethernet I an Stelle des Typfeldes die Länge des Dateninhalts im DATA-Teil angegeben (Längenfeld) sein. Da das Datenfeld in keinem Ethernet Frame länger als 1500 Bytes sein darf, können die Werte 1536 (0x0600) und darüber als Protokolltypen (Ethertype) verwendet werden. Die Verwendung der Werte 1501 bis 1535 ist nicht spezifiziert. Die Verwendung als Länge ist praktisch vollständig verschwunden – um das Ende eines Frames zu signalisieren, verwenden alle Ethernet-Varianten entweder ein spezielles Steuersymbol (100 Mbit/s aufwärts) oder beenden den Trägertakt (10 Mbit/s).
Das Typ-Feld wird als Big-Endian-Byte-Folge interpretiert und mit dem höherwertigen Byte voran verschickt.
Pro Datenblock können maximal 1500 Bytes an Nutzdaten übertragen werden. Die Nutzdaten werden von dem unter Type angegebenen Protokoll interpretiert. So genannte Jumbo Frames, Super Jumbo Frames und Jumbogramme erlauben auch größere Datenblöcke, diese Spezialmodi bewegen sich aber offiziell abseits von Ethernet beziehungsweise IEEE 802.3.
Das PAD-Feld wird verwendet, um den Ethernet-Frame auf die erforderliche Minimalgröße von 64 Byte zu bringen. Das ist bei alten Übertragungsverfahren wichtig, um Kollisionen in der sogenannten Collision-Domain sicher zu erkennen. Präambel und SFD (8 Bytes) werden bei der erforderlichen Mindestlänge des Frames nicht mitgezählt, wohl aber ein VLAN-Tag. Ein PAD-Feld wird somit erforderlich, wenn als Nutzdaten weniger als 46 bzw. 42 Bytes (ohne bzw. mit 802.1Q-VLAN-Tag) zu übertragen sind. Das in Type angegebene Protokoll muss dafür sorgen, dass diese als Pad angefügten Bytes (auch „Padding Bytes“ genannt) nicht interpretiert werden, wofür es üblicherweise eine eigene Nutzdaten-Längenangabe bereithält.
Das FCS-Feld stellt eine 32-Bit-CRC-Prüfsumme dar. Die FCS wird über den eigentlichen Frame berechnet, also beginnend mit der Ziel-MAC-Adresse und endend mit dem PAD-Feld. Die Präambel, der SFD und die FCS selbst sind nicht in der FCS enthalten.
Wenn ein Paket beim Sender erstellt wird, wird eine CRC-Berechnung über die Bitfolge durchgeführt und die Prüfsumme an den Datenblock angehängt. Der Empfänger führt nach dem Empfang die gleiche Berechnung aus. Stimmt die empfangene nicht mit der selbst berechneten Prüfsumme überein, geht der Empfänger von einer fehlerhaften Übertragung aus, und der Datenblock wird verworfen. Zur Berechnung der CRC-32-Prüfsumme werden die ersten 32 Bits der Mac-Adresse invertiert und das Ergebnis der Prüfsummenberechnung wird ebenfalls invertiert (Vermeidung des Nullproblems).
In üblichen CRC-Implementierungen als rückgekoppelte Schieberegister werden Datenbits in übertragener Reihenfolge, also vom LSB zum MSB, durch ein Schieberegister geschickt, das aber selbst vom LSB aus beschickt wird. In Schieberichtung steht damit das MSB der CRC zuerst zur Verfügung und gerät auch in Abweichung zu allen anderen Daten zuerst auf die Leitung. Wird nun der Datenstrom beim Empfänger inklusive empfangenem CRC-Wert in das Schieberegister geschrieben, enthält die CRC im fehlerfreien Fall den Wert Null. Ein von Null abweichender Wert deutet auf einen Übertragungsfehler hin.
Durch die Invertierung der ersten 32 Bit und der CRC-Summe ist das Ergebnis nicht mehr Null. Wenn kein Übertragungsfehler aufgetreten ist, dann enthält das Schieberegister immer dieselbe Zahl, auch Magic Number genannt. Beim Ethernet lautet sie 0xC704DD7B.
Bei Ethernet werden Bytes (Oktette) grundsätzlich mit dem niederstwertigen Bit voran übertragen (mit Ausnahme der Frame Check Sequence). Viele schnellere Varianten übertragen allerdings keine einzelnen Bits, sondern Mehrbit-Symbole oder ganze Oktette in einem Schritt. Felder, die aus mehreren Bytes bestehen, werden grundsätzlich mit dem höchstwertigen Oktett voran übertragen.
Nachdem der Datenstrom als Folge von Bytes bereitgestellt wurde, werden nun abhängig vom physischen Medium und der Übertragungsrate ein oder mehrere Bits in einen Leitungscode kodiert, um einerseits die physischen Eigenschaften des Mediums zu berücksichtigen und andererseits dem Empfänger eine Taktrückgewinnung zu ermöglichen. So wird, je nach Code, die erlaubte Frequenz-Bandbreite nach unten (Gleichspannungsfreiheit) und oben limitiert.
In übertragungsfreien Zeiten, also zwischen zwei Frames, kommt es definitionsgemäß zu Ruhepausen („Inter-Frame-Spacing“) mit einer gewissen Mindestlänge. Bei physischem Halbduplex-Modus schaltet sich in dieser Zeit der Sender ab, um anderen Stationen auf dem geteilten Medium Zugriff zu ermöglichen. Bei moderneren Medientypen mit physischem Vollduplex-Modus wird eine Trägerschwingung aufrechterhalten, die dem Empfänger ein schnelleres Aufsynchronisieren auf den Datenstrom ermöglicht. Außerdem können in der sendefreien Zeit Out-of-Band-Informationen zwischen den Stationen ausgetauscht werden.
Bei manchen physischen Vollduplex-Medientypen wie beispielsweise 10BASE-T deaktiviert sich die Sendestation trotz exklusiven Zugriffs auf das Medium zwischen den Frames. Hier wird die sendefreie Zeit zur Out-of-Band-Signalisierung (Link-Pulse, Fast-Link-Pulse) der Link-Parameter genutzt.
Die verschiedenen Ethernet-Varianten (PHYs) unterscheiden sich in Übertragungsrate, den verwendeten Kabeltypen und der Leitungscodierung. Der Protokollstack arbeitet bei den meisten der folgenden Typen identisch.
Eine erfolgreiche Verbindung zwischen zwei Anschlüssen (Ports) wird als Link bezeichnet. Einige Varianten teilen den Datenstrom in mehrere Kanäle (Lanes) auf, um Datenrate und Frequenzen auf das Medium anzupassen. Die jeweilige Reichweite ist die maximal mögliche Länge eines Links innerhalb der Spezifikation. Bei einer höheren Qualität des Mediums – insbesondere bei Glasfaser – können auch deutlich längere Links stabil funktionieren.
10, 100, 1000, 10G, ... – die nominelle, auf der Bitebene nutzbare Geschwindigkeit (kein Suffix = Megabit/s, G = Gigabit/s); die leitungskodierten Sublayer haben üblicherweise eine höhere Datenrate
-T, -S, -L, -C, -K, ... – Medium: T = Twisted-Pair-Kabel, S = (short) kurze Wellenlänge ca. 850 nm über Multimode-Faser, L = (long) lange Wellenlänge ca. 1300 nm, hauptsächlich Singlemode-Faser, E/Z = extralange Wellenlänge ca. 1500 nm (Singlemode), B = bidirektionale Faser mit WDM (meist Singlemode), P = Passive Optical Network, C = (copper) Twinaxialkabel, K = Backplane, 2/5 = Koaxialkabel mit 185/500 m Reichweite
X, R – PCS-Kodierung (generationsabhängig), zum Beispiel X für 8b/10b Blockkodierung (4B5B bei Fast Ethernet), R für große Blöcke (64b/66b)
1, 2, 4, 10 – Anzahl der Lanes pro Link oder Reichweite bei 100/1000 Mbit/s WAN PHYsBei 10-Mbit/s-Ethernet verwenden alle Varianten durchgehend Manchester-Code, keine Kodierung ist angegeben. Die meisten Twisted-Pair-Varianten verwenden spezielle Kodierungen, nur -T wird angegeben.
Die folgenden Abschnitte geben einen kurzen Überblick über alle offiziellen Ethernet-Medientypen. Zusätzlich zu diesen offiziellen Standards haben viele Hersteller proprietäre Medientypen entwickelt, häufig, um mit Lichtwellenleitern höhere Reichweiten zu erzielen.
Xerox Ethernet (Alto Aloha System) – Der Name entstand dadurch, dass das Konzept auf Alto-Computern getestet wurde. Xerox Ethernet ist die ursprüngliche Ethernet-Implementation, die während ihrer Entwicklung zwei Versionen hatte. Das Datenblock-Format der Version 2 wird zurzeit überwiegend benutzt.
10Broad36 (IEEE 802.3 Clause 11) – Obsolet. Ein früher Standard, der Ethernet über größere Entfernungen unterstützte. Es benutzte Breitband-Modulationstechniken ähnlich denen von Kabelmodems und arbeitete mit Koaxialkabeln.
StarLAN, standardisiert als 1BASE5 (IEEE 802.3 Clause 12) – Die erste Ethernet-Implementation über Twisted-Pair-Kabel, entwickelt von AT&T. 1 Mbit/s über die bereits weit verbreiteten (meist) Cat-3-Verkabelungen mit einer Link-Reichweite von 250 bis 500 m. Ein kommerzieller Fehlschlag, der aber die technische Grundlage für 10BASE-T lieferte.
Beim 10-Mbit/s-Ethernet kommt eine einfache Manchesterkodierung zum Einsatz, die je Datenbit zwei Leitungsbits überträgt (somit 20 MBaud). Mit dieser Verdopplung der Signalisierungsrate und dabei alternierend übertragenen Datenbits wird die Gleichspannung effektiv unterdrückt und gleichzeitig die Taktrückgewinnung im Empfänger nachgeführt, das Spektrum reicht bis 10 MHz. Die Leitung wird nur belegt, wenn ein Ethernet-Paket tatsächlich gesendet wird.
(auch bekannt als Thin Wire Ethernet, Thinnet oder Cheapernet) – Ein Koaxialkabel (RG58) mit einer Wellenimpedanz von 50 Ohm verbindet die Teilnehmer miteinander, jeder Teilnehmer benutzt ein BNC-T-Stück zur Anbindung seiner Netzwerkkarte. An den beiden Leitungsenden angebrachte Abschlusswiderstände sorgen für reflexionsfreie Signalübertragung. Ein Segment (das sind alle durch die BNC-T-Stücke miteinander verbundenen Koaxialkabelstücke) darf maximal 185 Meter lang sein und maximal 30 Teilnehmer versorgen. Jeweils zwei Teilnehmer am Bus müssen zueinander einen Abstand von mindestens 0,5 Meter einhalten. Im Unterschied zum ebenfalls Koaxialkabel verwendenden 10BASE5 sind die Transceiver in der NIC (Network Interface Card) integriert und müssen unmittelbar (ohne weiteres Koaxialkabel) an das T-Stück angeschlossen werden. Über Repeater können weitere Netzwerksegmente angeschlossen werden, sodass die maximale Ausdehnung des Netzwerks 5 Netzwerksegmente in einer Kette umfasst. Mit strukturierter Verkabelung lässt sich die Anzahl der Segmente weiter steigern. Damit ist eine maximale Gesamtausbreitung von 925 m Durchmesser erreichbar. Es wurden auch Ethernet-Anschlussdosen (EAD) verwendet. Bei 10BASE2 fällt das ganze Netzwerksegment aus, wenn an einer Stelle das Kabel oder eine Steckverbindung, insbesondere der Abschlusswiderstand, defekt ist. Besonders anfällig sind manuell konfektionierte Koaxialkabel, wenn bei ihnen der BNC-Stecker nicht korrekt befestigt wurde.
(auch Thicknet oder Yellow Cable) – ein früher IEEE-Standard, der ein 10 mm dickes Koaxialkabel (RG8) mit einer Wellenimpedanz von 50 Ohm verwendet. Zum Anschluss von Geräten muss mittels einer Bohrschablone ein Loch an einer markierten Stelle in das Kabel gebohrt werden, durch das ein Kontakt einer Spezialklemme (Vampirklemme) des Transceivers eingeführt und festgeklammert wird. An diesen Transceiver wird mittels der AUI-Schnittstelle über ein Verbindungskabel die Netzwerkkarte des Computers angeschlossen. Dieser Standard bietet 10 Mbit/s Datenrate bei Übertragung im Basisband und unterstützt auf jedem Segment maximal 500 m Kabellänge und 100 Teilnehmer. Die Leitung hat wie 10BASE2 keine Abzweigungen, und an den Enden sitzen 50-Ohm-Abschlusswiderstände. Wie auch bei 10BASE2 kann über Repeater das Netzwerk bis auf eine max. Länge von 2,5 km ausgedehnt werden. Dieser Typ ist eigentlich obsolet, aber aufgrund seiner weiten Verbreitung in den frühen Tagen noch immer in einigen Systemen in Benutzung.
10BASE-T, IEEE 802.3i Clause 14 – läuft über vier Adern (zwei verdrillte Paare) eines CAT-3 oder CAT-5-Kabels (Verkabelung nach TIA-568A/B). Ein Hub oder Switch sitzt in der Mitte, und jeder Teilnehmer wird über einen dedizierten Port angeschlossen. Die Übertragungsrate ist 10 Mbit/s und die maximale Länge eines Segments 100 Meter. Physisch sind die Steckverbindungen als 8P8C-Modularstecker und -buchsen ausgeführt, die meist als „RJ-45“- bzw. „RJ45“-Stecker/-Buchsen bezeichnet werden. Da normalerweise 1:1-Kabel zum Einsatz kommen, sind die Stecker von Computer (MDI) und Uplink (Hub, Switch, MDI-X) gegengleich belegt. Beim Computer gilt folgende Belegung: Pin1 – Transmit+;   Pin2 – Transmit−;   Pin3 – Receive+;   Pin6 – Receive−.
FOIRL – Fiber-optic inter-repeater link. Der ursprüngliche Standard für Ethernet über Glasfaserkabel.
10BASE-F, IEEE 802.3j (IEEE 802.3 Clause 15) – Allgemeiner Ausdruck für die neue Familie von 10-Mbit/s-Ethernet-Standards: 10BASE-FL, 10BASE-FB und 10BASE-FP. Der einzig weiter verbreitete davon ist 10BASE-FL.
10BASE-FB (IEEE 802.3 Clause 17) – Gedacht für Backbones, die mehrere Hubs oder Switches verbinden. Ist inzwischen technisch überholt.
10BASE-FP (IEEE 802.3 Clause 16) – Ein passives sternförmiges Netzwerk, das keinen Repeater braucht. Es gibt keine Implementationen.
Beim Übergang von 10- auf 100-Mbit/s-Ethernet (Fast Ethernet) wurde die Signalisierungsebene weiter unterteilt, um auf eine klarere Definition dessen zu kommen, was den PHY (die physische Schicht, OSI-Schicht 1) vom MAC trennt. Gab es bei 10-Mbit/s-Ethernet PLS (Physical Layer Signaling, Manchester-Codierung, identisch für alle 10-Mbit/s-Standards) und PMA (Physical Medium Attachment, Coaxial-, Twisted-Pair- und optische Anbindungen), sind es bei Fast Ethernet nunmehr PCS (Physical Coding Sublayer) mit PMA sowie PMD (Physical Medium Dependent). PCS, PMA und PMD bilden gemeinsam die physische Schicht. Es wurden drei verschiedene PCS-PMA-Kombinationen entworfen, von denen jene für 100BASE-T4 und 100BASE-T2 (IEEE 802.3 Clauses 23 und 32) aber keine wirtschaftliche Bedeutung erlangen konnten.
Durchgesetzt hat sich für Kupferkabel einzig 100BASE-TX (IEEE 802.3 Clause 24) für Twisted-Pair-Kabel, das wie die Glasfaser-Varianten statt der Manchesterkodierung den effizienteren 4B5B-Code einsetzt. Dieser ist zwar nicht gleichspannungsfrei, ermöglicht jedoch eine Taktrückgewinnung aus dem Signal und die Symbolrate liegt mit 125 MBaud nur geringfügig über der Datenrate selbst. Die verwendeten Leitungscodeworte garantieren eine für die Bitsynchronisation beim Empfänger ausreichende minimale Häufigkeit von Leitungszustandswechseln. Der Gleichspannungsanteil wird durch die zusätzliche Kodierung mit MLT-3 und mit einem Scrambling-Verfahren entfernt, das auch für ein (statistisch) gleichmäßiges Frequenzspektrum unabhängig von der Leitungsauslastung sorgt. Da es hier keine physischen Busse, sondern nur mehr Punkt-zu-Punkt-Verbindungen gibt, wurde eine kontinuierliche Übertragung favorisiert, die die aufwändigen Einschwingvorgänge des Empfängers auf die Hochfahrphase des Segments beschränkt.
Allgemeine Bezeichnung für die drei 100-Mbit/s-Ethernetstandards über Twisted-Pair-Kabel: 100BASE-TX, 100BASE-T4 und 100BASE-T2 (Verkabelung nach TIA-568A/B). Die maximale Länge eines Segments beträgt wie bei 10BASE-T 100 Meter. Die Steckverbindungen sind als 8P8C-Modularstecker und -buchsen ausgeführt und werden meist mit „RJ-45“ bezeichnet.
100 Mbit/s Ethernet über Category-3-Kabel (wie es in 10BASE-T-Installationen benutzt wird). Verwendet alle vier Aderpaare des Kabels. Es ist inzwischen obsolet, da Category-5-Verkabelung heute die Norm darstellt. Es ist darüber hinaus auf Halbduplex-Übertragung beschränkt.
Es existieren keine Produkte, die grundsätzliche Technik lebt aber in 1000BASE-T weiter und ist dort sehr erfolgreich. 100BASE-T2 bietet 100 Mbit/s Datenrate über Cat-3-Kabel. Es unterstützt den Vollduplexmodus und benutzt nur zwei Aderpaare. Es ist damit funktionell äquivalent zu 100BASE-TX, unterstützt aber ältere Kabelinstallationen.
Benutzt wie 10BASE-T je ein verdrilltes Aderpaar pro Richtung, benötigt allerdings mindestens ungeschirmte Cat-5-Kabel.Die Verwendung herkömmlicher Telefonkabel ist bei eingeschränkter Reichweite möglich. Entscheidend hierbei ist die richtige Zuordnung der beiden Ethernet-Paare zu einem verdrillten Paar des Telefonkabels. Ist das Telefonkabel als Sternvierer verseilt, bilden die gegenüberliegenden Adern jeweils ein Paar.Auf dem 100-Mbit/s-Markt ist 100BASE-TX heute die Standard-Ethernet-Implementation. 100BASE-TX verwendet 4B5B als Leitungscode und zur Bandbreitenhalbierung auf PMD-Ebene die Kodierung MLT-3. Dabei werden nicht nur zwei Zustände (positive oder negative Differenzspannung) auf dem Aderpaar unterschieden, es kommt ein dritter Zustand (keine Differenzspannung) dazu (ternärer Code). Damit wird der Datenstrom mit einer Symbolrate von 125 MBaud innerhalb einer Bandbreite von 31,25 MHz übertragen.Während der 4B5B-Code ausreichend viele Signalwechsel für die Bitsynchronisation beim Empfänger garantiert, kann MLT-3 zur benötigten Gleichspannungsfreiheit nichts beitragen. Als „Killer Packets“ bekannte Übertragungsmuster können dabei das Scrambling kompensieren und dem Übertragungsmuster eine signifikante Gleichspannung überlagern (baseline wander), die die Abtastung erschwert und zu einem Verbindungsabbruch der Endgeräte führt. Um gegen solche Angriffe immun zu sein, implementieren die PHY-Bausteine der Netzwerkkarten daher eine Gleichspannungskompensation.
100 Mbit/s Ethernet über Multimode-Glasfaser. Maximale Segmentlängen über Multi-Mode-Kabel: 400 Meter im Halbduplex-/Repeaterbetrieb, 2000 Meter im Vollduplex-/Switchbetrieb. Der gescrambelte 4B5B-Datenstrom wird direkt über einen optischen Lichtmodulator gesendet und in gleicher Weise empfangen, hierfür wird ein Faserpaar verwendet. Es wird eine Wellenlänge von 1300 nm verwendet, daher ist es nicht mit 10BASE-FL (10 MBit/s über Glasfaser) kompatibel, welches eine Wellenlänge von 850 nm benutzt.
Günstigere Alternative zu 100BASE-FX, da eine Wellenlänge von 850 nm verwendet wird; die Bauteile hierfür sind günstiger. Maximale Segmentlänge: 550 Meter über Multimode-Glasfaser. Durch die verwendete Wellenlänge optional abwärtskompatibel zu 10BASE-FL. Es wird ein Faserpaar benötigt.
Im Gegensatz zu 100BASE-FX, 100BASE-SX und 100BASE-LX10 wird hier Sende- und Empfangsrichtung über eine einzelne Single-Mode-Glasfaser übertragen. Hierfür wird ein Splitter benötigt, welcher die zu sendenden/empfangenden Daten auf die Wellenlängen 1310 und 1550 nm aufteilt. Dieser Splitter kann im Übertragungsbauteil, z. B. einem SFP-Modul, integriert sein. Dieser Standard erzielt Reichweiten von 10 km, erweiterte Versionen 20 oder 40 km.
Bei 1000-Mbit/s-Ethernet (Gigabit-Ethernet; kurz: GbE oder GigE) kommen im Wesentlichen zwei verschiedene Kodiervarianten zum Einsatz. Bei 1000BASE-X (IEEE 802.3 Clause 36) wird der Datenstrom in 8-Bit breite Einheiten zerlegt und mit dem 8b10b-Code auf eine Symbolrate von 1250 MBaud gebracht. Damit wird ein kontinuierlicher, gleichspannungsfreier Datenstrom erzeugt, der bei 1000BASE-CX über einen Transformator auf einem verdrillten Aderpaar zum Empfänger fließt oder bei 1000BASE-SX/LX/ZX die optische Trägerwelle moduliert. Bei 1000BASE-T hingegen wird der Datenstrom in vier Teilströme unterteilt, die jeweils mit PAM-5 und Trellis-Codierung in ihrer Bandbreite geformt und über die vier Aderpaare gleichzeitig gesendet und empfangen werden.
Die beim frühen Fast Ethernet noch weit verbreiteten Repeater Hubs wurden für Gigabit Ethernet anfangs zwar noch im Standard definiert, allerdings wurden keine Hubs hergestellt, so dass der Standard 2007 eingefroren wurde und GbE real ausschließlich über Switches im Vollduplex-Modus existiert.
1000BASE-T, IEEE 802.3 Clause 40 (früher IEEE 802.3ab) – 1 Gbit/s über Kupferkabel ab Cat-5 UTP-Kabel oder besser Cat-5e oder Cat-6 (Verkabelung nach TIA-568A/B). Die maximale Länge eines Segments beträgt wie bei 10BASE-T und 100BASE-TX 100 Meter. Wichtige Merkmale des Verfahrens sind:
Modulationsverfahren PAM-5 (Pulsamplitudenmodulation mit fünf Zuständen) übermittelt zwei Bit pro Schritt und Aderpaar
Übertragungsbandbreite 62,5 MHzIm Grundprinzip ist 1000BASE-T eine „hochskalierte“ Variante des seinerzeit erfolglosen 100BASE-T2, nur dass es doppelt so viele Aderpaare (nämlich alle vier Paare einer typischen Cat-5-Installation) verwendet und die gegenüber Cat-3 größere verfügbare Bandbreite eines Cat-5-Kabels ausnutzt.1000BASE-TX, 1000BASE-T2/4 (nicht in IEEE 802.3 standardisiert) – Erfolglose Versuche verschiedener Interessengruppen, die aufwändigen Modulier/Demodulier- und Echokompensationsschaltungen von 1000BASE-T durch eine höhere Signalisierungsrate auszugleichen. Statt Klasse-D-Verkabelung bei 1000BASE-T benötigen diese Übertragungsverfahren im Gegenzug Installationen nach Klasse E und Klasse F. Das Hauptargument für die Entstehung dieser Übertragungsverfahren, die hohen Kosten für Netzwerkanschlüsse mit 1000BASE-T-Unterstützung, ist längst entkräftet.
1000BASE-CX, IEEE 802.3 Clause 39 – Als Übertragungsmedium werden zwei Aderpaare eines Shielded-Twisted-Pair-Kabels (STP) mit einer maximalen Kabellänge von 25 m und einer Impedanz von 150 Ohm eingesetzt. Der Anschluss erfolgt über 8P8C-Modularstecker/-buchsen (meist als „RJ45“/„RJ-45“ bezeichnet) oder DE-9 in einer Sterntopologie. Im Vergleich zu 1000BASE-T werden bei 1000BASE-CX deutlich höhere Anforderungen an das Kabel gestellt. So ist etwa die verwendete Bandbreite um den Faktor 10 höher (625 MHz gegenüber 62,5 MHz). Die Komponenten sind außerdem zueinander nicht kompatibel.
1000BASE-SX, 1000BASE-LX, IEEE 802.3 Clause 38 (früher IEEE 802.3z) – 1 Gbit/s über Glasfaser. Die beiden Standards unterscheiden sich prinzipiell in der verwendeten Wellenlänge des optischen Infrarot-Lasers und der Art der Fasern: 1000BASE-SX verwendet kurzwelliges Licht mit 850 nm Wellenlänge und Multimode-Glasfasern, bei 1000BASE-LX strahlen die Laser langwelliges Licht mit 1310 nm Wellenlänge aus. Die Länge eines Glasfaserkabels muss mindestens 2 Meter betragen, die maximale Ausbreitung hängt von der Charakteristik der verwendeten Glasfaser ab. Multimode-Glasfaserkabel können je nach Faserquerschnitt und modaler Dämpfung zwischen 200 und 550 Meter erreichen, während 1000BASE-LX auf Singlemode-Glasfaserkabel bis 5 km spezifiziert sind.
1000BASE-LX10, manchmal auch 1000BASE-LH (LH steht für Long Haul) – Zum Einsatz kommen hierbei Singlemode-Glasfaserkabel mit einer maximalen Länge von 10 km. Die restlichen Eigenschaften gleichen denen von 1000BASE-LX.
1000BASE-BX10 verwendet eine einzige Singlemode-Faser mit bis zu 10 km Reichweite mit je Richtung verschiedenen Wellenlängen: downstream 1490 nm, upstream 1310 nm.
1000BASE-EX und -ZX sind keine IEEE-Standards – Zum Einsatz kommen Singlemode-Glasfaserkabel mit einer maximalen Länge von 40 km (-EX) bzw. 70 km (-ZX). Das verwendete Licht hat eine Wellenlänge von 1550 nm.
2.5GBASE-T und 5GBASE-T, auch 2.5GbE und 5GbE abgekürzt und bisweilen zusammen NBASE-T oder MGBASE-T genannt, wird wie 1000BASE-T oder 10GBASE-T über Kupferkabel übertragen.Effektiv sind 2.5GBASE-T und 5GBASE-T herunterskalierte Versionen von 10GBASE-T mit 25 % und 50 % der Signalrate. Durch die niedrigeren Frequenzen ist es möglich, geringerwertiges Kabel als das für 10GBASE-T notwendige Cat6A zu verwenden.
Hierbei dient für 2.5G eine Verkabelung mindestens nach Cat5e und für 5G eine nach mindestens Cat6.
Als IEEE 802.3bz offiziell verabschiedet, gab es bereits vorher Produkte von einigen Herstellern, darunter Broadcom, Intel und Marvell.
Der 10-Gbit/s-Ethernet-Standard (kurz: 10GbE oder 10GE) bringt zehn unterschiedliche Übertragungstechniken, acht für Glasfaserkabel und zwei für Kupferkabel mit sich. 10-Gbit/s-Ethernet wird für LAN, MAN und WAN verwendet. Der Standard für die Glasfaserübertragung heißt IEEE 802.3ae, die Standards für Kupfer sind IEEE 802.3ak und IEEE 802.3an.
Multimode10GBASE-SR überbrückt kurze Strecken über Multimode-Fasern, dabei wird langwelliges Licht mit einer Wellenlänge von 850 nm verwendet. Die Reichweite ist dabei abhängig vom Kabeltyp, so reichen 62,5 µm „FDDI-grade“ Fasern bis zu 26 m, 62,5-µm/OM1-Fasern bis zu 33 m weit, 50 µm/OM2 bis zu 82 m und 50 µm/OM3 bis zu 300 m.
10GBASE-LRM (Long Reach Multimode) verwendet eine Wellenlänge von 1310 nm, um über alle klassischen Multimode-Fasern (62,5 µm Fiber „FDDI-grade“, 62,5 µm/OM1, 50 µm/OM2, 50 µm/OM3) eine Distanz von bis zu 220 m zu überbrücken.
10GBASE-LX4 (Clause 53) nutzt Wellenlängenmultiplexierung, um Reichweiten zwischen 240 m und 300 m über die Multimode-Fasern OM1, OM2 und OM3 oder 10 km über Singlemode-Faser zu ermöglichen. Hierbei wird gleichzeitig auf den Wellenlängen 1275, 1300, 1325 und 1350 nm übertragen.Singlemode10GBASE-LW4 überträgt mit Hilfe von Singlemode-Fasern Licht der Wellenlänge 1310 nm über Distanzen bis zu 10 km.
10GBASE-LR verwendet eine Wellenlänge von 1310 nm, um über Singlemode-Fasern eine Distanz von bis zu 10 km zu überbrücken.
10GBASE-ER benutzt wie 10GBASE-LR Singlemode-Fasern zur Übertragung, jedoch bei einer Wellenlänge von 1550 nm, was die Reichweite auf bis zu 40 km erhöht. Da 10GBASE-ER mit dieser Wellenlänge die seltene Eigenschaft besitzt, kompatibel zu CWDM-Infrastrukturen zu sein, vermeidet er den Austausch der bestehenden Technik durch DWDM-Optik.OC-192 – STM-64Die Standards 10GBASE-SW, 10GBASE-LW und 10GBASE-EW benutzen einen zusätzlichen WAN-Phy, um mit OC-192- (SONET) bzw. STM-64-Equipment (SDH) zusammenarbeiten zu können. Der Physical Layer entspricht dabei 10GBASE-SR bzw. 10GBASE-LR bzw. 10GBASE-ER, benutzen also auch die gleichen Fasertypen und erreichen die gleichen Reichweiten. Zu 10GBASE-LX4 gibt es keine entsprechende Variante mit zusätzlichem WAN-Phy.Im LAN erreichen bedingt durch die Verfügbarkeit der Produkte die Standards 10GBASE-SR und 10GBASE-LR eine steigende Verbreitung.
Der Vorteil von Kupferverkabelung gegenüber Glasfasersystemen liegt in der schnelleren Konfektionierung und der unterschiedlichen Nutzbarkeit der Verkabelung (viele Anwendungen über ein Kabel). Darüber hinaus ist die Langlebigkeit von Kupfersystemen nach wie vor höher als bei Glasfasersystemen (Ausbrennen und Verschleiß der LEDs/Laser) und die Kosten bei zusätzlich notwendiger (teurer) Elektronik.
10GBASE-CX4 nutzt doppelt-twinaxiale Kupferkabel (wie InfiniBand), die eine maximale Länge von 15 m haben dürfen. Dieser Standard war lange der einzige für Kupferverkabelung mit 10 Gbit/s, verliert allerdings zunehmend an Bedeutung durch 10GBASE-T, das zu den langsameren Standards abwärtskompatibel ist und bereits vorhandene Verkabelung nutzen kann.
10GBASE-T verwendet wie schon 1000BASE-T vier Paare aus verdrillten Doppeladern. Die dafür verwendete strukturierte Verkabelung wird im globalen Standard ISO/IEC 11801 sowie in TIA-568A/B beschrieben. Die zulässige Linklänge ist vom eingesetzten Verkabelungstyp abhängig: Um die angestrebte Linklänge von 100 m zu erreichen, sind die Anforderungen von CAT-6a/7 zu erfüllen. Mit den für 1000BASE-T eingesetzten CAT-5-Kabeln (Cat-5e) ist nur die halbe Linklänge erreichbar. Der Standard ist in 802.3an beschrieben und wurde Mitte 2006 verabschiedet.
Bei der Übertragung wird der Datenstrom auf die vier Aderpaare aufgeteilt, so dass auf jedem Aderpaar jeweils 2,5 Gbit/s in Senderichtung und in Empfangsrichtung übertragen werden. Wie bei 1000BASE-T wird also jedes Aderpaar im Vollduplex-Betrieb genutzt. Zur Codierung werden die Modulationsverfahren 128-DSQ (eine Art doppeltes 64QAM) und schließlich PAM16 verwendet, wodurch die Nyquistfrequenz auf 417 MHz reduziert wird.Durch die hohe Signalrate mussten verschiedene Vorkehrungen getroffen werden, um die Übertragungssicherheit zu gewährleisten. Störungen innerhalb des Kabels werden passiv durch einen Kreuzsteg im Kabel vermindert, der für Abstand zwischen den Aderpaaren sorgt. Zusätzlich werden in den aktiven Komponenten digitale Signalprozessoren verwendet, um die Störungen herauszurechnen.
So genanntes Fremdübersprechen (Alien Crosstalk), also das Nebensprechen benachbarter, über längere Strecken eng gebündelter, ungeschirmter Kabel, kann auf diese Weise jedoch nicht verhindert werden. Deshalb sind in den Normen Kabel der Kategorie Cat 6A (Klasse EA) vorgesehen. Diese sind entweder geschirmt oder unterdrücken anderweitig (z. B. durch dickeren oder speziell geformten Mantel) das Fremdübersprechen ausreichend. Ungeschirmte Cat 6 Kabel (Klasse E) erreichen bei enger Bündelung (und nur dann) nicht die üblichen 100 m Leitungslänge. Zum anderen ist ein Mindestabstand der Steckverbindungen zueinander einzuhalten.
Converged 10 GbE ist ein Standard für Netzwerke bei denen 10 GbE und 10 GbFC verschmolzen sind. Zum Converged-Ansatz gehört auch das neue Fibre Channel over Ethernet (FCoE). Das sind FC-Pakete, die in Ethernet gekapselt sind und für die dann ebenfalls die Converged Ethernet-Topologie genutzt werden kann, z. B. sind dann entsprechend aktualisierte Switches (wegen Paketgrößen) transparent für FC- und iSCSI-Storage sowie für das LAN nutzbar.
25 Gigabit (25GbE) und 50 Gigabit Ethernet (50GbE) wurden von einem Industriekonsortium zur Standardisierung vorgeschlagen und von IEEE 802.3 in Form einer Study Group untersucht.25/50GbE sollen in Rechenzentren höhere Leistungen als 10GbE zu deutlich geringeren Kosten als 40GbE bereitstellen indem Technologie verwendet wird, die bereits für diejenigen 100GbE-Varianten definiert wurde, die auf 25-Gbit/s-Lanes basieren (IEEE 802.3bj). Außerdem lassen sich 25/50-Gbit/s-Verbindungen direkt auf 100 Gbit/s skalieren. Zusätzlich könnte das höhere Fertigungsvolumen von 25-Gbit/s-Komponenten zu einem schnelleren Preisverfall im 100-Gbit/s-Bereich führen.
Die bisher schnellste Generation unterstützt 40 und 100 Gbit/s sowohl über Kupferkabel (Twinax) als auch über Glasfaserkabel (single- und multimode).
Die Angaben entstammen der Spezifikation 802.3ba-2010 des IEEE und definieren folgende Reichweiten (Leitungen je Richtung):
40GBASE-CR4 40 Gbit/s (40GBASE-R mit 4 Leitungen eines geschirmten Twinax-Kupferkabels) mindestens 7 m
40GBASE-LR4 40 Gbit/s (40GBASE-R mit 1 OS2-Glasfaser und vier Farben/Wellenlängen, singlemode, CWDM) mindestens 10 km
100GBASE-CR10 100 Gbit/s (100GBASE-R mit 10 Leitungen eines geschirmten Twinax-Kupferkabels) mindestens 7 m
100GBASE-SR4 100 Gbit/s (100GBASE-R mit 4 OM4-Glasfasern, multimode) mindestens 100 m (IEEE 802.3bm)
100GBASE-LR4 100 Gbit/s (100GBASE-R mit 1 OS2-Glasfaser und vier Farben, singlemode) mindestens 10 km
100GBASE-ER4 100 Gbit/s (100GBASE-R mit 1 OS2-Glasfaser und vier Farben, singlemode) mindestens 40 km
Geschwindigkeiten und erwartete Standards schneller als 100 Gbit/s werden manchmal auch als Terabit Ethernet bezeichnet.
Im März 2013 begann die IEEE 802.3 400 Gb/s Ethernet Study Group mit der Arbeit an der nächsten Generation mit 400 Gbit/s, im März 2014 wurde die IEEE 802.3bs 400 Gb/s Ethernet Task Force gebildet. Im Januar 2016 wurde als zusätzliches Entwicklungsziel 200 Gbit/s hinzugefügt. Die neuen Standards wurden im Dezember 2017 veröffentlicht:
200GBASE-LR4 (Clause 122): 10 km über Monomodefaser, je vier Wellenlängen/Farben (CWDM)400 Gbit/s400GBASE-FR8 (Clause 122): 2 km über Monomodefaser, je acht Wellenlängen/Farben (CWDM)
Die zulässige Gesamtlänge der Übertragungsstrecke beträgt in der Regel 100 m. Darin enthalten sind:
2 Steckverbindungen (z. B. Dose und Patchfeld)Patchkabel haben schlechtere Übertragungseigenschaften. Sind die Patchkabel länger als 10 m, reduziert sich für jeden Meter Überschreitung die zulässige Länge des Installationskabels um jeweils 1,5 m.Besteht die Strecke nur aus Patchkabeln, ist die zulässige Regellänge ca. 70 m.
Wenn nicht anders angegeben, gelten die Längen für geschirmte und ungeschirmte Kabel gleichermaßen.
Die Werte für 10-Gbit/s-Ethernet entsprechen IEE 802.3-2008, Tabelle 55-13.*) Reduzierte Längen bei 10 Gbit/s ergeben sich durch Fremdübersprechen zwischen mehreren Kabeln und gelten nur ungeschirmt bei enger Bündelung über viele Meter Länge.
Der Wert für 10 Gbit/s über Cat 5e wurde in einem Entwurf vorgeschlagen, aber nicht in die endgültige IEEE 802.3 Norm übernommen. Allerdings bestätigen zahlreiche Hardwarehersteller die Funktion über 45 m Cat 5e UTP.Geschirmtes CAT 5e ist außerhalb von Europa ungebräuchlich und wurde von dem US-dominierten Gremium nicht getestet. Es ergeben sich dafür erheblich größere Längen, weil der längenbegrenzende Parameter das Fremdübersprechen ist. Geschirmte Kabel sind davon jedoch praktisch nicht betroffen.
Metro Ethernet Netze (MEN) sind ethernetbasierte Metropolitan Area Network (MAN) Netze, die auf Carriergrade-Ethernet basieren. Nachdem mit der Einführung ausgefeilter Glasfasertechniken die Längenbeschränkungen für Ethernet-Netze praktisch aufgehoben sind, gewinnt Ethernet auch bei Weitverkehrsnetzen wie den MAN an Bedeutung. MANs basieren vor allem auf Kundenseite auf kostengünstiger bekannter Technik und garantieren eine vergleichsweise hohe Effizienz bei geringer Komplexität.
Ebenfalls zur Familie der Ethernet-Standards gehört IEEE 802.3af (IEEE 802.3 Clause 33). Das Verfahren beschreibt, wie sich Ethernet-fähige Geräte über das Twisted-Pair-Kabel mit Energie versorgen lassen. Dabei werden entweder die ungenutzten Adern der Leitung verwendet, oder es wird zusätzlich zum Datensignal ein Gleichstromanteil über die vier verwendeten Adern übertragen. Eine Logik stellt sicher, dass nur PoE-fähige Geräte mit Energie versorgt werden. Gemäß 802.3af werden entsprechend ausgelegte Geräte mit 48 V und bis zu 15,4 Watt versorgt. Bis zu 30 W bei 54 V erreicht der Ende 2009 ratifizierte Standard 802.3at oder PoE+. 2018 wurde die dritte Generation 4PPoE als 802.3bt verabschiedet, die über alle vier Leitungspaare Geräte mit bis zu 100 W versorgen kann.
Folgende Netzwerkstandards gehören nicht zum IEEE-802.3-Ethernet-Standard, unterstützen aber das Ethernet-Datenblockformat und können mit Ethernet zusammenarbeiten:
WLAN (IEEE 802.11) – Eine Technik zur drahtlosen Vernetzung per Funktechnik auf kurzen Strecken (Distanzen sind von den örtlichen Gegebenheiten abhängig und vergleichbar mit LAN), anfänglich mit Übertragungsraten ab 1 Mbit/s, aktuell (2010) mit bis zu 600 Mbit/s.
VG-AnyLan (IEEE 802.12) oder 100BASE-VG – Ein früher Konkurrent zu 100-Mbit/s-Ethernet und 100-Mbit/s-TokenRing. Ein Verfahren das Multimedia-Erweiterungen besitzt und beispielsweise wie FDDI garantierte Bandbreiten kennt, es basiert auf einem Demand Priority genannten Zugriffsverfahren (Demand Priority Access Methode, kollisionsfrei, alle Zugriffe werden priorisiert vom Hub/Repeater zentral gesteuert), womit die Nachteile von CSMA eliminiert werden. 100BASE-VG läuft auch über Kategorie-3-Kabel, benutzt dabei aber vier Aderpaare. Federführend bei der Entwicklung waren Hewlett-Packard und AT&T beteiligt, kommerziell war VG-AnyLan ein Fehlschlag.
TIA 100BASE-SX – Von der Telecommunications Industry Association geförderter Standard. 100BASE-SX ist eine alternative Implementation von 100-Mbit/s-Ethernet über Glasfaser und ist inkompatibel mit dem offiziellen 100BASE-FX-Standard. Eine hervorstehende Eigenschaft ist die mögliche Interoperabilität mit 10BASE-FL, da es Autonegotiation zwischen 10 Mbit/s und 100 Mbit/s beherrscht. Die offiziellen Standards können das aufgrund unterschiedlicher Wellenlängen der verwendeten LEDs nicht. Zielgruppe sind Organisationen mit einer bereits installierten 10-Mbit/s-Glasfaser-Basis.
TIA 1000BASE-TX stammt ebenfalls von der Telecommunications Industry Association. Der Standard war ein kommerzieller Fehlschlag, und es existieren keine Produkte, die ihn umsetzen. 1000BASE-TX benutzt ein einfacheres Protokoll als der offizielle 1000BASE-T-Standard, benötigt aber Cat-6-Kabel (Gegner behaupten, dieser primär von der Kabelindustrie geförderte Standard sei gar nicht zur Produktentwicklung gedacht gewesen, sondern ausschließlich dafür, um eine erste Anwendung für diese bis dahin mit keinerlei Vorteilen gegenüber Cat-5 ausgestattete Kabelklasse vorweisen zu können).
InfiniBand ist ein bereits seit 1999 spezifiziertes schnelles Hochleistungsverfahren zur Überbrückung kurzer Strecken (über Kupferkabel bis zu 15 m). Es nutzt bidirektionale Punkt-zu-Punkt-Verbindungen zur kostengünstigen und latenzarmen Datenübertragung (unter 2 µs) und schafft pro Kanal theoretische Datenübertragungsraten von bis zu 2,5 Gbit/s in beide Richtungen und in der neueren DDR-Variante 5 Gbit/s. Bei InfiniBand können mehrere Kanäle transparent gebündelt werden, wobei dann ein gemeinsames Kabel verwendet wird. Üblich sind vier Kanäle (4×) also 10 Gbit/s bzw. 20 Gbit/s. Haupteinsatzgebiet sind Supercomputer (HPC-Cluster) wie sie auch in der TOP500-Liste zu finden sind.
PHY (physikalische Schnittstelle), Schaltkreis zur Datencodierung zwischen digitalen und analogen Systemen.
BroadR-Reach, ein Ethernet-Physical-Layer-Standard für Connectivity-Anwendungen im Automobilbereich.
Charles E. Spurgeon: Ethernet. The Definitive Guide. O’Reilly, Sebastopol CA 2000, ISBN 1-56592-660-9. 
Wolfgang Kemmler, Mathias Hein: Gigabit-Ethernet; Der Standard - die Praxis. FOSSIL-Verlag, 1998, ISBN 978-3-931959-10-4.

Die Ethik (griechisch ἠθική (ἐπιστήμη) ēthikē (epistēmē) „das sittliche (Verständnis)“, von ἦθος ēthos „Charakter, Sinnesart“ (dagegen ἔθος: Gewohnheit, Sitte, Brauch), vergleiche lateinisch mos) ist jener Teilbereich der Philosophie, der sich mit den Voraussetzungen und der Bewertung menschlichen Handelns befasst. Im Zentrum der Ethik steht das spezifisch moralische Handeln, insbesondere hinsichtlich seiner Begründbarkeit und Reflexion. Cicero übersetzte als erster êthikê in den seinerzeit neuen Begriff philosophia moralis. In seiner Tradition wird die Ethik auch als Moralphilosophie bezeichnet.
Die Ethik und ihre benachbarten Disziplinen (z. B. Rechts-, Staats- und Sozialphilosophie) werden auch als „praktische Philosophie“ zusammengefasst, da sie sich mit dem menschlichen Handeln befasst. Im Gegensatz dazu steht die „theoretische Philosophie“, zu der als klassische Disziplinen die Logik, die Erkenntnistheorie und die Metaphysik gezählt werden.
Als Bezeichnung für eine philosophische Disziplin wurde der Begriff Ethik von Aristoteles eingeführt, der damit die wissenschaftliche Beschäftigung mit Gewohnheiten, Sitten und Gebräuchen (ethos) meinte, wobei allerdings schon seit Sokrates die Ethik ins Zentrum des philosophischen Denkens gerückt war (Sokratische Wende). Hintergrund war dabei die bereits von den Sophisten vertretene Auffassung, dass es für ein Vernunftwesen wie den Menschen unangemessen sei, wenn dessen Handeln ausschließlich von Konventionen und Traditionen geleitet wird. Aristoteles war der Überzeugung, menschliche Praxis sei grundsätzlich einer vernünftigen und theoretisch fundierten Reflexion zugänglich. Ethik war somit für Aristoteles eine philosophische Disziplin, die den gesamten Bereich menschlichen Handelns zum Gegenstand hat und diesen Gegenstand mit philosophischen Mitteln einer normativen Beurteilung unterzieht und zur praktischen Umsetzung der auf diese Weise gewonnenen Erkenntnisse anleitet.
Die allgemeine Ethik – die im Folgenden einfach als Ethik bezeichnet wird – wird heute als eine philosophische Disziplin verstanden, deren Aufgabe es ist, Kriterien für gutes und schlechtes Handeln und die Bewertung seiner Motive und Folgen aufzustellen. Sie ist die Grundlagendisziplin der Angewandten Ethik, die sich als Individualethik und Sozialethik sowie in den Bereichsethiken mit den normativen Problemen des spezifischen Lebensbereiches befasst.
Die Ethik baut als philosophische Disziplin allein auf das Prinzip der Vernunft. Darin unterscheidet sie sich vom klassischen Selbstverständnis theologischer Ethik, die sittliche Prinzipien als in Gottes Willen begründet annimmt und insofern im Allgemeinen den Glauben an eine göttliche Offenbarung voraussetzt. Besonders im 20. Jahrhundert haben allerdings Autoren wie Alfons Auer theologische Ethik als weitgehend autonom zu konzipieren versucht.
Das Ziel der Ethik ist die Erarbeitung von allgemeingültigen Normen und Werten. Sie ist abzugrenzen von einer deskriptiven Ethik, die keine moralischen Urteile fällt, sondern die tatsächliche, innerhalb einer Gesellschaft gelebte Moral mit empirischen Mitteln zu beschreiben versucht. Die Metaethik, die sich zu Beginn des 20. Jahrhunderts als eigenständige Disziplin entwickelte, reflektiert die allgemeinen logischen, semantischen und pragmatischen Strukturen moralischen und ethischen Sprechens und stellt insofern die Grundlage für die deskriptive und normative Ethik dar.
Das Recht als Konfliktsentscheidung ist nicht denkbar ohne Anschluss an die Ethik. Die Lehre vom subjektiven Recht beschreibt den Zusammenhang von Ethik und Recht.
Die philosophische Disziplin Ethik (die auch als Moralphilosophie bezeichnet wird, lateinisch philosophia moralis, früher auch Philosophie der Sitten genannt.) beschäftigt  sich  überwiegend mit den folgenden drei Problemfeldern:
2. mit der Frage nach dem richtigen Handeln in bestimmten Situationen – also: „Wie soll ich mich in dieser Situation verhalten?“ (Die einfachste und klassische Formulierung einer solchen Frage stammt von Immanuel Kant: „Was soll ich tun?“) und
Als Hauptgegenstand der Ethik gelten den meisten Philosophen die menschlichen Handlungen und die sie leitenden Handlungsregeln. Die Ergebnisse bestehen in anwendbaren ethischen (bzw. moralischen) Normen, die beinhalten, dass unter bestimmten Bedingungen bestimmte Handlungen geboten, verboten oder erlaubt sind.
Insofern als in der Ethik nach allgemeingültigen Antworten auf die Frage nach dem richtigen Handeln gesucht wird, stellt sich die Frage nach der Möglichkeit allgemeingültiger ethischer Normen und deren Begründung. Diese Diskussion über die Grundlagen der Ethik, ihre Kriterien und Methoden, ist ein wichtiger Teil der philosophischen Disziplin Ethik, der auch als Metaethik bezeichnet wird.
Die Ethik ist von ihrer Zielsetzung her eine praktische Wissenschaft. Es geht ihr nicht um ein Wissen um seiner selbst willen (theoria), sondern um eine verantwortbare Praxis. Sie soll dem Menschen (in einer immer unüberschaubarer werdenden Welt) Hilfen für seine sittlichen Entscheidungen liefern. Dabei kann die Ethik allerdings nur allgemeine Prinzipien guten Handelns oder ethischen Urteilens überhaupt oder Wertvorzugsurteile für bestimmte Typen von Problemsituationen begründen. Die Anwendung dieser Prinzipien auf den einzelnen Fall ist im Allgemeinen nicht durch sie leistbar, sondern Aufgabe der praktischen Urteilskraft und des geschulten Gewissens. Aristoteles vergleicht dies mit der Kunst des Arztes und des Steuermanns. Diese verfügen über ein theoretisches Wissen, das aber situationsspezifisch angewendet werden muss. Entsprechend muss auch die praktische Urteilskraft allgemeine Prinzipien immer wieder auf neue Situationen und Lebenslagen anwenden. Dabei spielt für die richtige sittliche Entscheidung neben der Kenntnis allgemeiner Prinzipien die Schulung der Urteilskraft in praktischer Erfahrung eine wichtige Rolle.
Auch die Theorie der rationalen Entscheidung beantwortet die Frage: Wie soll ich handeln? Jedoch unterscheidet sie sich von ethischen Fragestellungen dadurch, dass Theorien rationalen Handelns nicht in jedem Falle auch Theorien des moralisch Guten sind. Von ethischen Theorien mit einem allgemeinverbindlichen Anspruch unterscheiden sich Theorien rationaler Entscheidung dadurch, dass nur die Ziele und Interessen eines bestimmten Individuums oder eines kollektiven Subjekts (z. B. eines wirtschaftlichen Unternehmens oder eines Staates) berücksichtigt werden. Zur Unterscheidung zwischen Ethik und Moral kann auf Hegel verwiesen werden und seinen Versuch einer Synthese aus dem klassischen Gemeinschafts- und dem modern-individualistischen Freiheitsdenken.Auch die Rechtswissenschaft fragt danach, wie gehandelt werden soll. Im Unterschied zur Ethik (welche seit Christian Thomasius und Kant von der Rechtslehre unterschieden wird) bezieht sie sich jedoch i. Allg. auf eine bestimmte, faktisch geltende Rechtsordnung (positives Recht), deren Normen sie auslegt und anwendet. Wo die Rechtswissenschaft als Rechtsphilosophie, Rechtspolitik oder Gesetzgebungslehre auch die Begründung von Rechtsnormen behandelt, nähert sie sich der Ethik an.
Auch religiös motivierte Ethiken geben Antworten auf die Frage, wie gehandelt werden soll. Im Unterschied zu philosophisch begründeten Ethiken beanspruchen diese jedoch nicht in jedem Fall, dass ihre Antworten auf für jeden nachvollziehbare Argumente gegründet sind, sondern können sich etwa auf eine göttliche Offenbarung als Quelle von Handlungsnormen berufen (siehe etwa die Sollens-Aussagen der Zehn Gebote im Judentum).
Mit gesellschaftlichen Normen des Handelns befassen sich auch empirische Wissenschaften wie Soziologie, Ethnologie und Psychologie. Im Unterschied zur normativen Ethik im philosophischen Sinne geht es dort jedoch um die Beschreibung und Erklärung faktisch bestehender ethischer Überzeugungen, Einstellungen und Sanktionsmuster und nicht um deren Rechtfertigung oder Kritik.
Die Frage, ob man überhaupt moralisch sein soll, wird in Platons Politeia im ersten Kapitel aufgeworfen. In der Moderne wurde der Diskurs um die Frage von Bradley und Prichard eingeleitet.
Metaethische Kognitivisten behaupten, erkennen zu können, wie man moralisch handeln solle. Somit stellt sich ihnen die Frage, ob man das überhaupt tun soll, nicht mehr, da sie auch gleich mit erkennen, dass man dies tun soll.
Die Diskussion wird in der Philosophie zumeist anhand der Frage „Warum soll man moralisch sein?“ geführt. Das Sollen innerhalb der Frage ist dabei kein moralisches Sollen, sondern verweist auf eine Akzeptanz besserer Gründe, z. B. anhand der Theorie der rationalen Entscheidung. Die Antwort auf die Frage hängt also ab vom jeweiligen Verständnis von Vernunft.
„Das muss jeder für sich selbst entscheiden“ von Dezisionisten.Die Situation des Menschen, der sich zwischen diesen Antworten entscheiden muss, hat ihre klassische Gestaltung in der so genannten Prodikos-Fabel von Herakles am Scheideweg gefunden, die auch von vielen christlichen Autoren rezipiert wurde.
Eine bekannte absolute Moralbegründung ist die der Letztbegründung von Apel. Angenommen jemand lehnt es ab, über Zwecke zu reden, dann sei diese Ablehnung bereits ein Reden über Zwecke. Insofern ist dies ein so genannter performativer Selbstwiderspruch.
Moralbegründung aus Sicht der Systemtheorie verzichtet darauf, zu begründen, warum Individuen moralisch handeln sollen. Stattdessen wird dargelegt, warum Moral als Regulierungsfunktion des Kommunikationssystems unentbehrlich ist (s. a. AGIL-Schema).
Viele Philosophen behaupten, dass man zwar nicht beweisen kann, dass Amoralismus logisch widersprüchlich ist, dass aber im wirklichen Leben Amoralisten viele Nachteile haben, so dass moralisches Verhalten größere Rentabilität im Sinne der Theorie der rationalen Entscheidung besitzt. Ethik wird mit dieser Form von Moralbegründung zu einer Spezialform von Zweckrationalität. Einer der wichtigsten Vertreter dieser Argumentationslinie ist David Gauthier.
Viele Philosophen dieser Richtung berufen sich auf den Grundsatz quid pro quo oder auf Tit for Tat-Strategien.
Andere meinen, Amoralisten seien auf Einsamkeit festgelegt, da man ihnen nicht vertrauen könne und auch sie niemandem vertrauen könnten. Daher könnten sie eines der wichtigsten Lebensgüter, soziale Gemeinschaft und Anerkennung, nie erreichen.
Nach R. M. Hare können Amoralisten keine moralischen Begriffe gebrauchen und daher nicht von ihren Mitmenschen fordern, sie fair zu behandeln. Die Möglichkeit entsprechender Lügen sah Hare nicht. Hare behauptete zudem, der Aufwand, den Amoralisten treiben müssten, um ihre Überzeugung zu verschleiern, wäre so groß, dass sie sozial immer im Nachteil seien.
Amoralisten kritisieren verschiedene Moralbegründungen, indem sie darauf verweisen, dass es in vielen Teilen der Welt relativ stabile Verhältnisse gibt, die üblichen moralischen Vorstellungen widersprechen, z. B. völkerrechtswidrige Kriege um Ressourcen, Sklaverei oder erfolgreiche Mafia-Organisationen.
Dezision (von latein decidere: entscheiden, fallen, abschneiden) bedeutet so viel wie Entscheidung.
Der Begriff des Dezisionismus wird oft in pejorativer Bedeutung gebraucht von Metaethischen Kognitivisten gegenüber Philosophen, die nur relative Begründungen der Moral anerkennen, z. B. Hare oder Popper und Hans Albert.
Dezisionisten sehen keine Alternative zu Prinzipienentscheidungen, die aus logischen oder pragmatischen Gründen ihrerseits nicht mehr weiter begründet werden können. So behauptete z. B. Henry Sidgwick, der Mensch müsse sich zwischen Utilitarismus und Egoismus entscheiden.
Dem Dezisionismus wird von seinen Kritikern ähnlich wie dem metaethischen Nonkognitivismus entgegengehalten, dass auch Entscheidungen wiederum einer Bewertung unterzogen werden könnten: Man entscheide sich nicht für bestimmte ethische Prinzipien, sondern diese würden umgekehrt die Grundlage von Entscheidungen darstellen.
Außerdem argumentieren Vertreter des Naturrechts dafür, dass sich die Objektivität der Ethik (also das Sollen) auf die Natur bzw. das Wesen des Seienden und letztlich auf das Sein selbst (z. B. Gott) zurückführen ließen.
Im Mittelpunkt deontologischer Ethiken steht der Begriff der Handlung. Sie wird in erster Annäherung definiert als „eine von einer Person verursachte Veränderung des Zustands der Welt“. Die Veränderung kann eine äußere, in Raum und Zeit beobachtbare oder eine innere, mentale Veränderung sein. Auch die Art und Weise, wie man von außen einwirkenden Ereignissen begegnet, kann im weiteren Sinne als Handlung bezeichnet werden.
Handlungen unterscheiden sich von Ereignissen dadurch, dass wir als ihre Ursache nicht auf ein weiteres Ereignis verweisen, sondern auf die Absicht des Handelnden. Die Absicht (intentio; nicht zu verwechseln mit dem juristischen Absichtsbegriff, dem dolus directus 1. Grades) ist ein von der Handlung selbst zu unterscheidender Akt. Geplanten Handlungen liegt eine zeitlich vorausgehende Absicht zugrunde. Wir führen die Handlung so aus, wie wir sie uns vorher schon vorgenommen hatten. Der Begriff der Absicht ist von dem der Freiwilligkeit zu unterscheiden. Die Freiwilligkeit ist eine Eigenschaft, die zur Handlung selbst gehört. Der Begriff der Freiwilligkeit ist weiter als der der Absicht; er umfasst auch die spontanen Handlungen, bei denen man nicht mehr von Absicht im engeren Sinne sprechen kann.
Die Unwissenheit kann dabei allerdings nur dann die Freiwilligkeit einer Handlung aufheben, wenn die handelnde Person sich nach besten Kräften vorher informiert hat, und sie mit dem ihr fehlenden Wissen anders gehandelt hätte. War dem Handelnden eine Kenntnis der Norm oder der Folgen zuzumuten, ist er für ihre Übertretung verantwortlich (ignorantia crassa oder supina). Noch weniger entschuldigt jene Unkenntnis, die absichtlich zum Vermeiden eines Konflikts mit der Norm herbeigeführt wurde (ignorantia affectata), wenn also z. B. bewusst vermieden wird, sich über ein Gesetz zu informieren, um sagen zu können, man hätte von einem bestimmten Verbot nicht gewusst. Das Sprichwort sagt zu Recht: „Unwissenheit schützt vor Strafe nicht“. Auch im deutschen Strafrecht wird diesem Sachverhalt Rechnung getragen. So heißt es z. B. in § 17 StGB:
Für die sittliche Bewertung einer Handlung ist außerdem das effektive Wollen wesentlich, die Absicht ihrer Verwirklichung. Das setzt voraus, dass zumindest der Handelnde der Auffassung war, dass ihm eine Verwirklichung seiner Absicht möglich sei, d. h. dass das Ergebnis von seinem Handeln kausal herbeigeführt werden könne. Unterliegt der Handelnde einem äußeren Zwang, hebt dieser die Freiwilligkeit der Handlung im Allgemeinen auf.
Absichten finden ihren Ausdruck in praktischen Grundsätzen. Diese können zunächst einmal in inhaltliche und formale Grundsätze unterschieden werden. Inhaltliche Grundsätze legen konkrete inhaltliche Güter (Leben, Gesundheit, Besitz, Vergnügen, Umwelt etc.) als Bewertungskriterium für das Handeln zugrunde. Sie sind teilweise subjektiv und haben unter Umständen einen dezisionistischen Charakter. In diesen Fällen können sie ihre eigene Vorrangstellung nicht gegenüber anderen, konkurrierenden inhaltlichen Grundsätzen begründen.
Formale Grundsätze verzichten auf einen Bezug zu konkreten inhaltlichen Gütern. Das bekannteste Beispiel ist der Kategorische Imperativ Kants.
Sätze, die Entscheidungen formulieren, indem sie Maximen auf konkrete Lebenssituationen anwendenDie Ethik ist häufig nur in der Lage, Aussagen zu den ersten beiden Ebenen zu machen. Die Übertragung von praktischen Grundsätzen auf eine konkrete Situation, erfordert das Vermögen der praktischen Urteilskraft. Nur mit seiner Hilfe können eventuell auftretende Zielkonflikte gelöst und die voraussichtlichen Folgen von Entscheidungen abgeschätzt werden.
Wesentlich für die ethische Bewertung von Handlungen sind die mit ihnen verbundenen Folgen. Diese werden unterschieden in motivierende und in Kauf genommene Folgen. Motivierende Folgen sind solche, um derentwillen eine Handlung ausgeführt wird. Sie werden vom Handelnden unmittelbar angezielt („Voluntarium in se“).
In Kauf genommene Folgen („Voluntarium in causa“) werden zwar nicht unmittelbar angezielt, aber als Nebenwirkung der motivierenden Folgen vorausgesehen und bewusst zugelassen (Prinzip der Doppelwirkung). So unterliegt beispielsweise bewusste Fahrlässigkeit als bedingter Vorsatz (dolus eventualis) der ethischen und rechtlichen Verantwortung: Volltrunkenheit entschuldigt nicht bei einem Verkehrsunfall.
Bereits Thomas von Aquin unterscheidet eine zweifache Kausalität des Willens: die „direkte“ Einwirkung des Willens, in der durch den Willensakt ein bestimmtes Ereignis hervorgerufen wird, und die „indirekte“, in der ein Ereignis dadurch eintritt, dass der Wille untätig bleibt. Tun und Unterlassen unterscheiden sich hierbei nicht hinsichtlich ihrer Freiwilligkeit. Beim Unterlassen verzichtet jemand auf das Eingreifen in einen Prozess, obwohl er die Möglichkeit dazu hätte. Auch das Unterlassen kann daher als Handlung aufgefasst werden und strafbar sein.
Die strikte Unterscheidung zwischen diesen beiden Handlungsformen, die z. B. in der medizinischen Ethik eine große Rolle spielt (vgl. aktive und passive Sterbehilfe etc.), erscheint daher vom ethischen Standpunkt aus gesehen als teilweise fragwürdig.
Im Mittelpunkt teleologischer Ethiken steht die Frage, was ich mit meiner Handlung letztlich bezwecke, welches Ziel ich mit ihr verfolge. Der Begriff „Ziel“ (finis, telos;) ist hier insbesondere als „letztes Ziel“ oder „Endziel“ zu verstehen, von dem all mein Handeln bestimmt wird.
In der Tradition wird als letztes Ziel des Menschen häufig das Glück oder die Glückseligkeit (beatitudo) genannt. Der Ausdruck „Glück“ wird dabei in einem mehrdeutigen Sinne gebraucht:
zur Bezeichnung eines gelungenen und guten Lebens, dem nichts Wesentliches fehlt („Lebensglück“, eudaimonia)
zur Bezeichnung des subjektiven Wohlbefindens (Glück als Lust, hedone)Philosophiegeschichtlich konkurrieren die Bestimmungen von Glück als „Lebensglück“ und als subjektives Wohlbefinden miteinander. Für die Eudämonisten (Platon, Aristoteles) ist Glück die Folge der Verwirklichung einer Norm, die als Telos im Wesen des Menschen angelegt ist. Glücklich ist dieser Konzeption zufolge vor allem, wer auf vernünftige Weise tätig ist.
Für die Hedonisten (Sophisten, klassische Utilitaristen) gibt es kein zu verwirklichendes Telos des Menschen mehr; es steht keine objektive Norm zur Verfügung, um zu entscheiden, ob jemand glücklich ist. Dies führt zu einer Subjektivierung des Glücksbegriffs. Es obliege allein dem jeweiligen Individuum, zu bewerten, ob es glücklich ist. Glück wird hier mit dem Erreichen von Gütern wie Macht, Reichtum, Ruhm etc. gleichgesetzt.
Das Wort „Sinn“ bezeichnet grundsätzlich die Qualität von etwas, das dieses verstehbar macht. Wir verstehen etwas dadurch, indem wir erkennen, worauf es „hingeordnet“ ist, wozu es dient. Die Frage nach dem Sinn steht also in einem engen Zusammenhang mit der Frage nach dem Ziel oder Zweck von etwas. Auch der Sinn einer Handlung oder gar des ganzen Lebens kann nur beantwortet werden, wenn die Frage nach seinem Ziel geklärt ist. Eine menschliche Handlung bzw. ein gesamtes Leben ist dann sinnvoll, wenn es auf dieses Ziel hin ausgerichtet ist.
„Gut“ gehört wie der Begriff „seiend“ zu den ersten und daher nicht mehr definierbaren Begriffen. Es wird zwischen einem adjektivischen und einem substantivischen Gebrauch unterschieden.
Als Adjektiv bezeichnet das Wort „gut“ generell die Hinordnung eines „Gegenstandes“ auf eine bestimmte Funktion oder einen bestimmten Zweck. So spricht man z. B. von einem „guten Messer“, wenn es seine im Prädikator „Messer“ ausgedrückte Funktion erfüllen – also z. B. gut schneiden kann. Analog spricht man von einem „guten Arzt“, wenn er in der Lage ist, seine Patienten zu heilen und Krankheiten zu bekämpfen. Ein „guter Mensch“ ist demnach jemand, der in seinem Leben auf das hin ausgerichtet ist, was das Menschsein ausmacht, also dem menschlichen Wesen bzw. seiner Natur entspricht.
Als Substantiv bezeichnet das Wort „das Gut“ etwas, auf das hin wir unser Handeln ausrichten. Wir gebrauchen es normalerweise in dieser Weise, um „eine unter bestimmten Bedingungen vollzogene Wahl als richtig oder gerechtfertigt zu beurteilen“. So kann beispielsweise eine Aussage wie „Die Gesundheit ist ein Gut“ als Rechtfertigung für die Wahl einer bestimmten Lebens- und Ernährungsweise dienen. In der philosophischen Tradition war man der Auffassung, dass prinzipiell jedes Seiende – unter einer gewissen Rücksicht – Ziel des Strebens sein könne („omne ens est bonum“). Daher wurde die „Gutheit“ des Seienden zu den Transzendentalien gerechnet.
Gemäß der Analyse von Richard Mervyn Hare werden wertende Wörter wie „gut“ oder „schlecht“ dazu verwendet, in Entscheidungssituationen Handeln anzuleiten bzw. Empfehlungen zu geben. Die Wörter „gut“ oder „schlecht“ haben demnach keine beschreibende (deskriptive), sondern eine vorschreibende (präskriptive) Funktion.
Dies kann an einer außermoralischen Verwendung des Wortes „gut“ verdeutlicht werden. Wenn ein Verkäufer zum Kunden sagt: „Dies ist ein guter Wein“, dann empfiehlt er den Kauf dieses Weines, er beschreibt damit jedoch keine wahrnehmbare Eigenschaft des Weines. Insofern es jedoch sozial verbreitete Bewertungsstandards für Weine gibt (er darf nicht nach Essig schmecken, man darf davon keine Kopfschmerzen bekommen etc.), so bedeutet die Bewertung des Weines als „gut“, dass der Wein diese Standards erfüllt und dass er somit auch bestimmte empirische Eigenschaften besitzt.
Die Bewertungskriterien, die an eine Sache angelegt werden, können je nach dem Verwendungszweck variieren. Ein herber Wein mag als Tafelwein gut, für sich selbst getrunken dagegen eher schlecht sein. Der Verwendungszweck einer Sache ist keine feststehende Eigenschaft der Sache selbst, sondern beruht auf menschlicher Setzung. Eine Sache ist „gut“ – immer bezogen auf bestimmte Kriterien. Wenn der Verkäufer sagt: „Dies ist ein sehr guter Tafelwein“ dann ist er so, wie er gemäß den üblichen Kriterien für Tafelwein sein soll.
Wenn das Wort „gut“ in moralischen Zusammenhängen gebraucht wird („Dies war eine gute Tat“), so empfiehlt man die Tat und drückt aus, dass sie so war, wie sie sein soll. Man beschreibt damit jedoch nicht die Tat. Wird auf allgemein anerkannte moralische Kriterien Bezug genommen, drückt man damit zugleich aus, dass die Tat bestimmte empirische Eigenschaften besitzt, z. B. eine Zurückstellung des Eigeninteresses zugunsten überwiegender Interessen von Mitmenschen.
Als das höchste Gut (summum bonum) wird das bezeichnet, was nicht nur unter einer bestimmten Rücksicht (für den Menschen) gut ist, sondern schlechthin, da es dem Menschen als Menschen ohne Einschränkung entspricht. Es ist identisch mit dem „unbedingt Gesollten“. Seine inhaltliche Bestimmung hängt ab von der jeweiligen Sicht der Natur des Menschen. In der Tradition wurden dabei die unterschiedlichsten Lösungsvorschläge präsentiert:
Der Begriff „Wert“ stammte ursprünglich aus der Nationalökonomie, wo man unter anderem zwischen Gebrauchs- und Tauschwert unterschied. Er wurde erst in der zweiten Hälfte des 19. Jahrhunderts ein philosophischer Terminus, wo er im Rahmen der Wertphilosophie (Max Scheler u. a.) eine zentrale Bedeutung einnahm. Dort führte man ihn als Gegenbegriff zur Kantischen Pflichtethik ein, in der Annahme, dass Werten vor allen Vernunftüberlegungen eine „objektive Gültigkeit“ zukommen würde.
In der Alltagssprache taucht der Begriff auch in jüngster Zeit wieder verstärkt auf, gerade wenn von „Grundwerten“, einem „Wertewandel“ oder einer „neuen Wertedebatte“ die Rede ist.
Der Wertbegriff weist große Ähnlichkeiten mit dem Begriff des Guten auf. Er wird wie dieser grundsätzlich in einer subjektiven und einer objektiven Variante gebraucht:
als „objektiver Wert“ bezeichnet er den „Wert“ von bestimmten Gütern für den Menschen – wie z. B. den Wert des menschlichen Lebens, der Gesundheit etc. Dies entspricht der Bedeutung von „bonum physicum“ („physisches Gut“).
als „subjektive Werthaltung“ bezeichnet er das, was mir wertvoll ist, meine „Wertvorstellungen“ – wie Treue, Gerechtigkeit etc. Dies entspricht der Bedeutung von „bonum morale“ („sittliches Gut“).Im Vergleich zum Begriff des Guten kommt dem Wertbegriff allerdings eine stärkere gesellschaftliche Bedingtheit zu. So spricht man von einem „Wertewandel“, wenn man ausdrücken will, dass sich bestimmte, in einer Gesellschaft allgemein akzeptierte Handlungsnormen im Verlauf der Geschichte verändert haben. Damit meint man aber in der Regel nicht, dass das, was früher für gut gehalten wurde, nun „tatsächlich“ nicht mehr gut sei, sondern nur, dass sich das allgemeine Urteil darüber geändert habe.
In ihrer klassischen Definition formuliert sie Aristoteles als „jene feste Grundhaltung, von der aus [der Handelnde] tüchtig wird und die ihm eigentümliche Leistung in vollkommener Weise zustande bringt“ (NE 1106a).Die Leistung der ethischen Tugenden besteht vor allem darin, im Menschen eine Einheit von sinnlichem Strebevermögen und sittlicher Erkenntnis zu bewirken. Wir bezeichnen einen Menschen erst dann als „gut“, wenn er zur inneren Einheit mit sich selbst gekommen ist und das als richtig Erkannte auch affektiv voll bejaht. Dies ist nach Aristoteles nur durch eine Integration der Gefühle durch die ethischen Tugenden möglich. Die ungeordneten Gefühle verfälschen das sittliche Urteil. Das Ziel der Einheit von Vernunft und Gefühl führt über eine bloße Ethik der richtigen Entscheidung hinaus. Es kommt nicht nur darauf an, was wir tun, sondern auch wer wir sind.
Tugend setzt neben Erkenntnis eine Gewöhnung voraus, die durch Erziehung und soziale Praxis erreicht wird. Wir werden gerecht, mutig etc., indem wir uns in Situationen begeben, wo wir uns entsprechend verhalten können. Die wichtigste Rolle kommt dabei der Tugend der Klugheit (phronesis) zu. Ihr obliegt es, die rechte „Mitte“ zwischen den Extremen zu finden und sich für die optimale Lösung in der konkreten Situation zu entscheiden.
Der Begriff „sollen“ ist ein Grundbegriff deontologischer Ethikansätze. Er bezieht sich – als Imperativ – auf eine Handlung, mit der ein bestimmtes Ziel erreicht werden soll. Dabei müssen folgende Bedingungen erfüllt sein:
das vorgegebene Ziel kann prinzipiell erreicht werden („Jedes Sollen impliziert ein Können“)Sprachanalytisch lässt sich das Sollen mit Hilfe der sogenannten deontischen Prädikatoren erklären. Diese beziehen sich auf die sittliche Verbindlichkeit von Handlungen. Folgende Varianten sind dabei zu unterscheiden:
moralisch unmöglich.Moralisch mögliche Handlungen sind sittlich erlaubt, d. h. man darf so handeln. Moralisch notwendige Handlungen sind sittlich geboten. Hier spricht man davon, dass wir etwas tun sollen bzw. die Pflicht haben, etwas zu tun. Moralisch unmögliche Handlungen sind sittlich verbotene Handlungen, die wir nicht ausführen dürfen; siehe auch Sünde.
So können wir in Situationen stehen, in denen wir nur zwischen schlechten Alternativen wählen können. Hier ist es gesollt, dass wir uns für das „geringere Übel“ entscheiden. Umgekehrt ist nicht alles Gute auch gesollt. Das kann z. B. der Fall sein, wenn das Erreichen eines Gutes ein anderes Gut ausschließt. Hier muss eine Güterabwägung erfolgen, die zum Verzicht eines Gutes führt.
Der Begriff der Gerechtigkeit ist seit der intensiven Diskussion um die „Theorie der Gerechtigkeit“ von John Rawls und vor allem seit der aktuellen politischen Debatte um die Aufgaben des Sozialstaates (Betonung der Chancen- und Leistungsgerechtigkeit gegenüber der Verteilungsgerechtigkeit) wieder stark ins Blickfeld geraten.
„Gerecht“ wird – wie der Begriff „gut“ – in vielerlei Bedeutungen gebraucht. Es werden Handlungen, Haltungen, Personen, Verhältnisse, politische Institutionen und zuweilen auch Affekte (der „gerechte Zorn“) als gerecht bezeichnet. Grundsätzlich kann zwischen einem „subjektiven“ und einem „objektiven“ Gebrauch unterschieden werden, wobei beide Varianten aufeinander bezogen sind.
Die subjektive oder besser personale Gerechtigkeit bezieht sich auf das Verhalten oder die ethische Grundhaltung einer Einzelperson. Eine Person kann gerecht handeln ohne gerecht zu sein und umgekehrt. Damit im Zusammenhang steht die kantische Unterscheidung zwischen Legalität und Moralität. Legale Handlungen befinden sind nach außen hin betrachtet in Übereinstimmung mit dem Sittengesetz, geschehen aber nicht ausschließlich aufgrund moralischer Beweggründe, sondern z. B. auch aus Angst, Opportunismus etc. Bei moralischen Handlungen dagegen stimmen Handlung und Motiv miteinander überein. In diesem Sinne wird Gerechtigkeit als eine der vier Kardinaltugenden bezeichnet.
Hier geht es immer um Pflichten innerhalb einer Gemeinschaft, die das Gleichheitsprinzip berühren. Es ist grundsätzlich zu unterscheiden zwischen der ausgleichenden Gerechtigkeit (iustitita commutativa) und Verteilungsgerechtigkeit (iustitita distributiva). Bei der ausgleichenden Gerechtigkeit tritt der Wert einer Ware oder Leistung in den Vordergrund. Bei der Verteilungsgerechtigkeit geht es um den Wert der beteiligten Personen.
Die Gerechtigkeit der Einzelpersonen und der Institutionen sind in einem engen Zusammenhang zueinander zu sehen. Ohne gerechte Bürger werden keine gerechten Institutionen geschaffen oder aufrechterhalten werden können. Ungerechte Institutionen erschweren andererseits die Entfaltung der Individualtugend der Gerechtigkeit.
Das Anliegen der Ethik beschränkt sich nicht auf das Thema „Gerechtigkeit“. Zu den Tugenden gehören noch diejenigen, die man vor allem sich selbst gegenüber hat (Klugheit, Mäßigung, Tapferkeit). Zu den ethischen Pflichten gegenüber anderen zählt noch die Pflicht des Wohltuns (beneficientia), die über die Gerechtigkeit hinausgeht und ihre Wurzel letztlich in der Liebe hat. Während der Gerechtigkeit das Gleichheitsprinzip zugrunde liegt, ist dies beim Wohltun die Notlage oder Bedürftigkeit des anderen. Diese Unterscheidung entspricht der zwischen „iustitia“ und „caritas“ (Thomas von Aquin), Rechts- und Tugendpflichten (Kant) bzw. in der Gegenwart der zwischen „duties of justice“ und „duties of charity“ (Philippa Foot).
Klassen ethischer oder moralphilosophischer Theorien lassen sich danach unterscheiden, welche Kriterien sie für die Bestimmung des moralisch Guten zugrunde legen. Das moralisch Gute kann bestimmt werden durch:
objektive moralische Tatsachen, etwa objektive moralische Güter oder Handlungsbewertungen betreffend (Deontologische Ethiken);
oder die Wohlfahrt.Dabei werden unterschiedlichste Kombinationen und feinere moraltheoretische Bestimmungen vertreten.
Die verschiedenen Ethikansätze werden traditionell prinzipiell danach unterschieden, ob sie ihren Schwerpunkt auf die Handlung selbst (deontologische Ethikansätze) oder auf die Handlungsfolgen (teleologische Ethikansätze) legen. Die Unterscheidung geht zurück auf C. D. Broad
und wurde bekannt durch William K. Frankena. In dieselbe Richtung geht auch die Aufteilung Max Webers in Gesinnungs- und Verantwortungsethiken, wobei diese von ihm als Polemik gegenüber Gesinnungsethiken verstanden wurde.
Das griechische Wort „telos“ bedeutet so viel wie Vollendung, Erfüllung, Zweck oder Ziel. Unter teleologischen Ethiken versteht man daher solche Theorieansätze, die ihr Hauptaugenmerk auf bestimmte Zwecke oder Ziele richten. In ihnen wird die Forderung erhoben, Handlungen sollten ein Ziel anstreben, das in einem umfassenderen Verständnis gut ist. Der Inhalt dieses Zieles wird von den verschiedenen Richtungen auf recht unterschiedliche Art und Weise bestimmt.
Teleologische Ethiken geben valuativen Sätzen einen Vorrang gegenüber normativen Sätzen. Für sie stehen Güter und Werte im Vordergrund. Die menschlichen Handlungen sind insbesondere insofern von Interesse, als sie hinderlich oder förderlich zum Erreichen dieser Güter und Werte sein können. „Eine Handlung ist dann auszuführen und nur dann, wenn sie oder die Regel, unter die sie fällt, ein größeres Überwiegen des Guten über das Schlechte herbeiführt, vermutlich herbeiführen wird oder herbeiführen sollte als jede erreichbare Alternative“ (Frankena).Innerhalb teleologischer Ethikansätze wird wiederum zwischen „onto-teleologischen“ und „konsequentialistisch-teleologischen“ Ansätzen unterschieden.
In onto-teleologischen Ansätzen – klassisch vertreten durch Aristoteles – wird davon ausgegangen, dass das zu erstrebende Gut in gewisser Weise dem Menschen selbst als Teil seiner Natur innewohne. Es wird gefordert, dass der Mensch so handeln und leben solle, wie es seiner Wesensnatur entspricht, um so seine artspezifischen Anlagen auf bestmögliche Weise zu vervollkommnen.
In konsequentialistisch-teleologischen Ansätzen hingegen wird nicht mehr von einer letzten vorgegebenen Zweckhaftigkeit des menschlichen Daseins ausgegangen. Das zu erstrebende Ziel wird daher durch einen außerhalb des handelnden Subjekts liegenden Nutzen bestimmt. Dieser Ansatz wird bereits in der Antike (Epikur) und später in seiner typischen Form durch den Utilitarismus vertreten.
Das griechische Wort „to deon“ bedeutet „das Schickliche, die Pflicht“. Deontologische Ethiken kann man daher mit Sollensethiken gleichsetzen. Sie sind dadurch gekennzeichnet, dass bei ihnen den Handlungsfolgen nicht dieselbe Bedeutung zukommt wie in teleologischen Ethiken. Innerhalb der deontologischen Ethiken wird häufig zwischen aktdeontologischen (z. B. Jean-Paul Sartre) und regeldeontologischen Konzeptionen (z. B. Immanuel Kant) unterschieden. Während die Regeldeontologie allgemeine Handlungstypen als verboten, erlaubt oder geboten ausweist (vgl. z. B. das Lügenverbot oder die Pflicht, Versprechen zu halten), bezieht sich den aktdeontologischen Theorien zufolge das deontologische Moralurteil unmittelbar auf spezifische Handlungsweisen in jeweils bestimmten Handlungssituationen.
In deontologischen Ethiken haben normative Sätze eine Vorrangstellung gegenüber valuativen Sätzen. Für sie bilden Gebote, Verbote und Erlaubnisse die Grundbegriffe. Es rücken die menschlichen Handlungen in den Vordergrund, da nur sie gegen eine Norm verstoßen können. Robert Spaemann charakterisiert sie als „moralische Konzepte, […] für welche bestimmte Handlungstypen ohne Beachtung der weiteren Umstände immer verwerflich sind, also z. B. die absichtliche direkte Tötung eines unschuldigen Menschen, die Folter oder der außereheliche Beischlaf eines verheirateten Menschen“.
Die Unterscheidung zwischen teleologischen und deontologischen Ethiken wird von einigen Kritikern als fragwürdig bezeichnet. In der Praxis sind auch selten Ansätze zu finden, die eindeutig einer der beiden Richtungen zugeordnet werden könnten.
Einer strikten deontologischen Ethik müsste es gelingen, Handlungen aufzuzeigen, die „in sich“, völlig losgelöst von ihren Folgen, als unsittlich und „in sich schlecht“ zu bezeichnen wären. Diese wären dann „unter allen Umständen“ zu tun oder zu unterlassen gemäß dem Spruch „Fiat iustitia et pereat mundus“ („Gerechtigkeit geschehe, und sollte die Welt darüber zugrunde gehen“, Ferdinand I. von Habsburg). Bekannte Beispiele solcher Handlungen sind die „Tötung Unschuldiger“ oder die nach Kant unzulässige Lüge. In den Augen der Kritiker liegt in diesen Fällen häufig eine „petitio principii“ vor. Wenn z. B. die Tötung Unschuldiger als Mord und dieser wiederum als unsittliche Handlung definiert wird, könne sie natürlich in jedem Fall als „in sich schlecht“ bezeichnet werden. Das gleiche gelte für die Lüge, wenn sie als unerlaubtes Verfälschen der Wahrheit bezeichnet wird.
Gerade in der Analyse ethischer Dilemmasituationen, in denen nur die Wahl zwischen mehreren Übeln möglich ist, zeige sich, dass es kaum möglich sein dürfte, bestimmte Handlungen unter allen Umständen als „sittlich schlecht“ zu bezeichnen. Nach einer strikten deontologischen Ethik wäre die „Wahl des kleineren Übels“ nicht möglich.
An strikt teleologisch argumentierenden Ethikansätzen wird kritisiert, dass sie das ethisch Gesollte von außerethischen Zwecken abhängig machen. Damit bleibe die Frage unbeantwortet, weshalb wir diese Zwecke verfolgen sollen. Eine Güterabwägung werde damit unmöglich gemacht, da die Frage, was ein oder das bessere „Gut“ ist, nur geklärt werden könne, wenn vorher allgemeine Handlungsprinzipien definiert wurden. In vielen teleologischen Ansätzen würden diese Handlungsprinzipien auch einfach stillschweigend vorausgesetzt, wie z. B. im klassischen Utilitarismus, für den Lustgewinnung und Unlustvermeidung die Leitprinzipien jeglicher Folgenabschätzung darstellen.
Ethische Positionen lassen sich auch danach unterscheiden, wie sich das Gesollte aus einem bestimmten Wollen ergibt.
Die aufgelisteten Positionen liegen auf unterschiedlichen logischen Ebenen und schließen sich deshalb auch nicht logisch aus. So ist z. B. die Verbindung einer religiösen Position mit einer intuitionistischen Position möglich. Denkbar ist auch eine Verbindung der konsenstheoretischen Position mit einer utilitaristischen Position, wenn man annimmt, dass sich ein Konsens über die richtige Norm nur dann herstellen lässt, wenn dabei der Nutzen (das Wohl) jedes Individuums in gleicher Weise berücksichtigt wird.
Außerdem ist zu beachten: Einige dieser Ansätze haben ausdrücklich nicht den Anspruch, umfassende ethische Konzepte zu sein, sondern z. B. nur Konzepte für die Beurteilung, ob eine Gesellschaft in politisch-ökonomischer Hinsicht gerecht eingerichtet ist; z. B. bei John Rawls, im Unterschied zu umfassenderen Ansätze, die auch Fragen privater, individueller Ethik betreffen – etwa, ob es eine moralische Pflicht gibt, zu lügen, wenn genau dies notwendig ist, um ein Menschenleben zu retten (und wenn ohne diese Lüge niemand sonst stattdessen gerettet würde). Auch z. B. Habermas beantwortet diese Frage nicht „inhaltlich“, aber sein Konzept beinhaltet den Bereich auch solcher Fragen, indem es „formal“ postuliert, richtig sei, was in dieser Frage alle, die an einem zwanglosen und zugleich vernünftigen Diskurs dazu teilnehmen würden, als verbindlich für alle dazu herausfinden und akzeptieren würden.
Wenn man fragt, warum Individuum A eine bestimmte Handlungsnorm N befolgen soll, so gibt es zwei Arten von Antworten.
Die eine Art von Antworten bezieht sich auf eine Institution oder ein Verfahren, wodurch die Norm gesetzt wurde. Beispiele hierfür sind:
… es mehrheitlich so beschlossen wurde etc.Die andere Art von Antworten bezieht sich auf die inhaltliche Beschaffenheit der Norm. Beispiele für diese Art von Antworten sind:
… N der Menschenwürde entspricht, etc.Offensichtlich liegen diese Begründungen auf zwei verschiedenen Ebenen, denn man kann ohne logischen Widerspruch sagen: „Ich halte den Beschluss der Parlamentsmehrheit zwar für inhaltlich falsch, aber dennoch ist er für mich verbindlich. Als Demokrat respektiere ich die Beschlüsse der Mehrheit.“
Man kann die ethischen Theorien nun danach unterscheiden, wie sie mit dem Spannungsverhältnis zwischen der Ebene der verfahrensmäßigen Setzung von verbindlichen Normen und der Ebene der argumentativen Bestimmung von richtigen Normen umgehen.
Auf der einen Seite stehen ganz außen die Dezisionisten. Für sie ist nur die verbindliche Setzung von Normen bedeutsam. Sie bestreiten, dass man in Bezug auf Normen überhaupt von inhaltlicher Richtigkeit und von einer Erkenntnis der richtigen Norm sprechen kann.
Das Hauptproblem der Dezisionisten ist, dass es für sie keine Berechtigung für einen Widerstand gegen die gesetzten Normen geben kann, denn „verbindlich ist verbindlich“. Außerdem können Dezisionisten nicht begründen, warum man das eine Normsetzungsverfahren irgendeinem anderen Verfahren vorziehen soll.
Auf der anderen Seite stehen ganz außen die ethischen Kognitivisten. Für sie ist das Problem ethischen Handelns allein ein Erkenntnisproblem, das man durch die Gewinnung relevanter Informationen und deren Auswertung nach geeigneten Kriterien lösen kann. Eine Legitimation von Normen durch Verfahren ist für sie nicht möglich.
Das Hauptproblem der Kognitivisten ist, dass es auch beim wissenschaftlichen Meinungsstreit oft nicht zu definitiven Erkenntnissen kommt, die als Grundlage der sozialen Koordination dienen könnten. Es werden deshalb zusätzlich verbindliche und sanktionierte Normen benötigt, die für jedes Individuum das Handeln der anderen berechenbar macht.
Teleologische Ethiken sind in der Regel Güter-Ethiken; sie bezeichnen bestimmte Güter (z. B. „Glück“ oder „Lust“) als für den Menschen gut und damit erstrebenswert.
Schon David Hume hat den Einwand erhoben, dass der Übergang von Seins- zu Sollensaussagen nicht legitim sei („Humes Gesetz“). Unter dem Stichwort „Naturalistischer Fehlschluss“ hat George Edward Moore damit eng verwandte Fragen aufgeworfen, die aber genau genommen nicht dieselben sind.
Für Hume sind logische Schlussfolgerungen von dem, was ist, auf das, was sein soll, unzulässig, denn durch logische Umformungen könne aus Ist-Sätzen kein völlig neues Bedeutungselement wie das Sollen hergeleitet werden.
Wie später die Positivisten betont haben, muss erkenntnistheoretisch zwischen Ist-Sätzen und Soll-Sätzen wegen ihres unterschiedlichen Verhältnisses zur Sinneswahrnehmung differenziert werden. Während der Satz „Peter ist um 14 Uhr am Bahnhof gewesen“ durch intersubjektiv übereinstimmende Beobachtungen überprüfbar, also verifizierbar oder falsifizierbar ist, lässt sich der Satz „Peter soll um 14 Uhr am Bahnhof sein“ mit den Mitteln von Beobachtung und Logik allein nicht begründen oder widerlegen.
Die erkenntnistheoretische Unterscheidung zwischen Sein und Sollen liegt den modernen Erfahrungswissenschaften zugrunde. Wer diese Unterscheidung nicht akzeptiert, der muss entweder ein Sein postulieren, das nicht direkt oder indirekt wahrnehmbar ist, oder er muss das Gesollte für sinnlich wahrnehmbar halten. Beiden Positionen mangelt es bisher an einer intersubjektiven Nachprüfbarkeit.
Die vermeintliche Herleitung ethischer Normen aus Aussagen über das Seiende wird oft nur durch die unbemerkte Ausnutzung der normativ-empirischen Doppeldeutigkeit von Begriffen wie „Wesen“, „Natur“, „Bestimmung“, „Funktion“, „Zweck“, „Sinn“ oder „Ziel“ erreicht.
So bezeichnet das Wort „Ziel“ einmal das, was ein Mensch tatsächlich anstrebt („Sein Ziel ist das Diplom“). Das Wort kann jedoch auch das bezeichnen, was ein Mensch anstreben sollte („Wer nur am Materiellen ausgerichtet ist, der verfehlt das wahre Ziel des menschlichen Daseins“).
Die unbemerkte empirisch-normative Doppeldeutigkeit bestimmter Begriffe führt dann zu logischen Fehlschlüssen wie: „Das Wesen der Sexualität ist die Fortpflanzung. Also ist Empfängnisverhütung nicht erlaubt, denn sie entspricht nicht dem Wesen der Sexualität.“
Aus der logischen Unterscheidung von Sein und Sollen folgt jedoch keineswegs, dass damit eine auf Vernunft gegründete Ethik unmöglich ist, wie dies sowohl von Vertretern des logischen Empirismus als auch des Idealismus geäußert wird. Zwar ließe sich allein auf Empirie und Logik keine Ethik gründen, aber daraus folgt noch nicht, dass es nicht andere allgemein nachvollziehbare Kriterien für die Gültigkeit ethischer Normen gibt. Ein aussichtsreiches Beispiel für eine nachpositivistische Ethik ist die am Kriterium des zwangfreien Konsenses orientierte Diskursethik.
Mit der Feststellung, dass das Gesollte nicht aus dem Seienden logisch ableitbar ist, wird eine Begründung von Normen noch nicht aussichtslos. Denn neben den Seinsaussagen und den normativen Sätzen gibt es Willensäußerungen. Die Willensäußerung einer Person: „Ich will in der nächsten Stunde von niemandem gestört werden“ beinhaltet die Norm: „Niemand soll mich in der nächsten Stunde stören“. Die Aufgabe der Ethik ist es, allgemeingültige Willensinhalte bzw. Normen zu bestimmen und nachvollziehbar zu begründen.
Die logische Unterscheidung zwischen Ist-Sätzen und Soll-Sätzen wird vor allem von Vertretern idealistischer Positionen als eine unzulässige Trennung von Sein und Sollen angesehen und es wird eingewandt, dass ihr ein verkürzter Seinsbegriff zugrunde liege. So argumentiert Vittorio Hösle, das Sollen könne nur vom realen, empirischen Sein strikt abgegrenzt werden, „... ein ideales Sein, das nicht vom Menschen gesetzt ist, wird dem Sollen damit ebenso wenig abgesprochen wie eine mögliche Prinzipiierungsfunktion gegenüber dem empirischen Sein“. Es könne gerade als Aufgabe des Menschen angesehen werden, „damit fertig zu werden, dass das Sein nicht so ist, wie es sein soll“. Das Gesollte solle eben sein und sei als solches bereits Prinzip des Seins:
Die Möglichkeit einer teleologischen Ethik scheint mit der logischen Unterscheidung von Seins- und Sollens-Aussagen grundsätzlich in Frage gestellt. Aus Sicht der klassischen Position des Realismus bezüglich der Ethik, insbesondere des Naturrechts, ist es aber gerade das Sein, aus dem das Sollen abgeleitet werden muss, da es (außer dem Nichts) zum Sein keine Alternative gibt. Weil das Gute das Seinsgerechte, also das dem jeweiligen Seienden gerechte bzw. entsprechende ist, muss demnach das Wesen des Seins zunächst erkannt und aus ihm die Forderung des Sollens (ihm gegenüber) logisch abgeleitet werden.
Trotz der teilweise apokalyptischen geschichtlichen Ereignisse des 20. Jahrhunderts wird der Begriff „böse“ in der Umgangssprache nur noch selten gebraucht. Stattdessen werden meist die Begriffe „schlecht“ („ein schlechter Mensch“) oder „falsch“ („die Handlung war falsch“) verwendet. Das Wort „böse“ gilt im gegenwärtigen Bewusstsein generell als metaphysikverdächtig und aufgrund der allgemeinen Dominanz des naturwissenschaftlichen Denkens als überholt.
In der philosophischen Tradition wird das Böse als eine Form des Übels betrachtet. Klassisch geworden ist die Unterscheidung von Leibniz zwischen einem metaphysischen (malum metaphysicum), einem physischen (malum physicum) und einem moralischen Übel (malum morale). Das metaphysische Übel besteht in der Unvollkommenheit alles Seienden, das physische Übel in Schmerz und Leid. Diese Übel sind Widrigkeiten, die ihren Ursprung in der Natur haben. Sie sind nicht „böse“, da sie nicht das Ergebnis des (menschlichen oder allgemeiner gesagt geistigen) Willens sind. Das moralische Übel oder das Böse hingegen besteht in der Nicht-Übereinstimmung einer Handlung mit dem Sittengesetz bzw. Naturrecht. Es kann, wie Kant betont, nur „die Handlungsart, die Maxime des Willens und mithin die handelnde Person selbst“ böse sein. Das Böse ist also als Leistung oder besser Fehlleistung des Subjekts zu verstehen.
Die Verhaltensforschung führt das Böse auf die allgemeine „Tatsache“ der Aggression zurück. Diese sei einfachhin ein Bestandteil der menschlichen Natur und als solcher moralisch irrelevant. Daher spricht Konrad Lorenz auch vom „sogenannten Bösen“. Dieser Erklärung wird von Kritikern eine reduktionistische Betrachtungsweise vorgeworfen. Sie übersehe, dass dem Menschen auf der Grundlage der Freiheit die Möglichkeit gegeben ist, zu seiner eigenen Natur Stellung zu nehmen.In der Philosophie stellte sich bereits Platon die Frage, wie das Böse überhaupt möglich sei. Das Böse werde nur getan, weil jemand im irrtümlichen Glauben annimmt, er (oder jemand) habe einen Nutzen davon. Somit wolle er aber den mit dem Bösen verbundenen Nutzen. Das Böse um seiner selbst willen könne niemand vernünftigerweise wollen:
Dieses in der Antike noch weit verbreitete Verständnis, das Böse ließe sich durch die Vernunft überwinden, wird allerdings durch die geschichtlichen Erfahrungen, insbesondere die des 20. Jahrhunderts in Frage gestellt. Diese lehren in den Augen vieler Philosophen der Gegenwart, dass der Mensch durchaus im Stande sei, das Böse auch um seiner selbst willen zu wollen.
Als Motiv für das Böse kann zunächst einmal der Egoismus ausgemacht werden. Er äußert sich in vielen Spielarten. In seiner harmlosen Variante zeigt er sich im Ideal einer selbstbezogenen Bedürfnisbefriedigung. In dieser Form stellt er letztlich auch die „Vertragsgrundlage“ des Utilitarismus dar, der nichts anderes als einen Interessensausgleich zwischen den Individuen schaffen möchte. Dieser Aspekt trifft – wie die geschichtliche Erfahrung zeigt – noch nicht den eigentlichen Kern des Bösen. Dieser wird erst dann sichtbar, wenn die eigene Bedürfnisbefriedigung nicht mehr im Vordergrund steht:
Die Ursache dieses „radikal Bösen“ ist nach Kant weder in der Sinnlichkeit noch in der Vernunft zu sehen, sondern in einer „Verkehrtheit des Herzens“, in der sich das Ich gegen sich selbst wendet:
Dieser Grundgedanke Kants von der Selbstwidersprüchlichkeit des Ichs als Ursache des Bösen wird vor allem in der Philosophie des Idealismus noch einmal vertieft. Schelling unterscheidet zwischen einem alle Bindung verneinenden „Eigenwillen“ und einem sich in Beziehungen gestaltenden „Universalwillen“. Die Möglichkeit zum Bösen bestehe darin, dass der Eigenwille sich seiner Integration in den Universalwillen widersetzt.
Das radikal Böse bewirke einen Umsturz der Ordnung in mir selbst und in Bezug zu anderen. Es erfolge um seiner selbst willen, denn „wie es einen Enthusiasmus zum Guten gibt, ebenso gibt es eine Begeisterung des Bösen“.Nach der klassischen Lehre (Augustinus, Thomas von Aquin etc.) ist das Böse selbst letztlich substanzlos. Als privativer Gegensatz des Guten besteht es nur in einem Mangel (an Gutem). Im Gegensatz zum absolut Guten (Gott) gibt es demnach das absolut Böse nicht.
Das Durchsetzungsproblem der Ethik besteht darin, dass die Einsicht in die Richtigkeit ethischer Prinzipien zwar vorhanden sein kann, daraus aber nicht automatisch folgt, dass der Mensch auch im ethischen Sinne handelt. Die Einsicht in das richtige Handeln bedarf einer zusätzlichen Motivation oder eines Zwangs.Das Problem erklärt sich daraus, dass die Ethik einerseits und das menschliche Eigeninteresse als Egoismus  andererseits oft einen Gegensatz bilden. Das Durchsetzungsproblem gewinne zudem durch die weltweite Globalisierung eine neue Dimension, die zu einer Ethik der Neomoderne führe.
Die Tatsache, dass die Menschen im Land X Hunger leiden und ihnen geholfen werden sollte, ja es moralisch geboten erscheint ihnen zu helfen, wird niemand bestreiten. Die Einsicht es auch zu tun, einen Großteil seines Vermögens dafür herzugeben, wird es im nennenswerten Umfang erst geben, wenn eine zusätzliche Motivation auftaucht, etwa die Gefahr einer Migration wegen Hungers ins eigene Land unmittelbar bevorsteht.
Das Durchsetzungsproblem zeigt sich auf andere Weise auch in der Erziehung, etwa wenn fest verinnerlichte Verhaltensregeln später auf entwickelte ethische Prinzipien stoßen.
Erkenntnisse der Evolutionären Spieltheorie lassen Rückschlüsse darauf zu, dass das Durchsetzungsproblem durch Selbstdurchdringung gelöst werden kann. Diese Auffassung vertraten zuerst Vertreter der Neuen Institutionenökonomik. So wiesen Eirik Furubotn und Rudolf Richter darauf hin, dass der Aufbau einer Reputation eine dominate Spielstrategie sein kann.
Die angewandte Ethik ist ein Teilbereich der allgemeinen Ethik. Teilbereiche der angewandten Ethik (oder Bereichsethik) sind beispielsweise Medizinethik, Umweltethik und  Wirtschaftsethik. Aufgabe der verschiedenen Bereichsethiken ist es, in Kommissionen, auf Instituten usw. Normen oder Handlungsempfehlungen für bestimmte Bereiche zu erarbeiten.
Ethik-InstituteIm deutschsprachigen Raum gibt es zahlreiche Ethik-Institute, die sich mit den weiten Problemfeldern der angewandten Ethik beschäftigen:
Institut für Ethik und Gesellschaftslehre an der Katholisch-Theologischen Fakultät der Karl-Franzens-Universität Graz (AT)
Institut für digitale Ethik (IDE), Hochschule der Medien, StuttgartEthikrateIn einer Reihe von Fachgebieten werden Ethikrate eingesetzt, welche begleitend zu oder auch in größeren Institutionen die Betrachtung ethischer Aspekte sicherstellen sollen. Diese sind teils gesetzlich fundiert wie beim Deutschen Ethikrat für die Medizin, teils nur per Proklamation aus einer Organisation selbst gebildet.
EinführungenArno Anzenbacher: Einführung in die Ethik. 3. Auflage. Patmos, Düsseldorf 2003, ISBN 3-491-69028-5 (gut lesbare Einführung)
Dieter Birnbacher: Analytische Einführung in die Ethik. De Gruyter, Berlin u. a. 2003, ISBN 3-11-017625-4 (systematische Darstellung der normativen Ethik aus Sicht eines analytischen Philosophen; moderne Ansätze stehen im Vordergrund)
Dagmar Fenner: Ethik. Wie soll ich handeln? UTB, Stuttgart 2008, ISBN 978-3-8252-2989-4 (gut strukturierte Einführung, etwas schulbuchhaft)
Dietmar Hübner: Einführung in die philosophische Ethik. UTB, 2. Aufl., Göttingen 2018, ISBN 978-3-8252-4991-5 (klare Systematik mit historischen Vertiefungen)
Annemarie Pieper: Einführung in die Ethik. 5. Auflage. Francke, Tübingen u. a. 2003, ISBN 3-8252-1637-3, ISBN 3-7720-1698-7 (vielzitierte Einführung in die Ethik)
Louis P. Pojman, James Fieser: Ethics. Discovering Right and Wrong. Wadsworth Pub. 2008, ISBN 978-0-495-50235-7. (exzellente, sehr klare, oft als Lehrbuch verwendete erste Einführung) (Inhaltsverzeichnis) (MS Word; 177 kB)
Der blaue reiter. Journal für Philosophie. Themenheft: Ethik. Nr. 3, 1995. Verlag der blaue reiter, ISBN 978-3-9804005-2-7.
Karl Hepfer. Philosophische Ethik. Eine Einführung. Göttingen 2008 (UTB 3117), ISBN 978-3-8252-3117-0 (Sehr übersichtliche und gut lesbare Darstellung aller gängigen Begründungsmodelle)
Michael Quante: Einführung in die allgemeine Ethik. Darmstadt 2003, ISBN 3-534-15464-9 (lehrbuchartig aufgebautes Werk mit Zusammenfassungen, Lektürehinweisen und Übungen am Ende jedes Kapitels; geht ausführlich auf metaethische Fragen ein)
Hans Reiner: Ethik. Eine Einführung. Studienausgabe, PAIS-Verlag, Oberried 2010, ISBN 978-3-931992-27-9 (gut verständliche Einführung)
Andreas Vieth: Einführung in die Philosophische Ethik. Münster/ München 2015, ISBN 978-3-7380-2658-0, PDF (themenorientiert, metaethisch, visuelle Themenaufbereitung, Lehrbuch)GesamtdarstellungenMarcus Düwell, Christoph Hübenthal, Micha H. Werner (Hrsg.): Handbuch Ethik. 2. akt. Auflage. Metzler, Stuttgart u. a. 2006, ISBN 3-476-02124-6 (derzeit das Standardhandbuch zur Ethik; enthält einen historischen und einen begrifflichen Teil; breite Berücksichtigung der aktuellen Diskussion; zum Teil sehr anspruchsvoll)
Hugh LaFollette (Hrsg.): Blackwell Guide to Ethical Theory. Blackwell, Oxford 2000. (Inhaltsverzeichnis)
Friedo Ricken: Allgemeine Ethik. 4. Auflage. Kohlhammer, Stuttgart 2003, ISBN 3-17-017948-9 (sehr fundiert und anspruchsvoll; versucht eine Synthese aus Aristotelischen und Kantischen Ansätzen mit Anleihen aus der analytischen Philosophie)
Hugh LaFollette (Hrsg.): Ethics in Practice: An Anthology. 4. Auflage. Wiley Blackwell, Oxford 2014, ISBN 978-0-470-67183-2.Lexika und GrundbegriffeOtfried Höffe (Hrsg.): Lexikon der Ethik. 6. Auflage. Beck, München 2002, ISBN 3-406-47586-8 (das Standardlexikon zur Einführung in die Begriffe der Ethik)
Gerhard Schweppenhäuser: Grundbegriffe der Ethik zur Einführung. 2. Auflage. Junius, Hamburg 2006, ISBN 3-88506-632-7 (konzentriert sich auf die Behandlung zentraler Grundbegriffe der Ethik)
Karl-Heinz Brodbeck: Ethik und Moral. eine kritische Einführung. (Memento  vom 14. Juni 2013 im Internet Archive) (PDF; 1,5 MB) Verlag BWT, Würzburg 2003, ISBN 3-9808693-1-8. (Freies, einführendes E-Book)
Henry S. Richardson: Moral Reasoning. In: Edward N. Zalta (Hrsg.): Stanford Encyclopedia of Philosophy.

Etowah County ist ein County im US-Bundesstaat Alabama. Es wurde 1868 gegründet und liegt im nordöstlichen Teil von Alabama nahe der Staatsgrenze zu Georgia. Etowah County zählt zum nördlichen, wirtschaftlich strukturschwachen Teil des Bundesstaats. Verwaltungssitz (County Seat) ist Gadsden. Zusammen mit den in der Peripherie von Gadsden liegenden Kleinstädten und Ortschaften bildet die Agglomeration Gadsden Metro Area im südlichen Teil des Countys das dominierende Ansiedlungszentrum.
Die im nordöstlichen Teil von Alabama gelegene Verwaltungseinheit hat eine Fläche von 1421 Quadratkilometern, wovon 36 Quadratkilometer Wasserfläche sind. 60 Kilometer entfernt von der Bundesstaatsgrenze zu Georgia, grenzt Etowah County im Uhrzeigersinn an folgende Countys: DeKalb, Cherokee, Calhoun, St. Clair, Blount und Marshall. Die Luftlinien-Entfernung zu Birmingham beträgt 78 Kilometer, die zur Bundesstaats-Hauptstadt Montgomery 175 Kilometer, die zu dem an der Golfküste gelegenen Hafenstadt Mobile 406 Kilometer und die zu Georgias Hauptstadt Atlanta 146 Kilometer. Von der Flächenausdehnung her ist Etowah – wenn auch nur knapp – das kleinste County Alabamas.
Die geografische Oberfläche der Verwaltungseinheit wird im Nordwesten und im Zentrum vom Cumberland-Plateau bestimmt. Südlich daran an schließt sich das den Appalachen vorgelagerte und vom Terrain her etwas gleichförmigere Piedmont-Plateau. Die Trennlinie bildet das Tal des Coosa River, welcher das County von Nordosten kommend in Richtung Südwest durchzieht und – auch aufgrund seiner Eigenschaft als wesentlicher Zufluss des Alabama River – die dominierende Wasserstraße ist. Zufluss und zweitgrößtes Fließgewässer im County ist der nördlich-parallel zum Coosa River verlaufende und in diesen einmündende Big Wills Creek. Die durchschnittlichen Ortshöhen variieren zwischen 150 und 350 Metern über dem Meeresspiegel. Die Landschaft ist hügelig und bewaldet. Die Böden gelten im Hinblick auf landwirtschaftliche Nutzung als weniger ergiebig; stattdessen finden sich im County zahlreiche Mineralien und Gesteinsarten. Ebenso wie der gesamte Norden Alabamas liegt auch Etowah County in der subtropischen Klimazone. Die Durchschnittstemperaturen schwanken – gemittelt aus Maximal- und Minimalwerten – zwischen rund 5 °C im Winter und über 25 °C im Sommer. Die Niederschlagsmenge beträgt gleichmäßig rund 120 mm pro Monat; lediglich in den Sommermonaten sinkt sie auf unter 100 mm ab (siehe auch Klimatabelle für Gadsden am Ende des Abschnitts).Besiedlungszentrum des Countys ist südliche Gebietsausbuchtung beidseits des Coosa River mit Gadsden als Mittelpunkt. Ansonsten prägt Streusiedlungsweise das Terrain mit einem entsprechend hohen Anteil an Unincorporated Areas – Ansiedlungen, die rein zu Verwaltungszwecken zusammengefasst sind und von der nächsthöheren Verwaltungsebene mitverwaltet werden. Größere Städte sind Attalla (Einwohnerzahl im Jahr 2010: 6.048), Boaz (lediglich mit südlichen Ausläufern in Etowah County hineinragend: 9.551 Einwohner), der County-Verwaltungssitz Gadsden (36.856 Einwohner), Glencoe (5.160), Hokes Bluff (4.286), Rainbow City (9.602) und Southside (8.412). Kleinere Ortschaften sind Altoona (933), Reece City (653), Ridgeville (112), Sardis City (1.704) und Walnut Grove (698). Hinzu kommen 15 nichtkorporierte Verwaltungseinheiten. Das Gebiet der größten – Ivalee – weist laut US-Zensus 2010 eine Einwohnerzahl von 12.349 auf. Die Einwohnerzahlen der restlichen 14 (darunter beispielsweise Anderson unmittelbar südwestlich von Gadsden) schwanken zwischen statistisch nicht erfasst und 2.137 (Carlisle-Rockledge). Tiefstgelegene Gemeindung ist mit 165 Meter über dem Meeresspiegel die County-Hauptstadt Gadsen, die höchste mit 339 Metern ein im Etowah County gelegener Ortsteil der Stadt Boaz mit der Bezeichnung Mountainboro.Wichtigste überregionale Verbindungsstraße ist die Interstate 59, welche von Norden nach Süden durch das County hindurchführt. Weitere wichtige Straßen sind die Bundesstraßen US-11 und US-411; beide verlaufen parallel zur Interstate 59. In Ost-West-Richtung verläuft die US-278, von Nordwesten nach Südosten die US-431.
Die weiter von der Golfküste weggelegenen Gebiete des heutigen Alabama waren bis Ende des 18. Jahrhunderts Stammesgebiet der Creek und Cherokee sowie der weiter westlich siedelnden Chickasaw und Choctaw. Politisch war das Territorium zwischen Golfküste, Mississippi und Appalachen vage als Teil von Georgia definiert. Erste strukturierende Maßnahmen erfolgten erst nach dem Ende des Unabhängigkeitskriegs. 1798 gründeten die USA das Mississippi-Territorium. Zunächst nur ein schmaler Streifen im Süden der heutigen Bundesstaaten Mississippi und Alabama, wurde es 1804 nach Norden bis zur Grenze zu Tennessee erweitert. Als letztes Teilstück kam 1812 die südwestliche Ecke Floridas mit der Hafenstadt Mobile hinzu. Bis zum Ende des Britisch-Amerikanischen Kriegs blieb Alabama Indianerland. Das nordöstliche Drittel – inklusive des Territoriums des späteren Etowah County – gehörte zum Stammesgebiet der Cherokee, die Gebiete südlich davon zum Territorium der Upper und Lower Creek.Größere Veränderungen ergaben sich Andrew Jacksons Krieg gegen die Creek von 1813 bis 1814, welcher das Gebiet von Alabama großflächig für weiße Siedler erschloss. 1819 wurde das (zuvor vom Mississippi-Territorium abgetrennte) Alabama-Territorium regulärer US-Bundesstaat. Im Gebiet des späteren Etowah County hatten sich nach dem Krieg gegen die Creek erste Besiedelungsaktivitäten entwickelt. Zunächst beschränkt auf die West- und Südwestecke des späteren County-Gebiets, erweiterte sich das zur Besiedlung freigegebene Terrain im Verlauf der 1830er-Jahre auf das gesamte County-Gebiet. Verknüpft war diese Gebietsfreigabe mit der Zwangsumsiedlung der in Ost-Alabama verbliebenen Cherokee und Creek ins Indianer-Territorium.
In den 1840ern wurden auch die bislang noch unerschlossenen Gebiete im Nordosten und Osten des späteren County zunehmend besiedelt. Als nächstgelegene Zentren fungierten in dieser frühen Phase Montgomery und Selma sowie Wetumpka am unteren Coosa River. 1836 erfolgte die Gründung der Poststation Double Springs. Nach der Überwindung logistischer Hindernisse – vor allem der zahlreichen Wasserfälle – wurde in den 1840er-Jahren auch der obere Teil des Coosa River für den Schiffsverkehr erschlossen. Auswirkung: ein verbesserter Anschluss an die in Georgia gelegenen Regionalzentren Augusta und Rome.
Double Springs war zwischenzeitlich zur beherrschenden Ansiedlung in dem Gebiet avanciert. 1846 wurde auf dem Gebiet die Stadt Gadsden gegründet. Lokalhistorische Quellen weisen als Begründer der Stadt die drei Expropriatoren Gabriel Hughes, Joseph Hughes und John S. Moragne auf. Die Stadtbenennung erfolgte nach James Gadsden – einem ehemaligen Offizier und Indianerkriegs-Veteran, der sich stark für eine südliche Transkontinental-Eisenbahnroute engagiert hatte und nach dem auch der Gadsden Purchase von 1853 benannt wurde.Im Verlauf des Amerikanischen Bürgerkriegs fanden in der Region 1863 einige größere Gefechte statt zwischen Unionstruppen unter Abel Streight und Konföderierten-Einheiten unter dem Kommando von Nathan Bedford Forrest, welche den Unionsvorstößen Richtung Süden hinhaltenden Widerstand entgegensetzten. Am 7. Dezember 1866 wurde das Etowah County offiziell begründet. Zusammengesetzt aus Gebieten umliegender Countys (vorwiegend Cherokee und DeKalb), firmierte es zunächst unter dem Namen Baine County. Da die Namenswahl – nach dem Konföderierten-General David W. Baine – unmittelbar nach Ende des Bürgerkriegs als Provokation empfunden wurde, annullierte das von Republikanern dominierte Staatsparlament von Alabama die County-Bezeichnung und begründete die Verwaltungseinheit 1868 als Etowah County neu.Erste Industrialisierungversuche hatte es in der hügelig-bewaldeten, für Landwirtschaft nur bedingt geeigneten Region bereits vor dem Beginn des Bürgerkriegs gegeben. 1845 wurde in Gadsden die erste Eisenhütte errichtet – die Coosa Furnace (später: Gadsden Furnace; 1911 stillgelegt). Die County-Hauptstadt selbst war bis 1888 auf 5000 Einwohner angewachsen. In den 1970ern wurde das County an das Eisenbahnnetz der Alabama Great Southern Railroad angeschlossen. Ende des 19. Jahrhunderts setzte die großflächigere industrielle Erschließung ein – vor allem der Abbau von Kohle sowie anderer Mineralien und Baustoffe wie Eisen, Mangan, Marmor, Schiefer und Kalkstein. Eine der bedeutendsten Kohleabbau-Gesellschaften war die Underwood Coal Company, die im östlichen Countyteil rund um die Kleinstadt Altoona elf Minen unterhielt. Eine für die Ökonomie des Countys bedeutende Gründung war die Baumwollspinnerei Dwight Mill in Alabama City, einem Nebenort von Gadsden. Flankierend hinzu kam 1903 der Bau eines Wasserkraftwerks am Big Wills Creek, welches die Stadt Attalla mit Strom versorgte. William Patrick Lay, der Erbauer, war einer der Mitbegründer von Alabamas wichtigstem Stromversorger – der Alabama Power Company.
In den ersten Jahrzehnten des 20. Jahrhunderts gesellten sich zu den bereits vorhandenen weitere Industrieansiedlungen. 1903 eröffnete die Gulf States Steel ein neues Stahlwerk im County. Zum größten wie auch beständigsten Arbeitgeber der Region avancierte der Reifenhersteller Goodyear Tire & Rubber Company. Ein herausragendes Ereignis der Lokalgeschichte war ein von Goodyear ausgerichteter Zeppelinflug 1930. Bei der wetterbedingten Notlandung des Zeppelins in Gadsen wurde ein Mitglied des Anseil-Teams hoch in die Luft gezogen und stürzte daraufhin tödlich ab. Während des Zweiten Weltkriegs war Etowah County ein wichtiger Rüstungsindustrie- und Ausbildungs-Stützpunkt – zum einen wegen der Granatenfabrik Gadsden Ordnance Plant, zum anderen wegen Camp Sibell, einem Trainingscamp des Chemical Warface Centers (CWC). Benannt war das Camp nach einem Kriegsteilnehmer des Ersten Weltkriegs. 1945 – nach Kriegsende – wurde es deaktiviert.Im Zug der Bürgerrechtsbewegung in den 1960er-Jahren geriet auch das Etowah County zeitweilig in den Fokus der Öffentlichkeit: 1963 wurde der Bürgerrechtler William Moore in der Nähe von Attalla ermordet. Die politische Entwicklung des Etowah County verlief weitgehend konform mit dem generellen politischen Trend im Solid South: In den 1930ern eine Hochburg der Demokraten sowie Mitprofiteur der unter Präsident Roosevelt eingeleiteten New-Deal-Politik, favorisierten die Wähler im County ab den 1960ern immer öfter konservativ-rechte Dixiecrat-Demokraten oder aber Kandidaten der Republikaner.Zeitlich parallel ging diese politische Umorientierung mit den Niedergang der die Region bestimmenden Industrien. Die Kohleminen im Osten rund um die Kleinstadt Altoona hatten bereits 1932 ihre Tore geschlossen; die Population der Stadt sank daraufhin von über 2000 auf weniger als die Hälfte. Die Dwight-Mill-Baumwollspinnerei – mit 2.600 Beschäftigten noch zu Anfang des Jahrzehnts der größte Arbeitgeber der Region – wurde 1959 stillgelegt. Das Stahlwerk von Gulf States Steel schloss – nach der Insolvenz des Unternehmens – im Jahr 2000 seine Pforten. Teilweise abgefangen wurde der wirtschaftliche Niedergang durch die Modernisierung des Reifenwerks von Goodyear in Gadsen sowie ein neues Automobil-Fertigungswerk von Honda im weiter südlich gelegenen Talladega County. Laut einem Artikel der Gadsden Times werden dessen 4.400 Beschäftigte zu rund einem Fünftel von Pendlern aus dem Etowah County bestritten.Die Infrastruktur-Probleme des Countys sorgten bereits Mitte der 1980er für Schlagzeilen. Anlass war ein Ranking von Reiseführer-Herausgeber Rand McNally, welches die County-Hauptstadt Gadsden als eine der sieben schlimmsten Städte der USA auflistete. Bedingt durch den Wegfall der Industrie sank die Belegungsdichte im Stadtkern der Countyhauptstadt Gadsden auf einen Wert von 60 Prozent; in den Bereichen Kunst und Kultur erreichte die Stadt einen Wert von Null. Ab den 1990er-Jahren brachte die Stadtverwaltung unterschiedliche Aufwertungsprojekte auf den Weg – unter anderen mittels einer Kulturstiftung sowie anderen Maßnahmen, die darauf abzielten, das Leben in der Stadt attraktiver zu gestalten. Im Jahr 2012 konstatierte Kay Moore, Direktorin der Downtown Gadsden Inc., gegenüber dem Wirtschaftsnachrichten-Portal Business Alabama, dass der Gebäudeauslastungsfaktor zwischenzeitlich auf einen Faktor von 92 Prozent angestiegen sei.Für Aufmerksamkeit sorgte die Bezirkshauptstadt Gadsden schließlich 2017 – anlässlich der Senatsnachwahl-Kandidatur des politisch rechtsaußen stehenden Republikaners Roy Moore. Wie die Washington Post enthüllte, hatte der aus dem County stammende Moore während seiner Zeit als stellvertretender Bezirksstaatsanwalt in Gadsden regelmäßig Frauen und auch Teenager belästigt – Vorkommnisse, die im Vorfeld der Senatsnachwahl für politische Turbulenzen sorgten und zum Ergebnis hatten, dass auch Teile des ortsansässigen evangelikalen Milieus zu Moore auf Abstand gingen.
Die Einwohnerentwicklung des Countys spiegelt die industrielle Erschließung zu Ende des 19. Jahrhunderts wider. Lebten um 1870 etwa 10.000 Bewohner in den Gebiet, steigerte sich die Einwohnerzahl bis 1900 auf über 25.000, bis 1930 auf über 63.000. Ihren Zenit erreichte die Einwohnerentwicklung in der zweiten Hälfte des 20. Jahrhunderts. Seit 1980 stagniert sie mit leichten Unterschieden bei über 100.000. Nach Angaben der Volkszählung im Jahr 2000 lebten im Etowah County 103.459 Menschen. Davon wohnten 2.043 Personen in Sammelunterkünften, die anderen Einwohner lebten in 41.615 Haushalten und 29.463 Familien. Die Bevölkerungsdichte betrug 75 Einwohner pro Quadratkilometer. Ethnisch betrachtet setzte sich die Bevölkerung zusammen aus 82,87 Prozent Weißen, 14,68 Prozent Afroamerikanern, 0,33 Prozent amerikanischen Ureinwohnern, 0,42 Prozent Asiaten, 0,03 Prozent Bewohnern aus dem pazifischen Inselraum und 0,73 Prozent aus anderen ethnischen Gruppen; 0,93 Prozent stammten von zwei oder mehr Ethnien ab. 1,70 Prozent der Bevölkerung waren spanischer oder lateinamerikanischer Abstammung.
Von den 41.615 Haushalten hatten 29,9 Prozent Kinder und Jugendliche unter 18 Jahren, die bei ihnen lebten. In 54,2 Prozent lebten verheiratete, zusammen lebende Paare, 13,1 Prozent waren allein erziehende Mütter, 29,2 Prozent waren keine Familien, 26,3 Prozent aller Haushalte waren Singlehaushalte und in 12,4 Prozent lebten Menschen im Alter von 65 Jahren oder darüber. Die Durchschnittshaushaltsgröße betrug 2,44 und die durchschnittliche Familiengröße betrug 2,93 Personen. 23,8 Prozent der Bevölkerung waren unter 18 Jahre alt, 8,7 Prozent zwischen 18 und 24, 27,4 Prozent zwischen 25 und 44, 24,1 Prozent zwischen 45 und 64 und 16,0 Prozent waren 65 Jahre oder älter. Das Durchschnittsalter betrug 38 Jahre. Auf 100 weibliche Personen kamen 91,8 männliche Personen und auf Frauen im Alter von 18 Jahren und darüber kamen 87,9 Männer. Das jährliche Durchschnittseinkommen eines Haushalts betrug 31.170 USD, das Durchschnittseinkommen einer Familie 38.697 USD. Männer hatten ein Durchschnittseinkommen von 31.610 USD, Frauen 21.346 USD. Das Prokopfeinkommen betrug 16.783 USD. 12,3 Prozent der Familien und 15,7 Prozent der Einwohner lebten unterhalb der Armutsgrenze.
Anlässlich einer Story über den umstrittenen republikanischen Nachwahl-Kandidaten Roy Moore charakterisierte das Magazin New Yorker die Region mit den Attributen „waldig, blau und religiös“. Im County, das wie meisten Südost-Bundesstaaten dem sogenannten Bible Belt zugerechnet wird, lebt selbst für Nord-Alabama eine überdurchschnittlich hohe Anzahl an evangelikalen Christen. Auch was die Favorisierung republikanischer Präsidentschaftskandidaten anbelangt, übertraf Etowah County seit 2004 die bundesstaatlichen Vergleichswerte. Gerundet auf ganze Prozentpunkte gewann George W. Bush 2004 in Etowah County mit 63 Prozent (Alabama: 60 Prozent). Gegen den demokratischen Kandidaten Barack Obama holten die Republikaner McCain und Romney 2008 und 1012 jeweils 68 Prozent (Alabama: 60 bzw. 61 Prozent).
Donald Trump schließlich wurde in Etowah County von 73 Prozent der Abstimmenden gewählt (Alabama: 62 Prozent). Deutliche Abweichungen vom bundesstaatlichen Trend gab es lediglich 1992 und 1996, als Bill Clinton für die Demokraten antrat: Während Clinton den Bundesstaat an seine republikanischen Konkurrenten George H. W. Bush und Dole verlor, votierte in Etowah County eine deutliche Mehrheit für ihn. Abweichungen zum Bundesstaats-Ergebnis gab es auch bei den Kandidaturen von Barry Goldwater 1964 und George Wallace 1968. Während der rechtsorientierte Republikaner Goldwater in Etowah County um zehn Prozentpunkte schlechter abschnitt als im landesweiten Durchschnitt, lag das Ergebnis des gleichfalls mit einer rechtskonservativen Agenda auftretenden Dixiecat-Demokraten Wallace vier Jahre später um drei Prozentpunkte über dem Landesdurchschnitt.Die Präferenz der Wählermehrheit für rechtskonservative Republikaner bestätigten auch die Vorkommnisse um die Senatsnachwahl-Kandidatur des republikanischen Hardliners und Trump-Favoriten Roy Moore. Bereits der bisherige Senatsvertreter Jeff Sessions, den Donald Trump als Justizminister in sein Kabinett berief, galt innerhalb der republikanischen Partei als erzkonservativer Hardliner. Bei der am 12. Dezember 2017 erfolgten Nachwahl votierte zwar eine knappe Mehrheit der Wähler in Alabama für Moores demokratischen Gegenkandidaten Doug Jones. In Etowah County allerdings konnte Moore seine Mehrheit mit 58 % gegen 39 % klar verteidigen.Obwohl ein Teil des evangelikalen Milieus angesichts der Missbrauchs-Anschuldigungen auf Abstand zu Moore gegangen war, zeigte sich bezüglich des Kandidaten Moore im County ein durchwachsenes Bild. Ein Bericht der Gadsden Times machte in diesem Milieu drei Abstufungen aus bezüglich der Unterstützung oder Nicht-Unterstützung von Moore: a) grundsätzliche Kritik am Verhalten beziehungsweise Unvereinbarkeit desselben mit christlichen Werten, b) Kritik am Verhalten, allerdings Relativierung aufgrund des Zeitpunkts der Veröffentlichungen, c) bedingungslose Solidarität aufgrund des Favorisierens konservativer Republikaner um (fast) jeden Preis. Eine ähnlich zwischen Ablehnung, Verunsicherung und Indifferenz changierende Stimmung dokumentierte auch eine Vor-Ort-Reportage des britischen Guardian.Defizite in Sachen Lebensqualität wurden seitens unterschiedlicher Medien mehrmals zum Thema gemacht. Die bekannteste dieser Veröffentlichungen ist der weiter oben angeführte Artikel von Rand McNally, welcher Gadsden als eine der sieben schlimmsten US-Städte klassifizierte. Ähnlich schlechte Zustände konstatierten im September 2017 die Breaking-News-Webseite al.com sowie die Site 24/7 Wall St. Basierend auf einer in Einzel-Problemfelder aufgegliederten Untersuchung hatte letztere ein Ranking erstellt bezüglich des Aspekts Lebensqualität für Frauen. Auch in diesem Ranking rangierte Gadsden aufgrund schlechter Bedingungen in Sachen Lohngleichheit, Bachelor-Abschlussmöglichketen und Lebenserwartung weit hinten. Strukturelle Probleme werden auch seitens offizieller County-Vertreter offen zugegeben. Hier hebt man allerdings hervor, dass – speziell im Bereich Infrastruktur-Modernisierung – im neuen Jahrtausend eine Menge passiert sei.
Eine der größten Werksniederlassungen in der Region ist nach wie vor das Reifenwerk von Goodyear in Alabama City. Als Großunternehmen hinzugekommen ist Honda, dessen Werk allerdings nicht in Etowah County selbst liegt, sondern 30 Autominuten südlich davon. Die Arbeitslosigkeit erreichte im Oktober 2000 mit 11,4 % ihren höchsten Stand. Gestützt auf Angaben des Alabama Department of Industrial Relations sowie des Center for Business and Economic Research der University of Alabama, klassifizierte das Gadsden-Etowah County Industrial Development Authority im Jahr 2008 12.000 Einwohner des Etowah County als unterbeschäftigt und 2.179 Einwohner als arbeitslos.Die derzeitigen Beschäftigungssektoren lassen sich laut Angaben des Webportals Encyclopedia of Alabama wie folgt aufteilen:
Dem Etowah County: Das Etowah County School System beschäftigt derzeit 540 Lehrer, die mehr als 8.400 Schüler in 22 Schulen betreuen.
Der Stadt Gadsden: Das Gadsden City School System beschäftigt derzeit 362 Lehrer, die mehr als 5.400 Schüler in 17 Schulen betreuen.
Der Stadt Attalla: Das Attalla City School System beschäftigt derzeit 115 Lehrer, die mehr als 1.800 Schüler in vier Schulen betreuen.Darüber hinaus gibt es im County vier private Schulen mit zusammen etwa 1.200 Schülern. Komplettiert wird das Bildungsangebot der Verwaltungseinheit vom Gadsden State Community College, einer zweijährigen koedukativen Einrichtung.
Die Erholungs- und Kulturangebote im Etowah County decken vor allem drei Bereiche ab: a) Outdoor- und Erholungsziele mit dem Noccalula-Wasserfall bei Gadsden als überregional bekanntem Highlight, b) den Historic Districts in Gadsden und Altoona als [Städtebau|städtetebaulich]]-architektonische Sehenswürdigkeiten und c) größere, regelmäßig ausgerichtete Veranstaltungen. Als besondere Attraktionen gelten:
die Noccalula Falls in Gadsden – ein 100-Fuß-Wasserfall. Im Zentrum des zwei Kilometer von Downtown Gadsden entfernten Parkgeländes liegt mit dem Noccalula-Wasserfall einer der spektakulärsten Wasserfälle am Oberlauf des Coosa River. Der Wasserfall selbst ist Teil des Black Greek, der bei Gadsden in den Coosa River einmündet.
der zum Robert Trent Jones Golf Trail gehörende Golfplatz Silver Lakes nahe Glencoe mit 36 Abschlagslöchern. Die in den 1990ern erbaute Golfplatz-Strecke, die in Nord-Süd-Richtung quer durch Alabama verläuft, wurde über Investitionen aus den Pensionsfonds öffentlicher Angestellter finanziert und gehört zu den international anerkannten Profigolf-Turnierplätzen.
Der Neely Henry Lake nahe Gadsden ist ein Stausee. Die dazugehörige Talsperre wurde 1966 von der Alabama Power Company errichtet. Der als Erholungsgebiet dienende See bietet einige der besten Angelmöglichkeiten der Gegend.
der Historic District Gadsden Downtown Historic Mall mit Gebäuden aus der Gründerzeit sowie dem frühen 20. Jahrhundert liegt im Stadtzentrum und repräsentiert ihre industrielle Blütezeit von den späten 1870ern bis in die 1940er. Als Historic Place ausgewiesen ist darüber hinaus auch das U.S. Post Office Attalla in der gleichnamigen Stadt.
The World’s Longest Yard Sale: ein bundesstaats-übergreifender Straßenflohmarkt, der einmal jährlich (in der Regel im August) stattfindet und drei Tage dauert.
Altoona Day: ein Stadtfest mit Musik-, Kunst- und Handwerksverkäufern, Lebensmittelverkäufern, einer Oldtimer-Show sowie prominenten Gästen. Darüber hinaus findet in Altoona eine jährliche Weihnachtsparade statt.
Thomas McAdory Owen: History of Alabama and Dictionary of Alabama Biography. S. J. Clarke Publishing Co., Chicago IL 1921.
Virginia O. Foscue: Place Names in Alabama. University of Alabama Press, Tuscaloosa u. a. 1989, ISBN 0-8173-0410-X.
Encyclopedia of Alabama. Enzyklopädieportal zu Alabama; enthält auch zum Etowah County eine Reihe Themenartikel (Engl.)
Overview of Etowah County, Alabama. Datenüberblick-Seite zum Etowah County bei statisticalatlas.com (Engl.)
Das Etschmiadsin-Evangeliar ist ein 989 im Kloster Bgheno-Norawank angefertigtes Evangeliar in Form eines armenischen illuminierten Buches, das in der Sammlung des Matenadaran in Jerewan unter der Nummer MS 2374, vormals 229, aufbewahrt wird. Die Ruine des Klosters liegt in der heutigen südarmenischen Provinz Sjunik. Das Etschmiadsin-Evangeliar gilt als die bedeutendste armenische Handschrift. Es enthält neben dem aus 224 Blättern bestehenden Textteil drei stilistisch unterschiedliche Gruppen von Miniaturmalereien: die zeitgleich mit dem Text entstandenen Kanontafeln vor dem Anfang des Kodex, im Text verstreute Randzeichnungen aus dem 11./12. Jahrhundert und – von besonderer kunsthistorischer Bedeutung – vier ganzseitige Miniaturen am Schluss, die Ende des 6. oder Anfang des 7. Jahrhunderts datiert werden. Sie sind die ältesten erhaltenen armenischen Buchmalereien. Der Einband aus reliefiertem Elfenbein ist eine byzantinische Arbeit aus dem 6. Jahrhundert und stellt das wertvollste Elfenbeinobjekt in einem armenischen Museum dar.
Die Armenische Apostolische Kirche führt ihre Tradition auf zwei Apostel zurück, die im 1. Jahrhundert das Christentum nach Armenien gebracht haben sollen. Drei Ereignisse in der frühchristlichen Zeit sind für die armenische Nation wesentlich: 314 erklärte König Trdat III. das Christentum zur Staatsreligion, im Jahr 405 führte Mesrop Maschtoz die armenische Schrift ein, was als religiöses Wunder aufgefasst wurde, und im 5. Jahrhundert verfasste Moses von Choren (Movses Chorenatsi) die erste Chronik Armeniens. In der frühchristlichen Zeit wurden die Bibel, liturgische Anweisungen und Texte der frühen Kirchenväter in altarmenischer Schrift aufgezeichnet, Mönche kopierten möglichst genau die Handschriften und statteten sie mit kostbaren Einbänden aus. Der Schrift selbst wurde nach alter vorchristlicher Tradition eine magische Bedeutung zuerkannt. Wer eine Handschrift kopierte, erwarb sich religiöse Verdienste. Es war für einen König, Fürsten oder einen hohen Geistlichen ebenso verdienstvoll, eine Handschrift wie eine Kirche zu stiften. Handschriften wurden neben Kirchen und Chatschkaren zu einem der Symbole armenischer Identität und genießen eine entsprechende Wertschätzung. Sie galten wie die Ikonen der Orthodoxen Kirchen als heilig und mussten in den Klöstern stehend aufbewahrt werden, um sie von weltlichen Gegenständen zu unterscheiden. Aus dem Mittelalter sind rund 30.000 Handschriften bekannt, die neben religiösen Themen alle damaligen Wissenschaften beinhalten.Es gab – wenn auch gegenüber der byzantinischen Kunst in bescheidenerem Umfang – großflächige armenische Wandmalereien in frühchristlicher Zeit; die armenische Bildkunst entwickelte sich jedoch vor allem im Format der Handschriften. Ein neben den Miniaturen weiterer wesentlicher Bestandteil der Handschriften ist das am Ende angefügte Kolophon (armenisch hischatakaran, „Gedächtnis“). Darin berichtet der Schreiber detailliert über seine Lebensumstände, Arbeitsweise, Auftraggeber und die mit ihm an der Herstellung des Buches beteiligten Personen. Ein Kolophon ist in den meisten armenischen Handschriften vorhanden. An das Hauptkolophon des Schreibers schließen sich häufig weitere Zusätze aus späterer Zeit an, aus denen sich die Veränderungen am Buch und dessen wechselnde Besitzer und Aufbewahrungsorte erkennen lassen.Die aus dem 9. und 10. Jahrhundert überlieferten armenischen Handschriften sind fast alle Evangeliare. Das undatierte und unvollständig erhaltene Evangeliar der Königin Mlke (Venedig, MS 1144/86), dessen Herstellungsort unbekannt ist, ist das älteste bekannte armenische Evangeliar; es wurde 851 abgeschrieben und 862 eingebunden. Das älteste datierte Manuskript ist das Lazarian- (Lazarev-)Evangeliar von 877 (Mat. MS 6200). Fünf weitere Handschriften aus dem 10. Jahrhundert sind bis zum Etschmiadsin-Evangeliar von 989 bekannt. Die Malereien zeigen im 9. Jahrhundert einen ausgereiften Stil, weshalb es in den Jahrhunderten zuvor bereits eine reiche Tradition von Buchillustrationen gegeben haben muss. Den wertvollsten Hinweis darauf geben die zwei dem Etschmiadsin-Evangeliar am Ende angefügten Blätter, ansonsten sind nur Rückschlüsse aus den Kommentaren einiger Geschichtsschreiber möglich.
Am Anfang der Evangeliare finden sich Kanontafeln, die von Schmuckarkaden umrahmt dem Leser einen Überblick über den biblischen Inhalt geben sollten. Die Indexierung folgt einem System, das Eusebius von Caesarea in Palaestina Anfang des 4. Jahrhunderts einführte. Damit konnten die in den vier Evangelien unterschiedlich dargestellten Episoden aus dem Leben Jesu einander zugeordnet werden. Eusebius erfand eine durchnummerierte Gliederung in mehrere hundert Abschnitte für alle Evangelien, die dadurch harmonisiert wurden, ohne den Inhalt anzutasten. Letzteres war erforderlich, damit die Texte als authentische Zeugnisse erhalten blieben. Im Jahr 331 bestellte Konstantin der Große bei Eusebius für die Liturgie in den Kirchen Konstantinopels mehrere Evangeliare und andere Schriften. Eusebius gestaltete sie den Vorgaben entsprechend aufwendig, indem er die aus der römischen Sakralarchitektur geläufigen Arkaden als Schmuckmotive übernahm.
Nur in den armenischen Handschriften wurde das ursprüngliche Format des Eusebius von zehn Kanontafeln unverändert überliefert. Das Etschmiadsin-Evangeliar hat von allen Handschriften des 1. Jahrtausends die Urform der von Konstantin bestellten Tetraevangeliare am getreuesten bewahrt, erst später erhielten die dargestellten spätrömischen Architekturformen durch eine charakteristische ornamentale Gestaltung eine armenische Prägung. Während üblicherweise die Evangelien in der Reihenfolge Matthäus, Markus, Lukas und Johannes sortiert werden, folgt das Etschmiadsin-Evangeliar dem alten eusebianischen Ordnungsprinzip und stellt den dritten Apostel Lukas vor den zweiten Apostel Markus.Die Arkaden auf den Kanontafeln bilden mit einer halbkreisförmigen Lünette darüber ein gemaltes Portal, das für den Leser den Zugang zum Text versinnbildlicht. Dieser fühlt sich in der Rolle des Gläubigen, der durch ein Portal ein Heiligtum betritt. Die Kanontafeln sind des Weiteren mit geometrischen Formen und Blumen verziert; die abgebildeten Vögel lassen sich als Pfauen, Reiher, Hähne, Rebhühner und Tauben identifizieren oder stellen Phantasievögel dar. Die symbolische Bedeutung der in vielen Variationen auftauchenden Vögel ist nur teilweise bekannt. Im Etschmiadsin-Evangeliar kommt ein Vogel in einem Käfig vor, der seit der Antike die in ihrem Körper gefangene Seele symbolisiert. Was das in armenischen Manuskripten einzigartige Motiv an dieser Stelle bedeutet, ist jedoch unklar.
Die ersten Erwähnungen des Etschmiadsin-Evangeliars stammen aus den Jahren 1840 und 1851 von Marie Felicité Brosset. Es folgten genauere Beschreibungen von Graf J. J. Uwarow (1862), Wladimir Stassow (1886), Josef Strzygowski (1891), Frédéric Macler (1920), Kurt Weitzmann (1933), Sirarpie Der Nersessian (1933 und 1964), Thomas F. Mathews (1980) und Matevosyan Artashes (1990). Die umfangreiche kunsthistorische Forschung zum Etschmiadsin-Evangeliar konzentrierte sich auf die beiden angefügten Blätter mit den vier frühchristlichen Miniaturen. Strzygowski bezeichnete in seiner detaillierten Beschreibung, mit der er das Evangeliar erstmals unter Kunsthistorikern bekannt machte, die erste Gruppe von 15 Abbildungen fälschlich als syrische Malereien aus dem 5. Jahrhundert und die vier abschließenden Miniaturen ebenfalls als syrisch aus dem 6. Jahrhundert. Die im Text am Rand verstreuten groben Zeichnungen hielt er für das Werk des Schreibers Hovhannes, die deshalb relativ plump ausgefallen seien, weil der Kalligraph offensichtlich kein gelernter Illustrator war. 1911 revidierte Strzygowski seine Auffassung zum Teil und hielt nun die Miniaturen am Anfang für im 10. Jahrhundert angefertigte Kopien der Originale des 6. Jahrhunderts. Für das 10. Jahrhundert plädierte auch Frédéric Macler in seinem Vorwort zur Faksimile-Edition von 1920.Die Forschungen von Sirarpie Der Nersessian schafften letztlich Klarheit über die Entstehungsgeschichte. Demnach entstanden die Miniaturen am Anfang zeitgleich mit dem Text im Jahr 989, während die Randzeichnungen im 11. oder 12. Jahrhundert hinzukamen, als das Evangeliar in einzelne Perikope aufgeteilt wurde. Nach Der Nersessian sind die vier abschließenden Miniaturen in vorarabischer Zeit – also vor der ersten arabischen Eroberung der Hauptstadt Dvin 640 – entstandene, armenische Schöpfungen.Die Elfenbeindeckel stammen nach einhelliger Ansicht aus dem 6. Jahrhundert und nach heutiger Erkenntnis aus einem Teil des Byzantinischen Reiches. Die in der ersten Hälfte des 20. Jahrhunderts angestellten Mutmaßungen über die Herkunft aufgrund der ikonografischen Analysen reichten von Ravenna (Strzygowski, 1891), Syrien (Strzygowski, 1902; Oskar Wulff, 1912) und aus dem östlichen Byzanz (Wolfgang Fritz Volbach, 1952). Ferner wurden Ägypten und Kleinasien genannt.
Laut dem ersten Kolophon fertigte der Schreiber Hovhannes (Johannes) im Kloster Norawank im Auftrag des Priesters Ter Stepanos im Jahr 989 eine Abschrift von einer alten und „originalen“ Vorlage an. Aus dem Kolophon geht nicht hervor, welches von den vier Norawank („neues Kloster“) benannten Orten gemeint war. Am bekanntesten ist heute das Norawank in der Provinz Wajoz Dsor. Josef Strzygowski ging 1891 noch von diesem Norawank als dem Herstellungsort aus. Bis 1933 war der Nachweis erbracht, dass es sich um das kleinere Norawank im Distrikt Bgheno in der Nähe von Goris handelt. Das dortige Kloster wurde 935/936 von einem Priester Ter Stepanos gegründet. Wie es in einem zweiten Kolophon von 1213 heißt, brachte Gurzhin, der Sohn von Vahram, das Evangeliar zur Kirche des heiligen Stephanus, des ersten christlichen Märtyrers, in das Magharda-Kloster (heute in der iranischen Provinz West-Aserbaidschan). Von dort nahm es Makar Petrosian (zu der Zeit Wardapet, von 1885 bis 1891 Katholikos) 1847 nach Etschmiadsin, wo sich der Sitz der Armenisch-Apostolischen Kirche befindet.
Im Hauptteil besteht das Buch aus 224 Blättern, die in 28 Gruppen nach dem armenischen Alphabet bezeichnet zu je acht Blättern geordnet sind. Hinzu kommen acht Blätter am Anfang, die mit den beiden hinteren Blättern die Gesamtzahl von 234 Blättern aus Pergament in den Abmessungen von 35 Zentimetern Höhe und 28 Zentimetern Breite ergeben. Die Textseiten sind wie bei den meisten armenischen Manuskripten zweispaltig mit neun Zentimetern Breite, die Schrift ist eine sorgfältig ausgeführte Unziale (armenisch erkathagir). Bei der Aufteilung im 11./12. Jahrhundert wurden die Verse mit kleineren Unzialen nummeriert. Ausbesserungen fehlender Wörter und Korrekturen erfolgten ebenfalls mit einer kleineren Schrift und wie die Textzeichnungen von anderer Hand. Die hinzugefügten Illustrationen des Textes sind von deutlich einfacherer Qualität als die ganzseitigen Miniaturen. Die Figuren von Maria oder anderen Heiligen überlappen sich zum Teil und sind auf dem beengten Raum häufig nur unvollständig ausgeführt.Das Evangeliar beinhaltet auf den ersten beiden Seiten (fol. 1, 1v) der Brief des Eusebius an Carpianus, worin er die Einteilung der Evangelien in einzelne Abschnitte erklärt, gefolgt von zehn Kanontafeln (fol. 2–5) und der Abbildung eines Miniaturtempels (Tempietto, fol. 5v). Die nächsten Seiten zeigen das Jesuskind mit den Aposteln Paulus und Petrus, Porträts der vier Evangelisten, ein Marienbildnis und das Opfer Abrahams. Die Textseiten der vier Evangelien schließen mit dem Johannesevangelium ab. Zwischen diesem und den Kolophonen sind zwei beidseitig bemalte Blätter eingefügt. Sie stellen auf Blatt 228 die Verkündigung an Zacharias (Ankündigung der Geburt des Johannes durch den Engel Gabriel an Zacharias), auf Blatt 228v Mariä Verkündigung, auf Blatt 229 die Anbetung der Könige und auf Blatt 229v die Taufe Jesu dar.
Der Text des auf den Seiten 1 und 1v ausgebreiteten Eusebius-Briefes wird seitlich von Säulen und einem Bogenfeld darüber umrahmt, das auf der ersten Seite von einer von Blättern umgebenen Knospe bekrönt wird, zu der zwei riesige Pfaue blicken. Über dem zweiten Teil des Textes ist stattdessen in der Mitte eine Fruchtschale mit kleineren Tauben zu beiden Seiten zu sehen. Ähnlich von Bögen eingerahmt und dekoriert sind die Textspalten der Kanontafeln.
Blatt 5 weicht mit einem Tempietto von diesem Muster ab. Vier Säulen mit marmorierten Schäften, die offensichtlich den Kreis einer Rotunde bilden, stehen auf von Wülsten gebildeten runden Basen. Die korinthischen Kapitelle tragen einen perspektivisch gekrümmten Architrav, der aus mehreren Streifen gebildet wird und im breiten mittleren Feld eine Reihe vereinfachter Palmetten enthält. Auf dem Dach sprießen Blumen, deren Blüten aus einem symmetrischen Blattpaar aufsteigen. Zwei zur Mitte blickende Enten auf jeder Seite füllen den Raum zwischen den Blüten, je eine Ente auf der Traufe scheint seitwärts wegfliegen zu wollen. Die Dachspitze bildet eine Kugel mit einem Kreuz. Die mit einem Knoten gerafften Vorhänge zwischen den Säulen geben einen realistischen Bühnenhintergrund (scenae frons des römischen Theaters) wieder. Durch das Kreuz an der Spitze ist das Gebäude als christlich gekennzeichnet, durch die in der Mitte herabhängende kleine Leuchte wird es möglicherweise zum Altarraum einer Kirche.
Die folgenden drei Seiten übernehmen wieder das anfängliche Bildprogramm der Kanontafeln des von einem Halbkreis überhöhten Säulenportals. Auf Blatt 6 sitzt Christus auf einem Thron, umgeben von zwei stehenden Heiligen. Die Lünette ist mit Blumen und Enten in leuchtendroten Farben vor einem blauen Hintergrund bemalt. Die Mitte in der Lünette bildet eine Muschel mit strahlenförmigen Zacken. Zu beiden Seiten der Fruchtschale an der Spitze des Bogens sind Papageien und Granatapfelzweige zu sehen. Der jugendliche Christus mit kurzem Haar und ohne Bart hebt segnend die angewinkelte rechte Hand. In der linken Hand, die unter dem langen dunkelroten Mantel verborgen ist, hält er ein Kreuz. Neben ihm stehen zwei alte Männer, die in einen Chiton und einen weiteren Umhang darüber gekleidet sind. Ihre Rechte ist wie bei Christus zum Segen erhoben, auf der linken Seite tragen sie eine Schriftrolle. Die Blätter 6v und 7 zeigen jeweils zwei stehende Evangelisten, die nicht identifizierbar sind. Wie vorher bei den Heiligen ist auf die linke Seite ihres Mantels der Buchstabe H gezeichnet, der sich an den Gewändern aller männlichen Figuren findet und laut Strzygowski eine Modeerscheinung darstellt, die bereits in der römischen Zeit weit verbreitet war. Links tragen sie eine Schriftrolle und ihr Haupt ist von einem Nimbus umgeben.
Die Motive auf den Blättern 7v und 8 sind anstelle der Säulen am Blattrand von einem roten Rahmen umgeben. Eine Muttergottes vom Zeichen sitzt auf einem Thron, seitlich begrenzt von Vorhängen, deren Knoten formal dem um den Thron drapierten Stoff entsprechen. Maria (Panagia) ist mit einem purpurfarbenen Chiton und einer Paenula darüber bekleidet. Das Jesuskind in ihrem Schoß hat wie zuvor der thronende Christus die rechte Hand zum Segen erhoben und hält in der linken ein Kreuz.
Bei der Opferung Isaaks auf Blatt 8 liegt Isaak auf der rechten Seite des Bildes mit auf den Rücken gebundenen Händen in einem langen gelben Chiton auf einer Art Steintreppe. Die Treppe führt hinauf zu einem Altar, aus dem Flammen schlagen. Neben Isaak steht Abraham mit einem langen Schwert in der rechten Hand, die linke Hand auf den Kopf seines Sohnes gelegt. In der linken oberen Ecke taucht die Hand Gottes aus lodernden Flammen auf, links unten ist ein Widder an eine Zypresse angebunden.Insgesamt verweisen die Miniaturen auf Vorbilder in der byzantinischen Kunst. Gegenüber dem wenige Jahre vorher entstandenen Mlke-Evangeliar sind die Tiere und Pflanzen weniger lebendig und stilisierter gemalt. Besonders die Figuren der biblischen Szenen wirken vor den schematisch arrangierten antiken Architekturkulissen erstarrt. Die Personen stehen oder sitzen dem Betrachter in steifer Körperhaltung frontal zugewandt. Die Vorliebe für strenge Muster anstelle plastischer Gestaltung kennzeichnet im Wesentlichen auch die armenische Steinbearbeitung. Ein herausragendes Beispiel für diesen armenischen Stil in der Wandmalerei und Bauplastik ist die Kirche von Akdamar.
Die vier am Schluss angefügten Miniaturmalereien gehören zum frühchristlichen Themenkreis. Sie sind auf die Vorder- und Rückseite von zwei Blättern gemalt und erinnern stilistisch an einige in Enkaustik-Technik hergestellte byzantinische Ikonen des 5. und 6. Jahrhunderts sowie an die Fresken in der Euphrasius-Basilika (543–553) in der kroatischen Stadt Poreč. Wegen der sassanidischen Kleidung der drei Weisen wurde außerdem angenommen, dass die Vorlage für die Miniaturen aus einer Zeit stammt, als Armenien unter persischer Oberherrschaft stand. Ein motivischer Ursprung liegt vermutlich in den Klöstern Palästinas, mit denen die Armenier in regem Kontakt standen. Ferner wurden Vergleiche zu den Wandmalereien der im 7. Jahrhundert erbauten Kirche Lmbatavank in der Nähe der nordarmenischen Stadt Artik gezogen: Die ovalen hellen Gesichter mit weit geöffneten Augen tauchen dort ähnlich an den Tetramorphen in der Apsiskalotte auf. Eine eigenständige armenische Handschrift ist erst ab dem 11. Jahrhundert erkennbar, etwa beim Mughni-Evangeliar um 1060, das in einer statischeren, kühleren Form auf dieselben frühen Vorbilder zurückgreift.
Die beiden Blätter zeigen die frühesten erhaltenen armenischen Miniaturen und stammen allein deshalb aus einer anderen Zeit als die übrige Handschrift, weil sie nachträglich an die Falze ausgeschnittener Blätter genäht wurden. Ihr umfassendes Thema ist die Erscheinung des Herrn (Epiphanie), die in den Ostkirchen am 6. Januar gefeiert wird, um Jesu Geburt und Taufe zu ehren. Mit diesem Tag beginnt das armenische Kirchenjahr.
In der Geschichte des Lukas-Evangeliums (fol. 228, (Lk 1,5-25;57-66 )) kündigt der Engel Gabriel Zacharias die Geburt seines Sohnes Johannes an, ein in der frühchristlichen Kunst sehr seltenes Thema. Die beiden einzigen anderen Darstellungen aus dieser Zeit sind im Augustin-Evangeliar aus dem 6. Jahrhundert, dem ältesten erhaltenen Evangeliar in lateinischer Sprache, und im syrischen Rabbula-Evangeliar von 586 enthalten.
In der Etschmiadsin-Miniatur ist im Hintergrund ein Giebeldach zu sehen, das von korinthischen Säulen getragen wird. Zwischen den beiden mittleren Säulen des Giebels spannt sich ein mit Edelsteinen verzierter Rundbogen. Ein zwischen die Säulen gespannter hellroter Vorhang verbirgt teilweise das Innere. Vor dem Gebäude steht ein rechteckiger Altar, der mit einer dunklen Decke überzogen ist. Auf der linken Seite des Bildes steht halb hinter dem Altar der Engel Gabriel. Er ist in ein langes weißes Gewand gehüllt und an einer Tänie in seinem Haar und an seinem Nimbus erkennbar. In seiner linken Hand hält er einen langen goldenen Stab, mit seiner erhobenen Rechten deutet er an, dass er mit Zacharias spricht, der auf der rechten Seite des Bildes steht. Zacharias ist ein Greis mit Bart und langen weißen Haaren, einem knöchellangen Gewand mit einem am Hals geknöpften Umhang darüber, der mit einer Weihrauchtasche auf den Engel zugeht.Dieser Komposition entspricht weitgehend die Miniatur im Rabbula-Evangeliar, wobei dort der Architekturhintergrund weniger detailliert herausgearbeitet ist als im Etschmiadsin-Evangeliar Dagegen steht im Augustin-Evangeliar Zacharias hinter einem zum Stehpult angewachsenen Altar und der geflügelte Engel erscheint im Profil von der linken Seite. Die syrisch-armenische Darstellung unterscheidet sich auch von späteren byzantinischen Versionen, die stets Zacharias hinter dem Altar zeigen. Sie ist damit näher am Text des Lukas-Evangeliums. Einzigartig auf der Miniatur von Etschmiadsin sind die lang herabhängenden Flügel des Engels aus Pfauenfedern. Namenlose Engel mit Pfauenflügeln sind aus dem Katharinenkloster auf dem Sinai (548–560) bekannt. Inschriftlich als Engel Gabriel mit Pfauenflügeln ist allein ein Relief aus prokonnesischem Marmor gesichert, das aus vorarabischer Zeit stammt und sich heute im archäologischen Museum von Antalya (Antalya Müzesi) befindet. In späterer Zeit scheint der Engel mit Pfauenflügeln nahezu gänzlich aus der christlichen Kunst verschwunden zu sein. Offensichtlich sollte das auffällige Gefieder auf einen in der höchsten Rangstufe stehenden Erzengel verweisen. Die Bildsprache weist ebenfalls Zacharias einen hohen Rang zu und macht ihn zum Hohepriester. Sein Umhang ist am Kragen mit drei Reihen von kostbaren Steinen besetzt, wie sie der alttestamentlichen Beschreibung der Priestergewänder im 2. Buch Mose (Ex 28,16-21 ) vorgegeben ist. Dahinter steht eine Glaubensüberzeugung, die nur in der syrischen und armenischen Kirche vorkommt, nach welcher das Priestertum in eine Traditionslinie mit den Priestern des Alten Testaments gestellt wird; also in eine kontinuierliche Linie, die mit Aaron beginnt und zu Zacharias führt, welcher die Tradition an Johannes den Täufer weitergibt, der schließlich Jesus tauft. Diese Bedeutung lässt die Armenier im Etschmiadsin-Evangeliar Zacharias an die erste Stelle der Epiphanie-Bebilderung setzen.
Die Verkündigung an Maria (fol. 228v, (Lk 1,26–38 )) wird nur im Lukas-Evangelium erzählt und gehört in der frühchristlichen Kunst zu den am häufigsten dargestellten Sujets. Die Darstellung im Etschmiadsin-Evangeliar ähnelt der vorangegangenen, mit einigen wesentlichen Unterschieden: Während Zacharias sich von rechts dem Engel auf der linken Seite nähert, tritt hier der wieder mit Pfauenflügeln ausgestattete Engel auf die still stehende Maria hinzu. Um Maria als statisches Bildzentrum zu betonen, ist das Giebelhaus im Hintergrund weiter nach rechts gerückt, so dass der Rundbogen in der Giebelwand nun ihren Nimbus umgibt und sie von den Säulen ihres Hauses eingerahmt wird. Maria trägt einen langen dunklen Chiton, an dessen Gürtel ein weißes Tuch herabhängt, und darüber eine lange Paenula, die am Saum mit Fransen besetzt ist. Die Bewegungsrichtung beider Figuren ist ebenso im Rabbula-Evangelium dargestellt, weil beide auf eine in syrischer Sprache verfasste Interpretation des Kirchenlehrers Ephräm der Syrer (um 306–373) zurückgehen, die im 5. Jahrhundert ins Armenische übersetzt wurde. In seinem Kommentar zum Diatesseron (Evangelienharmonie) erklärt Ephräm die Bedeutung der handelnden Personen und verweist auf das übliche Gebot der Höflichkeit, wonach Rangniedrige auf Höherstehende zugehen. Folglich nähert sich Zacharias dem Engel, um zu zeigen, dass sein zukünftiger Sohn niedriger im Rang als der Engel sein wird, aber indem sich der Engel Maria nähert, bringt er zum Ausdruck, dass Christus über die Engel gebieten wird.
Vom unerwarteten Auftritt des fremden Boten überrascht, steht Maria auf und lässt ihr Spinngarn in den Korb vor ihren Füßen fallen, mit der rechten Hand fasst sie sich an ihr Kinn und mit der linken ergreift sie den Saum ihres Mantels. Eine Frau, die sich – vor allem bei Abwesenheit ihres Gatten – tagsüber mit Spinnen beschäftigt, bewies zu der Zeit ihre vornehme Stellung und das Bemühen um einen ordentlichen Haushalt. Diese Symbolik ist von antiken Erzählungen und Reliefs auf Grabplatten bekannt. Auf vielen Abbildungen hat Maria ihre Hände vor der Brust verschränkt, hält Spindel und Garn oder streckt die Hände in einer Überraschungsgeste aus, nur selten führt sie die Hand ans Kinn, was in der antiken Kunst als Zeichen tiefer Verunsicherung gilt. Maria fragt nach, ob der Engel wirklich eine göttliche Botschaft bringt und sorgt sich um ihre Jungfräulichkeit (Lk 1,34–38 ).
Die Anbetung der Könige (fol. 229, (Mt 2,1-12 )) fehlt im Rabbula-Evangeliar. Die Szene ist vor einem gänzlich anderen Architekturhintergrund dargestellt. Das symmetrische Gebäude besitzt in der Mitte einen auf korinthischen Säulen ruhenden Rundbogen, der von einer strahlenförmigen blauen Muschelform ausgefüllt wird. Das sich über den Rundbogen erstreckende Dach endet seitlich in perspektivisch verzeichneten Giebeln, in denen sich wie Fenster aussehende Kreuze befinden. Die Außenecken des Gebäudes werden von Säulen getragen, die zugleich die Figurengruppe einrahmen. Die strenge Symmetrie ergänzen rote Vorhänge, die zwischen den Säulen angebracht schräg nach innen hängen. Nona Stepanjan sieht in der Architektur eine dreischiffige Basilika mit einer mittigen Rundapsis und fasst die Giebel als Seitenschiffe auf.In der Mitte unter dem Rundbogen sitzt Maria auf einem Thron mit einer hohen geschwungenen Lehne. Ihr blaues Gewand bildet den Hintergrund für das mit einem rötlichen Gewand und einem einfachen rötlichen Nimbus umgebene Christuskind, das sich vor einer in einem etwas helleren Blau gehaltenen ovalen Fläche deutlich abhebt, die eine Art Mandorla bildet. Rechts neben Maria steht ein Engel mit Tänie im Haar und einem Nimbus um den Kopf. Die übrigen drei Figuren, eine rechts vom Engel und zwei auf der linken Seite des Bildes, repräsentieren die heiligen drei Könige, die an ihrer orientalischen Kleidung zu erkennen sind. Sie tragen Pumphosen und kurze Röcke, die mit kunstvoll gemusterten Bordüren verziert sind. Offensichtlich sind sie unterschiedlich alt, denn der linke Weise trägt keinen, der mittlere einen schwarzen und der rechte einen grauen Bart. Alle drei halten einen Kranz waagrecht in den Händen, den sie Christus entgegenstrecken.Außer dem Matthäusevangelium liegt der Darstellung das Armenische Kindheitsevangelium zugrunde, eine apokryphe Kindheitsgeschichte, die Ende des 6. Jahrhunderts ostsyrische Christen von Mesopotamien nach Armenien brachten. Im armenischen Kindheitsevangelium kündet der Engel Gabriel vom Kommen der drei Weisen, während ansonsten Sterne am Himmel deren Vorboten sind. Gabriel informiert in der armenischen Erzählung zunächst Maria und lädt sodann die drei Weisen ein, damit sie dem Christuskind huldigen. In der verbreitetsten Darstellung kommen die drei Weisen in einer Reihe daher und überbringen ihre Geschenke. Das Blatt im Etschmiadsin-Evangeliar folgt dagegen einer Darstellung, die im 6. Jahrhundert auf ampullae (latein, Sg. ampulla, rundbauchige Trinkflasche für heiliges Wasser oder Öl mit zwei Henkeln) in Palästina auftaucht. Anstelle hoher Kronen tragen die Weisen flach um den Kopf gebundene Hauben, die mit Perlen verziert sind und deren Schleifen lose auf die Schultern herabhängen.
Nach Thomas F. Mathews verweist diese Kopfbedeckung zu den Magern, den Priestern des zoroastrischen Kults, wie sie auf sassanidischen Siegeln abgebildet sind. Die wie Reithosen aussehenden eleganten Beinkleider wurden von sassanidischen Fürsten getragen. Bereits Strzygowski fiel auf, dass die Weisen, obwohl sie sich auf Maria zubewegen, mit seltsam gespreizten O-Beinen dastehen. Möglicherweise steht dahinter der Gedanke an sassanidische Adlige, die typischerweise im Sitzen mit gespreizten Knien und geschlossenen Füßen gezeigt werden. Dementsprechend gelten die drei Besucher im armenischen Kindheitsevangelium als Könige. Eine weitere apokryphe Schriftsammlung, die vermutlich im 6. Jahrhundert aus Texten des 4. Jahrhunderts kompilierte Syrische Schatzhöhle, nennt die drei explizit „sassanidische Könige“. Aus einem religionsgeschichtlichen Aspekt war es erforderlich, die Besucher als Priester und Könige zugleich zu verstehen. Ein so hoch geborenes Kind sollte von niemand geringerem als von Königen den ersten Besuch erhalten. Zugleich demonstriert der Besuch von zoroastrischen Priestern, gegen deren Religion sich das frühe Christentum in Armenien behaupten musste, dass die Zoroastrier den neuen christlichen Herrn nunmehr als einen über ihnen stehenden Priester anerkannt haben.
In frühchristlicher Zeit war es noch ungewöhnlich, die drei Weisen als Jüngling, Erwachsener und Greis darzustellen. Sie so unterschiedlich alt erscheinen zu lassen, passt zur Erzählung des armenischen Kindheitsevangeliums. Darin berichten die Weisen sich gegenseitig ihre Eindrücke nach dem Besuch beim Christuskind. Kaspar sah ein Kind, den Sohn Gottes auf einem Thron sitzen. Balthasar erkannte den Herrn über die himmlischen Heerscharen, von denen er auf seinem Thron sitzend angebetet wurde, wohingegen Melchior den sterbenden und wiederauferstehenden Gott vorfand. Die Lebensalter der Weisen stimmen mit ihren Visionen überein. Die Herkunft dieser Erzählung wird in der zoroastrischen Vorstellung der ewigen Zeit (zurvan) vermutet, die als personifizierter Gott im Armenien des 5. Jahrhunderts verehrt wurde. Der über allen Göttern stehende Zurvan erschien nicht aus einem Anfang, denn er galt nicht nur als ewig, sondern erschuf überhaupt erst die Zeit, weil er vor allem anderen da war. Eine solcherart zeitlose, nach altiranischer Anschauung drei kosmische Zeitalter repräsentierende Christusfigur sollte in der Miniatur dargestellt werden. Die kosmisch-zeitlose Bedeutung Christi, dessen Gestalt im genauen Zentrum des Bildes die Weisen als Vision wahrnahmen, wollte der Maler mit der Mandorla um das Christuskind zum Ausdruck bringen.
Die abschließende Miniatur der Taufe Jesu (fol. 229v) ist als einzige von einem Rahmen umgeben. Der Rahmen enthält in den vier Ecken Medaillons mit den Porträts der vier Evangelisten. In den Rahmenfeldern dazwischen sind auf Kelchen und Schalen sitzende Vögel dargestellt, die sich ihre Brustfedern rupfen. Das mittlere Bild zeigt Johannes auf der linken Seite mit langen schwarzen Haaren, schwarzem Bart und einem Nimbus bei der Taufe. Die rechte Hand hält er auf den Kopf Christi, mit der linken rafft er sein langes faltenreiches Gewand, es ist ein schwerer Kamelhaarmantel, am Gürtelbereich zusammen. Christus steht mit nacktem Oberkörper im Wasser. Über ihm kommt eine Taube, als der Heilige Geist, im Sturzflug herab, und darüber erscheint die rechte Hand Gottes mit einer Segensgeste aus den Wolken.Porträts der Evangelisten beschließen typischerweise ein armenisches Evangeliar. Ihr gleich altes Aussehen ist ein Beleg für die frühchristliche Datierung der Malerei, denn später werden Matthäus und Johannes zumeist als ältere Männer, Markus und Lukas dagegen jünger dargestellt. Im Unterschied zum ansonsten ähnlichen Rabbula-Evangeliar trägt Christus keinen Bart. Dies folgt der armenischen Tradition, nach der mit der Epiphanie zugleich die Geburt und Taufe Christi gefeiert wird, der Täufling folglich als Kind erscheinen soll.
Die Vögel in der Umrahmung sind als rosafarbene Pelikane erkennbar. Sie stehen auf Abendmahlskelchen und Hostienschalen, die reich mit Edelsteinen verziert sind. In der Ikonografie des europäisch-christlichen Mittelalters erscheint der Pelikan als Symbol des Todes und der Auferstehung, in der byzantinischen Kunst kommt er als Sinnbild für Einsamkeit vor, in einem Abendmahlskelch sitzend ist er einzig aus dem Rabbula-Evangelium und aus dieser Miniatur bekannt. Die symbolische Bedeutung ist wohl dem Physiologus entnommen, einer Naturlehre, die bis zum 4. Jahrhundert auf Griechisch verfasst und anschließend ins Armenische und in andere Sprachen übersetzt wurde. Der Pelikan erweckt sein Junges mit seinem eigenen Blut wieder zum Leben und verhält sich so wie Christus, der sich am Kreuz opfert. Der Pelikan gibt sein Blut in den Kelch, mit dem in der Eucharistie symbolisch die Kreuzigung vergegenwärtigt wird.
In der Spätantike wurden byzantinische Handschriften von Konstantinopel nach Armenien gebracht, damit sie dort aus dem Griechischen übersetzt werden konnten. In einer zwischen 604 und 607 verfassten „Abhandlung über die Ikonoklasten“ (Yaghags Patkeramartits) beschreibt der Katholikos Vrtanes Kertogh die kostbaren byzantinischen Evangeliare, die es zu seiner Zeit in Armenien gab und preist deren aus Gold, Silber und Elfenbein bestehende Einbände. Die jeweils aus fünf reliefierten Elfenbeintafeln zusammengesetzten Buchdeckel des Etschmiadsin-Evangeliars messen jeweils 36,5 Zentimeter in der Höhe und 30,5 Zentimeter in der Breite. Vermutlich 1173 wurde die Handschrift neu gebunden und erhielt ihre heutigen Elfenbeindeckel.
Die fünf in der Regel für einen Deckel benötigten Teile wurden so angeordnet, dass in der Mitte Christus oder Maria erscheint. Die äußeren Paneele zeigen Szenen aus den Evangelien oder zu Gruppen angeordnete Engel und Apostel. Oft wurden einzelne, als Reliquien aufbewahrte Teile zusammengetragen und neu zu Buchdeckeln arrangiert. Die Elfenbeindeckel des Etschmiadsin-Evangeliars gehören zu einer Reihe Diptychen wie dem Einband des Evangeliars aus Saint-Lupicin  (2. Hälfte des 6. Jahrhunderts, Paris, Nationalbibliothek, Cod. lat. 9384) und dem Murano-Elfenbein (510–530), das im Nationalmuseum in Ravenna aufbewahrt wird. Die dargestellten Szenen entsprechen in ihrem Zusammenhang den Konventionen der ostkirchlichen Ikonografie.
Der vordere Deckel enthält im mittleren Feld eine Mariendarstellung vom Typus der Hodegetria, flankiert von zwei Engeln. Die seitlichen Tafeln sind zweigeteilt. Links oben ist Mariä Verkündigung zu sehen, darunter die Keuschheitsprobe mit bitterem Wasser nach dem 4. Buch Mose (5, 11–28). Rechts gegenüber sind Jesu Geburt und darunter die Flucht nach Ägypten dargestellt. Über den drei mittleren Teilen im Hochformat befindet sich ein horizontales Relief mit zwei aufeinander zufliegenden Engeln, die ein großes Kreuz in der Mitte, das von einem Lorbeerkranz (corona triumphalis) umrahmt wird, mit den Armen umfassen. Figuren in den oberen Ecken halten die Hände zum Gebet vorgestreckt. Die lebhafte Szene unten quer enthält die Anbetung der drei Könige.
Der hintere Deckel ist auf dieselbe Weise angeordnet. Die obere Tafel ist identisch mit der Vorderseite, während die Figurengruppe unten den Einzug in Jerusalem darstellt, mit dem auf einem Esel reitenden Jesus, der ein Kreuz hält, während ihm sieben Anhänger begeistert mit Palmwedeln zuwinken. Die Frau auf der rechten Seite mit einem Füllhorn in der Hand verkörpert die Stadt Jerusalem.
Das mittlere Feld gehört dem auf seinem Thron sitzenden jungen Christus, der in der linken Hand eine große Schriftrolle hält, seine rechte Hand ist zum Segen erhoben. Um seinen Kopf fehlt der Nimbus. Hinter ihm zu beiden Seiten stehen die Apostel Petrus und Paulus. Seitlich sind vier Wunderheilungen dargestellt. Die rechte Tafel enthält im oberen Bereich die Heilung des Gelähmten, zu dem Jesus sagt: „Steh auf, nimm dein Bett und geh in dein Haus!“ (Mt 9,1-8 ). Darunter folgt die Heilung der zwei vom Teufel besessenen Gadarener (Mt 8,28-34 ). Auf der linken Seite vollzieht Jesus oben die Heilung der Blutflüssigen (Mk 5,21 ) und darunter möglicherweise die Heilung am Teich Bethesda (Joh 5,1–16 ).Drei der dargestellten Heilungsszenen umrahmen ebenfalls auf den Diptychen in Paris und Ravenna die zentrale Christusfigur. Die linke untere Szene der Rückseite kann auch anders interpretiert werden, weil sie einen nackten Mann zeigt, was nach Pieter Singelenberg bedeutet, dass hier eine Heilung unter Wasser stattfindet. Demnach dürfte die Heilung eines Blindgeborenen nach dem Johannesevangelium gemeint sein (Joh 9,6-12 ). Jesus spuckt auf den Boden, weicht mit seinem Speichel Lehm auf, reibt diesen dem Mann auf die Augen und schickt ihn dann in den Teich von Siloah, damit er sich reinige.
Heide und Helmut Buschhausen: Das illuminierte Buch Armeniens. In: Armenien. Wiederentdeckung einer alten Kulturlandschaft. (Ausstellungskatalog) Museum Bochum 1995, S. 191–213.
Heide und Helmut Buschhausen: Codex Etschmiadzin. Kommentar zur Faksimile-Ausgabe. Codex 2374 des Matenadaran Mesrop Maštoc` in Erevan. Akademische Druck- und Verlagsanstalt, Graz 2001, ISBN 3-201-01739-6.
Beda O. Künzle: Das altarmenische Evangelium. Band 1: Edition zweier altarmenischer Handschriften: Edition nach dem Kodex Matenadaran 6200 (früher 1111 der Bibliothek des Instituts für orientalische Sprachen Lazarev, Moskau), geschrieben im Jahre 887, und nach dem Kodex Matenadaran 2374 (früher Ejmiacin 229), geschrieben im Jahre 989. Teil 2: Lexikon: vollständiges Wörterbuch mit grammatischer Analyse sämtlicher Wortformen in beiden edierten Handschriften. (Habilitationsschrift) Peter Lang, Bern/Frankfurt 1984
Frédéric Macler: L'Èvangile arménien. Edition phototypique du manuscrit No. 229 de la bibliothèque d'Etchmiadzin. Librairie Paul Geuthner, Paris 1920 (bei Internet Archive).
Thomas F. Mathews: The early Armenian iconographic program of the Ējmiacin Gospel (Erevan, Matenadaran Ms 2374 olim 229). In: Nina G. Garsoïan, Thomas F. Mathews, Robert W. Thomson (Hrsg.): East of Byzantium. Syria and Armenia in the Formative Period. A Dumbarton Oaks Symposium. Dumbarton Oaks, Washington (D.C.) 1982, S. 199–215.
Sirarpie Der Nersessian: The Date of the Initial Miniatures of the Etchmiadzin Gospel. In: The Art Bulletin 15, 1933, S. 327–360.
Vrej Nersessian: Treasures from the Ark. 1700 Years of Armenian Christian Art. The J. Paul Getty Museum, Los Angeles 2001, ISBN 978-0-89236-639-2, S. 155–158.
Pieter Singelenberg: The Iconography of the Etschmiadzin Diptych and the Healing of the Blind Man at Siloe. In: The Art Bulletin, 40, 1958, S. 105–112.
Nona Stepanjan: Wandmalerei, Buchmalerei und angewandte Kunst. In: Burchard Brentjes, Stepan Mnazakanjan, Nona Stepanjan: Kunst des Mittelalters in Armenien. Union Verlag, Berlin 1981, ISBN 978-3-7031-0548-7, S. 234–316.
Josef Strzygowski: Das Etschmiadzin-Evangeliar. Beiträge zur Geschichte der armenischen, ravennatischen und syro-ägyptischen Kunst. (= Byzantinische Denkmäler, Band 1) Wien 1891 (bei Internet Archive).
The Evolution of Armenian Gospel Illumination. The Formative Period (9th–11th Centuries). Armenian Studies Program, Fresno (CA)
