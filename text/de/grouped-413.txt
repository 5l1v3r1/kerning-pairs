
Radverkehr in Münster nimmt einen wichtigen Stellenwert ein, Fahrradstadt ist eine dem Namen der Stadt Münster in Westfalen zugefügte Eigenschaft, die sich in dem hier besonders ausgeprägten Fahrradverkehr begründet. Für dessen Förderung wurde Münster schon mehrmals ausgezeichnet, und sie gilt als Vorbild einer Verkehrswende für andere Städte im In- und Ausland.
Das Fahrrad wird aus dem Münsteraner Soziolekt Masematte stammend im Münsteraner Volksmund häufig als Leeze oder auch niederdeutsch Fietse bezeichnet.
Von den rund täglich 1,4 Millionen Fahrten in Münster entfallen circa 1,05 Millionen auf die Einwohner der Stadt. Von diesen rund 1,05 Millionen Fahrten werden je nach Quelle etwa 35 % bis 40 % mit dem Fahrrad zurückgelegt. Somit hat die Stadt den höchsten Radverkehrsanteil in Deutschland nach dem deutlich kleineren Greifswald mit 44 %. Nach einer Umfrage im Jahre 2007 wählten die Münsteraner sogar am häufigsten dieses Verkehrsmittel.Das Radwegenetz innerhalb der Stadt erstreckt sich dabei auf einer gesamten Länge von über 300 km, davon 293 km auf Bordsteinradwegen, 10 km sind Fahrradstraßen und auf einer Länge von insgesamt 3 km dürfen Radfahrer die Busspuren mitnutzen. Zur Orientierung wurden an 172 km Radwegweiser aufgestellt. Auf 142 km verlaufen gekennzeichnete Themenrouten.Zugleich ist Münster die Stadt mit den meisten Fahrraddiebstählen im deutschsprachigen Raum.
Das Radfahren in Münster ist unter anderem deshalb so beliebt, weil es in Münster kaum Steigungen gibt. Die Entfernungen sind in der Stadt selten über 5 km, zahlreiche Ziele finden sich in kürzeren Entfernungen. Nicht zuletzt ist die Sozialstruktur mit einem hohen Anteil an Schülern und Studenten günstig für das Radfahren. Diese Gründe treffen aber auch für zahlreiche andere Städte zu, in denen in weit geringerem Ausmaß Rad gefahren wird. Von besonderer Bedeutung scheinen daher weitere Gründe zu sein.
Zu diesen Gründen zählt, dass die gesamte Innenstadt zwischen Servatiiplatz über den Prinzipalmarkt bis zum Domplatz für private Pkw gesperrt ist oder aus Einbahnstraßen besteht. Dies macht für Autofahrer ein Umfahren des Innenstadtbereichs notwendig, während Radfahrer fast überall fahren dürfen und sie auch die Innenstadt, außer in den Fußgängerzonen, direkt durchqueren können. Auch die problematische Parkplatzsituation trägt ihren Teil zum Status quo bei: Kostenlose Parkplätze sind im Kernbereich, in dem viele der in Münster ansässigen Behörden angesiedelt sind, nicht vorhanden.
Wer per Rad die Innenstadt umrunden will, kann dies auch auf der 4,5 Kilometer langen Promenade tun. Hierbei handelt sich um eine Ringstraße, die in den zur Parkanlage umgewandelten ehemaligen spätmittelalterlichen Befestigungsanlagen angelegt wurde.
Die mittlere Bahn ist asphaltiert und ausschließlich für Radfahrer vorgesehen, allerdings wird sie ebenfalls von Skatern, manchen Joggern, Rollstuhlfahrern und gelegentlich auch von Segways benutzt.
Für Fußgänger ist parallel zur mittleren Bahn auf jeder Seite ein Gehweg angelegt, welche mit Ausnahme zweier kleiner Stücke auf dem inneren Gehweg gepflastert sind.
Zwischen Radspur und Fußgängerwegen zieht sich je eine Baumreihe von Linden, was die Promenade zur Allee macht.
Es gibt zehn aus der Innenstadt führende Straßen, welche die Promenade kreuzen und zwar an den Stellen, wo sich früher die Stadttore befanden. Fünf dieser Kreuzungen sind mit Zebrastreifen versehen.
Von den anderen Kreuzungen sind drei mit Ampeln versehen, eine weitere ist ohne Zebrastreifen oder Ampel und die letzt zu nennende ist eine Unterführung.
Ein weiterer Grund für den hohen Anteil an Radfahrern: Münster ist eine Pendlerstadt mit insgesamt rund 80.000 täglichen Einpendlern. Durch die Radialstruktur der Stadt mit nur sechs großen Ausfallstraßen ergeben sich im Berufsverkehr häufig Staus. Da die Ausfallstraßen durch Wohngebiete führen, greifen die dort wohnenden Arbeitnehmer häufig zum Fahrrad, um zu ihrer Arbeitsstelle zu kommen. Im Gegensatz zu anderen Städten vergleichbarer Größe wird in Münster der ÖPNV primär mit Bussen durchgeführt, die tendenziell unattraktiver als schienengebundene Verkehrsmittel sind. Der SPNV hingegen hat in Münster innerstädtisch eine geringe Bedeutung.
Die Gründe für die Vorliebe der Münsteraner für die Nutzung des Fahrrades liegen unter anderem in den zahlreichen Sonderregeln, die das Radfahren in Münster erleichtern. So gibt es an großen Kreuzungen eigene Fahrstreifen für Radfahrer, oder sie dürfen zwischen den Autos bis direkt vor die Ampel fahren (die sogenannte Fahrradschleuse), damit sie bei Grün immer im Blickfeld der anfahrenden Autos sind. Dies erhöht die Sicherheit der Radfahrer im Straßenverkehr. Außerdem wird so das Warten im unmittelbaren Abgasausstoß der Autos vermieden. Einige Ampeln gelten durch Sonderzeichen nicht für Radfahrer, zum Beispiel an vielen Fußgängerampeln. Ferner gilt in Münster häufig eine für Autofahrer vorgeschriebene Fahrtrichtung an Kreuzungen und Einmündungen nicht für Radfahrer. Da in Münster die Straßen der Innenstadt beim Wiederaufbau nach dem Zweiten Weltkrieg nicht verbreitert wurden, sind inzwischen viele davon nur als Einbahnstraßen freigegeben, jedoch betrifft auch dies nur selten Radfahrer, die die Straßen in beide Richtungen befahren dürfen. Inzwischen sind elf Straßen im Stadtgebiet als Fahrradstraße ausgewiesen.
Nicht zuletzt kommt hinzu, dass an nahezu allen Hauptverkehrsstraßen Radverkehrsanlagen bestehen, zumeist in Form von Bordsteinradwegen. Das Radverkehrsnetz entstand im Rahmen des Wiederaufbaus der Innenstadt nach dem Zweiten Weltkrieg, um Platz für den zunehmenden Kraftfahrzeugverkehr zu schaffen. Insofern war das Am-Stau-Vorbeifahren mit dem Rad auch zu Zeiten möglich, als die Nebenstraßen durch Schleichverkehre oder Einbahnstraßensysteme für Radfahrer noch nicht so attraktiv waren wie sie es in der heutigen Zeit mit Verkehrsberuhigung, Tempo-30-Zonen und Freigabe von Einbahnstraßen in Gegenrichtung sind.
Eine Ende 2011 erstellte Auswertung der Zahlen des Kraftfahrt-Bundesamtes durch das CAR-Center an der Universität Duisburg-Essen ergab, dass mit zunehmender Größe einer Stadt die Beliebtheit von Autos abnimmt. Der Bundesdurchschnitt liege bei 472 privaten Pkw-Zulassungen pro 1000 Einwohner. In Berlin werden mit 289 Pkw auf 1000 Einwohner bundesweit die wenigsten Fahrzeuge angemeldet. Münster liege mit 374 Zulassungen auf 1000 Einwohner in der Region am unteren Ende der Skala.
Im sogenannten Münster-Barometer, einer regelmäßig von der Westfälischen Wilhelms-Universität durchgeführten Umfrage, gaben in der Umfrage 2/2005 nur 7 % der 280.000 Münsteraner an, kein Fahrrad zu besitzen, immerhin knapp 45 % besaßen zu diesem Zeitpunkt zwei oder mehr Fahrräder. In offiziellen Schriften gibt die Stadt die Zahl der Fahrräder mit gut 500.000 an.In einer Stadt mit doppelt so vielen Fahrrädern wie Bewohnern tritt auch das Problem des Fahrraddiebstahls in den Vordergrund: So gaben weniger als die Hälfte der Befragten an, ihnen sei noch nie ein Fahrrad gestohlen worden, 15 % beklagten gar drei oder mehr verschwundene Räder. 2008 wurden in Münster bezogen auf 100.000 Einwohner im statistischen Mittel über 2.000 Fahrräder gestohlen. 2009 kamen 1971 Fahrräder abhanden, 2010 waren es 1828, 2011 ging die Quote weiter auf 1756 zurück. Damit wurden statistisch täglich etwa 13 Münsteraner Opfer eines Fahrraddiebstahls, womit Münster mit 179 % über dem Bundesdurchschnitt und wie bereits 2009 und 2010 auch 2011 an der Spitze des bundesweiten Vergleichs lag. Im Jahr 2011 wurden in Münster 4914 Fahrräder gestohlen. Fast jede sechste in Münster begangene Straftat ist ein Fahrraddiebstahl. Abgegebene sowie abgeschleppte Fahrräder können in der Fundfahrradstation der Stadt Münster abgeholt werden.Auswärtige wundern sich über die häufigen Verkehrskontrollen auch für Radfahrer. Die Münsteraner selber aber sagten zu jeweils knapp 40 %, die Kontrollen seien ausreichend beziehungsweise noch zu wenig, nur 11,5 % hielten die Anzahl der Kontrollen für übertrieben. Nach Aussage des Polizeidirektors Udo Weiss ließ sich durch die regelmäßigen Kontrollen die Quote der Radfahrer, die in der Dämmerung und Dunkelheit das Licht am Rad einschalten, zwischen 2006 und 2011 von 50 % auf weit über 90 % steigern. Bei einer nächtlichen Kontrolle wurde am 7. Dezember 2012 eine Quote von 98 % ermittelt. Während einer 14-tägigen Schwerpunktaktion der Polizei im Dezember 2012 wurden 96 % der Radfahrer mit eingeschaltetem Licht registriert. 88,8 % der 2012 befragten Münsteraner erachten die verstärkten Alkoholkontrollen bei Radfahrern für sinnvoll. Die Hälfte aller in Münster durchgeführten Alkoholproben werden bei Radfahrern vorgenommen. Die gemessenen Werte liegen zwischen 1,6 und 3,3 Promille.In der Umfrage 1/2007 gaben 47 % der Befragten an, überwiegend das Fahrrad zu benutzen, Auto und Bus folgten mit 34 beziehungsweise 13,9 %. Zudem nutzen 77 % das Fahrrad täglich oder mehrmals pro Woche.Das Verhalten der Radfahrer im Straßenverkehr wurde von Münsteranern 2005 als „rüpelhafter“ eingeschätzt als das der Autofahrer. Bei den Radfahrern ergab sich ein Mittelwert von 3,85, bei den Autofahrern von 3,14 („1“ = Verhalten sehr rücksichtsvoll).
Regelmäßig wird vom Allgemeinen Deutschen Fahrrad-Club (ADFC), teilweise unterstützt vom Bund für Umwelt und Naturschutz, der so genannte Fahrradklimatest durchgeführt, eine Umfrage, bei der die fahrradfreundlichste Stadt Deutschlands gekürt wird. Nach 1988 und 1991 gewann Münster auch in den Jahren 2003, 2005, 2012, 2014 und 2016 den Titel in der Kategorie der Städte über 200.000 Einwohnern, wobei zwischen 1991 und 2003 keine Erhebungen stattfanden. Dabei wurde im Jahr 2005 eine Note von 2,05 erzielt. Für das Jahr 2012 wurde eine durchschnittliche Zufriedenheit unter 640 Münsteraner Radfahrer von 2,61 auf der Schulnotenskala erhoben. Positiv wurde die Erreichbarkeit des Stadtzentrums sowie die Fahrgeschwindigkeit auf den Radwegen bewertet, wohingegen Polizeikontrollen, das Verhalten von Kraftfahrern sowie der anhaltende Fahrraddiebstahl negativ in die Bewertung einfloss. In dem Ranking derselben Umfrage aus dem Jahr 2012, das die Entwicklung in Relation zum Jahr 2005 setzt, belegt Münster hingegen mit Rang 27 den letzten Platz.2016 sank die Zufriedenheit der Radfahrer im ADFC-Fahrradklimatest weiter auf einen Wert von 3,07. Damit konnte Münster den ersten Platz nur knapp vor Karlsruhe (3,09) verteidigen. Positiv wurde dabei die Reinigung und der Winterdienst auf den Radwegen bewertet. Negativ wahrgenommen wird – neben dem hohen Diebstahlquote und Konflikten mit KFZ – insbesondere der Zustand der Radverkehrsinfrastruktur, also die geringe Breite der Radwege, die Oberflächen der Wege, Hindernisse und zu wenige Abstellmöglichkeiten.Auch der ADAC hat 2004 Münster – als einzige Stadt im Testfeld – mit der Note sehr gut ausgezeichnet. Während bei den ADFC-Tests dem Ergebnis Umfragen unter den Radfahrern selber zugrunde liegen, hat der ADAC seinen Test in 22 Städten mit Hilfe von Verkehrsexperten durchgeführt, die die Städte anhand objektiver Daten und Befahrungen bewertet haben.
Aufgrund der Auszeichnungen empfängt Münsters Stadtplanungsamt regelmäßig Verkehrsplaner aus allen Städten der Welt, um in der Praxis zu demonstrieren, wie Radverkehr als funktionierende Alternative zum Auto in einer kleineren Großstadt etabliert werden kann und was dafür getan werden muss. So waren in den letzten Jahren Abordnungen aus dem italienischen Florenz, dem norwegischen Kristiansand und dem US-amerikanischen Richfield in der westfälischen Stadt.
Neben den Bestrebungen der Stadt, eine nachhaltige Mobilität in der Stadt zu gestalten, nimmt seit einiger Zeit auch das bürgerschaftliche Engagement in Münster in Sachen Radfahren und Verkehrspolitik zu. Initiativen und Vereine engagieren sich auf verschiedenen Ebenen, um in der Öffentlichkeit für die Verkehrswende zu werben. Hierzu zählen unter anderem die Interessengemeinschaft Fahrradstadt Münster, der ADFC Münster, die Initiative der freien Lastenräder Lasse – dein Lastenrad für Münster und die Critical Mass Münster.
Seit dem Frühjahr 2012 droht Verkehrsteilnehmern in Münster ein Radfahrverbot, wenn sie mehrfach mit Alkohol oder Drogen im Blut erwischt werden. Diese Maßnahme des Münsteraner Ordnungsamtes ist bundesweit einzigartig. Dennoch gilt das Radfahrverbot bundesweit. Bei der ersten Missachtung dieses Verbots droht ein Zwangsgeld in Höhe von 500 Euro. Bei jedem weiteren Verstoß verdoppelt sich dieser Betrag. Die Höhe des Bußgelds kann bis zu 100.000 Euro betragen. Das Verbot wird erst aufgehoben, wenn der Beschuldigte nachweist, dass er nicht mehr alkoholisiert mit dem Fahrrad fährt. Hierzu ist ein medizinisch-psychologisches Gutachten erforderlich, das den Verzicht auf den Konsum von Alkohol oder die Erkenntnis, nach Alkoholgenuss nicht mit dem Fahrrad fahren zu dürfen, nachweist. Die Zielgruppe, die mit dem Verbot getroffen werden soll, sind nach Auskunft des Ordnungsamtes Wiederholungstäter, die mehrfach im Straßenverkehr alkoholisiert auffällig wurden.Die Ordnungsverfügung ist seit dem 27. April 2012 bestandskräftig. Verstöße werden dem Verkehrszentralregister in Flensburg gemeldet.Bei der Einführung des Radfahrverbots wurde seitens des Ordnungsamts mit ein paar Dutzend Fällen jährlich gerechnet. Das Verbot wurde Anfang März 2012 erstmals bei einem Radfahrer angewandt. Mehr als 30 Münsteraner Radfahrern drohte Ende März 2012 ein solches Fahrverbot. Bis Mitte August 2012 verloren elf Münsteraner die Erlaubnis zum Fahrradfahren. Ende Dezember 2012 wurde erstmals das Zwangsgeld in Höhe von 500 Euro für Radfahren trotz Radfahrverbots verhängt.Der Münsteraner Polizeipräsident Hubert Wimber forderte im Juli 2012, die Promillegrenze zur Fahruntüchtigkeit bei Radfahrern von 1,6 auf 1,1 Promille herabzusetzen und damit der Grenze für Autofahrer anzugleichen. Bereits 2011 schlug er die Kennzeichnungspflicht für Radfahrer vor, damit „Geisterfahrer, Rotlichtsünder und Unfallflüchtige“ auch nachträglich ermittelt werden können, obwohl auch bei gekennzeichneten Kraftfahrzeugen, beispielsweise PKW, trotz bekanntem Nummernschild der Fahrer häufig nicht ermittelt werden kann. Im Juli 2012 sprach er sich gegen eine solche Kennzeichnungspflicht aus, da seiner Aussage nach der Bürokratieaufwand hierfür zu groß wäre.Eine höhere Promillegrenze für Radfahrer als für Autofahrer sei laut der Münsteraner Polizei als „mehrere Jahrzehnte alte Regelung längst überholt“.Vergleichbar mit dem Radfahrverbot, entzieht die Stadt Münster bereits seit 2010 aggressiven Gewalttätern den Führerschein, damit diese ihr Kraftfahrzeug nicht als Waffe im Straßenverkehr einsetzen können. Im Jahr 2011 wurden mit dieser Begründung 165 Verfahren zum Führerscheinentzug eingeleitet, von denen 25 mit dem Verlust der Fahrerlaubnis endeten.
Im Jahr 2007 gab es in Münster 736 Unfälle, an denen insgesamt 843 Radfahrer beteiligt waren. Ein Radfahrer starb, 638 wurden verletzt. In 46 % der Fälle verursachte der Radfahrer den Unfall, bei weiteren 11 % war er an der Ursache beteiligt (dies beinhaltet auch Alleinunfälle von Radfahrern oder untereinander). Im Vergleich zum Jahr davor blieben die Zahlen relativ konstant, so gab es zwar 2006 mit 9.179 insgesamt weniger Verkehrsunfälle, an der Statistik für Radfahrer änderte sich jedoch nicht sehr viel. So ereigneten sich 720 Unfälle mit Beteiligung von 846 Radfahrern im Jahre 2006, wobei 653 von ihnen verletzt und keiner getötet wurde. Insgesamt blieb die Entwicklung seit Anfang des 21. Jahrhunderts relativ konstant, so lag die Unfallzahl mit Radfahrerbeteiligung zwischen 650 und 750 Unfälle und die Anzahl verletzter Radfahrer zwischen 580 und 660. 2011 wurden 758 Radfahrunfälle polizeilich registriert, die Dunkelziffer wird auf über 2000 geschätzt.Bei 710 Unfällen mit Beteiligung von Radfahrern (von 3.230 Unfällen insgesamt) kamen 633 Personen zu Schaden. Dabei wurden zwei Personen getötet,
117 schwer und 514 leicht verletzt. Radfahrer wurden 2015 nur bei 41 % ihrer Unfälle polizeilich als Hauptverursacher eingestuft. Dies beinhaltet auch Unfälle von Radfahrern allein oder untereinander, so dass weit überwiegend die Unfallgegner die Hauptunfallverursacher waren. Die Hauptunfallursachen der Unfallgeger werden von der Polizei nicht aufgezählt.Als häufigste Gründe für einen durch Radfahrer verursachten Unfall werden die Benutzung der falschen Fahrbahn, die Nichtbeachtung der Vorfahrt und die von Ampeln sowie Alkoholkonsum angegeben. Bei fünf der acht Radfahrer, die zwischen 2008 und 2012 auf den Straßen Münsters ums Leben kamen, wurde Alkohol im Blut nachgewiesen. Im Jahr 2010 wurden 53 Radfahrer mit mehr als 1,6 Promille Blutalkohol registriert. Im Folgejahr waren es 134 Radfahrer, die diesen Promillewert aufwiesen.In der Verunglücktenhäufigkeitszahl, das heißt die Anzahl an verletzten Personen pro 100.000 Einwohner, belegte Münster in Nordrhein-Westfalen bis 2012 den letzten Platz, allerdings sind nur in rund neun bis zwölf Prozent aller Unfälle Radfahrer beteiligt. Problematisch hingegen ist, dass Radfahrer und Fußgänger einen hohen Anteil von rund 50 % aller Verletzten stellen. Von diesen sind über 80 % leichtverletzt, was für das schlechte Abschneiden Münsters in den Unfallstatistiken verantwortlich ist. So werden nach Angaben der Polizei monatlich 180 Radfahrer nach Verkehrsunfällen in Münster ins Krankenhaus eingeliefert. Die größte Risikogruppe bilden Verkehrsteilnehmer im Alter zwischen 18 und 30 Jahren. Im Jahr 2012 gelang es den letzten Platz in der Statistik der Verunglücktenhäufigkeitszahl an Düsseldorf abzugeben und auf Platz 44 von 47 vorzurücken.Einer bundesweiten Kinderunfall-Statistik der Jahre 2006 bis 2010 der Bundesanstalt für Straßenwesen werden keinerorts in Nordrhein-Westfalen mehr Kinder im Alter von bis zu 14 Jahren im Straßenverkehr verletzt oder getötet als in Münster. In Münster kamen im Jahr 2012 etwa 3,47 Unfälle auf 1000 Kinder, womit die Stadt Rang 38 von 241 belegt. Hierbei werden Kinder als Radfahrer wie auch als Fußgänger gleichermaßen in Verkehrsunfälle verwickelt. Die Gründe für die hohen Unfallzahlen unter Kindern seien unbekannt.Die Polizei führt eine computergestützte Unfallsteckkarte. Nach dieser Karte lebten Radfahrer im Jahr 2008 an der Hammer Straße und vor den beiden Mensen an Aasee und Coesfelder Kreuz am gefährlichsten. Im Jahr 2012 zählten weiterhin unverändert die Weseler Straße, Hammer Straße sowie der Ring zu den Unfallschwerpunkten. Nach Analyse von Unfallforschern sind Kreuzungen und Straßeneinmündungen häufige Unfallschwerpunkte. Abbiegeunfälle und besonders das Linksabbiegen an Ampelkreuzungen sei ein Problem. Eine stadtweite Ordnungspartnerschaft zur Unfallprävention hat bislang ihr Ziel, die Unfallzahlen jährlich um 10 % zu senken, verfehlt. Die Direktion Verkehr der Polizei meint, dass die Minderung des Unfallrisikos nur mit flächendeckenden Überwachungsdruck auf die Verkehrsteilnehmer zu erreichen sei. Hierzu wurde unter anderem seit dem Frühjahr 2008 ein mit dem ProViDa-System ausgerüstetes Polizeifahrrad eingesetzt, jedoch Anfang 2010 wieder außer Dienst gestellt. Doch nicht nur die Polizei sowie das Ordnungsamt setzen auf Fahrräder, sondern seit der Skatenight am 17. August 2012 sind zwei Sanitäter der Johanniter-Unfall-Hilfe auf Fahrrädern unterwegs, um bei „mobilen Veranstaltungen“ wie dem Münster-Marathon, der Skatenight oder Reitturnieren schnell Erste Hilfe leisten zu können, bis ein Notarzt am Unfallort eintrifft.Im August 2012 wurde durch die Stadt und den ADFC beschlossen, zweimal jährlich die größten innerstädtischen Gefahrenstellen für Radfahrer in Augenschein zu nehmen, um die Verkehrssicherheit für Fahrradfahrer zu erhöhen.
Das heutige Radverkehrsnetz der Stadt Münster geht im Kern auf Radweg-Planungen der direkten Nachkriegsjahre zurück. Zuvor wurde der Radverkehr noch fast überall, auch an stärker Kfz-belasteten Hauptstraßen, auf denen damals allerdings auch nicht die heutige Höchstgeschwindigkeit von 50 km/h innerorts galt, auf der Fahrbahn geführt und in der Folge, bei zunehmender Motorisierung, als Hindernis für den Kraftverkehr wahrgenommen. So formulierte ein Gutachten aus dem Jahr 1946:
Erste Priorität genoss demzufolge bei der Planung der Radwege nicht – wie vielfach angenommen – die Sicherheit der Radfahrer, sondern die Erleichterung des Fortkommens mit dem Pkw. Deswegen wurden Radwege in Münster zuerst dort angelegt, wo starker Radverkehr von der Fahrbahn verschwinden sollte.
In der Folge wurden Radverkehrsanlagen zumeist als Bordsteinradwege abseits der Fahrbahn im Seitenraum der Straße angelegt. Der Platz hierfür wurde häufig den Gehwegen abgenommen. Derartige Radverkehrsanlagen stehen inzwischen bei Fachleuten und Fahrradverbänden in erheblicher Kritik, da sie das Unfallrisiko sowohl für Radfahrer als auch für Fußgänger deutlich steigern. „Viele Probleme mit Radfahrern sind in Münster auch hausgemacht“, urteilt der ADFC Münster, so würden weniger Radfahrer als Geisterfahrer die falsche Straßenseite befahren, wenn sich die Radwege nicht unmittelbar neben den Bürgersteigen, sondern abgesenkt auf der Straße befinden würden.Nur wenige Radverkehrsanlagen im Innenstadtbereich Münsters entsprechen gemäß den Vorgaben der Verwaltungsvorschrift zur Straßenverkehrs-Ordnung (VwV-StVO) und den Empfehlungen für Radverkehrsanlagen (ERA 2010) den Regelbreiten von 2,00 Metern, oft auch nicht der Mindestbreite von 1,60 Metern nach ERA (bei geringen Radfahrermengen, was in Münster nur an wenigen innerstädtischen Strecken zutrifft).
Anhand der Unfallsteckkarten der Polizei Münster lassen sich die Missstände nachvollziehen. Die Unfallsteckkarte für das Jahr 2011 weist deutliche Unfallhäufungslinien im Bereich stark frequentierter Radwege auf, etwa an der Wolbecker Straße, während die wenigen Hauptstraßen in Münster ohne Separierung des Radverkehrs weitgehend unauffällig sind, beispielsweise die Kanalstraße oder die Scharnhorststraße.Dennoch hält die Straßenverkehrsbehörde bisher weitgehend an der Benutzungspflicht fest.
Die Promenade ist eine aus dem Befestigungsring hervorgegangene, die gesamte Innenstadt umgebende Ringstraße mit begleitendem Fußweg, die jedoch dem Fahrradverkehr vorbehalten ist. Hier kommt es jährlich zu über 50 Zusammenstößen von Radfahrern, bei denen die Unfallteilnehmer Verletzungen davontragen. Zu weiteren 30 bis 40 Unfällen kommt es an den Stellen, an denen die Promenade von Straßen gequert wird. Im Juli 2011 wurde vorgeschlagen, die Promenade mit einer durchgängigen Mittellinie zu versehen, zumindest aber eine mehrere Meter lange Linie im Kreuzungsbereich der Promenade mit den kreuzenden Straßen aufzutragen, um die Unfallgefahr sowohl für als auch durch abbiegende Radfahrer zu verringern. Dies wurde jedoch von der Stadt abgelehnt. An der einzigen Unterführung der Promenade zwischen Salz- und Mauritzstraße konnte mit 1750 Radfahrern pro Stunde eines der größten Verkehrsaufkommen der Promenade gezählt werden, weswegen im Januar 2012 Pläne vorgestellt wurden, den Radweg an dieser Stelle für rund 120.000 Euro von 3,65 m auf 5,00 m Breite auszubauen.
Außerdem ist der große, zweispurige Kreisverkehr auf dem Ludgeriplatz eine für Radfahrer sehr gefährlich, wo jährlich bis zu 100 Unfälle zu teilweise schweren oder tödlichen Verletzungen führen. Bis April 2004 gab es an dieser Stelle drei Unfälle mit Todesfolge im Zusammenhang mit Radfahrern. Die Stadt versuchte bis 2005 diese Situation zu entschärfen. Dazu wurden die Außenseiten im Bereich der Zufahrtsstraßen des Kreisverkehrs, die nicht befahren werden dürfen, zwischen Hammer Straße, Hafenstraße und Schorlemerstraße mit kleinen Betonschwellen abgesperrt, da dort Radfahrer häufig von Autofahrern übersehen wurden und es bereits mehrmals zu Unfällen mit langen Fahrzeugen wie Lkw und Bussen gekommen war. Wegen der Absperrungen waren die Radfahrer gezwungen, auf der Fahrbahn und damit zwischen den Autos zu bleiben. Jedoch wurden die Absperrungen immer wieder vandaliert und von vielen Radfahrern nicht akzeptiert. Weiterhin kam es dadurch zu anderen gefährlichen Situationen, bei denen die Radfahrer durch große Fahrzeuge in die Betonsperren gedrückt werden konnten und keine Ausweichmöglichkeit für diesen Notfall bestand. Der Versuch, dieses System zu etablieren, wurde daraufhin im Frühjahr 2005 aufgegeben und die Betonsperren wieder abgebaut.
Stattdessen wurde die Anfahrt zum Kreisverkehr hin verändert und ein Schutzstreifen eingerichtet. Die Spuren sind nun so breit, dass Autos und Radfahrer gleichzeitig den Kreisverkehr anfahren können, im Falle eines großen Fahrzeugs die Anfahrt jedoch hintereinander erfolgen muss, da die Fahrzeugbreite bei Lkw und Bussen die Mitbenutzung des Schutzstreifens erfordert. So soll die Gefährdung im Toten Winkel der Autofahrer vermieden werden. Allerdings benutzen auch Autofahrer oftmals gebotswidrig den Schutzstreifen mit, so dass Radfahrer hier nicht an wartenden Autos vorbeifahren können. Außerdem warnen auch Schilder vor dem Toten Winkel und sollen die Radfahrer ermahnen, schon bei der Anfahrt hinter Lkw zu bleiben.
Ende 2007 wurde die Initiative „Sicher durch Münster“ als Ordnungspartnerschaft der Straßenverkehrsbehörde, der Polizei, der lokalen Gruppen von ADAC, ADFC und VCD sowie weiterer Partner wie Versicherungsgesellschaften und Lokalpresse ins Leben gerufen. Ziel ist es, die Zahl der Verkehrsunfälle jährlich um zehn Prozent zu senken. Nachdem die Verletzungen im Jahr 2010 erstmals entsprechend zurückgegangen waren, wurde das Projekt im November 2011 in Düsseldorf mit dem Landespreis für Innere Sicherheit vom Ministerium für Inneres und Kommunales des Landes Nordrhein-Westfalen ausgezeichnet. Bereits im Folgejahr überstieg die Zahl der Verletzten wieder die Marke von 2009. Die kurzfristige Verbesserung der Situation lässt sich vermutlich auf die starken Wintereinbrüche im Jahr 2010 zurückführen, in denen der Winterdienst zeitweise zusammenbrach und dementsprechend der Radverkehr beinahe zum Erliegen kam. Anfang 2012 wurde das Konzept der Ordnungspartnerschaft Verkehrsunfallprävention zur Verhängung bundesweiter Radfahrverbote in Münster eingeführt. Ihm wurde in der Anfangsphase eine große mediale Aufmerksamkeit zuteil, der Leiter des Münsteraner Ordnungsamtes stellte im weiteren Verlauf das Konzept im niedersächsischen Innenministerium vor, schließlich bekundete der Kreis Recklinghausen Anfang 2013 seinerseits Interesse an der Übernahme dieses Konzepts.
Damit Radfahrer eine Kreuzung zügig und sicher passieren können, wurden an vielen Stellen in Münster sogenannte Radfahrschleusen eingerichtet. Es handelt sich um Extra-Haltepunkte für Radfahrer an Ampeln, entweder an einer eigenen Haltlinie direkt vor den Autos oder auch auf einer gesonderten Radfahrerspur neben oder zwischen den wartenden Autofahrern:
Eine Straßen- und Anlagenordnung besagt, dass Fahrräder in Münster u. a. an Schilderpfählen, Lampenmasten, Absperrgittern und Buswartehallen nicht angeschlossen werden dürfen. In der Praxis erfolgt allerdings keine Durchsetzung dieser Verordnung.
Münster beherbergt mit der Radstation Münster das größte Fahrradparkhaus Deutschlands. Sie befindet sich direkt vor dem Hauptbahnhof. Während des Baus skeptisch von den Bewohnern der Stadt beäugt, wurde die Radstation schnell zum Erfolg: Die 3300 Stellplätze sind bei gutem Wetter ausgebucht, circa 2700 Kunden besitzen eine Dauerkarte. Nötig wurde die Anlage, da auf dem Bahnhofsvorplatz regelmäßig sämtliche Wege von abgestellten Fahrrädern blockiert wurden, so dass Fußgänger auf die Straße ausweichen mussten und Radfahrer, die ihr dort abgestelltes Fahrrad zurückhaben wollten, nicht zu ihrer Leeze kamen. Den Radfahrern standen zum Ende des Jahres 2004 im Stadtgebiet insgesamt 11.857 Stellplätze für ihre Räder zur Verfügung, darunter die schon genannten 3300 Plätze in der Fahrradstation, 8000 Fahrradständer sowie 557 Plätze an Umsteigemöglichkeiten, zum Beispiel Park+Ride-Stationen.
Nach Fertigstellung der Radstation war es untersagt, Fahrräder auf dem Vorplatz des Bahnhofs abzustellen. Zu dieser Zeit wurden Ruhebänke auf dem leerstehenden Platz aufgestellt. Wenige Jahre später hatte sich der Wildwuchs geparkter Fahrräder zu beiden Seiten des münsterschen Hauptbahnhofs erneut ausgebreitet, so dass er manchmal Zugänge versperrt und nur bedingt durch die Radstation eingedämmt werden konnte. Ein Teil dieser den Hauptbahnhof „zierenden“ Räder ist zudem herrenlos und muss regelmäßig vom Ordnungsamt der Stadt entfernt werden. Dazu werden u. a. Ein-Euro-Jobber eingesetzt. Ende 2005 wurde der Bereich hinter dem Hauptbahnhof neu gestaltet und mit 790 neuen Fahrradstellplätzen ausgestattet, so dass in diesem Bereich nur noch wenige Fahrräder wild abgestellt werden. Aber auch an anderen Stellen im Stadtgebiet sorgen die abgestellten Räder für Behinderungen. Besonders betroffen davon sind beispielsweise Einrichtungen der münsterschen Universität oder die Rothenburg südlich des Prinzipalmarktes, wo 2007 innerhalb des Einkaufszentrums Münster-Arkaden ein Fahrradparkhaus eröffnet wurde. Auch im neuen innerstädtischen Viertel, im ehemaligen Parkhaus Stubengasse, gibt es mittlerweile ein weiteres Parkhaus für 360 Fahrräder.
Die Friedensroute führt zur nördlichen Nachbarstadt Osnabrück. Für Touristen ist die bundesweit bekannte 100-Schlösser-Route besonders interessant. Weitere auch europaweite Radfernwege führen durch das Stadtgebiet:
Die Deutsche Fußballroute NRW ist eine 850 km lange Erlebnis-Radroute zum Thema Fußball in Nordrhein-Westfalen.
Die Pilgerroute (D7), Teil der D-Netz-Route, führt über ca. 1065 Kilometer als Radfernweg von Flensburg nach Aachen. Sie ist der deutsche Abschnitt der EuroVelo Pilgerroute (EV3).
Die Europaroute (D3) führt über 960 km von den Niederlanden quer durch Deutschland nach Polen. Er ist Teil des ca. 3600 km langen Europäischen Fernradwegs R1 von der französischen Kanalküste bis nach Sankt Petersburg in Russland.Darüber hinaus streift auch der EmsAuenWeg das Münsteraner Stadtgebiet. Dieser lässt sich mit dem Werse Rad Weg zu einem Rundweg durch das östliche Münsterland verbinden.
Viele Radfahrer steigen bei schlechtem Wetter auf den Öffentlichen Nahverkehr oder den eigenen Pkw um. Da in Münster im ÖPNV ausschließlich Busse zum Einsatz kommen, sind diese dann oftmals überfüllt und der Masse an Fahrgästen nicht gewachsen. Wetterunabhängig fühlen sich andererseits Radfahrer teilweise von den Linienbussen bedrängt.Am 27. November 2011 ging Münsters erstes Velotaxi, eine dreirädrige Rikscha mit Platz für zwei Fahrgäste, auf seine Jungfernfahrt. Mit dem zuschaltbaren Elektromotor kann es eine Höchstgeschwindigkeit von 20 km/h erreichen.
Die politischen Gremien haben 2004 ein „Radverkehrskonzept 2010“ beschlossen. Demnach sollten unter anderem die Unfallschwerpunkte systematisch untersucht und nach Möglichkeit entschärft werden. Lücken im bestehenden Radverkehrsnetz sollten geschlossen, Bordsteinradwege saniert und sogenannte Drängelgitter entfernt werden.Zur Verbesserung der Zufriedenheit der Radfahrer wurde vom ADFC Anfang 2012 der Bau von Radschnellwegen ins Umland sowie eine autofreie Altstadt vorgeschlagen.
Informationen über das Thema Fahrradstadt auf muenster.de (Memento  vom 5. Juni 2003 im Internet Archive)
Fachberichte zur Verkehrsunfallentwicklung in der Stadt Münster sowie auf den Autobahnen im Zuständigkeitsbereich des Polizeipräsidiums Münster, 2007–2015 unter Sicher durch Münster.de

Rafael Nadal Parera [rafaˈel naˈðal paˈɾeɾa] (* 3. Juni 1986 in Manacor, Mallorca) ist ein spanischer Tennisspieler. Er ist aktuell Zweiter hinter Weltranglistenführer Novak Đoković und beendete bereits vier Saisons (2008, 2010, 2013 und 2017) an der Spitzenposition. Insgesamt stand er 196 Wochen an deren Spitze.
Nadal gewann bisher 17 Grand-Slam-Titel im Einzel und liegt damit auf Platz 2 der Rekordliste hinter Roger Federer. Er ist der einzige Spieler der Tennisgeschichte, der ein Grand-Slam-Turnier – die French Open – elf Mal im Einzel gewinnen konnte. Dreimal war Nadal bei den US Open siegreich, dazu kommen zwei Erfolge in Wimbledon und ein Titelgewinn bei den Australian Open. Damit ist er einer von nur acht Spielern, die an jedem der vier Grand-Slam-Turniere wenigstens einmal als Sieger hervorgingen. Zudem gewann Nadal bei den Olympischen Spielen 2008 in Peking die Goldmedaille im Einzel und bei den Olympischen Spielen 2016 in Rio de Janeiro zusammen mit Marc López die Goldmedaille im Doppel. Viermal (2004, 2008, 2009 und 2011) gewann Nadal den Davis Cup mit der spanischen Mannschaft. Anfang 2011 wurde er für seine Leistungen zum Weltsportler des Jahres 2010 gewählt.
Nadal hält den Rekord der längsten Siegesserie auf Sand. Zwischen April 2005 und Mai 2007 gewann er auf Sand 81 Spiele in Folge, ehe er im Endspiel des Hamburger Masters-Turniers gegen Roger Federer verlor. Er gewann neben den French Open auch das Masters-Turnier in Monte Carlo und das ATP-500-Turnier in Barcelona je elfmal. Von vielen wird der erfolgreichste Sandplatzspieler der letzten Jahre als der beste Spieler auf diesem Belag in der Geschichte des Tennissports angesehen.
Rafael wurde als Sohn von Sebastián Nadal und Ana Maria Parera in Manacor auf der Insel Mallorca geboren. Sein Großvater väterlicherseits war der gleichnamige Musiker und Dirigent Rafael Nadal (1929–2015). Sein vollständiger Name lautet Rafael Nadal Parera, wobei sein erster Nachname Nadal in dem dort gesprochenen Katalanisch Weihnachten bedeutet. Er hat eine jüngere Schwester namens María Isabel (Maribel). Vater Sebastián besitzt ein Unternehmen für Glas und Fenster in Manacor, wo die Großfamilie Nadal einschließlich Rafael bis heute wohnt. Seit 2005 ist er mit Maria Francisca Perello liiert; beide sind bestrebt, ihr Privatleben möglichst von der Öffentlichkeit fernzuhalten. Nach der 10. Klasse ging Nadal mit 16 Jahren von der Schule ab. Zugunsten seiner Sportlerkarriere verzichtete er auf eine weitere schulische Ausbildung.
Schon früh in seiner Kindheit interessierte sich Rafael für Sport, nicht zuletzt deswegen, weil drei seiner Onkel Profisportler waren. Seine Onkel Rafael Nadal und Miguel Ángel Nadal waren beide professionelle Fußballspieler, wobei Rafael in der mallorquinischen Fußballliga spielte und Miguel Ángel für RCD Mallorca und den FC Barcelona aktiv war. Mit Barça gewann er während dieser Zeit fünf spanische Meisterschaften sowie einmal den Europapokal der Landesmeister und bestritt mehrere Dutzend Spiele für die spanische Nationalmannschaft. Sein Onkel Toni Nadal war ein auf der Heimatinsel bekannter Tennisspieler, der aber als Profi auf dem spanischen Festland nur mäßigen Erfolg hatte und sich dann dem Training der Jugend verschrieben hat. So kam es, dass er als kleiner Junge beide Sportarten betrieb, wobei Fußball seine große Leidenschaft war. Mit sieben Jahren begann er beim örtlichen Fußballverein Manacor als linker Stürmer, mit seiner Mannschaft holte er als Elfjähriger die Balearenmeisterschaft. Seine Begeisterung für Fußball hat sich bis heute erhalten, er ist immer noch leidenschaftlicher Real-Madrid-Fan.Im Alter von vier Jahren begann er mit dem Tennisspielen in einer kleinen Gruppe, die von seinem Onkel Toni trainiert wurde. Dieser erkannte und förderte früh sein Talent. Obwohl Tennis für den kleinen Rafael eher langweilig war und er lieber Fußball spielte, stellten sich früh Erfolge ein. Er nahm bereits im Alter von sieben Jahren an Turnieren teil. Mit acht Jahren gewann er die Balearenmeisterschaft der Unter-Zwölfjährigen und mit dreizehn die U14-Meisterschaft, obwohl er sich damals bei einem Sturz in der ersten Turnierrunde den kleinen Finger der linken Hand gebrochen hatte. Diese Siege zählen für ihn bis heute zu den wichtigsten auf seinem Weg zum Tennisprofi.Sein Onkel Toni war und ist als sein langjähriger Trainer maßgeblich an seinem Erfolg beteiligt. Die Familienmitglieder waren zwar früher besorgt, dass Toni den Jungen zu hart fordern würde, aber die ruhige und entspannte Art seines Vaters sorgte immer für einen gewissen Ausgleich zu Tonis Erfolgsstreben. Das intensive und strenge Training sowie die mentale Ausbildung durch Toni Nadal, gepaart mit Rafaels Talent, haben ihn aber erst zu einem der größten Tennisspieler der Welt werden lassen.Mit 14 erhielt Rafael Nadal durch Zufall erstmals die Gelegenheit, in einem Vorbereitungsspiel gegen einen großen Namen im Tennis anzutreten. Als Boris Becker, der als Gegner von Pat Cash vorgesehen war, kurzfristig ausfiel, bekam Nadal seine Chance; er schlug unerwartet den Wimbledonsieger von 1987.
Nadal suchte früh den Weg auf die Profitour und war 2001 mit 15 Jahren erstmals als Profispieler gemeldet. Er absolvierte in Spanien seine ersten beiden Turniere, die für ihn aber schon in den ersten beiden Runden beendet waren.2002 gelang ihm der erste Sieg auf der ATP Tour. In seiner Heimat besiegte er in der ersten Runde Ramón Delgado. Im selben Jahr gewann er sechs Titel bei kleineren spanischen Turnieren der Future-Serie. Im Juni erreichte er bei seinem einzigen Start auf der Junioren-Tour das Halbfinale von Wimbledon. Zum Jahresende 2002 hatte sich Nadal in der Weltrangliste um 611 Plätze auf Rang 200 verbessert.
Zum Saisonbeginn 2003 schaffte er einige Finalteilnahmen auf der ATP Challenger Tour, bevor er sich im April erstmals für ein Turnier der Masters Series qualifizieren konnte. In Monte Carlo feierte er Siege über Karol Kučera und Albert Costa, ehe er in der dritten Runde gegen den späteren Finalisten Guillermo Coria ausschied. Mit diesen Ranglistenpunkten konnte er nun regelmäßig an Turnieren der ATP Tour teilnehmen. Eine Verletzung verhinderte seinen ersten Start bei den French Open. Bei seinem ersten Auftritt in Wimbledon erreichte er dort mit 17 Jahren als jüngster Spieler seit Boris Becker die dritte Runde, in der er Paradorn Srichaphan unterlag. Sein bestes Abschneiden auf der Tour folgte dann mit dem Halbfinaleinzug beim Turnier in Umag, wo er seinem Landsmann Carlos Moyá unterlag. Bereits in dieser Saison zeigte sich seine besondere Stärke auf Sand – elf seiner 14 Siege erzielte er auf diesem Belag. Am Jahresende stand Nadal auf Platz 49 der Weltrangliste.
Zum Jahresbeginn 2004 erreichte er in Auckland sein erstes ATP-Turnier-Finale, das er gegen Dominik Hrbatý verlor. Bei seinem Australian-Open-Debüt zog Nadal in die dritte Runde ein, in der er Lleyton Hewitt unterlag. Beim Miami Masters konnte er zum ersten Mal den Weltranglistenersten Roger Federer besiegen und ins Achtelfinale vorstoßen. Beim Turnier von Estoril erreichte er das Viertelfinale, zu dem er wegen einer Stressfraktur im Mittelfußknochen jedoch nicht antreten konnte. Die Verletzung zwang ihn zu einer dreimonatigen Pause. Im August gewann Nadal dann erstmals ein ATP-Turnier, sein Finalgegner in Sopot war José Acasuso. Bei den US Open erreichte er an der Seite von Tommy Robredo zum ersten Mal das Halbfinale eines Grand-Slam-Turniers im Doppel.
Im Dezember gehörte Nadal der spanischen Mannschaft an, die im Davis-Cup-Finale die USA mit 3:2 bezwang. Bei seinem Sieg über Andy Roddick war er mit 18 Jahren und sechs Monaten der zweitjüngste Spieler (nach Boris Becker 1985), der ein Einzelmatch in einem Davis-Cup-Finale gewinnen konnte.
Das Jahr 2005 brachte Nadal mit elf Turniersiegen den endgültigen Durchbruch, so viele sind ihm in einer Saison seitdem nicht mehr gelungen. Den größten Erfolg feierte er bei den French Open: er war der erste Spieler seit Mats Wilander im Jahr 1982, der gleich bei seinem Debüt in Paris gewinnen konnte. Im Halbfinale besiegte er an seinem neunzehnten Geburtstag Federer mit 6:3, 4:6, 6:4 und 6:3. Mit seinem Finalsieg über Mariano Puerta war Nadal mit 19 Jahren und 2 Tagen zudem der jüngste Sieger eines Grand-Slam-Turniers seit Pete Sampras (US Open 1990). In Wimbledon musste er dagegen eine frühe Niederlage einstecken, als Nummer vier der Setzliste unterlag er in Runde zwei Gilles Müller. In Montreal gewann er dann mit einem Dreisatzsieg über Andre Agassi seinen ersten Titel auf Hartplatz. Bei den US Open musste er sich (gegen James Blake) erneut in der dritten Runde geschlagen geben. Acht seiner elf Titel gewann Nadal auf seinem Lieblingsbelag, seine Jahresbilanz auf Sand wies 50 Siege bei zwei Niederlagen auf; bis zum Jahresende blieb er 36 Matches in Folge ungeschlagen. Nadal war damit der erfolgreichste Sandplatzspieler seit Thomas Muster – der Österreicher hatte es 1995 auf 65 Siege bei nur zwei Niederlagen gebracht. Nadal beendete das Jahr als erster Teenager seit Boris Becker im Jahr 1986 auf Position 2 der Weltrangliste.
2006 gewann er fünf Turniere, vier davon gegen Federer: Dubai, Monte Carlo, Rom und die French Open. Seinen fünften Titel des Jahres gewann er in Barcelona gegen Tommy Robredo. Mit dem Finale in Rom blieb er auf Sand 53 Partien in Folge unbesiegt – damit stellte er den Rekord von Guillermo Vilas aus dem Jahr 1973 ein. Mit dem Titelgewinn bei den French Open hatte Nadal auf Sand 60 Siege in Folge errungen. Anschließend zeigte er sich auch auf Rasen stark verbessert. Als erster Spanier seit dem Titelgewinn seines Landsmannes Manuel Santana im Jahr 1966 erreichte er in Wimbledon das Endspiel. Das erneute Duell mit Federer gewann diesmal der Schweizer. Für Nadal war es die erste Finalniederlage nach 14 Siegen in Serie. Zum Abschluss des Jahres erreichte er noch das Halbfinale beim Tennis Masters Cup und beendete das Jahr wiederum auf Platz 2.
2007 erreichte er das Viertelfinale der Australian Open, unterlag jedoch Fernando González in drei Sätzen. Beim Masters-Turnier in Indian Wells konnte Nadal dann den ersten Turniersieg seit den French Open im Jahr zuvor feiern. Beim Masters-Turnier in Miami unterlag er im Viertelfinale Novak Đoković, den er im Finale von Indian Wells noch hatte bezwingen können. In Monte Carlo und Rom konnte er seine Titel aus dem Vorjahr verteidigen, sodass er nunmehr seit 78 Partien auf Sand unbesiegt war. Im Halbfinale von Rom stellte er den bisherigen Rekord John McEnroes von 75 Siegen in Serie auf einem Belag ein. Auch beim Turnier in Hamburg erreichte Nadal das Finale, verlor dort jedoch nach 81 Erfolgen auf Sand gegen Roger Federer. In Paris gelang ihm mit einem erneuten Sieg über Federer der Hattrick bei den French Open. Auch in Wimbledon kam es zu einer Neuauflage des Vorjahresfinales, die wiederum Federer gewann. Nach einer Niederlage im Achtelfinale der US Open gegen David Ferrer erreichte Nadal noch das Finale beim Paris Masters gegen David Nalbandian und das Halbfinale des Tennis Masters Cups gegen Roger Federer.
2008 gelang ihm der Halbfinaleinzug bei den Australian Open und in Indian Wells sowie der Finaleinzug in Miami. Mit einem Sieg über Federer sicherte sich Nadal in Monte Carlo seinen vierten Titel in Folge. Mit Tommy Robredo gewann er auch im Doppel, seinen insgesamt vierten Doppeltitel. Nadal war dann mit seinem Sieg in Hamburg erst der dritte Tennisspieler, der alle drei Masters-Turniere auf Sand in einem Jahr gewinnen konnte. Bei den French Open gelang ihm ohne Satzverlust der vierte Triumph in Folge, womit er Björn Borgs Open-Era-Rekord egalisierte. In einem nur 107 Minuten dauernden Finale, dem kürzesten seit 1980, besiegte er Federer klar mit 6:1, 6:3 und 6:0.
Auch in Wimbledon kam es erneut zu einem Finale zwischen ihm und Federer. In einer der dramatischsten Begegnungen der Tennisgeschichte gewann der Spanier mit 6:4, 6:4, 6:7, 6:7 und 9:7. Es war das längste Herren-Einzelfinale in der über 130-jährigen Turniergeschichte. John McEnroe und Björn Borg, die 1980 das bis dahin bedeutendste Wimbledonfinale bestritten hatten und das Match live verfolgten, sprachen später vom besten Spiel, das sie jemals gesehen hätten.Beim Turnier von Cincinnati verlor Nadal zwar sein Halbfinale, da Federer aber bereits im Achtelfinale ausgeschieden war, konnte Nadal ihn am 18. August 2008 von der Spitze der Weltrangliste verdrängen. Federer war 237 Wochen ununterbrochen die Nummer eins gewesen. Am 17. August 2008 sicherte sich Nadal bei den Olympischen Spielen in Peking mit einem Sieg über den Chilenen Fernando González die Goldmedaille. Bei den US Open erreichte er dann zum ersten Mal das Halbfinale, unterlag jedoch Andy Murray. Das letzte Masters-Turnier des Jahres in Paris musste er wegen einer Knieverletzung im Viertelfinale abbrechen und alle weiteren Turniere absagen.
Am 1. Februar 2009 gelang Nadal bei den Australian Open gegen Roger Federer der erste Grand-Slam-Turniersieg auf Hartplatz. Es war sein sechster Grand-Slam-Titel und der 13. Sieg im 19. Duell gegen Federer sowie der erste Einzeltitel eines Spaniers bei den Australian Open. Im weiteren Saisonverlauf siegte er auch in Barcelona, sein fünfter Erfolg in Serie dort. Dagegen schied er im Achtelfinale der French Open überraschend gegen den Schweden Robin Söderling aus. Es war seine erste Niederlage in Roland Garros überhaupt. Verletzungsbedingt trat Nadal in Wimbledon nicht an; so musste er Federer nach dessen Triumph dort wieder die Führung im ATP-Ranking überlassen – Nadal hatte bis zum 5. Juli 2009 46 Wochen lang Position 1 eingenommen. Er ging beim Rogers Cup wieder an den Start und wurde dort im Viertelfinale von Juan Martín del Potro gestoppt. Gegen ebendiesen Gegner verlor er auch das Halbfinale der US Open und beendete die Saison damit zum vierten Mal auf Platz 2.
Bei den Australian Open 2010 musste er als Titelverteidiger in seinem Viertelfinalmatch gegen den späteren Finalisten Andy Murray im dritten Satz wegen einer Knieverletzung aufgeben. Im April gelang ihm in Monte Carlo der sechste Erfolg in Serie. Auch in Rom gelang ihm die Titelverteidigung und in Madrid der 18. Titel bei einem Turnier der Masters Series. Damit übernahm Nadal in dieser Statistik den Spitzenplatz vor Andre Agassi, der es auf 17 Masters-Erfolge gebracht hat. Die French Open gewann er – wiederum ohne Satzverlust – bereits zum fünften Mal. Er besiegte im Finale Robin Söderling und übernahm, nachdem er zwischendurch für einige Wochen auf Platz 4 zurückgefallen war, erneut die Führung in der Weltrangliste. Am 4. Juli sicherte sich Nadal seinen zweiten Wimbledon-Titel; im Finale besiegte er Tomáš Berdych in drei Sätzen. Bei den US Open erreichte er dann erstmals das Finale, in dem er Novak Đoković in vier Sätzen bezwang. Im gesamten Turnierverlauf gab Nadal nur fünf Aufschlagspiele ab und wurde mit 24 Jahren der siebte Spieler und der jüngste in der Open Era, der alle vier Grand-Slam-Titel gewonnen hat.
Das Jahr begann für Nadal mit dem ATP-Turnier in Doha, wo er das Halbfinale gegen Nikolai Dawydenko verlor. Im Viertelfinale der Australian Open musste er sich seinem Freund David Ferrer geschlagen geben. Er zog sich im ersten Satz eine Oberschenkelzerrung zu und verlor das Match in drei Sätzen. Bei seinem Comeback konnte Nadal in Indian Wells und Miami jeweils das Finale erreichen, beide Endspiele endeten aber mit Niederlagen gegen Đoković. Seinen ersten Turniersieg des Jahres errang Nadal in Monte Carlo, wo er Ferrer diesmal im Finale besiegte; es war sein siebter Erfolg dort in Serie. Eine Woche später konnte er sich erneut gegen Ferrer durchsetzen und seinen sechsten Titel beim ATP-Turnier von Barcelona gewinnen. Anfang Juni gewann Nadal ebenfalls zum sechsten Mal das Finale der French Open. Er besiegte wieder einmal Federer (in 3:40 Stunden mit 7:5, 7:6, 5:7 und 6:1) und zog mit dem schwedischen Rekordsieger Björn Borg gleich, der in Paris sechsmal triumphierte.In Wimbledon verlor Nadal zum fünften Mal in diesem Jahr ein Finale gegen Đoković. Der Serbe hatte bereits durch das Erreichen des Endspiels Nadal als Weltranglistenersten abgelöst. Bei den Vorbereitungsturnieren auf die US Open bot Nadal eher mäßige Leistungen und schied in Montreal und Cincinnati jeweils früh aus. Bei den US Open steigerte er sich dann von Runde zu Runde. Schließlich kam es zur Neuauflage des Vorjahresfinales gegen Đoković, allerdings mit dem besseren Ende für den Serben, der auch das sechste Aufeinandertreffen der Saison für sich entscheiden konnte. Im September zog er dagegen nach souveränen Siegen gegen die Franzosen Tsonga und Gasquet mit Spanien ins Davis-Cup-Finale ein.
Im Oktober musste Nadal weitere Niederlagen hinnehmen, als er gegen Andy Murray in Tokio und später gegen Florian Mayer in zwei Sätzen beim Masters in Schanghai verlor. Auch beim Saisonfinale erreichte er nicht seine alte Form und schied nach Niederlagen gegen Federer und Tsonga frühzeitig aus.
Im Davis-Cup-Finale Anfang Dezember gegen die Mannschaft Argentiniens verhalf er dem spanischen Team mit seinen Erfolgen gegen Juan Mónaco und Juan Martín del Potro zum Sieg und schloss so das Jahr 2011 noch mit einem Erfolg ab.
Bei seinem Saisonauftakt in Doha zog Nadal ins Halbfinale gegen Gaël Monfils ein, verlor dieses aber glatt in zwei Sätzen. Bei den Australian Open traf er im Finale erneut auf Đoković, der sich in fünf Sätzen mit 7:5, 4:6, 2:6, 7:6, 5:7 durchsetzte. Mit einer Spielzeit von fünf Stunden und 53 Minuten war es nicht nur die längste Partie der Turniergeschichte, sondern auch das längste Finale bei einem Grand-Slam-Turnier. Nadal verlor damit zum dritten Mal in Folge ein solches Finale und stellte einen weiteren, wenn auch negativen Rekord auf.
Bei den anschließenden Masters-Turnieren in Indian Wells und Miami erreichte er jeweils das Halbfinale. In Indian Wells verlor er gegen Federer, konnte sich aber in der Doppelkonkurrenz mit seinem Landsmann Marc López den Titel sichern. Dies war nach 2010 sein zweiter Titel in Indian Wells und sein achter Doppelerfolg insgesamt. In Miami musste er das Halbfinalmatch gegen Andy Murray wegen einer Knieverletzung absagen. Danach stand er beim Masters von Monte Carlo erstmals wieder auf dem Platz und konnte gleich wieder ins Finale einziehen, wo er abermals dem Weltranglistenersten Đoković gegenüberstand. Nach zuletzt sieben verlorenen Endspielen behielt Nadal diesmal mit 6:3 und 6:1 die Oberhand und gewann in Monte Carlo seinen achten Titel in Folge – als erster Spieler der Open Era bei ein und demselben Turnier. Auch schon zum siebten Mal gewann er kurz darauf das ATP-Turnier von Barcelona, wo er im Endspiel David Ferrer nach 2008, 2009 und 2011 bereits zum vierten Mal besiegte.
Nach dem Aus im Achtelfinale des Master-Turniers in Madrid verlor er Platz 2 der Weltrangliste an Federer, der das Turnier gewann (Nadal war letztmals im Mai 2010 auf dem dritten Platz geführt worden). Beim anschließenden Turnier in Rom konnte er wie schon in Monte Carlo Đoković in zwei Sätzen schlagen. Mit seinem insgesamt sechsten Erfolg in der italienischen Hauptstadt sowie dem 21. Turniersieg bei einem Masters-Wettbewerb rückte er wieder auf Platz 2 der Weltrangliste vor.
Bei den French Open zog er ohne Satzverlust ins Finale ein, wo er zum vierten Mal in Folge bei einem Grand-Slam-Finale auf Đoković traf. Beim wegen Regenunterbrechungen über zwei Tage ausgetragenen Match setzte er sich in vier Sätzen durch. Er wurde mit seinem siebten Erfolg in Paris – zugleich sein 50. Einzeltitel – zum alleinigen Rekordsieger der French Open.
Am 28. Juni 2012 scheiterte er in Wimbledon zum ersten Mal seit 2005 bei einem Grand-Slam-Turnier in Runde zwei. Er unterlag dem in der Weltrangliste 98 Plätze hinter ihm platzierten Tschechen Lukáš Rosol in fünf Sätzen (7:6, 4:6, 4:6, 6:2, 4:6). Damit endete auch seine seit den French Open 2011 andauernde Serie von fünf Grand-Slam-Finalteilnahmen in Folge.
Im Juni benannte das Nationale Olympische Komitee Spaniens (Comité Olímpico Español) Nadal als Träger der spanische Flagge bei der Eröffnungsfeier der Olympischen Spiele in London. Am 19. Juli erklärte Nadal allerdings aufgrund einer Knieverletzung, die er sich bei seiner Zweitrundenniederlage in Wimbledon zugezogen hatte, seinen Verzicht auf Olympia. Auch die anschließenden Masters-Turniere in Nordamerika und die US Open musste er auslassen. Im September gab er auf seiner Website bekannt, dass er wegen seiner Kniebeschwerden mindestens zwei weitere Monate lang an keinem Wettkampf teilnehmen könne. Er verpasste damit die China Open, die Masters-Turniere in Schanghai und Paris sowie das Davis-Cup-Halbfinale gegen die USA.
Ende November stieg Nadal wieder ins Training ein und sagte seine Teilnahme für das Turnier in Abu Dhabi zu. Ein Magen-Darm-Virus zwang ihn am 25. Dezember jedoch zur Absage und nur drei Tage später sagte er auch die Australian Open ab.
Am 5. Februar 2013 kehrte Nadal nach 222 Tagen Verletzungspause auf den Tennisplatz zurück. Beim Sandplatzturnier in Viña del Mar nahm er sowohl am Einzel- als auch am Doppelturnier – im Doppel trat er mit dem Argentinier Juan Mónaco an – teil und kam jeweils ins Finale. Bei den Brasil Open in São Paulo gewann Nadal am 17. Februar das Endspiel gegen den Argentinier David Nalbandian. Es war acht Monate nach den French Open 2012 sein erster Titelgewinn nach der Verletzungspause und sein erster im Jahr 2013.
Im März gewann er das Endspiel in Acapulco gegen seinen Landsmann David Ferrer mit 6:0 und 6:2, ohne im gesamten Turnierverlauf einen Satz abzugeben. Auch beim anschließenden Masters-Turnier in Indian Wells erreichte Nadal das Finale. Zuvor hatte er im Viertelfinale Roger Federer und im Halbfinale Tomáš Berdych besiegt. Mit 4:6, 6:3 und 6:4 bezwang er im Endspiel auch Juan Martín del Potro und gewann damit seinen dritten Titel in Indian Wells. In der Weltrangliste zog er damit wieder an David Ferrer vorbei auf Platz 4.
In Monte Carlo erreichte Nadal das neunte Finale in Folge im Fürstentum, unterlag jedoch nach acht Turniersiegen in Serie Novak Đoković mit 2:6 und 6:7. Wie auch in den fünf vorangegangenen Turnieren erreichte Nadal in Barcelona das Finale. Er besiegte im Endspiel Nicolás Almagro mit 6:4 und 6:3 und feierte seinen achten Turniersieg in Barcelona und schon den vierten seit seinem Comeback im Februar. Bereits zwei Wochen später gewann er mit einem 6:2 und 6:4 im Finale der Mutua Open in Madrid gegen Stanislas Wawrinka seinen insgesamt 23. Titel bei einem ATP-Masters-1000-Turnier. Den 24. Titel feierte er sogleich beim darauffolgenden Masters in Rom. Im Finale besiegte er Federer deutlich mit 6:1 und 6:3; es war sein 20. Sieg im 30. Duell gegen Federer.
Am 9. Juni 2013 gewann Nadal zum achten Mal die French Open, indem er Ferrer in drei Sätzen mit 6:2, 6:3 und 6:2 besiegte. Er stellte damit einen neuen Rekord in der „Open Era“ auf: mit seinem achten Grand-Slam-Titel bei den French Open erreichte er die meisten Titel bei einem Grand-Slam-Turnier.
In Wimbledon schied Nadal in der ersten Runde aus, er unterlag dem in der Weltrangliste 130 Plätze hinter ihm geführten Belgier Steve Darcis in drei engen Sätzen mit 6:7, 6:7 und 4:6. Es war die erste Erstrundenniederlage bei einem Grand-Slam-Turnier in seiner Karriere. Im Anschluss gelang es Nadal als erstem Spieler seit Andy Roddick in der Saison 2003, die beiden Masters-Hartplatzturniere in Montreal und Cincinnati innerhalb einer Woche zu gewinnen. Damit baute er seine Hartplatz-Bilanz in der laufenden Saison auf 15:0 aus und stieg in der Weltrangliste auf Position zwei. Bei den US Open erreichte er mit nur einem Satzverlust das Endspiel. Dort bezwang er Novak Đoković mit 6:2, 3:6, 6:4 und 6:1 und gewann damit nach 2010 den zweiten US-Open-Titel seiner Karriere und den zweiten Grand Slam in dieser Saison.
Im Oktober 2013 zog Nadal nach der verletzungsbedingten Aufgabe von Berdych im Halbfinale beim Turnier in Peking ins Endspiel ein. Durch diesen Sieg sicherte er sich, erstmals wieder seit Juli 2011, die Weltranglistenführung. Im Finale unterlag er Đoković in zwei Sätzen.
Nadal startete gut in die neue Saison. Gleich bei seinem ersten Turnierstart in Doha erreichte er das Finale, das er gegen Gaël Monfils in drei Sätzen gewann. Bei den Australian Open zog er ebenfalls ins Endspiel ein, in dem er auf Wawrinka traf, den er in sämtlichen zwölf Aufeinandertreffen zuvor hatte besiegen können. Leicht angeschlagen verlor er aber diesmal mit 3:6, 2:6, 6:3 und 3:6.
Nach Siegen in Rio und Madrid ging er bei den French Open abermals als Topfavorit ins Turnier. Nadal, der die letzten vier Duelle gegen Novak Đoković verloren hatte, traf im Finale der French Open wiederum auf den Serben, behielt jedoch dieses Mal mit 3:6, 7:5, 6:2 und 6:4 die Oberhand. Mit dem Sieg sicherte er sich seinen neunten Titel in Roland Garros und den 14. Grand-Slam-Titel seiner Karriere. Damit zog er in dieser Statistik mit Pete Sampras gleich. Außerdem verteidigte er die Weltranglistenführung gegen Đoković und baute seine Bilanz bei den French Open auf 66:1 Siege aus.
Zum anschließenden Start der Rasensaison nahm Nadal am Turnier in Halle teil. Als Nummer 1 der Setzliste hatte er zunächst ein Freilos und schied dann im Achtelfinale mit 4:6 und 1:6 gegen Dustin Brown aus. Eine Woche später erreichte er in Wimbledon, als Nummer 2 gesetzt, mit drei Viersatzsiegen über Martin Kližan, Lukáš Rosol und Michail Kukuschkin das Achtelfinale, in dem er überraschend dem 19 Jahre alten Nick Kyrgios mit 6:7, 7:5, 6:7 und 3:6 unterlag.
Aufgrund einer Handgelenksverletzung musste er die folgenden ATP-Masters-1000-Turniere in Toronto und Cincinnati sowie die US Open, die er im Vorjahr alle gewonnen hatte, absagen. Er setzte in Peking die Saison fort, wo er im Viertelfinale Martin Kližan in drei Sätzen unterlag. Beim folgenden Masters in Schanghai verlor er, geschwächt durch eine Blinddarmentzündung, bereits seine Auftaktpartie gegen Feliciano López. Im Anschluss sagte er die Teilnahme an den World Tour Finals wegen einer bevorstehenden Entfernung des Blinddarms ab. Er trat noch bei den Swiss Indoors in Basel an, wo er nicht über das Viertelfinale hinauskam. Am 3. November unterzog er sich in Barcelona der angekündigten Operation.
Zu Beginn des Jahres 2015 scheiterte Nadal bei den Australian Open im Viertelfinale an Tomáš Berdych. In Buenos Aires gelang ihm Anfang der März der erste Saisontitel, doch es sollte das letzte Erfolgserlebnis für längere Zeit sein. Nadal blieb bis zu den French Open ohne weiteren Titel, zum ersten Mal seit elf Jahren verpasste er einen Titelgewinn bei einem der drei Masters-Turniere auf Sand. Bei den French Open kam es im Viertelfinale, wie schon in den drei Vorjahren, wieder zum Duell mit Đoković. Nadal musste sich mit 5:7, 3:6 und 1:6 deutlich geschlagen geben und verlor erstmals nach 39 Siegen wieder in Paris. Zu Beginn der Rasensaison gewann Nadal seinen zweiten Saisontitel in Stuttgart. Es war sein erster Erfolg auf diesem Belag seit fünf Jahren. Doch an diesen Erfolg konnte er auf dem Rasen von Wimbledon nicht anknüpfen; er scheiterte bereits in der zweiten Runde an Dustin Brown. Nachdem er in Hamburg seinen dritten Saisontitel gewonnen hatte, musste sich Nadal auch bei den US Open früh geschlagen geben. Nach einer deutlichen Zweisatzführung verlor er sein Drittrundenmatch gegen Fabio Fognini, zum ersten Mal in seiner Karriere verlor er ein Match nach einer solchen Führung. Damit endete auch Nadals Rekordserie mit mindestens einem Titelgewinn bei Grand-Slam-Turnieren pro Saison. In den letzten zehn Jahren hatte er immer mindestens eines der vier wichtigsten Turniere gewonnen. Zum Saisonende gelang ihm bei den ATP World Tour Finals noch der Halbfinaleinzug, doch er musste sich erneut Đoković geschlagen geben. Er beendete die Saison 2015 schließlich auf Platz 5 der Weltrangliste, seiner schlechtesten Platzierung seit elf Jahren am Saisonende.
Anfang 2016 scheiterte er bei den Australian Open schon in der ersten Runde an Fernando Verdasco. Im April holte er sich in Monte Carlo seinen neunten Titel. Im Finale bezwang er Gaël Monfils mit 7:5, 5:7 und 6:0. Im direkt folgenden Turnier in Barcelona gewann er seinen 69. Karrieretitel. Bei den French Open musste er sein Drittrundenmatch gegen Marcel Granollers aufgrund von Handgelenkschmerzen absagen. Diese hinderten ihn auch daran, in Wimbledon anzutreten. Erst zu den Olympischen Spielen kehrte er zurück, er musste sich dort im Halbfinale Juan Martín del Potro geschlagen geben. Im Spiel um Bronze unterlag er Kei Nishikori, wohingegen er im Doppel mit Marc López die Goldmedaille gewinnen konnte. Bei den US Open schied Nadal im Achtelfinale gegen Lucas Pouille aus. Kurz darauf beendete er die Saison 2016 aufgrund von gesundheitlichen Problemen vorzeitig und verzichtete damit, obwohl qualifiziert, auf eine Teilnahme an den ATP World Tour Finals.
2017 gelang Nadal bei den Australian Open der erste Grand-Slam-Finaleinzug seit Juni 2014. Im Finale traf er auf Roger Federer, der sich für die Niederlage im Australian-Open-Finale von 2009 revanchierte und die Partie in fünf Sätzen mit 6:4, 3:6, 6:1, 3:6 und 6:3 gewann. Durch den Finaleinzug verbesserte sich Nadal in der Weltrangliste auf Platz 6. Nach der Finalniederlage im März in Acapulco gegen Sam Querrey scheiterte Nadal weitere zweimal an Federer in Indian Wells im Achtelfinale sowie in der Finalpartie des Miami Masters. In der Sandplatzsaison sicherte sich Nadal nacheinander die Titel in Monte-Carlo, Barcelona und Madrid. In Rom unterlag er im Viertelfinale Dominic Thiem, den er zuvor noch in den Endspielen von Barcelona und Madrid besiegt hatte. Bei den French Open zog Nadal ohne Satzverlust, unter anderem gegen Dominic Thiem im Halbfinale, ins Finale ein, das er in drei Sätzen gegen Stan Wawrinka gewann. Er ist damit der erste Spieler, der eines der Grand-Slam-Turniere zehnmal gewinnen konnte. Auch bei den Turnieren in Barcelona und Monte Carlo gewann er jeweils seinen zehnten Titel (spanisch: „La Décima“). In Wimbledon erreichte Nadal das Achtelfinale, wo er sich in fünf Sätzen dem Luxemburger Gilles Müller nach 4:48 Stunden (3:6, 4:6, 6:3, 6:4, 13:15) geschlagen geben musste.
Zum 21. August 2017 kehrte Nadal erstmals nach rund drei Jahren Unterbrechung an die Spitze der Weltrangliste zurück. Hierbei profitierte er von den verletzungsbedingten Absagen Andy Murrays, dem bisherigen Weltranglistenführenden, und Roger Federers in Cincinnati. In der Vorbereitung auf die US Open gelangen Nadal keine größeren Erfolge. In New York selbst erreichte er dann aber das Endspiel, in dem er Kevin Anderson klar mit 6:3, 6:3 und 6:4 besiegte und damit seinen dritten Erfolg beim letzten Grand-Slam-Turnier der Saison feierte. Im weiteren Saisonverlauf gelang es Nadal, seine starke Hard-Court-Form zu konservieren und an der Seite seines langjährigen Dauerrivalen Roger Federer den erstmals in Prag ausgetragenen Laver Cup zu gewinnen. Darüber hinaus konnte Nadal seinen persönlichen Herbst-Fluch beenden, indem er das ATP-World-Tour-500-Turnier in Peking im Finale gegen Nick Kyrgios für sich entschied. Auch beim Masters-Turnier in Shanghai gelang ihm der Einzug ins Endspiel. Nach 16 Siegen in Folge musste er sich dort aber deutlich Roger Federer geschlagen geben. Aufgrund wiederkehrender Probleme im rechten Knie konnte Nadal im darauf folgenden Masters-Turnier von Paris seine Viertelfinalpartie nicht antreten. Auch seine Teilnahme an den ATP Finals stand lange Zeit in der Schwebe. Nach der Auftaktniederlage gegen den späteren Finalisten David Goffin erklärte Nadal seine Saison vorzeitig für beendet. Zum Jahresende behielt er seine Spitzenposition in der Weltrangliste.
Aufgrund anhaltender körperlicher Probleme verzichtete Nadal im Vorfeld des ersten Major des Jahres auf jegliche Vorbereitungsturniere. Seiner körperlichen Verfassung war es schließlich zuzuschreiben, dass er bei den Australian Open im Verlauf seines Viertelfinalmatches gegen den späteren Finalisten Marin Čilić beim Stand von 6:3, 3:6, 7:65, 2:6 und 0:2 aufgeben musste. Am 19. Februar 2018 verlor er zudem seine Spitzenposition in der Weltrangliste an seinen langjährigen Kontrahenten Roger Federer. Weiterhin verzichtete Nadal auf die ersten beiden Masters des Jahres, um sein Comeback erst zum Davis-Cup-Viertelfinale zwischen Spanien und Deutschland zu geben. Mit zwei deutlichen Dreisatzsiegen im Einzel war er dabei maßgeblich am Weiterkommen der Spanier beteiligt. Zu Beginn der europäischen Sandplatzsaison gelang ihm zudem der jeweils elfte Sieg beim Masters von Monte-Carlo und dem Turnier in Barcelona. Saisonübergreifend entschied er auf Sand damit 19 Partien bzw. 46 Sätze in Folge für sich, Letzteres stellt einen historischen Rekord dar. Bereits zum 2. April 2018 hatte Nadal Platz eins in der Weltrangliste, „kampflos“, da Federer die Sandplatzsaison ausließ, zurückerobern können. Beim Masters von Madrid erreichte Nadal das Viertelfinale unterlag dort allerdings Dominic Thiem in zwei Sätzen mit 5:7 und 3:6. Damit endete eine historische Serie von 50 Gewinnsätzen in Folge auf ein und demselben Belag. Die bisherige Rekordmarke datierte aus dem Jahr 1984 von John McEnroe, der 49 Sätze in Folge auf Teppich für sich entscheiden konnte. Durch diese Niederlage verlor Nadal abermals seine Spitzenposition in der Weltrangliste an Roger Federer, die er sich aber bereits eine Woche zum 21. Mai 2018 mit einem 6:1, 1:6 und 6:3 gegen Alexander Zverev im Finale des Masters von Rom zurückholte. Es war der 32. Masters-Titel Nadals und der achte in Rom. Am 10. Juni 2018 gewann Rafael Nadal das Finale der French Open mit 6:4, 6:3 und 6:2 gegen Dominic Thiem. Damit holte sich Nadal seinen 11. Titelgewinn bei den French Open, seinen 17. Grand-Slam-Titel insgesamt und war gleichzeitig der erste Spieler, der in zwölf verschiedenen Saisons ein Grand-Slam-Turnier gewinnen konnte. In Wimbledon zog Nadal zum ersten Mal seit 2011 wieder ins Halbfinale ein, unterlag dort jedoch seinem langjährigen Rivalen Novak Đoković im 52. Duell in fünf Sätzen mit 4:6, 6:3, 6:7, 6:3, 8:10. Damit verlor er erstmals seit den US Open 2009 wieder ein Halbfinale, nachdem Nadal zuvor 16 in Folge gewann. Den Rogers Cup in Toronto gewann Nadal zum vierten Mal, nachdem er sich im Finale gegen Stefanos Tsitsipas durchsetzte. Dieser Sieg war sein 33. Masters-Triumph und der 80. Titel der Karriere. Bei den US Open 2018 schaffte Nadal den Sprung ins Halbfinale. Nach einem 0:2-Satzrückstand gegen Juan Martín del Potro gab er verletzungsbedingt auf.
Bei den Australian Open erreichte er zum fünften Mal das Finale, verlor es jedoch gegen Novak Đoković beim 3:6, 2:6, 3:6 bereits zum vierten Mal in Serie. Zum ersten Mal in seiner Karriere verlor Nadal ein Grand-Slam-Finale in drei Sätzen.
Im März 2016 warf die ehemalige französische Sportministerin Roselyn Bachelot Nadal Doping vor. Nadal, der die Vorwürfe bestritt, reichte gegen Bachelot in Frankreich Klage ein. Im November 2017 wurde Bachelot wegen Diffamierung schuldig gesprochen, Nadal erhielt 10.000 Euro Schadensersatz.
Nadals bevorzugter Belag ist der Sandplatz, auf dem er die meisten Erfolge errungen hat. Aus diesem Grund wird er auch oft als Sandkönig (engl. king of clay) bezeichnet. Nadal gilt wie Roger Federer als besonders nervenstark und besitzt ein außergewöhnlich ausgeprägtes Antizipationsvermögen. Damit zwingt er seine Gegner zu einem offensiven und risikoreichen Spiel.
Seine Spielweise ist durch ein kraftvolles und aggressives Grundlinientennis geprägt. Seine Vor- und Rückhandschläge spielt er dabei mit einem äußerst starken Topspin, den er unter anderem mit Hilfe eines extremen Vorhandgriffs (Westerngriff) und einer starken Beschleunigung des Schlägerkopfes erreicht. Untersuchungen haben gezeigt, dass der Ball bei Nadals Vorhandschlägen mit 3000 bis 4000 Umdrehungen pro Minute rotiert und Spieler wie Sampras, Agassi und Federer nur etwa 2000 Umdrehungen pro Minute erreichen. Bemerkenswert ist auch seine außergewöhnliche Athletik, insbesondere seine enorme Schnelligkeit und Beweglichkeit, die es ihm erlaubt, auch schwierige Bälle auf der Rückhandseite zu umlaufen und seinen variableren Vorhandschlag einzusetzen. Seine beidhändige Rückhand gilt als eine der härtesten im Profitennis, wobei Nadal zugutekommt, dass seine Spielhand, obwohl er Rechtshänder ist, die linke ist. Verantwortlich dafür ist sein Onkel und langjähriger Trainer Toni Nadal, der früh der Ansicht war, dass seine beidhändige Rückhand davon profitieren und ihm die Möglichkeit eröffnen würde, beidseitig einen extrem starken Schlag zu entwickeln.Sein Aufschlag zählte zu Beginn zu seinen Schwächen, was vermutlich auch der genannten Tatsache der linken Spielhand als Rechtshänder geschuldet war. Über die Jahre hat er intensiv an der Verbesserung seiner Technik gearbeitet, wodurch sich sein Aufschlag von einem reinen Mittel zur Eröffnung des Ballwechsels hin zu einem der besten Aufschläge mit hohem Prozentsatz an Punktgewinnen mit dem ersten Aufschlag gewandelt hat. Nadal setzt dabei nicht nur auf Geschwindigkeit, sondern auf das geschickte Platzieren des Balles. Seine Strategie besteht darin, möglichst extreme Winkel zu spielen und seine Gegner so unter Druck zu setzen.Schon oft wurde Nadal für seine ritualisierte Bewegungsabfolge kritisiert, die speziell vor dem Aufschlag relativ viel Zeit in Anspruch nimmt. Manche Gegner fühlen sich dadurch in ihrem Spielrhythmus gestört. So kritisierte ihn sein ewiger Rivale Federer öffentlich für Überschreitungen der zugelassenen Zeit zwischen den Punkten (20 s bei Grand-Slam-Turnieren bzw. 25 s auf der ATP Tour). Seine Kritik richtete sich dabei auch gegen die Schiedsrichter, die das Nichteinhalten nicht ahnden würden. Eines von Nadals ungewöhnlichen Ritualen auf dem Tennisplatz ist die Verwendung von zwei Wasserflaschen, aus welchen er in jeder Pause trinkt und die er stets mit dem Etikett in die gleiche Richtung wieder ausrichtet. Er selbst beschreibt seine vielen Rituale vor und während eines Matches als förderlich für seine Konzentration und für die Fokussierung auf das Spiel bzw. die einzelnen Spielzüge.
Rafael Nadal ist einer von acht Spielern, die alle vier Grand-Slam-Turniere in ihrer Karriere gewinnen konnten. Dies gelang ihm 2010 mit 24 Jahren als jüngstem Spieler in der Open Era.
Nur Nadal und Andre Agassi gelang es in ihrer Karriere den Karriere-Golden-Slam zu gewinnen, d. h. alle vier Grand-Slam-Turniere und die Olympischen Spiele im Einzel zu gewinnen.
Zudem ist er der Einzige, der alle Grand-Slam-Turniere gewann und die Olympische Goldmedaille sowohl im Einzel als auch im Doppel holte.
Er stand elfmal im Finale der French Open, nur Roger Federer erreichte das in Wimbledon ebenfalls. Wobei Nadal der Einzige ist, der alle elf Finals eines Turniers gewinnen konnte.
Von 2005 bis 2014 gewann er zehn Jahre in Folge mindestens ein Grand-Slam-Turnier und brach damit den Rekord von Björn Borg, Pete Sampras und Federer (8 Jahre in Serie).
2010 gewann er die French Open, Wimbledon und die US Open und holte somit als erster Spieler Titel auf drei unterschiedlichen Belägen in einer Saison.
Von den French Open 2010 bis zu den French Open 2018 gewann Nadal 16 Grand-Slam-Halbfinals in Serie und überbot damit die Bestmarke von Björn Borg (14).
Nadal gewann in zwölf Saisons Grand-Slam-Turniere (2005–2014, 2017, 2018), damit liegt er knapp vor Roger Federer (11).
Neben Pete Sampras und Ken Rosewall ist er einer von drei Spielern, die Grand-Slams sowohl in ihrer Jugend, in ihren 20ern- und 30er-Jahren gewinnen konnten.
Nur Nadal bei den French Open (2008, 2010 und 2017) und Borg gewannen in der Open Era drei Turniere ohne Satzverlust.
Neben Rod Laver, Borg und Federer ist er einer von vier Spielern in der Open Era, die die French Open und Wimbledon hintereinander gewinnen konnten, das gelang ihm 2008 und 2010.
Mit elf Titeln beim Turnier von Monte-Carlo hält er den Rekord für die meisten Siege bei einem Turnier. Seine acht Erfolge hintereinander dort (2005–2012) sind ebenfalls Rekord.
Zudem ist er der einzige Spieler der zwei Turniere mindestens achtmal gewinnen konnte (Monte Carlo & Rom) und bei drei Sandplatzturniern mindestens fünfmal triumphierte.
In Monte-Carlo erreichte Nadal zwölf Finals und erreichte neun Finals bei drei unterschiedlichen Turnieren (Monte-Carlo, Rom & Madrid).
2010 gelang es Nadal als erstem Spieler alle drei Sandplatzturniere in einer Saison zu gewinnen (Monte Carlo, Rom & Madrid).
Neben Ivan Lendl, Federer und Đoković ist er einer von vier Spielern, die Finals von neun unterschiedlichen Turnieren erreichten.
2013 gewann er vier Turniere hintereinander (Rom, Madrid, Montréal & Cincinnati), Novak Đoković gelang das später ebenfalls, allerdings nicht in einer Saison.
Nadal hält die Open-Era-Rekorde für die meisten gewonnen Titel auf Sand (57) und auf Outdoor-Plätzen (78).
Von 2005 bis 2007 gewann er 81 Matches in Folge auf Sand, bis Federer die Serie im Finale von Hamburg beendete.
Bei den French Open, in Monte-Carlo und Barcelona holte er jeweils elf Titel, wobei er der einzige Spieler in der Open Era ist, der ein Turnier mindestens zehnmal gewann.
Er hält den Rekord für die meisten gewonnen Titel sowohl bei einem Grand-Slam-Turnier, bei einem Masters- als auch bei einem ATP-World-Tour-500-Turnier.
Von 2004 bis 2018 gewann er in 15 Saisons in Folge mindestens ein Turnier und stellte damit den Rekord von Roger Federer ein.
2014: Ehrendoktor der Universität der BalearenIm Juni 2008 wurde ein Asteroid nach Nadal benannt. Der 2003 vom mallorquinischen Planetarium entdeckte Asteroid trägt seitdem den Namen 128036 Rafaelnadal.
2008 und 2011 wurde er jeweils für zwei Jahre ins ATP Player Council gewählt, wo er neben dem Präsidenten Roger Federer die Funktion des Vizepräsidenten innehatte. Zu Beginn des Jahres 2011 gehörte er zu den Hauptakteuren einer Gruppe von Spielern, die sich für eine geringere Anzahl Turniere und höhere Preisgelder einsetzten. Er sorgte zudem für Aufsehen, als er dabei das Verhalten Federers als Vorsitzender der Spielergewerkschaft (ATP Player Council) kritisierte und ihm vorwarf, nicht hinter den anderen Spielern zu stehen. Im März 2012 trat er dann – vermutlich aufgrund von Differenzen mit Federer – von diesem Posten zurück.Im Frühjahr 2010 hatte Rafael Nadal einen Gastauftritt in dem Musikvideo Gypsy als Liebespartner der kolumbianischen Sängerin Shakira.Anfang 2013 gründete er zusammen mit seinem langjährigen Manager Carlos Costa seine eigene Sportvermarktungsfirma, mit der er Athleten aus verschiedenen Sportarten wie Tennis, Golf und Fußball repräsentieren und zudem Veranstaltungen organisieren will.
Rafael Nadal und John Carlin: Rafa – Mein Weg an die Spitze. Edel Verlag, Hamburg 2012, ISBN 978-3-8419-0123-1.

Die Raffinerieexplosion in Texas City ereignete sich am 23. März 2005 um 13:20 Uhr in der größten Erdölraffinerie des britischen Ölkonzerns British Petroleum (BP) in Texas City, einer Stadt im Galveston County des US-Bundesstaates Texas. Bei der Explosion, einem der größten Raffinerieunfälle in den Vereinigten Staaten, kamen 15 Arbeiter ums Leben, und über 180 Personen wurden verletzt. Die Wucht der Explosion ließ im Umkreis von mehreren Kilometern Fensterscheiben bersten, der Feuerball und die Rauchwolken waren bis zum 48 Kilometer entfernten Houston sichtbar.
Nachdem ein interner Untersuchungsbericht zunächst menschliches Versagen als Ursache der Katastrophe angesehen hatte, bat BP auf Anraten der US-Untersuchungsbehörde U.S. Chemical Safety and Hazard Investigation Board (CSB) ein unabhängiges elfköpfiges Gremium unter Leitung des ehemaligen Außenministers der Vereinigten Staaten James Baker um eine Überprüfung des konzernweiten Sicherheitsmanagements. Die Baker-Kommission legte die Untersuchungsergebnisse am 16. Juli 2007 in einem 374 Seiten starken Bericht, dem sogenannten Baker-Bericht (Baker Panel Report), dar und deckte verheerende Sicherheitsmängel bei BP auf.
Die Veröffentlichung des Baker-Berichts hatte weitreichende Folgen sowohl innerhalb des Konzerns als auch für die Sicherheitsprogramme in Raffinerien, der petrochemischen und chemischen Industrie sowie bei den aufsichtführenden Behörden. Beunruhigt durch die im Bericht aufgezeigten signifikanten Mängel, initiierte die Occupational Safety and Health Administration (OSHA), eine Bundesbehörde in den Vereinigten Staaten zur Durchsetzung der Arbeitssicherheitsgesetze, ein nationales Schwerpunktprogramm für das Prozesssicherheitsmanagement in Raffinerien.
John Browne, der damalige CEO von BP, gab kurz nach der Veröffentlichung des Baker-Berichts sein vorzeitiges Ausscheiden aus dem Amt bekannt, nicht ohne auf die Verpflichtung des Konzerns zur Umsetzung der Vorschläge des Baker-Gremiums hinzuweisen. Die Kosten der Raffinerieexplosion in Texas City betrugen etwa zwei Milliarden Dollar für Schadensersatz, Strafzahlungen sowie für den Wiederaufbau und die Betriebsunterbrechung.
Die BP-Texas-City-Raffinerie war zum Zeitpunkt des Unfalls die drittgrößte der damals 149 Raffinerien in den Vereinigten Staaten. Sie wurde im Jahr 1934 als Pan American Refinery gebaut und am 1. Januar 1999 von BP als Teil der Fusion mit Amoco übernommen. Die Verarbeitungskapazität betrug im Januar 2005 etwa 27 Milliarden Liter Rohöl pro Jahr, der Ausstoß an Motorenbenzin betrug etwa 11 Milliarden Liter pro Jahr. Die Raffinerie bezog die Rohstoffe zu 30 % über Pipelines und zu 70 % per Schiff, die Produkte wurden zu 75 % per Pipeline und zu 25 % per Schiff abtransportiert. Die Raffinerie produzierte im Jahr 2005 etwa 3 Prozent des US-amerikanischen Benzinbedarfs. Am Standort, der sich über 5 Quadratkilometer erstreckt, arbeiteten im Jahr 2005 etwa 1800 BP-Mitarbeiter sowie 800 Fremdfirmenmitarbeiter, die in 29 Raffinerie- und vier petrochemischen Prozessen Produkte wie Benzin, schwefelarmen Dieselkraftstoff, Kerosin, Chemierohstoffe und Schweröle herstellten.
In der Raffinerie kam es mehrfach zu Betriebs- und Arbeitsunfällen, die zwischen 1974 und 2003 zu 20 Todesfällen und mehreren Betriebsunterbrechungen führten. Bereits im Jahr 2002 gab BP eine Studie bei der Unternehmensberatung A. T. Kearney in Auftrag, um die Fakten zu verstehen, die zu den Unfällen und der Verschlechterung der Arbeitsleistung der Raffinerie geführt hatten. Der A. T. Kearney-Bericht stellte als eine der Ursachen fest, dass ein Top-down-Prozess die Mittelzuweisung für Instandhaltungsaufgaben steuerte. Durch diesen Prozess wurden Budgetkürzungen nicht unter Berücksichtigung der spezifischen Bedürfnisse der Raffinerie vorgenommen.Allein im Jahr 2004 starben drei weitere Mitarbeiter bei Arbeitsunfällen. Nach einer großen Explosion im März 2004, bei der niemand verletzt wurde, evakuierte BP die Raffinerie vorübergehend. Die Polizei sperrte die Zufahrtsstraßen und forderte die Anwohner auf, ihre Häuser nicht zu verlassen.Ende des Jahres 2004 beauftragte BP die Beratungsfirma Telos mit einer Umfrage zur Sicherheitskultur in der Texas City Raffinerie. Zwischen dem 8. und dem 30. November 2004 befragten die Berater mehr als 1000 Beschäftigte. Die Berater legten das Ergebnis im Telos-Bericht am 19. Januar 2005 vor. Im Bericht äußerten Mitarbeiter große Bedenken über die Sicherheitsstandards in der Raffinerie und bemängelten unrealistische Vorgaben, mangelnde Ausbildung, hohen Kostendruck, Ausfall von Sicherheitseinrichtungen und die Fälschung von Sicherheitsprotokollen. Die Untersuchung ergab weiter, dass Ausgaben gekürzt wurden, ohne dass die Auswirkungen auf die Anlagensicherheit geprüft worden waren.
Die Explosion ereignete sich in der Mitte der 1980er Jahre errichteten Isomerisierungsanlage der Raffinerie. Bei der Isomerisierung werden geradkettige Paraffine in verzweigte Iso-Paraffine umgewandelt. Damit wird die Oktanzahl von Motorenbenzin um 30 bis 40 Einheiten erhöht. Als Einsatzstoffe dienten hauptsächlich n-Pentan und n-Hexan. Die Produktionskapazität dieser Anlage betrug 4.300 Kubikmeter Leichtbenzin pro Tag.Die Isomerisierungsanlage bestand aus einer Entschwefelungsanlage, einer Rektifikationskolonne, dem sogenannten „Raffinate Splitter Tower“, welche die Rohstoffe für die Isomerisierung zur Verfügung stellte, dem Isomerisierungsreaktor und einer Gasrückführung.
Die Entschwefelung erfolgte nach dem Ultrafining-Verfahren, einem von Standard Oil of Indiana entwickelten und lizenzierten Verfahren zur Entschwefelung und Hydrierung von Naphtha mittels eines Festbettkatalysators. Die Isomerisierungsreaktion wurde in einem Festbettreaktor nach dem Penex-Prozess durchgeführt, einem weit verbreiteten, von Universal Oil Products (UOP) entwickelten und lizenzierten Prozess. Als Isomerisierungskatalysatoren verwendet das Verfahren saure, chloridaktivierte Platin-γ-Aluminiumoxid-Katalysatoren. Weiterhin verwendet das Verfahren eine Gasrückführung.
Am Morgen des 23. März 2005 nahmen die Schichtoperatoren gegen 2:00 Uhr die Rektifikationskolonne, den sogenannten Raffinat-Trennturm, nach einem vierwöchigen Wartungsstillstand wieder in Betrieb. Zunächst füllten die Schichtmitarbeiter die Kolonne mit flüssigen Kohlenwasserstoffen, ohne den Aufheizer in Betrieb zu nehmen oder Flüssigkeit aus der Kolonne abzuziehen. Der normale Flüssigkeitsstand in der Rektifikationskolonne betrug 2 Meter.
Gegen 3:00 Uhr quittierte ein Mitarbeiter den ersten Füllstandshochalarm, der bei einem Füllstand von 72 % ansprach, entsprechend einer Füllstandshöhe von 3 Metern. Ein weiterer Hochalarm bei 78 %, entsprechend einer Füllstandshöhe von etwa 3,25 Meter, sprach nicht an, da das Instrument defekt war.Als die Operatoren den Anfahrbetrieb gegen 5:00 Uhr unterbrachen, betrug der Füllstand in der Kolonne etwa 4 Meter. Nachdem der maximal erlaubte Füllstand überschritten war, konnte der Füllstand nicht mehr abgelesen werden, da das Instrument ihn nicht anzeigte. Durch einen Defekt zeigte das zweite Füllstandsinstrument sogar an, dass der Füllstand in der Kolonne falle.
Um 9:50 Uhr nahmen die Operatoren den Anfahrbetrieb wieder auf und förderten mehr Rohmaterial in den Raffinat-Trennturm. Um 10 Uhr nahmen sie den Aufheizer in Betrieb. Gegen 12:40 Uhr betrug der Füllstand bereits 42 Meter, obwohl die Füllstandsanzeige 3 Meter mit fallender Tendenz anzeigte. Der hohe Füllstand löste einen Hochdruckalarm aus. Als Gegenmaßnahme nahmen die Operatoren zwei Brenner im Aufheizer außer Betrieb, um die Temperatur und damit den Druck zu senken. Außerdem öffneten sie ein manuelles Ventil zum Ausblasebehälter, der zur Atmosphäre hin offen war.
Um 13:00 Uhr öffnete die Schicht den Weg zu den Lagertanks. Über den Wärmetauscher erhitzte sich das Rohmaterial um etwa 65 °C, wodurch es zu einer Überhitzung in der Rektifikationskolonne kam. Um 13:05 Uhr begannen die Kohlenwasserstoffe in der Kolonne zu sieden, um 13:10 Uhr wurde die Kolonne überfüllt und flutete eine Rohrleitung, die zu 47 Meter tiefer gelegenen Sicherheitsventilen führte. Um 13:14 Uhr öffneten die Sicherheitsventile, der Druck stieg jedoch durch das beginnende Sieden der Kohlenwasserstoffe in der Kolonne weiter von 1,5 bar auf etwa 4,5 bar an. Die Kohlenwasserstoffe liefen in den Ausblasebehälter, von da aus teilweise in das Abwasserkanalsystem. Gegen 13:09 Uhr war der gesamte Ausblasebehälter gefüllt, ein Füllstandsalarm in diesem Behälter funktionierte ebenfalls nicht.
Es kam zu einer Ausflutung des Behälters aus dem 34 Meter hohen Schornstein, etwa eine Minute lang. Etwa 28.000 Liter leichtflüchtige Kohlenwasserstoffe wurden geysirähnlich aus dem Kamin ausgeworfen. Die freigesetzte flüchtige Flüssigkeit verdampfte und bildete eine brennbare Dampfwolke. Die BP-Mitarbeiter bemerkten die Kohlenwasserstoffwolke und riefen über den Werkfunk dazu auf, alle Heißarbeiten einzustellen sowie alle potentiellen Zündquellen auszuschalten. Die Kohlenwasserstoffwolke entzündete sich jedoch um 13:20 Uhr an einem etwa 8 Meter entfernten im Leerlauf wartenden Pickup-Truck.
Die Wucht der Explosion tötete 15 Fremdfirmenmitarbeiter, die in Baucontainern in einer Entfernung von etwa 45 Metern von der Explosionsstelle eine Besprechung abhielten und dort von der Stoßwelle der Explosion getroffen wurden. Es handelte sich um elf Angestellte der Firma Jakobs Merit, die an einem Wartungsstillstand der Ultracracker-Anlage arbeiteten, sowie vier Mitarbeiter der Firma Fluor Corporation.Der leitende Rechtsmediziner des Galveston County, Stephen Pustilnik, gab als Todesursache im medizinischen Gutachten Schädel-, Hirn- sowie andere Traumen an, verursacht durch stumpfe Gewalteinwirkung umherfliegender Trümmer. Das Feuer war nach etwa einer Stunde unter Kontrolle und nach zwei Stunden vollkommen gelöscht.
Sowohl BP-interne Experten als auch verschiedene Behörden und Kommissionen untersuchten die Explosion in technischer Hinsicht und unter Aspekten der Organisation und der Sicherheitskultur. Die Ergebnisse der technischen Untersuchung legte ein konzerninternes Expertenteam im sogenannten „Mogford-Bericht“ nieder, die Untersuchungsergebnisse hinsichtlich der organisatorischen Aspekte und der Verantwortung des Managements im sogenannten „Bonse-Bericht“. Das U.S. Chemical Safety and Hazard Investigation Board untersuchte sowohl die technischen Aspekte als auch die Verantwortung der aufsichtführenden Behörden. Die Occupational Safety and Health Administration (OSHA) überprüfte im Nachgang die Einhaltung der verschiedenen Auflagen in der Raffinerie.
Ein Expertenteam unter Leitung von John Mogford, dem Senior Group Vice President Safety and Operations, untersuchte die technischen Aspekte der Explosion und schlug Korrekturmaßnahmen vor. Am 9. Dezember 2005 veröffentlichte BP diesen Unfalluntersuchungsbericht. Als Hauptursachen identifizierte der Bericht vier kritische Faktoren, ohne die der Vorfall nicht passiert wäre oder erheblich geringere Auswirkung gehabt hätte. Dazu gehörten die unbeabsichtigte Stofffreisetzung, die Betriebsanweisungen sowie deren Befolgung bei der Inbetriebnahme der Rektifikationskolonne, die Arbeitssteuerung sowie der Aufbau der Baucontainer und das Design des Ausblasebehälters.
Ein weiterer interner Bericht, der sogenannte Bonse-Bericht, der unter Leitung des Aufsichtsratsvorsitzenden von BP Deutschland, Wilhelm Bonse-Geuking, erstellt wurde, machte zahlreiche Managementfehler für die Raffinerieexplosion verantwortlich.
Am 3. Mai 2007 ordnete ein Gericht die Veröffentlichung des eigentlich nur für interne Zwecke erstellten Berichts an. Der Report maß die Verantwortlichkeiten des Managements an internen Managementanweisungen (BP Management Framework, BPMF) und dem BP Verhaltenskodex. Die Raffinerieorganisation definierte diese Verantwortlichkeiten in Anlehnung an die konzernweiten Vorschriften in einem sogenannten Blue Book, das im Jahr 2005 veröffentlicht wurde. Neben persönlichen Verfehlungen stellte der Report unklare Verantwortlichkeiten im Konzern auf allen Ebenen von der Anlagenmannschaft über die Texas City Raffinerieorganisation bis zum gesamten Raffineriesegment des Konzerns als zum Vorfall beitragende Faktoren fest. Als weiteren beitragenden Faktor prangerte der Report den schlechten Zustand der Anlagen sowie die zu geringen Ausgaben für Instandhaltung an. Obwohl auf allen Ebenen des Konzerns deren Unaufschiebbarkeit bekannt war, waren für sie zu geringe Ressourcen bereitgestellt worden.
Als Hauptfaktor erkannte der Report die Sicherheitskultur in der Raffinerie, eine hohe Risikobereitschaft sowie das Fehlen oder Nichtbefolgen klarer Betriebsanweisungen. Der Störfallbeauftragte übernahm sein Amt im Januar 2005, galt aber für seine Aufgabe als nicht genügend ausgebildet. Außerdem zogen die Verantwortlichen keine Lehren aus den Fehlern der Vergangenheit. So kam es allein im Jahr 2000 zu 80 ungewollten Freisetzungen von Kohlenwasserstoffen; das Management hatte entsprechende Verbesserungsmaßnahmen nicht konsequent umgesetzt.
Es fehlten klare Strukturen, und das hatte zur Folge, dass die Operatoren die Anlage ohne die notwendige Kommunikation mit anderen Abteilungen und Anlagen in Betrieb nahmen. Die zuständigen Schichtführer kamen zu spät zum Dienst oder verließen ihn zu früh, nicht genügend ausgebildete Operatoren übernahmen den Start der Anlage. Alarme, die die Überfüllung der Rektifikationskolonne signalisierten, wurden falsch interpretiert. Der Report legte dem Raffinerieleiter zur Last, dass der Schwerpunkt seiner Aktionen auf der Arbeitssicherheit und nicht auf der Anlagensicherheit lag.
Aufgrund der Bedeutung der Katastrophe untersuchte das U.S. Chemical Safety and Hazard Investigation Board sowohl das Sicherheitsmanagement der Raffinerie in Texas City als auch die Rolle der BP-Gruppe und der Occupational Safety and Health Administration (OSHA) als Aufsichtsbehörde. Die Untersuchungsergebnisse legte die Behörde in einem über dreihundertseitigen Bericht am 20. März 2007 vor.Das CSB stellte fest, dass organisatorische und sicherheitstechnische Mängel auf allen organisatorischen Ebenen der BP die Raffinerieexplosion mitverursachten, etwa durch Kostensenkungen und Ausgabenkürzungen im sicherheitstechnischen Bereich, obwohl sich ein großer Teil der Raffinerie-Infrastruktur und der Prozess-Anlagen in einem schlechten Zustand befand. Außerdem hatten die Verantwortlichen das Budget für die Schulungen gekürzt und Personal abgebaut.Weiterhin stellte das CSB fest, dass die OSHA als aufsichtsführende Behörde es versäumt hatte, geplante Inspektionen der Raffinerie durchzuführen und Sicherheitsvorschriften durchzusetzen, obwohl es viele Warnzeichen gab. Nach der Explosion entdeckte die OSHA 301 Verstöße gegen Auflagen und verhängte eine Geldstrafe von 21 Millionen USD. Das CSB stellte fest, dass nur eine begrenzte Anzahl von OSHA-Inspektoren die fachliche Ausbildung und notwendige Erfahrung für die komplexen Untersuchungen in Raffinerien aufwies.
Das CSB gab eine Empfehlung zur Entwicklung einer Richtlinie für das Verständnis, das Erkennen und den Umgang mit Erschöpfungszuständen während der Schichtarbeit. Die daraufhin erarbeitete Richtlinie API Recommended Practice 755 empfahl für Raffinerien, petrochemische und chemische Anlagen und ähnliche Einrichtungen die Erstellung von Leitlinien zum Umgang mit Erschöpfungssyndromen (Fatigue Risk Management-System, FRMS). Diese Leitlinien beinhalten Empfehlungen für Arbeiten auf Wechselschicht, etwa für die Anzahl der Überstunden und die Anzahl der Tage, an denen ohne Unterbrechung gearbeitet werden soll.
Der CSB-Bericht stellte außerdem fest, dass BP und Amoco es versäumt hatten, Sicherheitsempfehlungen, die zum Teil lange vor der Explosion gegeben worden sind, zu beachten und umzusetzen. So hatte im Jahr 1991 die Planungsabteilung der Amoco den Ersatz der atmosphärischen Ausblasebehälter durch Fackelsysteme vorgeschlagen. Verschiedene Abteilungen erneuerten diese Vorschläge, zuletzt im Jahr 2002. Das nötige Budget wurde jedoch nicht bereitgestellt. Zwischen 1994 und 2004 kam es zu acht Fällen, bei denen Kohlenwasserstoffdämpfe über einen Ausblasebehälter an die Atmosphäre abgegeben wurden. In einem hausinternen Prozesssicherheitsstandard verbot Amoco den Bau von neuen Ausblasebehältern in Raffinerien. Dennoch ersetzte Amoco im Jahr 1997 den betroffenen Ausblasebehälter, der aus den 1950er Jahren stammte, durch einen baugleichen Behälter. Als Folge des Unfalls sagte BP zu, alle Ausblasebehälter in der Umgebung von brennbaren Kohlenwasserstoffen zu eliminieren. Wegen der in kurzer Zeit freigesetzten Menge an brennbaren Kohlenwasserstoffen wird jedoch zum Teil bezweifelt, dass ein Fackelsystem die Explosion verhindert hätte.Auf Grund eines Unglücks im Jahr 1995 in einer Pennzoil-Raffinerie, bei dem eine Explosion zweier Lagertanks fünf Arbeiter tötete, die in Baucontainern untergebracht waren, gab es eine Empfehlung für die Standortwahl von Baucontainern. BP ignorierte die Empfehlung in der Annahme, dass die Gefahr gering sei, weil die Container die meiste Zeit leer standen.
Die Explosion führte zu einer Reihe von Klagen und Prozessen im privat- sowie umweltrechtlichen Bereich. Zunächst reichten zwei bei der Explosion verletzte Arbeiter eine Klage gegen die BP Amoco Chemical Company of Delaware, JE Merit, einer Tochtergesellschaft von Johnson Engineering und gegen den Raffineriedirektor ein. Betroffene reichten insgesamt über 3000 Klagen ein, in 1350 Fällen kam es bereits nach kurzer Zeit zum Vergleich. BP stellte 1,6 Milliarden US-Dollar für Kompensationszahlungen zurück. Weiterhin wurde BP wegen Verstößen gegen Umweltgesetze angeklagt, die OSHA verhängte große Geldstrafen.
Nach der Explosion reichten viele Betroffene zivilrechtliche Klagen gegen BP ein. Nach verschiedenen Angaben zahlte BP über eine Milliarde US-Dollar an die Kläger als Vergleich aus. Aufsehen erregte der Fall Eva Rowe.Die junge Frau verlor bei der Explosion ihre Eltern, die als Subunternehmer für JE Merit arbeiteten. Sie ließ verbreiten, sie werde kein Geld annehmen und den Konzern zur Rechenschaft ziehen. Ed Bradley, ein bekannter US-amerikanischer Journalist, machte ihre Geschichte im Fernsehmagazin 60 Minutes landesweit bekannt.Am 9. November 2006 schloss BP mit Rowe als letzter Klägerin einen Vergleich, nachdem die Anwälte von Rowe versucht hatten, John Browne als Zeugen zu laden. Die Höhe der Abfindung für Eva Rowe blieb unbekannt. BP zahlte im Zuge des Vergleichs 32 Millionen USD an von Rowe genannte Hochschulen und Krankenhäuser, darunter das Mary Kay O’Connor Process Safety Center an der Texas A&M University, die medizinische Fakultät der University of Texas in Galveston, die Truman G. Blocker Adult Burn Unit und das College of the Mainland in Texas City. Weiterhin veröffentlichte BP etwa sieben Millionen Seiten interner Dokumente, darunter den Telos- und den Bonse-Report.
Auf Veranlassung der Texas Kommission für Umweltqualität (Texas Commission on Environmental Quality; TCEQ) eröffnete die Generalstaatsanwaltschaft von Texas (Texas Attorney General) ein Verfahren gegen BP Products North America Inc. wegen Verstößen gegen die Gesetze zur Luftreinhaltung (Texas Clean Air Act), die Arbeitsschutzgesetze (Texas Health & Safety Code) und die Wasserschutzgesetze (Texas Water Code) im Zusammenhang mit der Explosion.In einer Vereinbarung stimmte BP der Zahlung einer Geldstrafe von 50 Millionen USD zu. Darin enthalten waren Verfahrenskosten von 500.000 USD. Außerdem vereinbarten das Gericht und BP eine Bewährungsstrafe von drei Jahren, während deren BP sich verpflichtete, die von staatlichen Stellen eingeleiteten Untersuchungen zum Unfallhergang zu unterstützen. Eine weitere Bewährungsauflage war die Einhaltung aller Anordnungen, die von der OSHA und von der TCEQ erlassen worden waren. Im Gegenzug verpflichtete sich das Justizministerium, keine zusätzlichen Strafanzeigen gegen BP in Verbindung mit der Raffinerieexplosion zuzulassen.
Im Oktober 2005 berief John Browne, der damalige CEO der BP, auf Empfehlung des US Chemical Safety and Hazard Investigations Board (CSB) eine elfköpfige Kommission unter der Leitung des früheren US-Außenministers James A. Baker ein. Die Kommission sollte konzernweit das Sicherheitsmanagement untersuchen und Empfehlungen zur Verbesserung der Sicherheit erarbeiten. Das Gremium interviewte dazu über 700 BP-Mitarbeiter auf allen Organisationsebenen bei Ortsterminen in den fünf Raffinerien der BP. Die Mitarbeiter und technischen Berater wurden zur Sicherheitskultur befragt, und Unterlagen zur Prozesssicherheit wurden überprüft. Weiterhin führte das Gremium öffentliche Sitzungen in den Gemeinden der Raffineriestandorte durch und befragte andere Unternehmen in Bezug auf das Prozesssicherheitsmanagement. Die Untersuchungsergebnisse wurden am 16. Juli 2007 in einem umfangreichen Bericht vorgelegt.Die Kommission unterteilte die Erkenntnisse ihrer Untersuchung in die drei Abschnitte Sicherheitskultur des Unternehmens, Prozesssicherheitsmanagement inklusive der Bewertung der Leistungsfähigkeit des Managementsystems und Korrekturmaßnahmen sowie der Aufsichtsführung im Unternehmen. Im Bericht identifizierte die Kommission zum Teil erhebliche Defizite im Sicherheitsmanagement der Raffinerien. Weiterhin wurde festgestellt, dass BP nicht zwischen Arbeitsschutz, Umweltschutz und Anlagensicherheit unterschieden hatte und eine gute Unfallstatistik als Indikator für eine hohe Anlagensicherheit ansah. Das Gremium forderte BP auf, der Prozesssicherheit die gleiche Priorität wie dem Arbeitsschutz einzuräumen.
Die Kommission empfahl als Ergebnis ihrer Untersuchungen insgesamt zehn Maßnahmen zur Verbesserung der Prozesssicherheit. Gleichzeitig übertrug die Baker-Kommission die Untersuchungsergebnisse auf andere Industriebereiche.
Die erste Empfehlung betraf die Verantwortung der obersten Leitung für Prozesssicherheit. Die Geschäftsleitung und der Verwaltungsrat sollten wirksame Führungsinstrumente und -strukturen schaffen und geeignete Ziele für die Prozesssicherheit im Unternehmen definieren. Durch ihr Engagement, durch Anweisungen und Aktionen sollte die oberste Führungsebene sich vor der Belegschaft zur Bedeutung der Prozesssicherheit bekennen. Weiterhin empfahl das Gremium die Einrichtung eines integrierten und umfassenden Prozesssicherheits-Managementsystems, das systematisch und kontinuierlich Risiken identifiziert, reduziert und verwaltet.Drittens sollte BP ein System einführen, das sicherstellt, dass den Mitarbeitern auf allen organisatorischen Ebenen des Konzerns ein angemessenes Wissen über Prozesssicherheit vermittelt wird. Weiterhin wurde BP aufgefordert, eine vertrauensvolle und offene Sicherheitskultur in den einzelnen Raffineriestandorten zu entwickeln und zu pflegen. Eine weitere Empfehlung betraf die Definition klarer Verantwortlichkeiten und Ziele auf allen Organisationsebenen, verbunden mit klaren Berichts- und Rechenschaftspflichten. Die sechste Empfehlung betraf die Unterstützung der direkten Linienvorgesetzten in Raffinerien im Bereich der Prozesssicherheit.Die Kommission empfahl auch die Definition von hinweisenden und zu befolgenden Kennzahlen für Prozesssicherheit. Die achte Empfehlung betraf die Festlegung und Durchführung eines Systems zur Funktionsüberprüfung von Prozesssicherheit. Die Einführung und Umsetzung der Empfehlungen sollten auf der Vorstandsebene geschehen. Die Summe der Empfehlungen sollte BP zu einem führenden Unternehmen im Bereich der Prozesssicherheit machen und nachhaltig zu deren Verbesserung in allen Raffinerien führen.
Nach der Explosion in der Isomerisierungsanlage ereigneten sich in der Raffinerie weitere sicherheitsrelevante Vorfälle. Am 28. Juli 2005 setzte ein Riss in einem Wärmetauscher einer Entschwefelungsanlage Wasserstoffgas frei. Die nachfolgende Knallgasexplosion verletzte eine Person leicht und verursachte einen Sachschaden in Höhe von 30 Millionen USD. Das CSB untersuchte den Vorfall und stellte als Ursache fest, dass bei Reparaturarbeiten ein Subunternehmer Rohrbögen aus niedrig legiertem Stahl verwendet hatte. Diese Rohrleitungsstücke versagten im Betrieb mechanisch durch Wasserstoffversprödung.Bereits zwei Wochen später ereignete sich ein weiterer Vorfall in einer Entschwefelungsanlage, der dazu führte, dass die Nachbarn der Raffinerie dazu aufgefordert werden mussten, ihre Häuser zu verlassen und Schutzräume aufzusuchen. In den folgenden Jahren ereigneten sich weitere Vorfälle, teilweise mit tödlichen Folgen. Am 14. Januar 2008 starb ein Mitarbeiter an Kopfverletzungen bei Wartungsarbeiten an der Ultracrackeranlage.Im Jahr 2009 bewertete die Occupational Safety- and Health Administration in einer sechsmonatigen Prüfung die von BP als Verpflichtung aus der Raffinerieexplosion eingeleiteten Verbesserungsmaßnahmen. Die OSHA stellte 270 Verfehlungen gegen bestehende Auflagen sowie 439 neue Verstöße fest und verhängte eine Ordnungsstrafe in Höhe von 87,43 Millionen USD gegen BP Products North America Inc., die höchste in der Geschichte der Behörde.Im Jahr 2012 verkaufte BP die Texas-City-Raffinerie sowie Teile des Vertriebs- und Logistiknetzwerks an das US-Unternehmen Marathon Petroleum für 2,5 Milliarden USD. Der Erlös soll laut BP für die Entschädigungszahlungen der Deepwater-Horizon-Ölkatastrophe im Golf von Mexiko verwendet werden.
Die Veröffentlichung des Baker-Berichts sowie der internen und externen Untersuchungsberichte veranlasste zahlreiche Unternehmen und staatliche Stellen weltweit, die Stellung der Anlagensicherheit und der Sicherheitskultur zu überdenken. Der Arbeitskreis Texas City der Kommission für Anlagensicherheit beim Bundesministerium für Umwelt, Naturschutz und Reaktorsicherheit gab Empfehlungen für eine Weiterentwicklung der Sicherheitskultur als Lehre aus der Raffinerieexplosion heraus.Der Verband der Europäischen chemischen Industrie CEFIC empfahl seinen Mitgliedern, verstärkte Aufmerksamkeit auf das Gebiet der Prozesssicherheit zu legen. Der englische Verband Chemical Industries Association besuchte seine Mitgliedsfirmen und befragte Vorstands- und Aufsichtsratsmitglieder, ob sie den Baker-Bericht gelesen hätten und die Empfehlungen umsetzen würden.Als Folge der Explosion investierten nicht nur BP, sondern auch viele andere Unternehmen in die Verbesserung der Prozesssicherheit. Unternehmen überprüften die Arbeitsorganisation und änderten beispielsweise die Regeln für die Aufstellung von Baucontainern in gefährdeten Bereichen, unabhängig von deren Nutzungsdauer. Die Explosion gab den wichtigsten Anstoß zu den hohen Investitionen in den Anlagenbau in den Jahren nach der Katastrophe.
Trevor A. Kletz: What Went Wrong: Case Histories of Process Plant Disasters and How They Could Have Been Avoided, 5. Auflage, Butterworth-Heinemann, Oxford 2009, ISBN 1-85617-531-6.
Andrew Hopkins: Failure to Learn the BP Texas City Refinery Disaster, CCH Australia, Sydney 2008, ISBN 1-921322-44-6.
U.S. Chemical Safety and Hazard Investigation Board, Investigation Report, Report No. 2005-04-I-TX, Refinery Explosion and Fire (PDF; 3,4 MB), offizieller Unfalluntersuchungsbericht, bei csb.gov
Fatal Accident Investigation Report: Isomerization Unit Explosion, Final Report, Texas City, Texas, USA (PDF; 1,7 MB), BP interner Unfalluntersuchungsbericht, bei BP.com

Ein RAID-System dient zur Organisation mehrerer physischer Massenspeicher (üblicherweise Festplattenlaufwerke oder Solid-State-Drives) zu einem logischen Laufwerk, das eine höhere Ausfallsicherheit oder einen größeren Datendurchsatz erlaubt als ein einzelnes physisches Speichermedium. Der Begriff ist ein Akronym für englisch „redundant array of independent disks“, also „redundante Anordnung unabhängiger Festplatten“ (ursprünglich englisch „redundant array of inexpensive disks“; deutsch „redundante Anordnung kostengünstiger Festplatten“; was aus Marketinggründen aufgegeben wurde).
Während die meisten in Computern verwendeten Techniken und Anwendungen darauf abzielen, Redundanzen (in Form von mehrfachem Vorkommen derselben Daten) zu vermeiden, werden bei RAID-Systemen redundante Informationen gezielt erzeugt, damit beim Ausfall einzelner Speichermedien das RAID als Ganzes seine Integrität und Funktionalität behält und nach Ersetzen der ausgefallenen Komponente durch einen Rebuild der ursprüngliche Zustand wiederhergestellt werden kann. Diese Redundanz darf keinesfalls mit einer Datensicherung gleichgesetzt werden.
Das Akronym RAID wurde erstmals 1987 durch David A. Patterson, Garth A. Gibson und Randy H. Katz von der University of California, Berkeley, USA als Abkürzung für „Redundant Array of Inexpensive Disks“ definiert. In der zur International Conference on Management of Data (1988, Chicago, Illinois, USA) veröffentlichten Publikation „A case for redundant arrays of inexpensive disks (RAID)“ entwickeln die Autoren Vorschläge, um langsame Plattenzugriffe zu beschleunigen und die Mean Time Between Failures (MTBF) zu erhöhen. Dazu sollten die Daten auf vielen kleineren (billigeren) Platten anstatt auf wenigen großen (teuren) abgelegt werden; deshalb die ursprüngliche Leseweise als „Redundant Arrays of Inexpensive Disks“ – RAID steht damit als Gegensatz zu den damaligen SLEDs (Single Large Expensive Disk). In ihrer Arbeit untersuchen die Autoren die Möglichkeit, kostengünstige kleinere Festplatten (eigentlich Zubehör für Personalcomputer) im Verbund als größeres logisches Laufwerk zu betreiben, um so die Kosten für eine große (zum damaligen Zeitpunkt überproportional teure) SLED-Festplatte (Großrechner-Technologie) einzusparen. Dem gestiegenen Ausfallrisiko im Verbund sollte durch die Speicherung redundanter Daten begegnet werden. Die einzelnen Anordnungen wurden als RAID-Level diskutiert.
Die Varianten RAID 0 und RAID 6 wurden erst später von der Industrie geprägt. Seit 1992 erfolgt eine Standardisierung durch das RAB (RAID Advisory Board), bestehend aus etwa 50 Herstellern. Die weitere Entwicklung des RAID-Konzepts führte zunehmend zum Einsatz in Serveranwendungen, die den erhöhten Datendurchsatz und die Ausfallsicherheit nutzen. Der Aspekt der Kostenersparnis fiel damit weg. Die Möglichkeit, in einem solchen System einzelne Festplatten im laufenden Betrieb zu wechseln, entspricht der heute gebräuchlichen Übersetzung: Redundant Array of Independent Disks (redundante Anordnung unabhängiger Festplatten).
Der Betrieb eines RAID-Systems setzt mindestens zwei Speichermedien voraus. Diese werden gemeinsam betrieben und bilden einen Verbund, der unter mindestens einem Aspekt betrachtet leistungsfähiger ist als die einzelnen Medien. Mit RAID-Systemen kann man folgende Vorteile erreichen (allerdings schließen sich einige gleichzeitig aus):
Kostenreduktion durch Einsatz mehrerer kleiner, preiswerter MedienDie genaue Art des Zusammenwirkens der einzelnen Speichermedien wird durch den RAID-Level spezifiziert. Die gebräuchlichsten RAID-Level sind RAID 0, RAID 1 und RAID 5. Sie werden unten beschrieben.
In der Regel erkennen die RAID-Implementierungen nur den Gesamtausfall eines Mediums beziehungsweise Fehler, die vom Medium signalisiert werden (siehe z. B. SMART). Auch können viele Implementierungen schon von der Theorie her nur einzelne Bitfehler erkennen und nicht korrigieren. Da Bitfehler mittlerweile selten sind und das Erkennen von Einzelfehlern ohne Korrekturmöglichkeit nur von relativ geringem Nutzen ist, verzichten heute einige Implementierungen auf die zusätzliche Integritätsprüfung beim Schreiben (read after write) oder Lesen (read and compare) und bieten hierdurch eine zum Teil beachtlich höhere Performance. Beispielsweise verzichten manche RAID-5-Implementierungen heute beim Lesen auf das Überprüfen der Plausibilität mittels Paritäts-Stripes, analog arbeiten auch viele RAID-1-Implementierungen. So erreichen diese Systeme beim Lesen Datendurchsätze, wie sie sonst nur bei RAID 0 erzielt werden. Auch wird bei solchen Implementierungen nicht notwendigerweise der Cache eines Speichermediums deaktiviert. Dennoch legen einige RAID-Level (RAID 2, je nach Hersteller auch RAID 6) ihr besonderes Augenmerk auf die Datenintegrität und Fehlerkorrektur (ECC), dort sind folglich die Cache-Speicher der Platten deaktiviert und zusätzlich werden dann jederzeit alle möglichen Prüfungen durchgeführt (read after write usw.), woraus zum Teil erhebliche Performanceeinbußen resultieren.
Aus Sicht des Benutzers oder eines Anwendungsprogramms unterscheidet sich ein RAID-System nicht von einem einzelnen Speichermedium.
Von Hardware-RAID spricht man, wenn das Zusammenwirken der Speichermedien von einer speziell dafür entwickelten Hardware-Baugruppe, dem RAID-Controller, organisiert wird. Der Hardware-RAID-Controller befindet sich typischerweise in physischer Nähe der Speichermedien. Er kann im Gehäuse des Computers enthalten sein. Besonders im Rechenzentrumsumfeld befindet er sich häufiger in einem eigenen Gehäuse, einem Disk-Array, in dem auch die Festplatten untergebracht sind. Die externen Systeme werden oft auch als DAS oder SAN bezeichnet, oder auch NAS, wenngleich nicht jedes dieser Systeme auch RAID implementiert. Professionelle Hardware-RAID-Implementierungen verfügen über eigene eingebettete CPUs; sie nutzen große, zusätzliche Cache-Speicher und bieten somit höchsten Datendurchsatz und entlasten dabei gleichzeitig den Hauptprozessor. Durch eine durchdachte Handhabung und einen soliden Herstellersupport wird gerade auch bei Störungen eine bestmögliche Unterstützung des Systemadministrators erreicht. Einfache Hardware-RAID-Implementierungen bieten diese Vorteile nicht in gleichem Maße und stehen daher in direkter Konkurrenz zu Software-RAID-Systemen.
Im unteren Preissegment (praktisch ausschließlich für IDE/ATA- oder SATA-Festplatten) werden sogenannte Host-RAID-Implementierungen angeboten. Rein äußerlich ähneln diese Lösungen den Hardware-RAID-Implementierungen. Es gibt sie als Kartenerweiterungen aus dem Niedrigpreis-Sektor, häufig sind sie aber auch direkt in die Hauptplatinen (engl. mainboards) für den Heimcomputer und Personal Computer integriert. Meistens sind diese Implementierungen auf RAID 0 und RAID 1 beschränkt. Um solche nichtprofessionellen Implementierungen so erschwinglich wie möglich zu halten, verzichten sie weitestgehend auf aktive Komponenten und realisieren die RAID-Level durch eine Software, die in den Treibern der Hardware integriert ist, allerdings für die notwendigen Rechenarbeiten den Hauptprozessor nutzt und auch die internen Bussysteme deutlich mehr belastet. Es handelt sich also eher um eine Software-RAID-Implementierung, die an eine spezielle Hardware gebunden ist. Die Bindung an den Controller ist ein bedeutender Nachteil, erschwert die Wiederherstellung und birgt bei einer Fehlfunktion desselben die Gefahr eines Datenverlustes. Solche Controller werden im Linux-Jargon daher oft auch als Fake-RAID bezeichnet (vgl. auch die sogenannten Win- oder Softmodems, die ebenfalls den Hauptprozessor und Bussysteme zusätzlich belasten).
Von Software-RAID spricht man, wenn das Zusammenwirken der Festplatten komplett softwareseitig organisiert wird. Auch der Begriff Host based RAID ist geläufig, da nicht das Speicher-Subsystem, sondern der eigentliche Computer die RAID-Verwaltung durchführt. Die meisten modernen Betriebssysteme wie FreeBSD, OpenBSD, Apple macOS, HP HP-UX, IBM AIX, Linux, Microsoft Windows ab Windows NT oder SUN Solaris sind dazu in der Lage. Die einzelnen Festplatten sind in diesem Fall entweder über einfache Festplattencontroller am Computer angeschlossen oder es werden externe Storage-Geräte wie Disk-Arrays von Unternehmen wie EMC, Promise, AXUS, Proware oder Hitachi Data Systems (HDS) an den Computer angeschlossen. Die Festplatten werden zunächst ohne RAID-Controller als sogenannte JBODs („just a bunch of disks“) in das System integriert, dann wird per Software-RAID (z. B. unter Linux mit dem Programm mdadm) die RAID-Funktionalität realisiert. Eine besondere Variante des Software RAID sind Dateisysteme mit einer integrierten RAID-Funktionalität. Ein Beispiel dafür ist das von Sun Microsystems entwickelte RAID-Z.
Der Vorteil von Software-RAID ist, dass kein spezieller RAID-Controller benötigt wird. Die Steuerung wird von der RAID-Software erledigt, diese ist entweder schon Teil des Betriebssystems oder wird nachträglich installiert. Dieser Vorteil kommt besonders bei der Disaster Recovery zum Tragen, wenn der RAID-Controller defekt und nicht mehr verfügbar ist. Praktisch alle derzeit verfügbaren Software-RAID-Systeme benutzen die Festplatten so, dass diese auch ohne die spezifische Software ausgelesen werden können.
Bei einem Software-RAID werden bei Festplattenzugriffen neben dem Hauptprozessor des Computers auch die System-Busse wie PCI stärker belastet als bei einem Hardware-RAID. Bei leistungsschwachen CPUs und Bus-Systemen verringert dies deutlich die Systemleistung; bei leistungsstarken, wenig ausgelasteten Systemen ist dies belanglos. Storage-Server sind in der Praxis oft nicht voll ausgelastet; auf solchen Systemen können Software-RAID-Implementierungen unter Umständen sogar schneller sein als Hardware-RAIDs.
Ein weiterer Nachteil ist, dass bei vielen Software-RAID kein Cache genutzt werden kann, dessen Inhalt auch nach einem Stromausfall erhalten bleibt, wie es bei Hardware-RAID-Controllern mit einer Battery Backup Unit der Fall ist. Dieses Problem lässt sich mit einer unterbrechungsfreien Stromversorgung für den gesamten PC vermeiden. Um die Gefahr von Datenverlusten und Fehlern in der Datenintegrität bei einem Stromausfall oder Systemabsturz zu minimieren, sollten außerdem die (Schreib-)Caches der Festplatten deaktiviert werden.Da die Platten eines Software-RAIDs prinzipiell auch einzeln angesprochen werden können, besteht bei gespiegelten Festplatten die Gefahr, dass Änderungen nur noch an einer Platte durchgeführt werden – wenn etwa nach einem Betriebssystem-Update die RAID-Software oder der Treiber für einen RAID-Festplatten-Controller nicht mehr funktionieren, eine der gespiegelten Festplatten aber weiterhin über einen generischen SATA-Treiber angesprochen werden kann. Entsprechende Warnhinweise oder Fehlermeldungen während des Bootens sollten deshalb nicht ignoriert werden, nur weil das System trotzdem funktioniert. Ausnahmen bilden hier Software-RAID mit Datenintegrität wie z. B. ZFS. Unvollständige Speichervorgänge werden zurückgesetzt. Fehlerhafte Spiegeldaten werden erkannt und durch korrekte Spiegeldaten ersetzt. Es wird wohl beim Lesen eine Fehlermeldung geben, da die fehlerhafte oder alte Spiegelseite nicht mit dem aktuellen Block übereinstimmt.
Mit einem Software-RAID-ähnlichen Ansatz lassen sich auch (logische) Volumes, die von unterschiedlichen Storage-Servern zur Verfügung gestellt werden, auf Seite des Anwendungsservers spiegeln. Das kann in hochverfügbaren Szenarien nützlich sein, weil man damit unabhängig von entsprechender Cluster-Logik in den Storage-Servern ist, welche häufig fehlt, andere Ansätze verfolgt oder herstellerabhängig und somit in gemischten Umgebungen nicht zu gebrauchen ist. Allerdings muss das Host-Betriebssystem entsprechende Features mitbringen (z. B. durch Einsatz von GlusterFS, des Logical Volume Manager oder von NTFS). Solche Storage-Server sind üblicherweise in sich schon redundant. Ein übergreifendes Cluster richtet sich also eher gegen den Ausfall des ganzen Servers oder eines Rechnerraumes (Stromausfall, Wasserschaden, Brand usw.) Ein einfacher Spiegel, vergleichbar mit RAID 1, reicht hier aus; siehe auch Hauptartikel Storage Area Network.
Für Größenänderungen von Raids bestehen grundsätzlich zwei Alternativen. Entweder das bestehende Raid wird übernommen und angepasst oder die Daten werden andernorts gesichert, das Raid in der gewünschten Größe/dem gewünschten Level neu aufgesetzt und die zuvor gesicherten Daten zurückgespielt. Letzterer trivialer Fall wird nachfolgend nicht weiter behandelt; es geht ausschließlich um die Größenveränderung eines bestehenden Raid-Systems.
Bezüglich einer Vergrößerung gibt es allgemein keine Garantie, dass ein bestehendes Raid-System durch das Hinzufügen weiterer Festplatten erweitert werden kann. Das gilt sowohl für Hardware- als auch für Software-Raids. Nur wenn ein Hersteller sich der Erweiterung als Option explizit angenommen hat, besteht diese Möglichkeit. Sie ist üblicherweise recht zeitintensiv, da sich durch eine veränderte Laufwerksanzahl die Organisation sämtlicher Daten und Paritätsinformationen ändert und daher die physische Ordnung restrukturiert werden muss. Weiterhin müssen das Betriebssystem und das verwendete Dateisystem in der Lage sein, den neuen Plattenplatz einzubinden.
Eine Raid-Vergrößerung setzt einen neuen Datenträger voraus, der mindestens die Größe des kleinsten bereits verwendeten Datenträgers aufweist.
Muss zum Beispiel nach einem Plattenfehler ein RAID-Array wiederhergestellt werden, so benötigt man eine Festplatte, die mindestens so groß wie die ausgefallene Festplatte ist. Dies kann problematisch sein, wenn man zum Beispiel Platten maximaler Größe verwendet. Ist ein Plattentyp auch nur zeitweise nicht lieferbar und die alternativ erhältlichen Platten sind auch nur ein Byte kleiner, kann das RAID-Array nicht mehr einfach wiederhergestellt werden. Vorsorglich nutzen daher manche Hersteller (z. B. HP oder Compaq) Platten mit einer geänderten Plattenfirmware, welche die Platte gezielt geringfügig verkleinert. So wird sichergestellt, dass sich auch Platten anderer Hersteller mit ebenfalls angepasster Firmware auf die vom RAID-Array genutzte Größe einstellen lassen. Ein anderer Ansatz, den einige Hersteller von RAID-Controllern verfolgen, ist die Plattenkapazität beim Einrichten des Arrays geringfügig zu beschneiden, somit können auch Platten unterschiedlicher Serien oder verschiedener Hersteller mit annähernd gleicher Kapazität problemlos verwendet werden. Ob ein Controller diese Funktion unterstützt, sollte aber vor Einrichten eines Arrays überprüft werden, da eine nachträgliche Größenänderung meist nicht möglich ist. Manche RAID-Implementierungen überlassen es dem Benutzer, einigen Plattenplatz nicht auszunutzen. Es empfiehlt sich dann, aber natürlich auch bei Software-RAID, bereits von Anfang an einen geringen Plattenplatz für den Fall eines Modellwechsels zu reservieren und nicht zu nutzen. Aus diesem Grund sollte man auch maximal große Platten, für die es nur einen Hersteller gibt, im Bereich redundanter RAID-Systeme behutsam einsetzen.
Auch Hardware kann defekt sein (z. B. in Form eines RAID-Controllers). Das kann zum Problem werden, besonders dann, wenn kein identischer Ersatzcontroller verfügbar ist. In der Regel kann ein intakter Plattensatz nur am gleichen Controller beziehungsweise an der gleichen Controller-Familie betrieben werden, an dem er auch erstellt wurde. Häufig kommt es (besonders bei älteren Systemen) auch vor, dass nur exakt der gleiche Controller (Hardware + Firmware) den Plattensatz ohne Datenverlust ansprechen kann. Im Zweifelsfall sollte man unbedingt beim Hersteller nachfragen. Aus dem gleichen Grund sind unbekannte Hersteller, aber auch onboard RAID-Systeme mit Vorsicht einzusetzen. In jedem Fall sollte sichergestellt sein, dass man auch nach Jahren einen leicht zu konfigurierenden, passenden Ersatz bekommt.
Abhilfe schafft unter Umständen Linux, die Plattensätze einiger IDE-/SATA-RAID-Controller (Adaptec HostRAID ASR, Highpoint HPT37X, Highpoint HPT45X, Intel Software RAID, JMicron JMB36x, LSI Logic MegaRAID, Nvidia NForce, Promise FastTrack, Silicon Image Medley, SNIA DDF1, VIA Software RAID und Kompatible) können direkt mit dem dmraid-Tool vom Betriebssystem eingelesen werden.
Festplatten können, wie andere Produkte auch, in fehlerhaften Serien produziert werden. Gelangen diese dann zum Endverbraucher und in ein RAID-System, so können solche serienbehafteten Fehler auch zeitnah auftreten und dann zu Mehrfachfehlern – dem gleichzeitigen Ausfall mehrerer Festplatten – führen. Solche Mehrfachfehler lassen sich dann üblicherweise nur durch das Rückspielen von Datensicherungen kompensieren. Vorsorglich kann man Diversitäts-Strategien nutzen, also einen RAID-Array aus etwa leistungsgleichen Platten mehrerer Hersteller aufbauen, wobei man beachten muss, dass die Plattengrößen geringfügig variieren können und sich die maximale Arraygröße ggf. von der kleinsten Platte ableitet.
Ein verdecktes Problem liegt in dem Zusammenspiel von Arraygröße und statistischer Fehlerwahrscheinlichkeit der verwendeten Laufwerke. Festplattenhersteller geben für ihre Laufwerke eine Wahrscheinlichkeit für nicht korrigierbare Lesefehler an (unrecoverable read error, URE). Der URE-Wert ist ein Durchschnittswert, der innerhalb der Gewährleistungszeit zugesichert wird, er erhöht sich alters- und verschleißbedingt. Für einfache Laufwerke aus dem Consumer-Bereich (IDE, SATA) garantieren die Hersteller typischerweise URE-Werte von maximal 
   Bit (etwa 12 TB) maximal zu einem URE kommen darf. Besteht ein Array also beispielsweise aus acht je 2 TB großen Platten, so garantiert der Hersteller nur noch, dass der Rebuild statistisch gesehen mindestens in einem von drei Fällen ohne URE klappen muss, obwohl alle Laufwerke korrekt nach Herstellerspezifikation funktionieren. Für kleine RAID-Systeme stellt dies kaum ein Problem dar: Der Rebuild eines RAID 5-Arrays aus drei 500 GB großen Consumer-Laufwerken (1 TB Nutzdaten) wird im Schnitt in 92 von 100 Fällen erfolgreich sein, wenn nur URE-Fehler betrachtet werden. Deswegen stoßen praktisch schon jetzt alle einfachen redundanten RAID-Verfahren (RAID 3, 4, 5 usw.) außer RAID 1 und RAID 2 an eine Leistungsgrenze. Das Problem kann natürlich durch höherwertige Platten, aber auch durch kombinierte RAID-Level wie RAID 10 oder RAID 50 entschärft werden. In der Realität ist das Risiko, dass tatsächlich ein derartiger URE-basierter Fehler eintritt, durchaus geringer, denn es handelt sich bei den Herstellerangaben nur um garantierte Maximalwerte. Dennoch ist der Einsatz hochwertiger Laufwerke mit Fehlerraten von 
Als Rebuild bezeichnet man den Wiederherstellungsprozess eines RAID-Verbundes. Dieser wird notwendig, wenn eine oder mehrere Festplatten (je nach RAID-Level) im RAID-Verbund ausgefallen oder entfernt worden sind und anschließend durch neue Festplatten ersetzt wurden. Da die neuen Festplatten unbeschrieben sind, müssen mit Hilfe der noch vorhandenen Nutzdaten bzw. Paritätsdaten die fehlenden Daten auf diese geschrieben werden. Unabhängig von der Konfiguration des RAID-Systems bedeutet ein Rebuild immer eine höhere Belastung der beteiligten Hardwarekomponenten.
Ein Rebuild kann, abhängig vom RAID-Level, der Plattenanzahl und -größe, durchaus mehr als 24 Stunden dauern. Um bei einem Rebuild weitere Plattenausfälle zu vermeiden, sollte man Festplatten aus unterschiedlichen Herstellungschargen für ein RAID verwenden. Da die Platten unter gleichen Betriebsbedingungen arbeiten und dasselbe Alter besitzen, besitzen sie auch eine ähnliche Lebens- bzw. Ausfallerwartung.
Ein automatischer Rebuild kann je nach verwendetem Controller über Hot-Spare-Platten, welche dem RAID-Verbund zugeordnet sind, erfolgen. Allerdings verfügt nicht jeder Controller oder jede Software über die Möglichkeiten, eine oder mehrere Hot-Spare-Platten anzubinden. Diese ruhen im normalen Betrieb. Sobald der Controller eine defekte Festplatte erkennt und aus dem RAID-Verbund entfernt, wird eine der Hot-Spare-Platten in den Verbund eingefügt, und der Rebuild startet automatisch. Es findet aber i. A. weder bei Controllern noch bei Softwarelösungen eine regelmäßige Überprüfung der Hot-Spare-Platte(n) auf Verfügbarkeit und Schreib-/ Lesefunktionalität statt.
Unter den vorgenannten Problempunkten sollte der Zustand „Rebuild“ eines Systems neu betrachtet werden. Die ursprüngliche Bedeutung „RAID“ (s. a. Erläuterung zu Beginn des Artikels) und dessen Priorität „Ausfallsicherheit durch Redundanz“ wandelt sich in der Praxis mehr und mehr zu einem System, dessen Priorität in der „Maximierung des zur Verfügung gestellten Speicherplatzes durch einen Verbund von günstigen Festplatten maximaler Kapazität“ liegt. Der ehemals kurzzeitige „Betriebszustand“ Rebuild, in dem verfügbare Hotspare-Festplatten sehr kurzfristig den Normalzustand wiederherstellen konnten, entwickelt sich dabei zu einem tagelangen Notfallszenario. Der verhältnismäßig lange Zeitraum unter maximaler Beanspruchung erhöht dabei das Risiko eines weiteren Hardwareausfalls oder anderer Störfälle beträchtlich. Eine weitere Erhöhung der Redundanz, z. B. durch Verwendung eines RAID 6, scheint in solchen Szenarien angeraten.
Bei der Präsenz von 3 Platten à 1 TB, die jeweils eine Ausfallwahrscheinlichkeit von 1 % in einem gegebenen Zeitraum haben, gilt
RAID 0 stellt 3 TB zur Verfügung. Die Ausfallwahrscheinlichkeit des RAIDs beträgt 2,9701 % (1 in 34 Fällen).
RAID 1 stellt 1 TB zur Verfügung. Die Ausfallwahrscheinlichkeit des RAIDs beträgt 0,0001 % (1 in 1.000.000 Fällen).
RAID 5 stellt 2 TB zur Verfügung. Die Ausfallwahrscheinlichkeit des RAIDs beträgt 0,0298 % (1 in 3.356 Fällen).Technisch wird dieses Verhalten wie folgt erreicht.
Bei RAID 0 fehlt die Redundanz, daher gehört es streng genommen nicht zu den RAID-Systemen, es ist nur ein schnelles „Array of Independent Disks“.
RAID 0 bietet gesteigerte Transferraten, indem die beteiligten Festplatten in zusammenhängende Blöcke gleicher Größe aufgeteilt werden, wobei diese Blöcke quasi im Reißverschlussverfahren zu einer großen Festplatte angeordnet werden. Somit können Zugriffe auf allen Platten parallel durchgeführt werden (engl. striping, was „in Streifen zerlegen“ bedeutet, abgeleitet von stripe, oder „Streifen“). Die Datendurchsatzsteigerung (bei sequentiellen Zugriffen, aber besonders auch bei hinreichend hoher Nebenläufigkeit) beruht darauf, dass die notwendigen Festplattenzugriffe in höherem Maße parallel abgewickelt werden können. Die Größe der Datenblöcke wird als Striping-Granularität (auch stripe size, chunk size oder interlace size) bezeichnet. Meistens wird bei RAID 0 eine chunk size von 64 kB gewählt.
Fällt jedoch eine der Festplatten durch einen Defekt (vollständig) aus, kann der RAID-Controller ohne deren Teildaten die Nutzdaten nicht mehr vollständig rekonstruieren. Die Daten teilweise wiederherzustellen, ist unter Umständen möglich, nämlich genau für jene Dateien, die nur auf den verbliebenen Festplatten gespeichert sind, was typischerweise nur bei kleinen Dateien und eher bei großer Striping-Granularität der Fall sein wird. (Im Vergleich dazu würde die Benutzung von je einem getrennten Dateisystem pro Festplatte bei einem Ausfall eines einzelnen Speichermediums die nahtlose Benutzbarkeit der verbliebenen Medien beziehungsweise der dortigen Dateisysteme garantieren, während der vollständige Ausfall eines einzelnen und entsprechend größeren Speichermediums einen vollständigen Verlust aller Daten zur Folge hätte.) RAID 0 ist daher nur in Anwendungen zu empfehlen, bei denen Ausfallsicherheit kaum von Bedeutung ist. Auch wenn überwiegend lesende Zugriffe auftreten (während ändernde Zugriffe durch entsprechende Verfahren redundant auch auf einem anderen Medium ausgeführt werden), kann RAID 0 empfehlenswert sein. Die bei einfachem RAID 0 unvermeidbare Betriebsunterbrechung infolge eines Festplatten-Ausfalls (auch einzelner Platten) sollte bei der Planung berücksichtigt werden.
Der Einsatzzweck dieses Verbundsystems erstreckt sich demnach auf Anwendungen, bei denen in kurzer Zeit besonders große Datenmengen vor allem gelesen werden sollen, etwa auf die Musik- oder Videowiedergabe und die sporadische Aufnahme derselben.
   einer Festplatte statistisch unabhängig von den übrigen Festplatten und für alle Festplatten identisch ist.
Eine Sonderform stellt ein Hybrid-RAID-0-Verbund aus SSD und konventioneller Festplatte dar (s. a. Fusion Drive unter OS X), wobei die SSD als großer Cachespeicher für die konventionelle Festplatte dient. Ein echtes RAID 0 entsteht hier aber nicht, da nach einer Trennung der beiden Laufwerke die Daten beider Datenträger auch separat noch lesbar sind.
RAID 1 ist der Verbund von mindestens zwei Festplatten. Ein RAID 1 speichert auf allen Festplatten die gleichen Daten (Spiegelung) und bietet somit volle Redundanz. Die Kapazität des Arrays ist hierbei höchstens so groß wie die kleinste beteiligte Festplatte.
Ein enormer Vorteil von RAID 1 gegenüber allen anderen RAID-Verfahren liegt in seiner Einfachheit. Beide Platten sind identisch beschrieben und enthalten alle Daten eines Systems, somit kann (die passende Hardware vorausgesetzt) normalerweise auch jede Platte einzeln in zwei unabhängigen Rechnern (intern oder im externen Laufwerk) unmittelbar betrieben und genutzt werden. Aufwändige Rebuilds sind nur dann notwendig, wenn die Platten wieder redundant betrieben werden sollen. Im Störfall wie auch bei Migrationen beziehungsweise Upgrades bedeutet das einen enormen Vorteil.
Fällt eine der gespiegelten Platten aus, kann jede andere weiterhin alle Daten liefern. Besonders in sicherheitskritischen Echtzeitsystemen ist das unverzichtbar. RAID 1 bietet eine hohe Ausfallsicherheit, denn zum Totalverlust der Daten führt erst der Ausfall aller Platten.
   einer Festplatte statistisch unabhängig von den übrigen Festplatten und für alle Festplatten identisch ist.
Aus historischen Gründen wird zwischen Mirroring (alle Festplatten am selben Controller) und Duplexing (für jede Festplatte ein eigener Controller) unterschieden, was heute jedoch nur bei Betrachtungen über den Single Point of Failure eine Rolle spielt: Festplatten-Controller fallen im Vergleich zu mechanisch beanspruchten Teilen (also Festplatten) relativ selten aus, so dass das Risiko eines Controller-Ausfalls auf Grund seiner geringen Wahrscheinlichkeit häufig noch toleriert wird.
Zur Erhöhung der Leseleistung kann ein RAID-1-System beim Lesen auf mehr als eine Festplatte zugreifen und gleichzeitig verschiedene Sektoren von verschiedenen Platten einlesen. Bei einem System mit zwei Festplatten lässt sich so die Leistung verdoppeln. Die Lesecharakteristik entspricht hierbei einem RAID-0-System. Diese Funktion bieten aber nicht alle Controller oder Softwareimplementierungen an. Sie erhöht die Lesegeschwindigkeit des Systems enorm, geht aber auf Kosten der Sicherheit. Eine solche Implementierung schützt vor einem kompletten Datenträgerausfall, aber nicht vor Problemen mit fehlerhaften Sektoren, zumindest falls diese erst nach dem Speichern (read after write verify) auftreten.
Zur Erhöhung der Sicherheit kann ein RAID-1-System beim Lesen stets auf mehr als eine Festplatte zugreifen. Dabei werden die Antwortdatenströme der Festplatten verglichen. Bei Unstimmigkeiten wird eine Fehlermeldung ausgegeben, da die Spiegelung nicht länger besteht. Diese Funktion bieten nur wenige Controller an, auch reduziert sie die Geschwindigkeit des Systems geringfügig.
Eine Spiegelplatte ist kein Ersatz für eine Datensicherung, da sich auch versehentliche oder fehlerhafte Schreiboperationen (Viren, Stromausfall, Benutzerfehler) augenblicklich auf die Spiegelplatte übertragen. Dies gilt insbesondere für unvollständig abgelaufene, schreibende Programme (etwa durch Stromausfall abgebrochene Update-Transaktionen auf Datenbanken ohne Logging-System), wobei es hier nicht nur zu der Beschädigung der Spiegelung, sondern auch zu einem inkonsistenten Datenzustand trotz intakter Spiegelung kommen kann. Abhilfe schaffen hier Datensicherungen und Transaktions-Logs.
Eine Sonderform stellt ein Hybrid-RAID-1-Verbund aus SSD und konventioneller Festplatte dar, welche die Vorteile einer SSD (Lesegeschwindigkeit) mit der Redundanz verbindet.
In der Praxis wird jedoch während des Rebuild-Vorganges das gesamte Array hoch belastet, so dass weitere Ausfälle in diesem Zeitraum mit höherer Wahrscheinlichkeit zu erwarten sind.
RAID 5 implementiert Striping mit auf Block-Level-verteilten Paritätsinformationen. Zur Berechnung der Parität wird durch die jeweils an gleicher Adresse anliegenden Datenblöcke der am RAID-Verbund beteiligten Festplatten eine logische Gruppe gebildet. Von allen Datenblöcken einer Gruppe enthält ein Datenblock die Paritätsdaten, während die anderen Datenblöcke Nutzdaten enthalten. Die Nutzdaten von RAID-5-Gruppen werden wie bei RAID 0 auf alle Festplatten verteilt. Die Paritätsinformationen werden jedoch nicht wie bei RAID 4 auf einer Platte konzentriert, sondern ebenfalls verteilt.
RAID 5 bietet sowohl gesteigerten Datendurchsatz beim Lesen von Daten als auch Redundanz bei relativ geringen Kosten und ist dadurch eine sehr beliebte RAID-Variante. In schreibintensiven Umgebungen mit kleinen, nicht zusammenhängenden Änderungen ist RAID 5 nicht zu empfehlen, da bei zufälligen Schreibzugriffen der Durchsatz aufgrund des zweiphasigen Schreibverfahrens deutlich abnimmt. An dieser Stelle wäre eine RAID-01-Konfiguration vorzuziehen. RAID 5 ist eine der kostengünstigsten Möglichkeiten, Daten auf mehreren Festplatten redundant zu speichern und dabei das Speichervolumen effizient zu nutzen. Dieser Vorteil kann bei wenigen Platten allerdings durch hohe Controllerpreise vernichtet werden. Daher kann es in einigen Situationen dazu führen, dass ein RAID 10 kostengünstiger ist.
Die nutzbare Gesamtkapazität errechnet sich aus der Formel: (Anzahl der Festplatten − 1) × Kapazität der kleinsten Festplatte.
Da ein RAID 5 nur dann versagt, wenn mindestens zwei Platten gleichzeitig ausfallen, ergibt sich bei einem RAID 5 mit 
   einer Festplatte statistisch unabhängig von den übrigen Festplatten und für alle Festplatten identisch ist. In der Praxis wird jedoch während des Rebuild-Vorganges das gesamte Array hoch belastet, so dass weitere Ausfälle in diesem Zeitraum mit höherer Wahrscheinlichkeit zu erwarten sind.
Die Berechnung der Paritätsdaten eines Paritätsblocks erfolgt durch XOR-Verknüpfung der Daten aller Datenblöcke seiner Gruppe, was wiederum zu einer leichten bis erheblichen Verminderung der Datentransferrate im Vergleich zu RAID 0 führt. Da die Paritätsinformationen beim Lesen nicht benötigt werden, stehen alle Platten zum parallelen Zugriff zur Verfügung. Dieser (theoretische) Vorteil greift allerdings nicht bei kleinen Dateien ohne nebenläufigen Zugriff, erst bei größeren Dateien oder geeigneter Nebenläufigkeit tritt eine nennenswerte Beschleunigung ein. Bei n Festplatten erfordert der Schreibzugriff entweder ein Volumen, das genau (n−1) korrespondierende Datenblöcke ausfüllt, oder ein zwei-phasiges Verfahren (alte Daten lesen; neue Daten schreiben).
Jüngere RAID-Implementierungen berechnen die neue Paritätsinformation bei einem Schreibzugriff nicht durch XOR-Verknüpfung über die Daten aller korrespondierenden Datenblöcke, sondern durch XOR-Verknüpfung von altem und neuen Datenwert sowie des alten Paritätswerts. Anders gesagt: Wechselt ein Datenbit den Wert, dann wechselt auch das Paritätsbit den Wert. Das ist mathematisch dasselbe, aber es sind nur zwei Lesezugriffe erforderlich, nämlich auf die beiden alten Werte und nicht n−2 Lesezugriffe auf die sonstigen Datenblöcke wie früher. Dies erlaubt den Aufbau von größeren RAID-5-Arrays ohne Performanceabfall, beispielsweise mit n = 8. In Verbindung mit Schreibcaches erreicht man im Vergleich zu RAID 1 beziehungsweise RAID 10 hiermit ähnlichen Datendurchsatz bei geringeren Hardwarekosten. Storage-Server werden daher, wenn überhaupt noch klassische RAID-Verfahren zur Anwendung kommen, üblicherweise in RAID-5-Arrays aufgeteilt.
Bei RAID 5 ist die Datenintegrität des Arrays beim Ausfall von maximal einer Platte gewährleistet. Nach Ausfall einer Festplatte oder während des Rebuilds auf die Hotspare-Platte (bzw. nach Austausch der defekten Festplatte) lässt die Leistung deutlich nach (beim Lesen: jeder (n−1)-te Datenblock muss rekonstruiert werden; beim Schreiben: jeder (n−1)-te Datenblock kann nur durch Lesen der entsprechenden Bereiche aller korrespondierenden Datenblöcke und anschließendes Schreiben der Parität geschrieben werden; hinzu kommen die Zugriffe des Rebuilds: (n−1) × Lesen; 1 × Schreiben). Bei dem Rebuild-Verfahren ist daher die Berechnung der Parität zeitlich zu vernachlässigen; im Vergleich zu RAID 1 dauert somit das Verfahren unwesentlich länger und benötigt gemessen am Nutzdatenvolumen nur den (n−1)-ten Teil der Schreibzugriffe.
Eine noch junge Methode zur Verbesserung der Rebuild-Leistung und damit der Ausfallsicherheit ist präemptives RAID 5. Hierbei werden interne Fehlerkorrekturstatistiken der Platten zur Vorhersage eines Ausfalls herangezogen (siehe SMART). Vorsorglich wird nun die Hotspare-Platte mit dem kompletten Inhalt der ausfallverdächtigsten Platte im RAID-Verbund synchronisiert, um zum vorhergesagten Versagenszeitpunkt sofort an deren Stelle treten zu können. Das Verfahren erreicht bei geringerem Platzbedarf eine ähnliche Ausfallsicherheit wie RAID 6 und andere Dual-Parity-Implementierungen. Allerdings wurde präemptives RAID 5 aufgrund des hohen Aufwands bislang nur in wenigen „High-End“-Speichersystemen mit serverbasierten Controllern implementiert.
Zudem zeigt eine Studie von Google (Februar 2007), dass SMART-Daten zur Vorhersage des Ausfalls einer einzelnen Festplatte nur eingeschränkt nützlich sind.Einfluss der Anzahl der Festplatten
Bei RAID-5-Systemen sind Konfigurationen mit 3 oder 5 Festplatten häufig anzutreffen – das ist kein Zufall, denn die Anzahl der Festplatten hat einen Einfluss auf die Schreibleistung.
Sie wird weitestgehend durch die Anzahl der Festplatten, aber auch durch Cache-Größen bestimmt, mehr ist hier immer besser.
Im Unterschied zur Read-Performance ist das Ermitteln der Write-Performance bei RAID 5 deutlich komplizierter und hängt sowohl von der zu schreibenden Datenmenge, als auch von der Anzahl der Platten ab. Ausgehend von Festplatten mit weniger als 2 TB Plattenplatz, ist die atomare Blockgröße (auch Sektorgröße genannt) der Platten häufig 512 Byte (siehe Festplattenlaufwerk). Geht man weiter von einem RAID-5-Verbund mit 5 Platten (4/5 Daten und 1/5 Parität) aus, so ergibt sich folgendes Szenario: Will eine Anwendung 2.048 Byte schreiben, wird in diesem günstigen Fall auf alle 5 Platten genau je ein Block zu 512 Byte geschrieben, wobei einer dieser Blöcke keine Nutzdaten enthält. Im Vergleich zu RAID 0 mit 5 Platten ergibt sich daraus eine Effizienz von 80 % (bei RAID 5 mit 3 Platten wären es 66 %). Möchte eine Anwendung nur einen Block von 512 Byte schreiben, so ergibt sich ein ungünstigerer Fall, es müssen zuerst der abzuändernde Block und der Paritätsblock eingelesen werden, danach wird der neue Paritätsblock berechnet und erst dann können beide 512-Byte-Blöcke geschrieben werden. Das bedeutet einen Aufwand von 2 Lesezugriffen und 2 Schreibzugriffen, um einen Block zu speichern. Geht man vereinfacht davon aus, dass Lesen und Schreiben gleich lange dauern, so beträgt die Effizienz in diesem ungünstigsten Fall, dem sogenannten RAID 5 write Penalty, noch 25 %. In der Praxis wird dieser Worst-Case-Fall bei einem RAID 5 mit 5 Platten aber kaum eintreten, denn Dateisysteme haben häufig Blockgrößen von 2 kB, 4 kB und mehr und zeigen daher praktisch ausschließlich das Well-Case-Schreibverhalten. Gleiches gilt analog für RAID 5 mit 3 Platten. Unterschiedlich verhält sich hingegen etwa ein RAID-5-System mit 4 Platten (3/4 Daten und 1/4 Parität), soll hier ein Block von 2.048 Byte geschrieben werden, sind zwei Schreibvorgänge notwendig, es werden dann einmal 1.536 Byte mit Well-Case-Performance geschrieben und noch einmal 512 Byte mit Worst-Case-Verhalten. Diesem Worst-Case-Verhalten wirken zwar Cache-Strategien entgegen, aber dennoch ergibt sich hieraus, dass bei RAID 5 möglichst ein Verhältnis von zwei, vier oder auch acht Platten für Nutzdaten plus einer Platte für Paritätsdaten eingehalten werden sollte. Daher haben RAID-5-Systeme mit 3, 5 oder 9 Platten ein besonders günstiges Performanceverhalten.
RAID 2 spielt in der Praxis keine Rolle mehr. Das Verfahren wurde nur bei Großrechnern verwendet. Die Daten werden hierbei in Bitfolgen fester Größe zerlegt und mittels eines Hamming-Codes auf größere Bitfolgen abgebildet (zum Beispiel: 8 Bit für Daten und noch 3 Bit für die ECC-Eigenschaft). Die einzelnen Bits des Hamming-Codeworts werden dann über einzelne Platten aufgeteilt, was prinzipiell einen hohen Durchsatz erlaubt. Ein Nachteil ist jedoch, dass die Anzahl der Platten ein ganzzahliges Vielfaches der Hamming-Codewortlänge sein muss, wenn sich die Eigenschaften des Hamming-Codes nach außen zeigen sollen (diese Forderung entsteht, wenn man einen Bit-Fehler im Hamming-Code analog zu einem Festplatten-Ausfall im RAID 2 sieht).
Der kleinste RAID-2-Verbund benötigt drei Festplatten und entspricht einem RAID 1 mit zweifacher Spiegelung. Im realen Einsatz sah man daher zumeist nicht weniger als zehn Festplatten in einem RAID-2-Verbund.
Der wesentliche Gedanke bei RAID 3 ist ein möglichst hoher Leistungsgewinn mit Redundanz im Verhältnis zum Anschaffungspreis. Im RAID 3 werden die eigentlichen Nutzdaten normal auf einer oder mehreren Datenplatten gespeichert. Außerdem wird eine Summeninformation auf einer zusätzlichen Paritätsplatte gespeichert. Für die Paritätsplatte werden die Bits der Datenplatten zusammengezählt und die errechnete Summe wird darauf untersucht, ob sie eine gerade oder eine ungerade Summe darstellt; eine gerade Summe wird auf der Paritätsplatte mit dem Bit-Wert 0 gekennzeichnet; eine ungerade Summe wird mit dem Bit-Wert 1 gekennzeichnet. Die Datenplatten enthalten also normale Nutzdaten, während die Paritätsplatte nur die Summeninformationen enthält.
Der Gewinn durch ein RAID 3 ist folgender: Man kann beliebig viele Datenplatten verwenden und braucht für die Paritätsinformationen trotzdem nur eine einzige Platte. Die eben dargestellten Berechnungen ließen sich auch mit 4 oder 5 oder noch mehr Datenplatten (und nur einer einzigen Paritäts-Platte) durchführen. Damit ergibt sich auch gleich der größte Nachteil: Die Paritätsplatte wird bei jeder Operation, vor allem Schreiboperation, benötigt, sie bildet dadurch den Flaschenhals des Systems; auf diese Platten wird bei jeder Schreiboperation zugegriffen.
RAID 3 ist inzwischen vom Markt verschwunden und wurde weitgehend durch RAID 5 ersetzt, bei dem die Parität gleichmäßig über alle Platten verteilt wird. Vor dem Übergang zu RAID 5 wurde RAID 3 zudem partiell durch RAID 4 verbessert, bei dem Ein- beziehungsweise Ausgabe-Operationen mit größeren Blockgrößen aus Geschwindigkeitsgründen standardisiert wurden.
Zusätzlich sei hier bemerkt, dass ein RAID-3-Verbund aus nur zwei Festplatten (eine Datenplatte + eine Paritätsplatte) dazu führt, dass die Paritätsplatte die gleichen Bit-Werte enthält wie die Datenplatte, was in der Wirkung einem RAID 1 mit zwei Festplatten entspricht (eine Datenplatte + eine Kopie der Datenplatte).
Es werden ebenfalls Paritätsinformationen berechnet, die auf eine dedizierte Festplatte geschrieben werden. Allerdings sind die Einheiten, die geschrieben werden, größere Datenblöcke (englisch stripes oder chunks) und nicht einzelne Bytes, was die Gemeinsamkeit zu RAID 5 ausmacht.
Ein Nachteil bei klassischem RAID 4 besteht darin, dass die Paritätsplatte bei allen Schreib- und Leseoperationen beteiligt ist. Dadurch ist die maximal mögliche Datenübertragungsgeschwindigkeit durch die Datenübertragungsgeschwindigkeit der Paritätsplatte begrenzt. Da bei jeder Operation immer eine der Datenplatten und die Paritätsplatte verwendet werden, muss die Paritätsplatte weit mehr Zugriffe durchführen als die Datenplatten. Sie verschleißt dadurch stärker und ist daher häufiger von Ausfällen betroffen.
Eine Ausnahme bildet ein Systemdesign, bei dem die Lese- und Schreiboperationen auf ein NVRAM erfolgen. Das NVRAM bildet einen Puffer, der die Übertragungsgeschwindigkeit kurzfristig erhöht, die Lese- und Schreiboperationen sammelt und in sequenziellen Abschnitten auf das RAID-4-Plattensystem schreibt. Dadurch werden die Nachteile von RAID 4 vermindert und die Vorteile bleiben erhalten.
NetApp nutzt RAID 4 in ihren NAS-Systemen, das verwendete Dateisystem WAFL wurde speziell für den Einsatz mit RAID 4 entworfen. Da RAID 4 nur bei sequentiellen Schreibzugriffen effektiv arbeitet, verwandelt WAFL wahlfreie Schreibzugriffe (random writes) im NVRAM-Cache in sequentielle – und merkt sich jede einzelne Position für den späteren Abruf. Beim Lesen tritt allerdings das klassische Fragmentierungsproblem auf: Zusammengehörige Daten stehen nicht notwendigerweise auf physisch hintereinanderliegenden Blöcken, wenn sie im Nachhinein aktualisiert oder überschrieben wurden. Die verbreitetste Beschleunigung von Lesezugriffen, der Cache prefetch, ist daher ohne Wirkung. Die Vorteile beim Schreiben ergeben somit einen Nachteil beim Lesen. Das Dateisystem muss dann regelmäßig defragmentiert werden.
RAID 6 (unter diversen Handelsnamen angeboten, zum Beispiel Advanced Data Guarding) funktioniert ähnlich wie RAID 5, verkraftet aber den gleichzeitigen Ausfall von bis zu zwei Festplatten. Insbesondere beim intensiven Einsatz hochkapazitiver SATA-/IDE-Festplatten kann die Wiederherstellung der Redundanz nach dem Ausfall einer Platte viele Stunden bis hin zu Tagen dauern; bei RAID 5 besteht währenddessen kein Schutz vor einem weiteren Ausfall.
RAID 6 implementiert Striping mit doppelten, auf Block-Level verteilten Paritätsinformationen. Im Gegensatz zu RAID 5 gibt es bei RAID 6 mehrere mögliche Implementierungsformen, die sich insbesondere in der Schreibleistung und dem Rechenaufwand unterscheiden. Im Allgemeinen gilt: Bessere Schreibleistung wird durch erhöhten Rechenaufwand erkauft. Im einfachsten Fall wird eine zusätzliche XOR-Operation über eine orthogonale Datenzeile berechnet, siehe Grafik. Auch die zweite Parität wird rotierend auf alle Platten verteilt. Eine andere RAID-6-Implementierung rechnet mit nur einer Datenzeile, produziert allerdings keine Paritätsbits, sondern einen Zusatzcode, der 2 Einzelbit-Fehler beheben kann. Das Verfahren ist rechnerisch aufwändiger. Zum Thema Mehrbit-Fehlerkorrektur siehe auch Reed-Solomon-Code.
Für alle RAID-6-Implementierungen gilt gemeinsam: Der Performance-Malus bei Schreiboperationen (Write Penalty) ist bei RAID 6 etwas größer als bei RAID 5, die Leseleistung ist bei gleicher Gesamtplattenzahl geringer (eine Nutzdatenplatte weniger) beziehungsweise der Preis pro nutzbarem Gigabyte erhöht sich um eine Festplatte je RAID-Verbund, also im Schnitt um ein Siebtel bis zu ein Fünftel. Ein RAID-6-Verbund benötigt mindestens vier Festplatten.
Bei RAIDn handelt es sich um eine Entwicklung der Inostor Corp., einer Tochter von Tandberg Data. RAIDn hebt die bisher starre Definition der RAID-Level auf.
Dieses RAID wird definiert durch die Gesamtzahl der Festplatten (n) sowie die Anzahl der Festplatten, die ohne Datenverlust ausfallen dürfen (m). Als Schreibweise hat sich RAID(n,m) oder RAID n+m eingebürgert.
Kapazität = (n − m) × Kapazität der EinzelplatteEinige spezielle Definitionen wurden wie folgt festgelegt:
RAID DP (double, dual oder manchmal diagonal parity) ist eine von NetApp weiterentwickelte Version von RAID 4. Hierbei wird eine zweite Parität nach derselben Formel wie die erste Parität P berechnet, jedoch mit anderen Datenblöcken. Die erste Parität wird horizontal, die zweite Parität Q diagonal berechnet. Zudem wird bei der Berechnung der diagonalen Parität jeweils die erste Parität mit einbezogen, dafür aber abwechselnd eine Festplatte nicht. Da in einem RAID DP zwei beliebige Festplattenfehler kompensiert werden können, ist die Verfügbarkeit eines solchen Systems gegenüber einer Single-Paritätslösung (also z. B. RAID 4 oder RAID 5) gesteigert.
RAID-DP-Sets bestehen in der Regel aus 14 + 2 Platten. Somit liegt der Brutto-Netto-Verschnitt ähnlich niedrig wie bei RAID 4/RAID 5.
  RAID DP vereinfacht die Wiederherstellung. Hierbei werden zuerst mit der diagonalen Parität die Daten der ersten ausgefallenen Festplatte berechnet und danach aus der horizontalen Parität der Inhalt der zweiten Festplatte.
Die Rechenoperationen beschränken sich im Gegensatz zum RAID 6, wo ein Gleichungssystem zu lösen ist, auf einfache XOR-Operationen. RAID DP kann jederzeit auf RAID 4 umgeschaltet werden (und umgekehrt), indem man einfach die zweite Paritätsplatte abschaltet (bzw. wiederherstellt). Dies geschieht ohne ein Umkopieren oder Umstrukturieren der bereits gespeicherten Daten im laufenden Betrieb.
Details zu RAID DP können in der USENIX Veröffentlichung Row-Diagonal Parity for Double Disk Failure Correction gefunden werden.RAID DP erfüllt die SNIA-RAID-6-Definition.
Streng genommen handelt es sich bei NRAID (= Non-RAID: kein eigentlicher RAID) nicht um ein wirkliches RAID – es gibt keine Redundanz. Bei NRAID (auch als linear mode oder concat(enation) bekannt) werden mehrere Festplatten zusammengeschlossen. Die von einigen RAID-Controllern angebotene NRAID-Funktion ist mit dem klassischen Herangehen über einen Logical Volume Manager (LVM) zu vergleichen und weniger mit RAID 0, denn im Gegensatz zu RAID 0 bietet NRAID kein Striping über mehrere Platten hinweg und daher auch keinen Gewinn beim Datendurchsatz. Dafür kann man Festplatten unterschiedlicher Größe ohne Speicherverlust miteinander kombinieren (Beispiel: eine 10-GB-Festplatte und eine 30-GB-Festplatte ergeben in einem NRAID eine virtuelle 40-GB-Festplatte, während in einem RAID 0 nur 20 GB (10 + 10 GB) genutzt werden könnten).
Da es bei dem zugrunde liegenden linear mode keine Stripes gibt, wird bei einem solchen Verbund erst die erste Festplatte mit Daten gefüllt und erst dann, wenn weiterer Platz benötigt wird, kommt die zweite Platte zum Einsatz. Reicht auch diese nicht aus wird – falls vorhanden – die nächste Platte beschrieben. Folglich gibt es bei einem Ausfall einer Platte zwei Möglichkeiten: Zum einen kann diese (noch) keine Daten enthalten, dann gehen – je nach Implementierung der Datenwiederherstellung – möglicherweise auch keine Daten verloren. Zum anderen kann die defekte Platte bereits Daten enthalten haben, dann hat man auch hier den Nachteil, dass der Ausfall der einzelnen Platte den gesamten Verbund beschädigt. Das fehlende Striping erleichtert aber auch das Wiederherstellen einzelner nicht betroffener Dateien. Im Unterschied zu RAID 0 führt der Ausfall einer Platte hier also nicht unbedingt zu einem kompletten Datenverlust, zumindest solange sich die Nutzdaten komplett auf der noch funktionierenden Platte befinden.
NRAID ist weder einer der nummerierten RAID-Levels, noch bietet es Redundanz. Man kann es aber durchaus als entfernten Verwandten von RAID 0 betrachten. Beide machen aus mehreren Festplatten eine einzige logische Einheit, deren Datenkapazität – mit denen für RAID 0 genannten Einschränkungen – der Summe der Kapazitäten aller verwendeten Platten entspricht. Heutzutage sind Controller, die mit der Eigenschaft NRAID verkauft werden, in der Lage, dies zu tun. Die Platten, die auch von unterschiedlicher Größe sein können, werden einfach aneinandergehängt. Im Unterschied zu RAID 0 werden allerdings keine Stripesets gebildet. Es gibt weder Ausfallsicherheit, noch Performancegewinn. Der Vorteil liegt im Wesentlichen in der Größe des resultierenden Laufwerks, so wie in einer etwas verbesserten Situation bei der Datenrettung. Ein Vorteil im Vergleich zu einer LVM-Lösung ist, dass es bei NRAID problemlos möglich ist vom RAID-Verbund zu Booten.
Da die meisten modernen Betriebssysteme mittlerweile über einen Logischen Volume Manager (LVM, wird manchmal auch als Manager für dynamische Datenträger bezeichnet) verfügen, ist es aber oft sinnvoller diesen zu benutzen. Der im Betriebssystem integrierte LVM birgt praktisch keine messbaren Performancenachteile und arbeitet unabhängig von spezieller Hardware, er kann daher auch Festplatten verschiedener Typen (SCSI, SATA, USB, iSCSI, AoE, uvm.) miteinander zusammenfassen. Auch braucht man bei einem defekten RAID-Controller nicht nach einem baugleichen Modell zu suchen, die Platten können in der Regel einfach an jeden beliebigen Controller mit gleichem Festplatten-Interface angeschlossen werden. Die Wiederherstellung erfolgt dann über das jeweilige Betriebssystem. Wurde allerdings (falls dies überhaupt möglich ist) direkt vom zusammengesetzten logischen Volume gebootet, kann dies die Wiederherstellung enorm erschweren.
Von VIA wird in seiner RAID-Konfiguration unter anderem die Option SPAN angeboten. Sie dient zur Kapazitätserweiterung ohne Leistungsgewinn wie bei NRAID. Während bei RAID 0 (Striping) die Daten gleichzeitig auf mehrere Festplatten verteilt werden, gelangen die Daten bei SPAN zusammenhängend auf eine Festplatte. Bei RAID 0 sollten nach Möglichkeit gleich große Festplatten verwendet werden, da die überschüssige Kapazität des größeren Datenträgers verlorengeht. Bei SPAN sind unterschiedlich große Festplatten ohne Kapazitätsverlust zu einer großen Festplatte zusammenfassbar, was Linear Mode oder NRAID entspricht.
JBOD bedeutet Just a Bunch of Disks, also: Nur ein Haufen Festplatten. Bei JBOD fehlt die Redundanz, daher gehört es nicht zu den RAID-Systemen, es ist nur ein einfaches „Array of Independent Disks“. Der Begriff wird in der Praxis in Abgrenzung zu RAID-Systemen auf drei verschiedene Arten verwendet:
Konfiguration eines RAID-Controllers mit mehreren Festplatten, die keinen Verbund bilden. Viele Hardware-RAID-Controller sind in der Lage, die angeschlossenen Festplatten dem Betriebssystem einzeln zur Verfügung zu stellen; die RAID-Funktionen des Controllers werden dabei abgeschaltet und er arbeitet als einfacher Festplatten-Controller.
Ein JBOD kann auch, unabhängig vom Controller, eine auf beliebige Arten an den Computer angeschlossene Anzahl von Festplatten bezeichnen. Mithilfe einer Volume Management Software kann ein solches JBOD zu einem gemeinsamen logischen Volume zusammengeschaltet werden.
Konfiguration eines RAID-Controllers als Aneinanderreihung („concatenation“) einer oder mehrerer Festplatten, die so als ein einziges Laufwerk erscheinen. Es ist jedoch auch möglich, eine Festplatte in mehrere logische Datenträger aufzuteilen, um diese für das Betriebssystem als mehrere Festplatten erscheinen zu lassen, zum Beispiel um Kapazitätsgrenzen zu umgehen. Diese Konfiguration ist identisch mit NRAID oder SPAN und ist genau genommen auch kein RAID-System.
Obwohl die RAID-Level 0, 1 und 5 die weitaus größte Verwendung finden, existieren neben den Leveln 0 bis 6 noch „RAID-Kombinationen“. Hier wird ein RAID nochmals zu einem zweiten RAID zusammengefasst. Beispielsweise können mehrere Platten zu einem parallelen RAID 0 zusammengefasst werden und aus mehreren dieser RAID-0-Arrays zum Beispiel ein RAID-5-Array gebildet werden. Man bezeichnet diese Kombinationen dann etwa als RAID 05 (0+5). Umgekehrt würde ein Zusammenschluss von mehreren RAID-5-Arrays zu einem RAID-0-Array als RAID 50 (oder RAID 5+0) bezeichnet werden. Auch RAID-1- und RAID-5-Kombinationen sind möglich (RAID 15 und RAID 51), die beliebtesten Kombinationen sind allerdings das RAID 01, bei dem je zwei Platten parallel arbeiten und dabei von zwei anderen Platten gespiegelt werden (insgesamt vier Platten), oder RAID 10, bei dem (mindestens) zwei mal zwei Platten gespiegelt werden und dabei per RAID 0 zu einem Ganzen ergänzt werden.
Ein RAID-01-Verbund ist ein RAID 1 über mehrere RAID 0. Dabei werden die Eigenschaften der beiden RAIDs kombiniert: Sicherheit (geringer als beim RAID 10) und gesteigerter Datendurchsatz.
Häufig wird behauptet, ein konventioneller, aktueller RAID-01-Verbund benötige mindestens vier Festplatten. Das ist nicht ganz richtig. Mindestens vier (oder genereller: eine gerade Anzahl ≥ 4) Festplatten werden nur für den bekannteren, klassischen RAID-10-Verbund benötigt. Aber auch mit nur drei Festplatten lässt sich auf vielen RAID-Controllern ein RAID-01-Verbund bilden. Die Vorgehensweise ist folgende: Zunächst werden die Platten (genau wie bei RAID 0) nur in fortlaufend nummerierte Chunks (= Blöcke, Nummerierung beginnend mit 1) eingeteilt, dann werden alle Chunks mit ungeraden Nummern mit dem nächsthöheren Nachbarn mit gerader Nummer gespiegelt. Die Platten werden dabei jeweils zu 50 % mit Nutzdaten belegt, die übrigen 50 % jeder Platte enthalten eine Kopie der Nutzdaten einer der anderen Platten. Die Nutzdaten sowie die gespiegelten Daten werden verteilt (striped). Bei drei Platten sieht das so aus:
Platte C: 50 % Nutzdaten + 50 % Spiegelung Nutzdaten Platte BDie Nutzdaten werden dabei ebenso wie die gespiegelten Daten RAID-0-typisch über die Platten A, B und C verteilt (striped). Bei Ausfall einer Platte sind immer noch alle Daten vorhanden. Hinsichtlich der Ausfallwahrscheinlichkeit gibt es theoretisch keinen Unterschied zu RAID 5 mit drei Festplatten. Zwei von drei Laufwerken müssen intakt bleiben, damit das System funktioniert. Im Unterschied zu RAID 5 steht bei RAID 01 mit vier Festplatten jedoch weniger Speicherplatz zur Verfügung.
Ein RAID-05-Verbund besteht aus einem RAID-5-Array, das aus mehreren striped RAID 0 besteht. Er benötigt mindestens drei RAID 0, somit mind. 6 Festplatten. Bei RAID 05 besteht fast doppelte Ausfallchance im Vergleich zu einem herkömmlichen RAID 5 aus Einzelplatten, da bei einem RAID 0 schon beim Defekt eines Laufwerkes alle Daten verloren sind.
Ein RAID-10-Verbund ist ein RAID 0 über mehrere RAID 1. Es werden dabei die Eigenschaften der beiden RAIDs kombiniert: Sicherheit und gesteigerte Schreib-/Lesegeschwindigkeit.
Während die RAID-1-Schicht einer RAID-0+1-Implementation nicht in der Lage ist, einen Schaden in einem untergeordneten RAID 0 differenziert den einzelnen Festplatten zuzuordnen, bietet RAID 10 gegenüber RAID 0+1 eine bessere Ausfallsicherheit und schnellere Rekonstruktion nach einem Plattenausfall, da nur ein Teil der Daten rekonstruiert werden muss. Auch hier hat man wie bei RAID 0+1 nur die Hälfte der gesamten Festplattenkapazität zur Verfügung.
RAID 1.5 ist eigentlich kein eigenes RAID-Level, sondern ein durch das Unternehmen Highpoint eingeführter Ausdruck für eine RAID-1-Implementierung mit gesteigerter Performance. Es nutzt typische Vorteile einer RAID-0-Lösung auch bei RAID 1. Die Optimierungen können im Unterschied zu RAID 10 bereits mit nur zwei Festplatten verwendet werden. Dabei werden die beiden Platten in einfacher Geschwindigkeit wie bei RAID 1 üblich gespiegelt beschrieben, während beim Lesen beide Platten mit hohem Datendurchsatz wie bei RAID 0 genutzt werden. Die RAID-1.5-Erweiterungen, nicht zu verwechseln mit RAID 15, werden allerdings nicht nur von Highpoint implementiert. Versierte RAID-1-Implementierungen, wie die unter Linux oder Solaris, lesen ebenfalls von allen Platten und verzichten auf den Ausdruck RAID 1.5, der keinen Extravorteil bietet.
Das RAID-15-Array wird gebildet, indem man mindestens drei RAID-1-Arrays als Bestandteile für ein RAID 5 verwendet; es ist im Konzept ähnlich wie RAID 10, außer dass das Striping mit einer Parität erfolgt.
Bei einem RAID 15 mit acht Festplatten dürfen bis zu drei beliebige Platten gleichzeitig ausfallen (insgesamt bis zu fünf, sofern maximal ein Mirrorset komplett ausfällt).
Der Datendurchsatz ist gut, aber nicht sehr hoch. Die Kosten sind mit denen anderer RAID-Systeme nicht direkt vergleichbar, dafür ist das Risiko eines kompletten Datenverlustes recht gering.
Beim RAID 1E werden einzelne Datenblöcke auf die jeweils nächste Festplatte gespiegelt. Es dürfen hierbei weder zwei benachbarte noch die erste und die letzte Festplatte gleichzeitig ausfallen. Für ein RAID 1E wird immer eine ungerade Anzahl von Festplatten benötigt. Die nutzbare Kapazität reduziert sich um die Hälfte.
Es gibt allerdings noch andere Versionen von RAID 1E, die flexibler sind als die hier dargestellte Variante.
Bei einem RAID 1E0 werden mehrere RAID 1E mit einem RAID 0 zusammengeschaltet. Die maximale Anzahl der redundanten Platten und die Nettokapazität entspricht dem zugrundeliegenden RAID 1E.
RAID 30 wurde ursprünglich von American Megatrends entwickelt. Es stellt eine Striped-Variante von RAID 3 dar (das heißt ein RAID 0, welches mehrere RAID 3 zusammenfasst).
Ein RAID-30-Verbund benötigt mindestens sechs Festplatten (zwei Legs mit je drei Festplatten). Es darf eine Festplatte in jedem Leg ausfallen.
Ein RAID-45-Verbund fasst, ähnlich dem RAID 55, mehrere RAID 4 mit einem RAID 5 zusammen. Man benötigt hierfür mindestens drei RAID-4-Legs zu je drei Festplatten und damit neun Festplatten. Bei neun Festplatten sind nur vier Festplatten nutzbar, das Verhältnis verbessert sich allerdings mit der Anzahl der verwendeten Festplatten. RAID 45 wird daher nur in großen Festplattenverbänden eingesetzt. Die Ausfallsicherheit ist sehr hoch, da mindestens drei beliebige Festplatten, zusätzlich eine Festplatte in jedem Leg und dazu noch ein komplettes Leg ausfallen dürfen.
Ein RAID-50-Verbund benötigt mindestens sechs Festplatten, beispielsweise zwei RAID-5-Controller mit jeweils drei Platten pro Controller zusammengeschaltet mit einem Software-Stripe RAID 0. Das garantiert einen sehr hohen Datendurchsatz beim Schreiben und Lesen, da die Rechenarbeit auf zwei XOR-Units verteilt wird.
Bei einem RAID-50-Verbund mit sechs Festplatten darf nur eine Platte gleichzeitig ausfallen (insgesamt bis zu zwei, sofern die beiden Platten nicht zum selben RAID-5-Verbund gehören).
Ein RAID-50-Verbund wird bei Datenbanken verwendet, bei denen Schreibdurchsatz und Redundanz im Vordergrund stehen.
Der RAID-51-Verbund wird ähnlich wie RAID 15 gebildet, indem man die gesamte Reihe eines RAID 5 spiegelt, und ist ähnlich wie RAID 01, abgesehen vom Paritätsschutz.Bei einem Sechs-Festplatten-RAID-51 dürfen bis zu drei beliebige gleichzeitig ausfallen. Darüber hinaus dürfen vier Festplatten ausfallen, solange nur eine Platte aus dem gespiegelten RAID-5-Verbund betroffen ist.
Die Datenübertragungs-Leistung ist gut, aber nicht sehr hoch. Die Kosten sind mit denen anderer RAID-Systeme nicht direkt vergleichbar.
Der RAID-55-Verbund wird ähnlich wie RAID 51 gebildet, indem mehrere RAID-5-Systeme über ein weiteres RAID 5 zu einem RAID 55 zusammengeschaltet werden. Im Gegensatz zu RAID 51 ist der Overhead geringer und es ist möglich, schneller die Daten zu lesen.
Bei einem Neun-Festplatten-RAID-55-System dürfen bis zu drei beliebige Festplatten gleichzeitig ausfallen. Insgesamt dürfen maximal fünf Festplatten ausfallen (3+1+1). Ein RAID-55-Verbund benötigt mindestens neun Festplatten (drei Legs zu je drei Festplatten). Die Datenübertragungs-Geschwindigkeit ist gut, aber nicht sehr hoch. Die Kosten sind mit denen anderer RAID-Systeme nicht direkt vergleichbar.
RAID 5E ist die Abkürzung für RAID 5 Enhanced. Es kombiniert ein RAID 5 mit einem Hot-Spare. Der Hot-Spare wird dabei allerdings nicht als getrenntes Laufwerk ausgeführt, sondern auf die einzelnen Platten aufgeteilt. Anders ausgedrückt wird auf jeder Platte Speicherplatz für den Fall eines Ausfalles reserviert. Sollte eine Festplatte ausfallen, wird der Inhalt dieser Platte im freien Speicherplatz mit Hilfe der Parität wiederhergestellt und das Array kann als RAID 5 weiterbetrieben werden.
Der Vorteil liegt nicht in einer gesteigerten Sicherheit gegenüber RAID 5, sondern in der höheren Geschwindigkeit durch ständige Nutzung aller vorhandenen Plattenspindeln, inklusive der üblicherweise leer mitlaufenden Hot-Spare-Platte.
Die Technik wird schon lange bei IBM für RAID-Controller eingesetzt, jedoch immer mehr durch RAID 5EE ersetzt.
RAID 5EE arbeitet ähnlich wie RAID 5E. Allerdings wird hierbei der freie Speicherplatz nicht am Ende der Festplatten reserviert, sondern ähnlich der RAID-5-Parität über die Platten diagonal verteilt. Dadurch bleibt beim Ausfall eine höhere Übertragungsgeschwindigkeit bei der Wiederherstellung der Daten.
RAID 5DP ist die von Hewlett-Packard verwendete Bezeichnung der Implementierung für RAID 6 in den Speicher-Systemen der VA-Baureihe. Durch die Übernahme von Compaq AG durch Hewlett Packard ging die für die durch Compaq entwickelte RAID-6-Variante RAID ADG für die Compaq Smart Arrays ebenfalls in das geistige Eigentum von Hewlett Packard über. Das Akronym ADG steht hier für Advanced Data Guarding.
Ein RAID-60-Verbund besteht aus einem RAID-0-Array, das mehrere RAID 6 kombiniert. Hierzu sind mindestens zwei Controller mit je vier Festplatten, also gesamt acht Festplatten, notwendig. Prinzipiell skalieren sich die Unterschiede von RAID 5 und RAID 6 hoch auf die Unterschiede zwischen RAID 50 und RAID 60: Der Durchsatz ist geringer, während die Ausfallsicherheit höher ist. Der gleichzeitige Ausfall von zwei beliebigen Laufwerken ist jederzeit möglich; weitere Ausfälle sind nur dann unkritisch, wenn maximal zwei Platten je gestriptem RAID 6 betroffen sind.
Ab der Intel ICH6R-Southbridge ist seit etwa Mitte 2004 erstmals eine Technik integriert, die als „Matrix-RAID“ vermarktet wird und die Idee von RAID 1.5 aufgreift. Sie soll die Vorteile von RAID 0 und RAID 1 auf nur zwei Festplatten vereinen. Jede der beiden Platten wird vom Controller zu diesem Zweck in zwei Bereiche aufgeteilt. Ein Bereich wird dann auf die andere Festplatte gespiegelt, während im verbleibenden Bereich die Daten auf beide Platten aufgeteilt werden. Man kann dann zum Beispiel im aufgeteilten Bereich sein „unwichtiges“ Betriebssystem und Programme installieren, um von RAID 0 zu profitieren, während man im gespiegelten Bereich dann seine wichtigen Daten abspeichern kann und auf die Redundanz von RAID 1 vertrauen kann. Im Falle eines Plattencrashes müsste man dann nur sein Betriebssystem und Programme neu aufspielen, während die wichtigen Daten im anderen Festplattenbereich erhalten bleiben.
Mit mehreren Festplatten kann man in einem Matrix-RAID auch andere RAID-Typen einsetzen und beispielsweise ab drei Festplatten eine Partition als RAID 5 betreiben.
RAID S beziehungsweise Parity RAID, manchmal auch als RAID 3+1, RAID 7+1 oder RAID 6+2 beziehungsweise RAID 14+2 bezeichnet, ist ein proprietäres Striped-Parity-RAID des Herstellers EMC. Ursprünglich nannte EMC diese Form RAID S bei den Symmetrix-Systemen. Seit dem Marktauftritt der neuen Modelle DMX heißt diese RAID-Variante Parity-RAID. Inzwischen bietet EMC auch Standard-RAID-5 an. Laut Angaben von EMC dürfen bei Parity-RAID-„+2“-Typen bis zu zwei Festplatten ausfallen.
RAID S stellt sich wie folgt dar: Ein Volume ist jeweils auf einem physischen Laufwerk, mehrere Volumes (meistens drei bzw. sieben) werden willkürlich zu Paritätszwecken kombiniert. Dies ist nicht genau identisch mit RAID 3, 4 oder 5, denn bei RAID S geht es immer um zahlreiche (evtl. 100 oder 1.000) Laufwerke die nicht alle gemeinsam einen einzelnen Verbund bilden. Vielmehr bilden wenige Platten (typisch: 4 bis 16) einen RAID-S-Verbund, ein oder mehrere dieser Verbünde bilden logische Einheiten – eine Ähnlichkeit zu den RAID-Leveln 50 oder 60 und RAID-Z ist unverkennbar. Außerdem werden bei RAID 5 die auf physischen Laufwerken befindlichen Volumes, abweichend Chunks genannt. Mehrere Chunks für Daten werden zusammen mit einem Paritäts-Chunk zu einem Stripe zusammengefasst. Aus beliebig vielen Stripes wird dann die Basis für eine Partition oder ein logisches Volume gebildet.
Ein Parity RAID 3+1 beinhaltet drei Daten-Volumes und ein Paritäts-Volume. Hiermit ist eine Nutzung von 75 % der Kapazität möglich. Beim Parity RAID 7+1 hingegen sind sieben Daten-Volumes und ein Paritäts-Volume vorhanden. Hiermit ist jedoch bei geringerer Ausfallsicherheit eine Nutzung von 87,5 % der Kapazität möglich. Bei normalem RAID 5 aus vier Platten beinhaltet ein Stripe drei Chunks mit Daten und einen Paritäts-Chunk. Bei normalem RAID 5 aus acht Platten beinhaltet ein Stripe dann sieben Chunks mit Daten und ebenfalls einen Paritäts-Chunk.
Darüber hinaus bietet EMC noch als Option für diese RAID-Varianten die Hypervolume Extension (HVE) an. HVE erlaubt mehrere Volumes auf demselben physischen Laufwerk.
Hinweis: A1, B1 etc. stellen einen Datenblock dar; jede Spalte stellt eine Festplatte dar. A, B etc. sind gesamte Volumes.
RAID TP beziehungsweise RAID Triple Parity ist ein proprietäres RAID mit dreifacher Parität vom Hersteller easyRAID. Laut Herstellerangaben können bei RAID TP ohne Datenverlust bis zu drei Festplatten ausfallen. Eine weitere Triple Parity Implementierung stammt von Sun Microsystems und wird als Triple-Parity RAID-Z oder RAID-Z3 vermarktet. Diese in das ZFS integrierte Version nutzt zur Absicherung der Daten einen Reed-Solomon-Code, auch hier können ohne Datenverlust bis zu drei Festplatten eines RAID-Verbunds defekt sein.
Beim RAID TP von easyRAID werden die Datenblöcke und die Paritäten zeitgleich jeweils auf die einzelnen physischen Festplatten geschrieben. Die drei Paritäten werden auf verschiedene Stripes auf unterschiedlichen Platten abgelegt. Der RAID-Triple-Parity-Algorithmus benutzt einen speziellen Code mit einem Hamming-Abstand von mindestens 4.
Hinweis: A1, B1 etc. stellen einen Datenblock dar; jede Spalte stellt eine Festplatte dar. A, B etc. sind gesamte Volumes.
Hierzu benötigt man mindestens vier Festplatten. Die Kapazität errechnet sich aus Festplattenanzahl minus drei.
RAID-Z ist ein von Sun Microsystems im Dateisystem ZFS integriertes RAID-System. ZFS ist ein weiterentwickeltes Dateisystem, welches zahlreiche Erweiterungen für die Verwendung im Server- und Rechenzentrumsbereich enthält. Hierzu zählen die enorme maximale Dateisystemgröße, eine einfache Verwaltung selbst komplexer Konfigurationen, die integrierten RAID-Funktionalitäten, das Volume-Management sowie der prüfsummenbasierte Schutz vor Platten- und Datenübertragungsfehlern. Bei redundanter Speicherung ist so eine automatische Fehlerkorrektur möglich. Die Integration der RAID-Funktionalität in das Dateisystem hat den Vorteil, dass Blockgrößen des Dateisystems und der RAID-Volumes aufeinander abgestimmt werden können, wodurch sich zusätzliche Optimierungsmöglichkeiten ergeben. Das ins Dateisystem integrierte RAID-Subsystem bietet gegenüber klassischen Hardware- oder Software-RAID-Implementierungen den Vorteil, dass durch das integrierte RAID-System zwischen belegten und freien Datenblöcken unterschieden werden kann und somit bei der Rekonstruktion eines RAID-Volumens nur belegter Plattenplatz gespiegelt werden muss, woraus im Schadensfall, besonders bei wenig gefüllten Dateisystemen, eine enorme Zeitersparnis resultiert. Elementare redundante Einheiten nennt das ZFS Redundancy Groups: Diese sind als Verbünde aus RAID 1, RAID Z1 (~RAID 5) und RAID Z2 (~RAID 6) realisiert. Eine oder mehrere Redundancy Groups bilden (analog zu kombiniertem RAID 0) zusammen ein ZFS-Volume (oder ZFS-Pool), aus dem dynamisch „Partitionen“ angefordert werden können. RAID-Z1 arbeitet analog zu RAID 5, gegenüber einem traditionellen RAID-5-Array ist RAID-Z1 gegen Synchronisationsprobleme („write hole“) geschützt und bietet daher Performance-Vorteile – analog gilt dies auch für RAID-Z2 und RAID 6. Seit Juli 2009 ist auch RAID-Z3, also eine RAID-Z-Implementierung mit drei Paritätsbits, verfügbar. Der Begriff write hole bezeichnet eine Situation, die bei Schreibzugriffen entsteht, wenn die Daten bereits auf die Festplatten geschrieben wurden, die dazugehörige Paritätsinformation aber noch nicht. Sollte während dieses Zustands ein Problem beim Berechnen oder Schreiben der Paritätsinformation auftreten, passen diese nicht mehr zu den gespeicherten Datenblöcken.
   an. Dies entspricht der Anzahl der benötigten Festplatten ohne RAID, die die gleiche Speicherkapazität aufweisen.
   gibt an, wie viele Festplatten ausfallen dürfen, ohne dass ein Datenverlust auftritt. Zu beachten ist, dass es insbesondere bei den Kombinations-RAIDs einen Unterschied geben kann zwischen der Anzahl der Festplatten, die auf jeden Fall ausfallen können (
Ein Leg (englisch für Bein) oder lower level RAID ist ein RAID-Array, welches mit anderen gleichartigen Legs über ein übergeordnetes RAID-Array (upper level RAID) zusammengefasst wird. Hierbei ist in unten stehender Tabelle 
   die Anzahl der Legs im übergeordneten Array (sofern das RAID tatsächlich kombiniert ist).Anmerkung: Die RAIDs 3 und 4 können prinzipiell auch mit zwei Festplatten benutzt werden, allerdings erhält man dann exakt die gleiche Ausfallsicherheit wie mit RAID 1 bei der gleichen Anzahl Festplatten. Dabei ist aber RAID 1 technisch einfacher und würde in dem Fall immer bevorzugt werden. Dasselbe trifft für übergeordnete Arrays oder Legs in Kombinations-RAIDs zu.
   angegebenen Fälle, welche Geräte genau ausfallen, dienen zur anschaulichen Darstellung. Die Werte geben lediglich an, dass in jedem beliebigen Fall genau diese Anzahl an Geräten ausfallen kann, ohne dass Daten verloren gehen. Die Angabe erhebt nicht den Anspruch darauf, dass in dem speziellen Fall nicht noch weitere Festplatten ohne Datenverlust ausfallen können.
Der Cache-Speicher spielt bei RAID eine große Rolle. Grundsätzlich sind folgende Caches zu unterscheiden:
Enterprise Disk-ArrayEine Schreibanforderung wird heute üblicherweise bereits quittiert, wenn die Daten im Cache angelangt sind und somit bevor die Daten tatsächlich permanent gespeichert wurden; weiterhin kann es vorkommen, dass der Cache nicht in der Reihenfolge bereinigend entleert wird, in der er gefüllt wurde; hierdurch kann ein Zeitraum entstehen, in dem bereits als gespeichert angenommene Daten durch einen Strom- oder Hardware-Ausfall verloren gehen können, was zu fehlerhaften Datei-Inhalten führen kann (etwa wenn das Dateisystem von einer Verlängerung der Datei ausgeht, obwohl aber die entsprechenden Daten noch gar nicht geschrieben wurden).
In Enterprise-Speichersystemen überlebt der Cache daher Resets. Der Schreib-Cache bringt einen Geschwindigkeitsgewinn, solange der Cache (RAM) nicht voll ist, oder solange die Schreibanforderungen in suboptimaler Reihenfolge oder überlappend eingehen, da das Schreiben in den Cache schneller ist als das Schreiben auf Platte.
Der Lese-Cache ist heute in Datenbank-Anwendungen oft von großer Bedeutung, da hierdurch fast nur noch zum Schreiben auf das langsame Speichermedium zugegriffen werden muss.
Die Pufferbatteriefunktion ist nicht zu verwechseln mit einer unterbrechungsfreien Stromversorgung. Diese schützt zwar auch vor Stromausfall, kann aber keine Systemabstürze oder das Einfrieren des Systems verhindern. Höherwertige RAID-Controller bieten daher die Möglichkeit, den eigenen Cachespeicher vor Ausfall zu schützen. Dies wird klassisch durch eine Pufferbatterie für den Controller oder bei neueren Systemen mit einem nichtflüchtigen NAND-Flash-Speicher und hochkapazitativen Kondensatoren erreicht. Diese Sicherung des Cachespeichers soll dafür sorgen, dass Daten, die bei einem Stromausfall oder einem Systemausfall im Cache liegen und noch nicht auf die Platten geschrieben wurden, nicht verloren gehen und damit das RAID konsistent bleibt. Die Pufferbatterie wird häufig als BBU (Battery Backup Unit) oder BBWC (Battery Backed Write Cache) bezeichnet. Vor allem für performanceoptimierte Datenbanksysteme ist das zusätzliche Absichern des Cache wichtig, um gefahrlos den Schreibcache zu aktivieren. Den Cachespeicher zu schützen ist für diese Systeme besonders wichtig, da bei einem eventuellen Beschädigen der Datenbank Daten verloren gehen können und dieses nicht sofort erkannt werden kann. Da aber in den Datenbanken kontinuierlich weitergearbeitet wird, können die Daten im Fall, dass ihr Verlust später bemerkt wird, auch nicht einfach durch Rücksicherung aus dem (hoffentlich vorhandenen) Backup wiederhergestellt werden, da die nachfolgenden Änderungen dann verloren wären. Um optimal zu funktionieren, setzt dies voraus, dass der in den Festplatten eingebaute und nicht per Batterie abgesicherte Cache der Festplatten deaktiviert ist. Würde der Plattencache zusätzlich zum Controllercache Daten vor dem Schreiben zwischenspeichern, würden diese beim Systemausfall natürlich verloren gehen.
Die Funktionen eines Logical Volume Managers (LVM) werden oft mit denen eines Software-RAID-Systems vermischt. Das liegt wohl hauptsächlich daran, dass die Verwaltung beider Subsysteme meist über eine gemeinsame grafische Benutzeroberfläche erfolgt. Dabei gibt es eine klare Abgrenzung. Echte RAID-Systeme bieten immer Redundanz (außer RAID 0) und verfügen folglich auch immer über eine RAID-Engine, welche die zusätzlichen, für die Redundanz benötigten Daten erzeugt. Die häufigsten Engine-Varianten sind bei RAID 1 die Datenduplizierung und bei RAID 5 und den meisten anderen Verfahren die XOR-Bildung. Es werden bei RAID also immer zusätzliche Datenströme in erheblichem Umfang erzeugt, der Datendurchsatz der RAID-Engine ist daher ein wichtiger Performancefaktor. Aufgabe eines LVM ist es, physische Datenträger (oder Partitionen) auf logische Datenträger abzubilden. Einer der häufigsten Anwendungsfälle ist das nachträgliche Vergrößern von Partitionen und Dateisystemen, die durch den LVM verwaltet werden. Ein LVM erzeugt hierbei aber keine zusätzlichen Datenströme, er hat auch keine Engine und bietet daher auch keine Redundanz, somit erzeugt er auch nur minimalen Rechenaufwand. Daher hat er auch praktisch keinen Performance-Einfluss (wenngleich auch einige LVM-Implementierungen integrierte RAID-0-Erweiterungen besitzen). Die Aufgabe des LVMs besteht im Wesentlichen also darin, Datenströme aus den Dateisystemen auf die jeweils zugehörigen Datenträgerbereiche zu verteilen, sie ähnelt am ehesten der Arbeitsweise einer MMU. In einigen Systemen (z. B. HP-UX oder Linux) sind Software-RAID und LVM optionale Erweiterungen und können völlig unabhängig voneinander installiert und genutzt werden. Manche Hersteller lizenzieren daher das Volume-Management und RAID (Mirroring und/oder RAID 5) auch separat.
Zur Bestimmung der Leistungsfähigkeit eines Festplattensubsystems gibt es zwei wesentliche Parameter, die Anzahl der bei zufälligem Zugriff möglichen I/O-Operationen pro Zeit, also IOPS (I/O pro Sekunde), und der Datendurchsatz bei sequentiellem Zugriff gemessen in MB/s. Die Leistung eines RAID-Systems ergibt sich aus der kombinierten Leistung der verwendeten Festplatten.
Die Anzahl der IOPS leitet sich direkt von der mittleren Zugriffszeit und der Drehzahl einer Festplatte ab. Bei SATA-Platten liegt die Zugriffszeit bei 8 bis 10 ms, eine halbe Umdrehung dauert je nach Drehzahl etwa 2 bis 4 ms. Vereinfacht ist das gleichbedeutend mit einer Dauer für einen nicht sequentiellen Zugriff von gerundet 10 ms. Hieraus ergibt sich, dass pro Sekunde maximal 100 I/Os möglich sind, also ein Wert von 100 IOPS. Leistungsstärkere SCSI- oder SAS-Platten haben Zugriffszeiten von unter 5 ms und arbeiten mit höheren Drehzahlen, daher liegen deren I/O-Werte bei ungefähr 200 IOPS. Bei Systemen, die viele gleichzeitige Benutzer (oder Tasks) abzuarbeiten haben, ist der IOPS-Wert eine besonders wichtige Größe.
Der sequentielle Datendurchsatz ist ein Wert, der im Wesentlichen von der Drehzahl und der Schreibdichte und der Anzahl der beschriebenen Oberflächen einer Festplatte abhängt. Erst beim wiederholten Zugriff hat auch der Platten-Cache einen leistungssteigernden Einfluss. Bei gleicher Konfiguration (Schreibdichte, Oberflächen) liefert eine aktuelle Platte mit 10.000 Umdrehungen pro Minute also etwa doppelt so schnell Daten wie eine einfache IDE-Platte, die mit 5.200 Umdrehungen pro Minute arbeitet. Beispielsweise liefert die Samsung Spinpoint VL40 mit 5.400 min−1 einen Datenstrom mit im Mittel etwa 33 MB/s, die WD Raptor WD740GD mit 10.000 min−1 hingegen schafft im Mittel 62 MB/s.
Bei kleinen Chunk-Größen (2 kB) nutzt ein RAID-0-Verbund aus zwei Platten praktisch bei jedem Zugriff beide Platten, also verdoppelt sich der Datendurchsatz (pro I/O-Prozess), jedoch bleiben die IOPS unverändert. Bei großen Chunks hingegen (32 kB) wird für einen einzelnen Zugriff meist nur eine Platte genutzt, folglich kann ein weiterer I/O auf der zweiten Platte stattfinden, somit verdoppeln sich die IOPS-Werte, wobei der Datenstrom (pro I/O-Prozess) gleich bleibt und sich nur bei Systemen mit vielen gleichzeitigen Tasks kumuliert. Besonders Server profitieren also von größeren Chunks. Systeme mit zwei Platten (Typ: 100 IOPS) im RAID 0 können theoretisch also bis zu 200 IOPS bei etwa 60 MB/s erreichen.
Moderne RAID-1-Implementierungen verhalten sich beim Schreiben wie die einzelnen verwendeten Platten. Die Chunkgröße ist für schreibende Zugriffe nicht von Belang. Beim Lesen hingegen arbeiten zwei Platten gleichzeitig, analog zu den RAID-0-Systemen. Daher haben auch bei RAID 1 die Chunkgrößen den gleichen Einfluss auf die Performance von Lesezugriffen wie bei RAID 0, auch hier profitieren besonders Server von größeren Chunks. Systeme mit zwei Platten (Typ: 100 IOPS) im RAID 1 können also theoretisch lesend bis zu 200 IOPS bei etwa 60 MB/s erreichen, schreibend bis zu 100 IOPS bei etwa 30 MB/s.
Die Kombination aus RAID 0 und RAID 1 verdoppelt die Leistung im Vergleich zu purem RAID 1. Aktuelle Systeme mit acht Platten (Typ: 100 IOPS) im RAID 10 können also theoretisch lesend bis zu 800 IOPS bei etwa 240 MB/s erreichen, schreibend bis zu 400 IOPS bei etwa 120 MB/s.
RAID 5 arbeitet mit mindestens drei Platten, wobei sich auch hier die Lese- und Schreibperformance stark unterscheidet. In minimaler RAID-5-Konfiguration mit drei Platten (Typ: 100 IOPS) ergeben sich lesend bis zu 300 IOPS. Da aber für einen Schreibzugriff bei kleinen Blöcken (< Stripe-Size) immer zwei lesende (Sektor-alt, Parity-alt) und zwei schreibende Zugriffe (Sektor-neu, Parity-neu) nötig sind, werden beim Schreiben eines RAID-5-Verbunds mit drei Platten im Mittel nur 50 bis 75 IOPS erreicht (je nachdem, ob die dritte Platte für andere Zugriffe genutzt wird), beim Schreiben kompletter Stripes kann der Lesezugriff allerdings entfallen und dann bis zu 200 IOPS erreichen. Die Schreibleistung steht also in Abhängigkeit zur linear zusammenhängenden Datenmenge (> Stripe-Size), zur Lage der Daten (Stripe-Alignment) und steht im direkten Zusammenhang zur Stripe-Size. Daher sollten zum Erzielen einer guten Schreibleistung bei Systemen mit vielen Platten die Chunk-Size niedrig gewählt werden, im Besonderen niedriger als bei Systemen, die mit wenig Platten (drei) im Verbund arbeiten. Eine Chunk-Size von 128 kB im RAID 5 mit drei Platten führt beispielsweise zu einer Stripe-Size von 256 kB, für ein RAID-5-System mit fünf Platten ergibt sich eine Stripe-Size von 256 kB jedoch aus einer Chunk-Size von 64 kB. Die Schreibleistung kann also im ungünstigen Fall durchaus geringer sein als die eines RAID-1-Verbunds oder einer einzelnen Platte, besonders wenn häufig kleinere nicht zusammenhängende Daten geschrieben werden sollen. Aktuelle Systeme mit beispielsweise acht Platten (Typ: 100 IOPS) im RAID 5 können theoretisch lesend bis zu 800 IOPS erreichen, schreibend je nach Zugriffsmuster von 200 bis zu 700 IOPS.
Bei RAID 0 hat die Anzahl der beteiligten Festplatten einen linearen Einfluss auf die Performance. Die I/O-Geschwindigkeit lesend ist bei acht Platten auch achtmal so hoch wie die einer einzelnen Platte, ebenso wie die Schreibgeschwindigkeit. Bei RAID 10 hingegen steigt nur die Lesegeschwindigkeit genau so schnell wie bei RAID 0, die Schreibgeschwindigkeit hingegen nur halb so schnell. Weitaus komplizierter verhält sich der Zugewinn bei RAID 5 (siehe vorigen Absatz). Der Zugewinn beim Schreibzugriff liegt im ungünstigen Fall bei nur einem Viertel. Im Mischbetrieb (50 % read, 50 % write) ergeben sich bei solchen Zugriffsmustern für ein RAID 10 aus acht Platten etwa 533 IOPS und für RAID 5 ungefähr 320 IOPS. RAID 6 verhält sich ganz ähnlich wie RAID 5 jedoch sind beim Schreiben die Zusammenhänge noch etwas komplexer, lesend ist der Zugewinn gleich hoch, schreibend jedoch noch niedriger. Vorteilhafterweise ist der netto zur Verfügung stehende Platz bei RAID 5 im Vergleich zu RAID 10 (bei 8 Platten im Verhältnis 7:4) deutlich höher. Aus diesen Gesichtspunkten sind RAID 5 und RAID 6 besonders dort hervorragend geeignet, wo Daten in großen Blöcken verarbeitet werden oder Lesezugriffe dominieren, etwa bei der Bild- und Videobearbeitung und generell auch bei Archiven. Hat man häufig nicht sequentielle Schreibzugriffe auf kleine Datenmengen (< Stripe-Size), zum Beispiel bei einem gut ausgelasteten Messaging- (SMS) oder Mailserver oder einer interaktiven Datenbankanwendung, ist RAID 10 die bessere Wahl.
Ein Mail-Server führt heute neben dem Mailversand häufig noch eine Virenprüfung und eine Spam-Analyse durch. Daher gehen wir davon aus, dass pro Mail inklusive Anhang im Mittel rund 25 Schreibzugriffe und etwa 50 Lesezugriffe (IOPS) notwendig sind. Weiter gehen wir davon aus, dass keine weiteren Ressourcen einen begrenzenden Einfluss haben. Das RAID-1-System mit zwei einfachen Platten (siehe weiter oben) kann folglich etwa 240 Mails pro Minute verarbeiten, das RAID-5-System mit acht Platten hingegen kommt auf maximal 480 Mails pro Minute. Als RAID-10-Konfiguration mit acht Platten sind bereits knapp 1.000 Mails pro Minute möglich. Wechselt man bei dem RAID-10-System auf Hochleistungsplatten, so kann der Durchsatz auf das Zwei- bis Dreifache gesteigert werden, also maximal etwa 2.500 Mails pro Minute.
Vielerorts wird die Frage diskutiert, ob RAID 10 (auch RAID 01) oder RAID 5 für verschiedene Anwendungen das bessere System sei. Dabei sind generell zwei Aspekte zu beachten, die Read-Performance und die Write-Performance eines Systems. Eine genauere Betrachtung lohnt eigentlich nur bei Verwendung von hochwertigen Hardware-Cache-Controllern. Bei Verwendung von Software-RAID- oder „Fake-RAID“-Systemen ist die Performance stark von anderen Faktoren wie Prozessorleistung und verfügbaren Arbeitsspeicher abhängig und daher schwer vergleichbar.
Sie ist bei beiden Systemen praktisch identisch und wird weitestgehend durch die Anzahl der Festplatten aber auch durch Cache-Größen bestimmt, mehr ist hier immer besser.
Hier verhalten sich beide Systeme unterschiedlich. Da bei RAID 10 alles doppelt geschrieben wird, reduziert sich die nutzbare Write-Performance im Vergleich zur maximal möglichen Schreibleistung, die sich ebenfalls aus der Anzahl der Platten ergibt, immer auf 50 % (Effizienz ist 50 %). Blockgrößen oder andere Parameter haben bei RAID 10 keinen Einfluss auf die Leistung. Bei RAID 5 ist der Sachverhalt komplizierter und hängt von der zu schreibenden Datenmenge ab. Ausgehend von Festplatten mit weniger als 2 TB Plattenplatz ist die atomare Blockgröße (auch Sektorgröße genannt) der Platten häufig 512 Byte (siehe Festplatte: Speichern und Lesen von Daten). Geht man weiter von einem RAID-5-Verbund mit 5 Platten (4/5 Daten und 1/5 Parität) aus, so ergibt sich folgendes Szenario: Will eine Anwendung 2.048 Byte schreiben, wird in diesem günstigen Fall auf alle 5 Platten genau je ein Block zu 512 Byte geschrieben, wobei einer dieser Blöcke keine Nutzdaten enthält. Daraus ergibt sich eine theoretische Effizienz von 80 % (bei RAID 5 mit 3 Platten wären es 66 %). Möchte eine Anwendung aber nur einen Block von 512 Byte schreiben, so ergibt sich ein ungünstigerer Fall (siehe RAID 5 write Penalty), es müssen zuerst der abzuändernde Block und der Paritätsblock eingelesen werden, danach wird der neue Paritätsblock berechnet und erst dann können beide 512-Byte-Blöcke geschrieben werden. Das bedeutet einen Aufwand von 2 Lesezugriffen und 2 Schreibzugriffen, um einen Block zu speichern; geht man vereinfacht davon aus, dass Lesen und Schreiben gleich lange dauern, so beträgt die Effizienz in diesem ungünstigsten Fall nur noch 25 %. Zusätzlich zur Blockgröße muss bei RAID-5-Systemen noch die Paritätsberechnung beachtet werden. Bei sehr schreibintensiven Zugriffen (z. B. Datenbank-Log-Dateien) führt die zeitintensive Parity-Berechnung bei RAID-5-Systemen zu einer weiteren, möglichen Geschwindigkeitseinbußen gegenüber RAID 10.
In der Praxis hat das schlechtere Worst-case-Verhalten und die zeitintensive Parityberechnung von RAID 5 merklich negative Einflüsse nur dann, wenn viele, schnelle und kleine Schreibzugriffe erfolgen. Bei Zugriffen als Fileserver überwiegen dann die Vorteile von RAID 5 der höheren Lesegeschwindigkeit und günstigeren Kosten pro Kapazität. Bei performanceorientierten Datenbanksystemen wird jedoch RAID 1 beziehungsweise 10 empfohlen da hier keine Paritätsberechnung stattfindet und die Blockgröße nicht relevant ist. Bei neueren Festplatten, deren atomare Blockgröße oft 4.096 Byte beträgt, gewinnt die schlechtere Worst-Case-Effizienz weiter an Bedeutung. Für alle performanceorientierten Systeme mit Schutz vor Datenverlust bei Plattenausfall gilt der deutlich erhöhte Kostenfaktor. Für den sicheren Betrieb wird ein hochwertiger Hardware-RAID-Controller mit entsprechender Hardware-Cacheabsicherung (BBU) benötigt. Dafür erhält man ein deutlich sichereres und schnelleres System. Viele Hersteller von Datenbanksystemen empfehlen für ihre Systeme RAID 1 oder 10, zumindest für die schreibintensiven Log-Dateien.
Mit dem Drive Extender des Microsoft Windows Home Servers findet sich eine Art virtuelles RAID, das aber auf JBOD basiert. Neue Dateien landen in der Regel zunächst auf der Systemplatte und werden dann erst später auf eine der anderen Festplatten verschoben, hinterlassen aber einen Verweis (Tombstone), der 4 kB Festplattenspeicher belegt. Der Benutzer kann dadurch arbeiten, als ob der Server über eine einzige große Festplatte verfügen würde.
Stripes (Chunks) sind Untereinheiten eines Stripe-Set, die Stripe Size bezeichnet die Größe eines Datenblocks, der auf einer Platte gespeichert wird. Alle Blöcke oder Sektoren eines Stripes liegen auf der gleichen Platte. Ein Stripe-Set setzt sich aus je einem Stripe pro Datenträger eines RAID-Verbunds zusammen. So besitzt beispielsweise ein aus vier Festplatten bestehendes RAID-0-Array mit einer Stripe Size von 256 KiB einen Stripe Set von 1 MiB. Unabhängig davon wird beim Formatieren eines Arrays die File System Block Size für das jeweilige Dateisystem gesetzt. Die Performanceauswirkungen der eingestellten Stripe (Chunk) Size im Verhältnis zu der gewählten File System Block Size sind komplex.
Mit Hot Swapping bietet sich die Möglichkeit, Speichermedien im laufenden Betrieb auszutauschen. Dazu muss der Bus-Controller Hot-Plugging unterstützen (i. d. R. nur SCSI, SAS oder SATA). Damit es nicht zu einem Ausfall des Systems führt, ist ein Austausch nur in Arrays mit redundanter Datenhaltung möglich – bei einem RAID-0-System würde das Ersetzen (bzw. Entfernen) eines Mediums unweigerlich zum Ausfall führen.
Das Hot-Spare-Laufwerk ist ein unbenutztes Reservelaufwerk. Fällt ein Laufwerk innerhalb des RAID-Verbundes aus, wird es durch das Reservelaufwerk ersetzt. Dadurch ist die Redundanz schnellstmöglich wiederhergestellt. Während der Rebuild-Phase hat man allerdings keine Redundanz. Zur Vermeidung dieses Problems kann ein RAID 6 oder RAID DP statt RAID 5 verwendet werden, da hier zwei Paritätsplatten vorhanden sind. Außerdem gibt es Speichersysteme, die intern ständig alle Plattenbereiche prüfen. Sollte ein Datenblock „dirty“ sein, so wird die Platte bis zu diesem Bereich kopiert, der Dirty-Block aus der Prüfsumme beziehungsweise der Spiegelplatte rekonstruiert und dann weiter kopiert. Dadurch kann die Wiederherstellungszeit reduziert werden.
In größeren RAID-Systemen, in denen die Möglichkeit besteht, an einem RAID-Controller mehrere unterschiedliche RAID-Arrays einzurichten, gibt es darüber hinaus auch die Möglichkeit, dass ein Hot-Spare-Laufwerk entweder einem einzelnen RAID-Array oder einer Geräteeinheit (Shelf, Enclosure) zugeordnet ist oder für die Verwendung im gesamten RAID-System zur Verfügung steht. In diesem Fall spricht man dann von einem Local-Spare-Laufwerk oder auch Dedicated Hot-Spare-Laufwerk (bei Zuordnung zu einem bestimmten Array) oder einem Global-Spare-Laufwerk (wenn das Laufwerk für alle Arrays verwendet werden kann).
Viele RAID-Controller bieten die Möglichkeit, auftretende Laufwerksfehler durch Medien-Tests oder den SMART-Status frühzeitig zu erkennen. Wenn ein Laufwerk zu viele dieser meist korrigierbaren Fehler liefert, besteht die Möglichkeit, das betroffene Laufwerk schon vor dem endgültigen Ausfall zu ersetzen.
Dazu kopiert der Controller alle vorhandenen Daten der einen Festplatte auf ein bisher unbenutztes Spare-Laufwerk. Beim Ausfall der Originalfestplatte verkürzt sich dadurch die Wiederherstellungszeit und damit auch die kritische Zeit für einen weiteren Ausfall auf ein Minimum.
Zertifizierter Standard von Intel zur Verwendung der vorhandenen Anschlüsse. Es wird nur der fehlende I/O-Controller (mit 0 Kanälen) nachgerüstet. Kostengünstige und ökonomische Variante.
Auch Hersteller von Backup-Programmen haben das Problem, dass durch defekte Medien (Bänder wie DAT, DLT, QIC usw.) Daten einer Datensicherung oder Archivierung teilweise oder ganz verloren gehen können. Daher nutzen umfangreiche Backup-Lösungen die gleichen Mechanismen zur Herstellung redundanter Datenträgersätze, wie sie auch bei Festplatten zum Einsatz kommen. Häufig werden die Daten über mehrere Bandlaufwerke verteilt auf mehreren Bändern im RAID-3- oder RAID-5-Modus gespeichert. Da sich Backups auch über viele Bänder hinweg durchführen lassen, muss dies auch bei Tape-RAID möglich sein. Die eingesetzte Software muss folglich in der Lage sein, den Bandsatz dynamisch zu erweitern. Das bedeutet, die Software muss aus zusätzlichen Bändern automatisch weitere zusätzliche RAID-Verbünde herstellen und diese automatisch an bestehende Bandsätze anhängen können.
RAIL: Eine ähnliche Implementierung ist ein Redundant Array of Independent Libraries (redundante Reihe unabhängiger Bandbibliotheken). Dabei werden die RAID-Level nicht über mehrere Bandlaufwerke verteilt gebildet, sondern über mehrere unabhängige Bandbibliotheken (Tape-Libraries) hinweg, wobei die Librarys auch auf mehrere Standorte verteilt sein können, um die Daten, beispielsweise auch im Katastrophenfall, sicher und redundant zu lagern.

Rainer Maria Rilke (* 4. Dezember 1875 in Prag, Österreich-Ungarn; † 29. Dezember 1926 im Sanatorium Valmont bei Montreux, Schweiz; eigentlich: René Karl Wilhelm Johann Josef Maria Rilke) war Lyriker deutscher Sprache. Mit seiner in den Neuen Gedichten vollendeten, von der bildenden Kunst beeinflussten Dinglyrik gilt er als einer der bedeutendsten Dichter der literarischen Moderne.Aus Rilkes Werk sind etliche Erzählungen, ein Roman und Aufsätze zu Kunst und Kultur sowie zahlreiche Übersetzungen von Literatur und Lyrik bekannt. Sein umfangreicher Briefwechsel gilt als wichtiger Bestandteil seines literarischen Schaffens.
Rilke wurde als René Karl Wilhelm Johann Josef Maria Rilke am 4. Dezember 1875 in Prag geboren, das damals, wie ganz Böhmen, zu Österreich-Ungarn gehörte und dessen Bürger die einheitliche altösterreichische  Staatsbürgerschaft besaßen. Der Vater, Josef Rilke (1839–1906), war nach gescheiterter militärischer Karriere Bahnbeamter geworden. Die Familie stammte väterlicherseits aus dem nordböhmischen Türmitz. Seine Mutter Sophie „Phia“ Entz (1851–1931) entstammte einer wohlhabenden Prager Fabrikantenfamilie. Ihre Hoffnungen auf ein vornehmes Leben fand sie in ihrer Ehe nicht erfüllt. Im Jahr 1884 brach die Ehe der Eltern auseinander.
Auch das Verhältnis zwischen der Mutter und dem einzigen Sohn war belastet, weil sie den frühen Tod der älteren Tochter nicht verkraftete, die 1874 – ein Jahr nach der Eheschließung – geboren worden und nach einer Woche gestorben war. Aus emotionaler Hilflosigkeit heraus band sie René – französisch für „der Wiedergeborene“ – an sich und drängte ihn in die Rolle seiner verstorbenen Schwester. Bis zu seinem sechsten Lebensjahr fand sich Rilke so als Mädchen erzogen, frühe Fotografien zeigen ihn mit langem Haar, im Kleidchen.
Ab dem Jahr 1885 besuchte der dichterisch und zeichnerisch begabte Junge auf Druck der Eltern eine Militärrealschule in St. Pölten in Niederösterreich zur Vorbereitung auf eine Offizierslaufbahn. Die Zumutungen militärischen Drills und die Erfahrungen einer reinen Männergesellschaft traumatisierten den zarten Knaben. Nach sechs Jahren brach er seine militärische Ausbildung krankheitshalber ab. Daran schloss sich ein Besuch der Handelsakademie Linz, Oberösterreich, an. Im Mai 1892 musste er Linz wegen einer nicht geduldeten Liebesaffäre mit einem einige Jahre älteren Kindermädchen unfreiwillig verlassen. Damit war nach der militärischen auch eine wirtschaftliche Karriere aussichtslos geworden. Zurück in Prag, konnte Rilke sich von 1892 bis 1895 in privatem Unterricht auf die Matura vorbereiten, die er 1895 bestand. Im selben Jahr begann er an der Karls-Universität seiner Geburtsstadt Literatur, Kunstgeschichte und Philosophie zu studieren. Er wechselte im Folgejahr zur Rechtswissenschaft und setzte seine Studien ab September 1896 an der Ludwig-Maximilians-Universität München fort.
Im März 1897 besuchte Rilke das erste Mal Venedig. Am 12. Mai 1897 traf er in München die weitgereiste Intellektuelle und Literatin Lou Andreas-Salomé und verliebte sich in sie. Auch änderte er seinen Vornamen von René in Rainer, weil Andreas-Salomé den Namen für einen männlichen Schriftsteller angemessener fand. Die folgende intensive Beziehung mit der älteren und verheirateten Frau dauerte bis 1900 an. Auch nach der Trennung erwies sich Lou Andreas-Salomé bis zu Rilkes Lebensende als seine wichtigste Freundin und Beraterin. Dabei werden ihre psychoanalytischen Kenntnisse und Erfahrungen, die sie sich 1912/1913 bei Sigmund Freud angeeignet hatte, eine erhebliche Rolle gespielt haben. Freud berichtet, „daß sie dem großen, im Leben ziemlich hilflosen Dichter Rainer Maria Rilke zugleich Muse und sorgsame Mutter gewesen war“ (Sigmund Freuds Gedenkworte zum Tode Lou Andreas-Salomés, 1937).
Rilke folgte Lou Andreas-Salomé im Herbst 1897 nach Berlin und bezog eine Wohnung in ihrer unmittelbaren Nachbarschaft. In Berlin lernte er das Geschwisterpaar Mathilde und Karl Gustav Vollmoeller anlässlich einer Lesung Stefan Georges im Hause des Künstlerehepaares Sabine und Reinhold Lepsius kennen. 1898 unternahm er eine mehrwöchige Reise nach Italien. Rilke hatte sich im Jahre 1898 mit Heinrich Vogeler während dessen Florenz-Aufenthaltes angefreundet und kam nun als Gast Vogelers zu Besuch nach Worpswede.
In den beiden Jahren darauf besuchte er zweimal Russland: 1899 reiste er mit dem Ehepaar Andreas nach Moskau, wo er Lew Tolstoi traf. Von Mai bis August des Jahres 1900 folgte eine zweite Russlandreise mit Lou Andreas-Salomé allein, nach Moskau und Sankt Petersburg, aber auch quer durch das Land und die Wolga stromauf. Auf dieser Reise lernten sie durch Zufall Boris Pasternak kennen, der diese Begegnung in der autobiographischen Erzählung Der Schutzbrief beschreibt.Im Herbst 1900, unmittelbar nachdem Andreas-Salomé den Entschluss gefasst hatte, sich von ihm zu trennen, hielt Rilke sich zu einem längeren Besuch bei Heinrich Vogeler in Worpswede auf. Vogeler veranstaltete im Weißen Saal seines Barkenhoffs sonntägliche Treffen, bei denen die bildenden Künstler Otto Modersohn und dessen Frau Paula Modersohn-Becker, der Schriftsteller Carl Hauptmann sowie auch die Bildhauerin Clara Westhoff verkehrten. Westhoff und Rilke heirateten im folgenden Frühjahr. 
Im Dezember 1901 wurde ihre Tochter Ruth (1901–1972) geboren. Im Sommer 1902 gab Rilke die gemeinsame Wohnung auf und reiste nach Paris, um dort eine Monografie über den Bildhauer Auguste Rodin zu verfassen. Die Beziehung zwischen Rilke und Clara Westhoff blieb zeit seines Lebens bestehen, doch war er nicht der Mensch für ein bürgerliches und ortsgebundenes Familienleben. Gleichzeitig drückten ihn finanzielle Sorgen, die durch Auftragsarbeiten nur mühsam gemildert werden konnten.
Die erste Pariser Zeit war für Rilke schwierig, da die fremde Großstadt für ihn viele Schrecken barg. Diese Erfahrungen hat er später im ersten Teil seines einzigen Romans, Die Aufzeichnungen des Malte Laurids Brigge, gestaltet. Zugleich aber brachte die Begegnung mit der Moderne zahlreiche Anregungen: Rilke setzte sich intensiv zunächst mit den Plastiken Auguste Rodins, dann mit dem Werk des Malers Paul Cézanne auseinander. Mehr und mehr wurde in diesen Jahren Paris zum Hauptwohnsitz des Dichters. Von 1905 bis 1906 war er für acht Monate als Sekretär bei Rodin angestellt, der ihm gleichzeitig eine idealisierte Vaterfigur war. Das Dienstverhältnis beendete Rodin im Mai 1906 abrupt. Kurz zuvor war Rilkes Vater gestorben. Im selben Jahr lernte Rilke Sidonie Nádherná von Borutín kennen, mit der er eine erotisch desinteressierte, aber von Eifersucht nicht ungetrübte literarische Freundschaft und einen ausgedehnten Briefwechsel bis zu seinem Tod führte: Nachdem Sidonie Nádherná 1913 in Wien den Schriftsteller Karl Kraus kennengelernt hatte, war es Rilke, der sie vor Kraus warnte. Diese Einmischung in eine komplizierte Liebesbeziehung hat er später bereut.
Den Sommer 1903 verbrachte Rilke in Florenz, den Winter 1903/1904 in Rom, wo er in der Villa Strohl-Fern wohnte und wo er auch die Briefe an einen jungen Dichter verfasste. Der Maler Otto Sohn-Rethel, ein Freund der Maler der Künstlerkolonie Worpswede, hatte ihm sein „Studio al Ponte“ überlassen. Rilkes Frau Clara Westhoff hatte zur selben Zeit in Sichtweite ein eigenes Studio auf dem Gelände.Ab 1906 intensivierte sich der Kontakt Rilkes zu Mathilde und Karl Gustav Vollmoeller. Zunächst nutzte er in Abwesenheit Mathildes deren Pariser Atelier mehrmals. Gleichzeitig versuchte Rilke anlässlich seiner Italienreise 1907, Vollmoeller in dessen Villa in Sorrent zu besuchen. Erst über Ostern 1908 kam es zum neuerlichen Treffen zwischen Rilke und Vollmoeller in Florenz. Rilke war hier für mehrere Tage Gast in Vollmoellers Florentiner Domizil, der Renaissancevilla Gilli-Pozzino. Anwesend waren auch der Schriftsteller Felix Salten sowie das Ehepaar Lepsius. In den folgenden Jahren trafen Rilke und Vollmoeller einander mehrmals in Paris. Die wichtigsten dichterischen Erträge der Pariser Zeit waren die Neuen Gedichte (1907), Der neuen Gedichte anderer Teil (1908), die beiden Requiem-Gedichte (1909) sowie der bereits 1904 begonnene und im Januar 1910 vollendete Roman Die Aufzeichnungen des Malte Laurids Brigge.
Für den Leipziger Insel Verlag, dessen Leitung Anton Kippenberg 1905 übernommen hatte, wurde Rilke zum wichtigsten zeitgenössischen Autor. Kippenberg erwarb für den Verlag bis 1913 die Rechte an allen bis dahin verfassten Werken Rilkes.
Nachdem er Die Aufzeichnungen des Malte Laurids Brigge 1910 in Leipzig vollendet hatte, begann für Rilke eine tiefe, zwölf Jahre währende Schaffenskrise. Er beschäftigte sich mit Übersetzungen literarischer Werke aus dem Französischen. Auf der Suche nach neuer Inspiration setzte er sich mit klassischen Schriftstellern, erstmals auch intensiver mit dem Werk Goethes und mit Shakespeare, auseinander. 1912 begann er die Duineser Elegien, die er jedoch erst im Februar 1922 abschließen konnte. Dieser Gedichtzyklus verdankt seinen Namen dem Aufenthalt Rilkes auf dem Schloss Duino der Prinzessin Marie von Thurn und Taxis bei Triest in der Zeit von Oktober 1911 bis Mai 1912.
1912 erschien eine Neuausgabe der lyrischen Erzählung Die Weise von Liebe und Tod des Cornets Christoph Rilke als Nummer 1 der Insel-Bücherei, mit der das Werk hohe Auflagen und ungewöhnliche Popularität erlangen sollte, nachdem es zunächst 1906 von Rilkes erstem Verleger, Axel Juncker, recht erfolglos als Liebhaberausgabe herausgebracht worden war.
Der Ausbruch des Ersten Weltkrieges überraschte Rilke während eines Deutschlandaufenthaltes. Nach Paris konnte er nicht mehr zurückkehren; sein dort zurückgelassener Besitz wurde beschlagnahmt und versteigert. Den größten Teil der Kriegszeit verbrachte Rilke in München. Er wohnte in der Ainmillerstraße 34 im Stadtteil Schwabing. Von 1914 bis 1916 hatte er eine stürmische Affäre mit der Malerin Lou Albert-Lasard. Die Freundschaft zwischen Rilke und Vollmoeller intensivierte sich während des Ersten Weltkriegs, als beide einander auch in Gegenwart von Lou Albert-Lasard sowohl in Berlin wie in München trafen. Rilke nutzte Vollmoellers Beziehungen zum deutschen Generalstab, um ihn bei der Fahndung nach einem vermissten Vetter einzusetzen. Wie der unveröffentlichte Briefwechsel (Deutsches Literaturarchiv, Marbach) ausweist, war Vollmoeller erfolgreich und konnte Rilke und dessen Familie mit den gewünschten Informationen versorgen.
Anfang 1916 wurde Rilke eingezogen und musste in Wien eine militärische Grundausbildung absolvieren, wo er in der Breitenseer Kaserne im Westen der Stadt stationiert war. Auf Fürsprache einflussreicher Freunde wurde er zur Arbeit ins Kriegsarchiv und ins k.u.k. Kriegspressequartier überstellt und am 9. Juni 1916 aus dem Militärdienst entlassen. Während seines Aufenthaltes in Wien wohnte er in der Viktorgasse 5 und in der Gußhausstraße 9, beides Adressen im 4. Bezirk unweit von Stadtzentrum und Schloss Belvedere. Die Zeit danach, in der er auch – zum Teil gemeinsam mit Oskar Maria Graf – die dortigen revolutionären Bewegungen erlebte – verbrachte er wieder in München, unterbrochen durch einen Aufenthalt auf Hertha Koenigs Gut Böckel in Westfalen. Das traumatische Erlebnis des Militärdienstes, empfunden auch als eine Wiederholung in der Militärschulzeit erfahrener Schrecken, ließ Rilke als Dichter eine Zeit lang nahezu völlig verstummen.
Am 11. Juni 1919 reiste Rilke von München in die Schweiz. Äußerer Anlass war eine Vortragseinladung aus Zürich, eigentlicher Grund aber der Wunsch, den Nachkriegswirren zu entkommen und die so lange unterbrochene Arbeit an den Duineser Elegien wieder aufzunehmen. Die Suche nach einem geeigneten und bezahlbaren Wohnort erwies sich als sehr schwierig. Rilke lebte unter anderem in Soglio, Locarno und Berg am Irchel. Erst im Sommer 1921 fand er im Château de Muzot, einem Schlösschen oberhalb von Siders im Kanton Wallis, eine dauerhafte Wohnstätte. Im Mai 1922 erwarb Rilkes Mäzen Werner Reinhart (1884–1951) das Gebäude und überließ es dem Dichter mietfrei.
In einer intensiven Schaffenszeit vollendete Rilke hier innerhalb weniger Wochen im Februar 1922 die Duineser Elegien. In unmittelbarer zeitlicher Nähe entstanden auch die beiden Teile des Gedichtzyklus’ Sonette an Orpheus. Beide Dichtungen zählen zu den Höhepunkten in Rilkes Werk.
Seit 1923 musste Rilke mit großen gesundheitlichen Beeinträchtigungen kämpfen, die mehrere lange Sanatoriumsaufenthalte nötig machten. Auch der Paris-Aufenthalt von Januar bis August 1925 war ein Versuch, der Krankheit durch Ortswechsel und Änderung der Lebensumstände zu entkommen. Dennoch entstanden auch in den letzten Jahren zwischen 1923 und 1926 noch zahlreiche Einzelgedichte (etwa Gong und Mausoleum) und ein umfangreiches lyrisches Werk in französischer Sprache.
Im Januar und Februar 1926 schrieb Rilke der Mussolini-Gegnerin Aurelia Gallarati Scotti drei Briefe nach Mailand, in denen er die Herrschaft Benito Mussolinis lobte und den Faschismus als ein Heilmittel pries. Über die Rolle der Gewalt war Rilke sich dabei nicht im Unklaren. Er war bereit, eine gewisse, vorübergehende Gewaltanwendung und Freiheitsberaubung zu akzeptieren. Es gelte, auch über Ungerechtigkeiten hinweg zur Aktion zu schreiten. Italien sah er als das einzige Land, dem es gut gehe und das im Aufstieg begriffen sei. Mussolini sei zum Architekten des italienischen Willens geworden, zum Schmied eines neuen Bewusstseins, dessen Flamme sich an einem alten Feuer entzünde. „Glückliches Italien!“ rief Rilke aus, während er den Ideen der Freiheit, der Humanität und der Internationale eine scharfe Absage erteilte. Sie seien nichts als Abstraktionen, an denen Europa beinahe zusammengebrochen wäre.Erst kurz vor Rilkes Tod wurde seine Krankheit als Leukämie diagnostiziert, und zwar in einer damals noch wenig bekannten Form. Der Dichter starb am 29. Dezember 1926 im Sanatorium Valmont sur Territet bei Montreux und wurde am 2. Januar 1927 – seinem Wunsch entsprechend – in der Nähe seines letzten Wohnorts auf dem Bergfriedhof von Raron (Schweiz) beigesetzt. Auf seinem Grabstein steht der von Rilke selbst verfasste und für den Grabstein ausgewählte Spruch:
Beeinflusst durch die Philosophen Schopenhauer und vor allem Nietzsche, deren Schriften er früh kennengelernt hatte, ist Rilkes Werk durch eine scharfe Kritik an der Jenseitsorientierung des Christentums und an einer einseitig naturwissenschaftlich-rationalen Weltdeutung geprägt. Seine kurze Orientreise, die ihn 1911 nach Tunesien, Ägypten und Spanien brachte, setzte ihn mit der Welt des Islams in Kontakt, aus der schon früher nachvollziehbare Einflüsse in Weltanschauung und Werk ersichtlich wurden. Rilke fühlte sich sehr stark zur arabischen Sprache hingezogen.  Der Islam ist ihm die Religion des „unverstellten Weltraums“, des reinen Kreaturgefühls: Die Erde wird als „pures Gestirn“ erfahrbar. Die Geschöpflichkeit der Erde kann so rein und unverstellt erscheinen.  
Zu den frühen Werken Rilkes gehören die Gedichtbände Wegwarten, Traumgekrönt und Advent. Mit dem Band Mir zur Feier (1897/1898) wendet er sich zum ersten Mal systematisch einer Betrachtung der menschlichen Innenwelt zu. Die unveröffentlichte Gedichtsammlung Dir zur Feier (entstanden 1897/1898) ist eine einzige Liebeserklärung an die verehrte Lou Andreas-Salomé. 1899 entstand das kurze Prosawerk Die Weise von Liebe und Tod des Cornets Christoph Rilke.
Das Stunden-Buch (drei Teile, entstanden 1899–1903, Erstdruck 1905), benannt nach traditionellen Gebetbüchern des Mittelalters, bildet den ersten Höhepunkt des Frühwerkes und ist Ausdruck eines pantheistischen Gottesbildes. Mit seinen kunstvoll verschlungenen Reimbändern und seinem fließenden Rhythmus ist dieser Gedichtzyklus eines der Hauptwerke des literarischen Jugendstils. Dieser Schaffensperiode ist auch die 1902 erschienene und 1906 um zahlreiche Gedichte erweiterte, im impressionistischen Stil gehaltene Gedichtsammlung Das Buch der Bilder zuzurechnen.
Nietzsches Philosophie – auch vermittelt durch beider intime Freundin Lou Andreas-Salomé – gewinnt in den Jahren um die Jahrhundertwende erheblichen Einfluss auf Rilke. Die Anerkennung der Wirklichkeit ohne Jenseitsvertröstungen oder soziale Entwicklungsromantik prägte auch Rilkes Weltverständnis. Dafür stehen intensive Beobachtungen der Natur sowie des menschlichen Verhaltens und Gefühlslebens. Dies alles bildete Rilkes „Weltinnenraum“, in dem sich Außen- und Innenwelt verbinden.
Aus den Werken der mittleren Phase zwischen 1902 und 1910 ragen vor allem die Neuen Gedichte und der Roman Die Aufzeichnungen des Malte Laurids Brigge hervor. Rilke wendet sich in diesen Werken der Welt menschlicher Grunderfahrungen zu; nun aber nicht mehr, indem er das Innenleben beobachtet, sondern in einer das Subjekt zurückdrängenden symbolischen Spiegelung dieses Innen in den erlebten Dingen. So entstehen seine „Dinggedichte“, zu denen die Blaue Hortensie, Der Panther oder der Archaïsche Torso Apollos gehören, die den literarischen Symbolismus weiterentwickeln. Dieses Welterfassen bezieht ausdrücklich die schmerzvollen und fremden Aspekte des Lebens ein: Hässliches, Krankheit, Trieb und Tod.
Im späten Werk (1912–1922) verleiht Rilke seiner Lebensbejahung in den Zyklen Duineser Elegien und Die Sonette an Orpheus poetische Gestalt und bezieht sich auf das ganze, Leben und Tod umgreifende Dasein. Die Gedichte der letzten Jahre zerfallen in unterschiedliche Gruppen: einerseits heiter-entspannte, oft lakonisch-pointierte Natur- und Landschaftsgedichte, andererseits poetisch kühne Experimente, die rein aus der Sprache herausgearbeitet sind.
Rilkes Werke sind häufig vertont oder musikalisch bearbeitet worden. Die folgende chronologisch geordnete Übersicht listet die wichtigsten Werke der ernsten Musik auf:
Anton Webern: Zwei Lieder nach Gedichten von Rainer Maria Rilke, op. 8 (1910) (1. Du, der ich’s nicht sage. 2. Du machst mich allein.)
Arnold Schönberg: Alle, welche dich suchen, Mach mich zum Wächter deiner Weiten und Vorgefühl. op. 22, No. 2, 3 und 4, 1914.
Paul von Klenau: Die Weise von Liebe und Tod des Cornets Christoph Rilke. Für Bariton, gemischten Chor und Orchester, 1915.
Hanns Eisler: Wenn es nur einmal so ganz still wäre. (Stundenbuch). Für Alt, Violine, Viola, Violoncello. Wiesbaden 1918.
Ferruccio Busoni: Vom mönchischen Leben. Gesänge aus Rainer Maria Rilke’s Stundenbuch für Bariton Solo, gemischten Chor, Orchester und Orgel. op. 44, 1919.
Clemens Krauss: Acht Gesänge nach Gedichten von Rainer Maria Rilke für eine hohe Frauenstimme und Pianoforte. 1920.
Willy Burkhard: Rilke Liederzyklus I und II. op. 20,1–2 Jeweils fünf Gesänge für Bass und Sopran. 1927.
Paul Hindemith: Six Chansons. 6 Gedichte für Chor a cappella nach französischen Gedichten von R. M. Rilke. 1939.
Zoltán Gárdonyi: Fünf Lieder für Sopran und Klavier nach Gedichten von R. M. Rilke. 1941/1942, revidiert 1978, Edition Walhall Magdeburg.
Frank Martin: Die Weise von Liebe und Tod des Cornets Christoph Rilke. Für Alt und Kammerorchester, 1942.
Viktor Ullmann: Die Weise von Liebe und Tod des Cornets Christoph Rilke. Für Sprecher und Orchester oder Klavier, 1944.
Josef Schelb: Drei Lieder nach Gedichten von Rainer Maria Rilke. Für hohe Stimme und Klavier, 1946.
Gerhard Frommel: 3 Lieder. (Rainer Maria Rilke, Stefan George, Otto Frommel). Für tiefe Stimme und Klavier. Manuskript, 1953.
Einojuhani Rautavaara: Die Liebenden. Liederzyklus für hohe Stimme und Streichorchester. [Liebes-Lied (Neue Gedichte), Der Schauende, (Buch der Bilder), Die Liebende, Der Tod der Geliebten (Neue Gedichte)]. 1958/1959.
Leonard Bernstein: Two Love Songs (When my soul toches you …, Extinquish my eyes …). Boosey & Hawkes, New York, 1949. [Lösch mir die Augen aus (Stundenbuch)]
Michael Denhoff: O Orpheus singt – fünf lyrische Stücke für Oktett. op. 15, 1977, nach Motiven aus den Sonetten an Orpheus.
Klaus Miehling: Fünf Lieder nach Rainer Maria Rilke. Für mittlere Stimme und Klavier, op. 67, 1996.
Helmut Schmidinger: „Was uns anrührt, dich und mich.“ Sieben Verhältnisse für Violine und Klavier nach Versen von Rainer Maria Rilke. Wien 2004.
Krzysztof Penderecki: Lieder der Vergänglichkeit. Für Soli, gemischter Chor und Orchester. Nach Gedichten von Johann Wolfgang von Goethe, Achim von Arnim, Joseph von Eichendorff, Karl Kraus, Rainer Maria Rilke [Ende des Herbstes (Buch der Bilder), Herbsttag (Buch der Bilder)] und Hermann Hesse. Schott, Mainz/London u. a. 2005.
François Cotinaud: Verwandlung. Spiegele Malerei. L’Orphée de Rilke. Ensemble Luxus. Nach Sonette an Orpheus. Mit Pascale Labbé (Stimme), François Cotinaud (Klarinette, Saxophon), und Jérôme Lefebvre (Gitarre). Label Musivi (Musea), 2015.
Gérard Zinsstag: Rilke-Lieder für Mezzosopran, Flöte, Klarinette, Klavier, Violine, Viola und Violoncello (2015).Jenseits des engeren Bereiches der E-Musik hat sich die englische Künstlerin Anne Clark 1998 auf ihrem Album Just After Sunset musikalisch mit dem Werk Rilkes auseinandergesetzt.
Populär geworden ist vor allem die musikalische Annäherung an Rilkes lyrisches Werk durch das Rilke Projekt, das im Jahr 2001 begonnen wurde. In bisher vier CD-Veröffentlichungen interpretieren bekannte zeitgenössische Schauspieler und Musiker Texte von Rilke.
Seit einigen Jahren verbreitet sich Rilkes Werk auch außerhalb literarisch interessierter Kreise. Ein materieller Grund für diese größere Aufmerksamkeit liegt im Wegfall der Urheberrechtsbindung seines Werkes an den Insel-Verlag im Jahr 1996, 70 Jahre nach Rilkes Tod. Neben musikalischen Vertonungen seiner Gedichte („Rilke Projekt“) wurde unter anderem unter dem Namen „Rilke Zauber“ eine abendfüllende Zaubertheater-Vorführung entwickelt, bei der der Künstler Ulrich Rausch erstmals Gedichte Rilkes mit Zauberkunststücken verknüpft.
Ein Teil des schriftstellerischen Nachlasses von Rilke liegt im Deutschen Literaturarchiv Marbach. Im Marbacher Literaturmuseum der Moderne sind Manuskripte aus dem Nachlass in der Dauerausstellung zu sehen, so zum Beispiel vom Stunden-Buch und von Die Aufzeichnungen des Malte Laurids Brigge.
Im deutschsprachigen Raum und den Nachbarländern sind zahlreiche Straßen nach Rilke (mit oder ohne Vornamen) benannt. Im Prager Stadtzentrum wurden am 7. Dezember 2011 am Gebäude der früheren deutschen Schule eine Gedenktafel und eine Rilke-Büste enthüllt, gestaltet von der tschechischen Bildhauerin Vlasta Prachatická. Es gibt mindestens drei Schulen, die Rilke in ihrem Namen führen: die Rilke-Realschule in Stuttgart (seit 1960), das Rainer-Maria-Rilke-Gymnasium in Icking (Oberbayern) (seit 2011) und in Alaska die Rilke School in Anchorage (seit 1997).
Nach Marbach, Bern und Zürich wurde im Moskauer Literaturmuseum vom 7. Februar bis 31. März 2018 eine Rilke-Ausstellung gezeigt. Vom 21. September 2018 bis 6. Januar 2019 zeigt das Paula Modersohn-Becker Museum in Bremen die Ausstellung "Rilke in Bremen".
Sämtliche Werke. 7 Bände. Hg. vom Rilke-Archiv in Verbindung mit Ruth Sieber-Rilke, besorgt durch Ernst Zinn. Insel Verlag, Frankfurt am Main 1955–1966 (Bd. 1–6), 1997 (Bd. 7)
auf Basis der ersten 6 Bände der Sämtlichen Werke erschienen ab 1966 mehrere Gesamtausgaben in 6 bzw. 12 Bänden; sowie (weniger umfangreiche) Werkausgaben in 3 bzw. 6 Bänden
Werke. Kommentierte Ausgabe. 4 Bände und ein Supplementband. Hrsg. von Manfred Engel, Ulrich Fülleborn, Dorothea Lauterbach, Horst Nalewski und August Stahl. Insel, Frankfurt am Main/Leipzig 1996 (Bd. 1–4), 2003 (Supplement), ISBN 978-3-458-06697-2.
Gesammelte Werke. 5 Bände. Hrsg. v. Manfred Engel, Ulrich Fülleborn, Horst Nalewski und August Stahl. Insel, Frankfurt am Main 2003, ISBN 978-3-458-17186-7. [Textauswahl auf Basis der Kommentierten Ausgabe.]
Silberne Schlangen. Die frühen Erzählungen aus dem Nachlaß. Hrsg. vom Rilke-Archiv in Zusammenarbeit mit Hella Sieber-Rilke, besorgt durch August Stahl. Insel, Frankfurt am Main/Leipzig 2004, ISBN 978-3-458-17226-0.
Rainer Maria Rilke: Das Testament. Faksimile der Handschrift aus dem Nachlass. Im Anhang Transkription der Handschrift. Erläuterungen und Nachwort von Ernst Zinn. Insel Verlag, Frankfurt am Main 1974.
Erste Gedichte (1913; enthält unveränderte Nachdrucke von Larenopfer, Traumgekrönt und Advent) UB Bielefeld
Die Weise von Liebe und Tod des Cornets Christoph Rilke (geschrieben 1899, zuerst erschienen 1904 in: Deutsche Arbeit. Jg. 4. 1904, H. 1, S. 59–65. Dieser „ersten Fassung“ lag ein Manuskript zu Grunde, dessen Faksimile später o. J. bei Insel/Leipzig in 525 nummerierten Stücken erschien; 1906 erste selbständige Ausgabe bei A. Juncker, Berlin/Leipzig/Stuttgart; Neuauflage als Band 1 der Insel-Bücherei, Leipzig 1912 ff.)
Auguste Rodin (1903); Greifenverlag, Berlin/Rudolstadt 2009, ISBN 978-3-86939-207-3 (= GreifenFundstücke).
Schriften zur Literatur und Kunst. Kommentierte Auswahl der wichtigsten Schriften, herausgegeben von Torsten Hoffmann. Reclam, Stuttgart 2009, ISBN 978-3-15-018670-1.
Im ersten Augenblick. Bildbetrachtung. Herausgegeben von Rainer Stamm. Insel Verlag, Berlin 2015, ISBN 978-3-458-19407-1.
Briefe. Hrsg. vom Rilke-Archiv in Weimar. 2 Bände. Wiesbaden 1950 (Neuauflage 1987 in einem Band – als Taschenbuchausgabe in drei Bänden: Frankfurt am Main 1987, ISBN 3-458-32567-0).
Rainer Maria Rilke – Sidonie Nádherny von Borutin: Briefwechsel 1906–1926. Hrsg. Joachim W. Storck, Waltraud und Friedrich Pfäfflin. Wallstein Verlag, Göttingen 2005, ISBN 978-3-89244-983-6.
Briefe an die Mutter. 1896–1926. Herausgegeben von Hella Sieber-Rilke. Insel Verlag, Frankfurt am Main/Leipzig 2009, ISBN 978-3-458-17318-2.
Briefe an eine venezianische Freundin. Hrsg. und aus dem Französischen übersetzt von Margret Millischer. Leipziger Literaturverlag, 2010, ISBN 978-3-86660-117-8. (Nicht in den Gesamtausgaben enthalten.)
Briefwechsel mit Thankmar von Münchhausen 1913 bis 1925. Herausgegeben von Joachim W. Storck. Insel, 2004.
Rainer Maria Rilke – Marie Gagarine-Obolenski. Transatlantischer Briefwechsel. Hrsg. von Rätus Luck. Futura Edition, Wolfenbüttel 2011.
Larenopfer. Zweisprachige, kommentierte Ausgabe, übersetzt von Alfred de Zayas. Red Hen Press, Los Angeles 2005-
The Essential Rilke. Ausgewählte Gedichte ins Englische übertragen von Galway Kinnell und Hannah Liebmann. The Ecco Press, Hopewell, New Jersey 1999.
Rilke. Selected Poems. Übersetzt von C.F. MacIntyre, University of California Press, Berkeley 1940.
The best of Rilke. Übersetzt von Walter Arndt. University Press of New England, Hanover 1984, ISBN 0-87451-460-6 / ISBN 0-87451-461-4.
Briefe an eine venezianische Freundin. Hrsg. und aus dem Französischen übersetzt von Margret Millischer. Leipziger Literaturverlag, 2010, ISBN 978-3-86660-117-8.
Himmelsherrin, Herrscherin auf Erden (Ballade, die Villon auf Bitten seiner Mutter verfasste, um die Muttergottes anzurufen) von François Villon
Ralph Freedman: Rainer Maria Rilke. 2 Bände. Aus dem amerikanischen Englisch von Curdin Ebneter. Frankfurt am Main/Leipzig 2001, 2002, ISBN 3-458-17124-X.
Ralph M. Köhnen: Rilke, Rainer Maria. In: Neue Deutsche Biographie (NDB). Band 21, Duncker & Humblot, Berlin 2003, ISBN 3-428-11202-4, S. 621–623 (Digitalisat).
Donald A. Prater: Ein klingendes Glas. Das Leben Rainer Maria Rilkes. Carl Hanser, München/Wien 1986, ISBN 3-446-13362-3.
Fritz J. Raddatz: Rainer Maria Rilke. Überzähliges Dasein. Eine Biographie. Arche Verlag, Zürich 2009, ISBN 978-3-7160-2606-9.
Heimo Schwilk: Rilke und die Frauen. Biografie eines Liebenden. Piper, München 2015, ISBN 978-3-492-05637-3.
Ingeborg Schnack: Rainer Maria Rilke. Chronik seines Lebens und seines Werkes. Frankfurt am Main 1996, ISBN 3-458-16827-3; Erweiterte Neuausgabe, hrsg. von Renate Scharffenberg. Insel, Frankfurt am Main/Leipzig 2009, ISBN 978-3-458-17433-2.
Joseph-François Angelloz: Rainer Maria Rilke: Leben und Werk. Übertr. aus dem Franz. von Alfred Kuoni. Nymphenburger, München 1955.
Manfred Engel, Dorothea Lauterbach (Hrsg.): Rilke Handbuch. Leben – Werk – Wirkung. Metzler, Stuttgart/Weimar 2004, ISBN 3-476-01811-3; Sonderausgabe: Metzler, Stuttgart/Weimar 2013, ISBN 978-3-476-02526-5. [Mit ausführlichem Verzeichnis von Ausgaben und Forschungsliteratur.]
Hans Egon Holthusen: Rainer Maria Rilke. rororo Monographien. 35. Auflage. Rowohlt Taschenbuch Verlag, 2004, ISBN 978-3-499-50022-0.
Hermann Kunisch: Rainer Maria Rilke. Dasein und Dichtung. Zweite, neu gefasste und stark erweiterte Auflage. Duncker und Humblot, Berlin/München 1975, ISBN 3-428-03429-5. (Erstauflage 1944)
Gunter Martens, Annemarie Post-Martens: Rainer Maria Rilke. Rowohlt, Reinbek 2008, ISBN 978-3-499-50698-7.
Günther Anders: Über Rilke und die deutsche Ideologie (Aus dem Nachlass: Cornet-Relektüre 1948). In: sans phrase. Zeitschrift für Ideologiekritik, Heft 7, Herbst 2015, S. 109–131. ISSN 2194-8860.
Ulrich C. Baer (Hrsg.): Rainer Maria Rilke: Die Prosa. Frankfurt am Main 2016, ISBN 978-3-458-17685-5.
Dieter Bassermann: Der andere Rilke: gesammelte Schriften aus dem Nachlass. Hrsg. von Hermann Mörchen. Gentner, Bad Homburg vor der Höhe 1961.
Paul Claes: Rilkes Rätsel: Eine neue Deutung der Neuen Gedichte. Aus dem Niederländischen von Marlene Müller-Haas. Athena-Verlag 2009, ISBN 978-3-89896-335-0.
Johannes Cramer: Nah ist und schwer zu fassen der Gott. Ein Versuch über Rainer Maria Rilke, Hans Carossa und Gertrud von le Fort. Paulus-Verlag 1948.
Günther Däss: Wirklichkeitsintuition und Wirklichkeitstreue in Rilkes Duineser Elegien. Haarlem University Press, 1970.
Gunnar Decker: Rilkes Frauen oder Die Erfindung der Liebe. Reclam, Leipzig 2004, ISBN 978-3-379-00816-7.
Ilija Dürhammer (Hrsg.): Mystik, Mythen & Moderne: Trakl, Rilke, Hofmannsthal. 21 Gedicht-Interpretationen. Praesens Verlag, 2010, ISBN 978-3-7069-0614-2.
Manfred Engel, Dieter Lamping (Hrsg.): Rilke und die Weltliteratur. Artemis und Winkler, München 1998, ISBN 3-538-07084-9.
Richard Exner: Rainer Maria Rilke: Das Marien-Leben. Vorgestellt von Richard Exner. Frankfurt am Main 1999, ISBN 978-3-458-16981-9.
Barbara Fritz: Rainer Maria Rilkes Leser in Schule und Gesellschaft: Rezeption 1904–1936. Diss. Frankfurt 2009, ISBN 978-3-631-59006-5.
Ralph Gleis, Maria Obenaus (Hrsg.): Rodin – Rilke – Hofmannsthal. Der Mensch und sein Genius. Berlin 2017, ISBN 978-3-95732-297-5.
Wolfram Groddeck: Interpretationen: Gedichte von Rainer Maria Rilke. Reclam Verlag, 1999, ISBN 978-3-15-017510-1.
Rüdiger Görner: Rainer Maria Rilke. Im Herzwerk der Sprache. Hanser/Zsolnay Verlag, München/Wien 2004, ISBN 978-3-379-00816-7.
Ulrich K. Goldsmith: Rainer Maria Rilke, a verse concordance to his complete lyrical poetry. W.S. Maney, Leeds 1980.
Gisela Götte, Jo-Anne Birnie Danzker (Hrsg.): Rainer Maria Rilke und die bildende Kunst seiner Zeit. München 1996, ISBN 3-7913-1750-4.
Romano Guardini: Rainer Maria Rilkes Deutung des Daseins. Eine Interpretation der Duineser Elegien. 1953. Nachdr. Mainz 1996, ISBN 3-7867-1948-9; Paderborn 1996, ISBN 3-506-74552-2.
Erich Heller: Nirgends wird Welt sein als innen: Versuche über Rilke. Suhrkamp, Frankfurt am Main 1975.
Alfred Hermann: Rilkes ägyptische Geschichte. ‚Ein Versuch wechselseitiger Erhellung von Dichtung und Altkultur‘. In: Symposion, Jahrbuch für Philosophie. Hrsg. von Max Müller. Band IV. Freiburg i. Br./München 1955, S. 367–461.
Gertrud Höhler: Niemandes Sohn: Zur Poetologie Rainer Maria Rilkes. Wilhelm Fink-Verlag, 1979, ISBN 978-3-7705-1574-5.
Anette und Peter Horn: „Ich lerne sehen.“ Zu Rilkes Lyrik. Athena-Verlag, 2010, ISBN 978-3-89896-397-8.
Andrea Hübener, Erich Unglaub (Hrsg.): Blätter der Rilke-Gesellschaft 29/2008: Rilkes Dresden. Das Buch der Bilder. Insel-Verlag 2008, ISBN 978-3-458-17424-0.
Maria Jansen: Poetik des Grauens: Über Rainer Maria Rilkes „Aufzeichnungen des Malte Laurids Brigge“. AV Akademikerverlag, 2013, ISBN 978-3-639-47549-4.
Sung-Kie Im: Dynamik des Raumes. Die Motive des Windes und des Atems in der Lyrik Rilkes. Diss. Karlsruhe 1979.
Heinrich Imhof: Rilkes Gott. R. M. Rilkes Gottesbild als Spiegelung des Unbewußten. Stiehm Verlag, Heidelberg 1988, ISBN 978-3-7988-0036-6.
Gerhard Junge: Motivuntersuchungen zu den französischen Gedichten Rainer Maria Rilkes. Dissertation Univ. Marburg 1956. GoogleBooks
Martina King: Pilger und Prophet: Heilige Autorschaft bei Rainer Maria Rilke. Vandenhoeck & Ruprecht, 2009, ISBN 978-3-525-20603-4.
Karl-Josef Kuschel: Rilke und der Buddha. Die Geschichte eines einzigartigen Dialogs. Guetersloher Verlagshaus, 2010, ISBN 978-3-579-07020-9.
Karen Leeder, Robert Vilain (Hrsg.): Nach Duino: Studien zu Rainer Maria Rilkes späten Gedichten. Wallstein-Verlag, 2010, ISBN 978-3-8353-0425-3.
Sascha Löwenstein: Poetik und dichterisches Selbstverständnis. Eine Einführung in Rainer Maria Rilkes frühe Dichtungen. Königshausen & Neumann, Würzburg 2004.
Bernhard Marx: ‚Meine Welt beginnt bei den Dingen‘. Rainer Maria Rilke und die Erfahrung der Dinge. Königshausen und Neumann, Würzburg 2015, ISBN 978-3-8260-5622-2.
Gisli Magnússon: Dichtung als Erfahrungsmetaphysik: esoterische und okkultistische Metaphysik bei R. M. Rilke. Univ. habil., Würzburg 2009, ISBN 978-3-8260-4076-4.
Silke Pasewalck: „Die fünffingrige Hand“: Die Bedeutung der sinnlichen Wahrnehmung beim späten Rilke. de Gruyter Verlag, Berlin 2002, ISBN 3-11-017265-8.
Jörg Paulus, Erich Unglaub (Hrsg.): Im Schwarzwald. Blätter der Rilke-Gesellschaft; Band 31. Göttingen, Wallstein-Verlag 2012, ISBN 3-8353-1137-9.
Sabine Prilop, Ursula Brunbauer: Rainer Maria Rilke: Die russischen Reisen. HerzRosen, 1999, ISBN 3-934114-00-8.
Marcel Reich-Ranicki (Hrsg.): Rainer Maria Rilke. Und ist ein Fest geworden. 33 Gedichte mit Interpretationen. Insel-Verlag 2000, ISBN 978-3-458-34311-0.
Walter Rehm: Orpheus. Der Dichter und die Toten. Selbstdeutung und Totenkult bei Novalis, Hölderlin, Rilke. Düsseldorf 1950.
Jessica Riemer: Rilkes Frühwerk in der Musik: rezeptionsgeschichtliche Untersuchungen zur Todesthematik. Heidelberg, Univ., Diss., 2009, ISBN 978-3-8253-5698-9.
Adolf J. Schmid: Rilke in Rippoldsau : 1909 u. 1913 – sympathische Seiten im Gästebuch des verlässlichen Kurtales. Apis-Verlag, Freiburg im Breisgau 1984.
August Stahl: Rilke – Kommentar zu den Aufzeichnungen des Malte Laurids Brigge. Winkler-Verlag, München 1990, ISBN 978-3-538-07027-1.
Jacob Steiner: Rilke. Vorträge und Aufsätze. Herausgegeben von der Literarischen Gesellschaft (Scheffelbund) Karlsruhe (= Jahresgabe). von Loeper-Verlag, Karlsruhe 1986.
Raoul Walisch: „Daß wir nicht sehr verläßlich zu Haus sind in der gedeuteten Welt“: Untersuchung zur Thematik der gedeuteten Welt in Rilkes „Die Aufzeichnung des Malte Laurids Brigge“, „Duineser Elegien“ und spätester Lyrik. Univ. Diss., Würzburg 2012, ISBN 978-3-8260-4927-9.
Judith Ryan: Umschlag und Verwandlung. Poetische Struktur und Dichtungstheorie in R. M. Rilkes Lyrik der Mittleren Periode (1907–1914). München 1972.
Erich Unglaub: Panther und Aschanti: Rilke-Gedichte in kulturwissenschaftlicher Sicht. Peter Lang-Verlag, Frankfurt am Main 2005, ISBN 978-3-631-53791-6.
Gunna Wendt: Lou Andreas-Salomé und Rilke – eine amour fou. Insel-Verlag 2010, ISBN 978-3-458-35352-2.
Marek Zybura: Hundert Jahre polnische Rilke-Rezeption. In: ders: Querdenker, Vermittler, Grenzüberschreiter. Dresden 2007, ISBN 978-3-934038-87-5.
Nachlass von Rainer Maria Rilke in der Archivdatenbank HelveticArchives der Schweizerischen Nationalbibliothek
Publikationen von und über Rainer Maria Rilke im Katalog Helveticat der Schweizerischen Nationalbibliothek
Rilke and his Reviewers. An Annotated Bibliography (1978) (PDF; 5,5 MB) Eine Bibliographie von zeitgenössischen Rezensionen.
Rainer Maria Rilke in der Datenbank von Find a Grave (englisch)Vorlage:Findagrave/Wartung/Gleiche Kenner im Quelltext und in WikidataLinksammlungen
Kommentierte Linksammlung der Universitätsbibliothek der FU Berlin (Memento  vom 11. Oktober 2013 im Internet Archive) (Ulrich Goerdten)Audio-Darstellungen
1. Duineser Elegie (Anfang) und Sonette an Orpheus (1. u. 2. Sonett) in der Rezitation von Irene LaettRezensionen zu Neuausgaben
