
Elagabal (altgriechisch Ἐλαγάβαλος Elagábalos, latinisiert Elagabalus oder Heliogabalus) ist der Name eines antiken Sonnengottes, dessen Kult sein Zentrum in der syrischen Stadt Emesa hatte. Dort befand sich ein Elagabal-Tempel, in dem ein heiliger Stein verehrt wurde. Im Zeitraum von 219 bis 222 dehnte sich der Elagabal-Kult infolge seiner Förderung durch den Kaiser Elagabal auf die Stadt Rom aus; sein Zentrum wurde durch Überführung des Steins dorthin verlegt und ein großer Tempel in der Reichshauptstadt errichtet. Die als fremdartig empfundene syrische Religion löste jedoch in der Führungsschicht des Reichs heftige Irritationen aus. Mit dem Tod des Kaisers im Jahr 222 endete der staatliche Kult in Rom. Fortan beschränkte sich die Verehrung der Sonnengottheit von Emesa wieder weitgehend auf ihr Ursprungsgebiet.
Bei verschiedenen Völkern im Mittelmeerraum, besonders den Phöniziern einschließlich der Punier, wurde Steinkult praktiziert. Bestimmte Steine erhielten religiöse Verehrung. Sie wurden mit Göttern in Verbindung gebracht oder galten selbst als göttlich. Solche Steine nennt man Bätyle (auch Betyle, griechisch baitýlia oder baítyloi). Das Wort ist abgeleitet von aramäisch bet el („Haus Gottes“). Im Zentrum des Elagabalkults stand ein bienenkorbförmiger, höckriger schwarzer Bätyl, der im Tempel von Emesa aufbewahrt wurde. Vielleicht handelte es sich um einen Meteoriten; man nahm an, er sei vom Himmel gefallen. Zu manchen Zeiten – wohl an Festtagen – wurde der Stein mit Stoff umhüllt. Ein ähnlicher Steinkult bestand in Harran. Der dortige Bätyl war dem Mondgott Sin zugeordnet.
Die früher umstrittene Herkunft und Bedeutung des Namens Elagabal ist geklärt. Er besteht aus den semitischen Wörtern ilaha (aramäisch und arabisch „Gott“) und gabal (arabisch „Berg“). Die ursprüngliche Bedeutung war „der Gott Berg“, nicht „Gott des Berges“, denn ilaha liegt nicht im Status constructus, sondern im Status emphaticus vor. Die Verehrung von Bergen als Gottheiten war im Nahen Osten verbreitet. Gemeint ist hier der felsige Festungshügel im Süden von Homs, auf dem sich der große, prunkvolle Elagabal-Tempel befand; dieser „Berg“ ist allerdings nur 30 m hoch (gabal kann auch eine Anhöhe bezeichnen). Es dürfte sich um einen lokalen Kult handeln, der später von einwandernden arabischen Nomaden, die dort sesshaft wurden, übernommen wurde.Von einem Sonnenkult in Emesa berichtete schon im 3. Jahrhundert v. Chr. der Geschichtsschreiber Phylarchos, der bei Athenaios zitiert wird; nach seinen Angaben durfte damals diesem Gott – anders als später Elagabal – nur Honig und kein Wein geopfert werden, da Trunkenheit dem weltbeherrschenden Gott fremd sei. Ein Zusammenhang der Sonnenverehrung mit dem Elagabalkult ist aber vor der römischen Kaiserzeit nicht bezeugt.
Schon im 1. Jahrhundert v. Chr. wurde Emesa von arabischen Fürsten beherrscht, die vielleicht bereits Elagabal als Schutzgott betrachteten. Der älteste Beleg für den Kult und den Gottesnamen ist eine Stele aus dem 1. Jahrhundert n. Chr. Etwa ab dem Beginn der römischen Kaiserzeit erlebte die Stadt dank ihrer Lage an der Karawanenstraße zum Persischen Golf einen wirtschaftlichen und kulturellen Aufschwung. Die Fürsten von Emesa standen in einem Abhängigkeitsverhältnis zum Römischen Reich. Ihre Eigenständigkeit endete im späten 1. Jahrhundert, als die Römer das Gebiet in ihr Reich eingliederten.
Elagabal war nicht nur die Gottheit des Hügels und des Bätyls von Emesa, sondern für seine Anhänger in der römischen Kaiserzeit auch der Sonnengott und als solcher der höchste aller Götter. Wie sein Name zeigt, war seine ursprüngliche Funktion nur die eines Ortsgottes. Die gewaltige Ausweitung der ihm zugeschriebenen Macht und Zuständigkeit, die mit seiner Rolle als Sonnengott verbunden war, ist erst in einer späteren Phase seiner Verehrung eingetreten.Als oberster Gott entsprach Elagabal dem griechischen Zeus bzw. dem römischen Iuppiter, und wie diesen war auch ihm der Adler als vornehmster Vogel heilig. In seiner Eigenschaft als Sonnengott wurde er mit dem griechischen Helios gleichgesetzt. Daher deutete man seinen Namen volksetymologisch als Heliogabalos.
Ab der Zeit des Kaisers Antoninus Pius (138–161) wurde der Bätyl auf Münzen Emesas abgebildet. Im 3. Jahrhundert stand der Elagabal-Kult in voller Blüte. Auswärtige Machthaber übersandten jährlich kostbare Weihegaben für den Tempel, der mit viel Gold und Silber und kostbaren Steinen geschmückt war. Nach dem Vorbild der Pythischen Spiele in Delphi wurden in Emesa regelmäßig Sportfeste (Helia Pythia) zu Ehren Elagabals veranstaltet. Außerdem erteilte Elagabal auch als Orakelgott Auskünfte an Ratsuchende. Die Würde des Oberpriesters war in einer Familie erblich, die vermutlich von dem alten Fürstengeschlecht von Emesa abstammte.
Eine Inschrift aus dem Jahr 249 bezeugt für Elagabal den Beinamen Ammudates. Dies ist die latinisierte Form des aramäischen Wortes ʿammūdā (Steinsäule, stehender Stein).253 wurde in Emesa Uranius Antoninus zum Gegenkaiser erhoben. Er ließ auf seinen Münzen den Bätyl abbilden, womit er sich zu dem örtlichen Sonnenkult bekannte. Kaiser Aurelian soll sich nach seinem Sieg über das Heer der palmyrenischen Herrscherin Zenobia 272 nach Emesa begeben haben, um dort im Elagabaltempel ein Gelübde zu erfüllen und sich für den Beistand des Sonnengottes in der Schlacht dankbar zu erweisen. Angeblich stieß er im Tempel auf diejenige Gestalt der Gottheit, die ihm im Kampf als Helfer erschienen war. In der Folgezeit trat Aurelian als Anhänger des römischen Gottes Sol invictus („unbesiegte Sonne“) hervor, den er als Schutzherrn des römischen Reichs verehren ließ. Dass er seinen Sieg dem Eingreifen des Sonnengottes zuschrieb, dürfte zutreffen, aber er hatte dabei nicht speziell Elagabal im Sinn; der staatliche Sonnenkult, den er in Rom organisierte, knüpfte nicht an den in Emesa praktizierten an.Im 4. Jahrhundert beschrieb der Dichter Avienus in seiner „Beschreibung des Erdkreises“ (Descriptio orbis terrae) den Elagabaltempel von Emesa, dessen Existenz durch diese Erwähnung letztmals bezeugt ist.
In der späten Adoptivkaiserzeit war der Oberpriester des Elagabal-Tempels in Emesa ein Mann namens Julius Bassianus. Der Name „Bassianus“ (weiblich „Bassiana“), den auch einige seiner Nachkommen führten, war wohl von dem Priestertitel Basus abgeleitet. Julius Bassianus hatte zwei Töchter, Julia Domna und Julia Maesa. Weltgeschichtliche Bedeutung erlangte seine Sippe durch die Heirat Julia Domnas mit dem aus Afrika stammenden künftigen Kaiser Septimius Severus (193–211), dem Gründer der Dynastie der Severer. Julia Domna war die Mutter des Kaisers Caracalla (211–217), der nach seinem Großvater Bassianus genannt wurde, und seines jüngeren Bruders Geta, der im Jahr 211 Mitregent Caracallas war. Ihre Schwester Julia Maesa war die Großmutter der beiden letzten Kaiser der Severerdynastie, Elagabal (218–222) und Severus Alexander (222–235). Somit stammten alle severischen Kaiser außer dem Dynastiegründer selbst von dem Elagabal-Priester Julius Bassianus ab.
Der ältere der beiden kaiserlichen Enkel Julia Maesas war Varius Avitus Bassianus. Wegen seiner intensiven Elagabal-Verehrung pflegt man ihn seit der Spätantike „Elagabal“ zu nennen. Dies ist aber nur ein Spitzname, denn zu seinen Lebzeiten war der Gottesname der Gottheit vorbehalten. Als Dreizehnjähriger übernahm er 217 im Elagabal-Tempel von Emesa das erbliche Amt des Oberpriesters, denn er war damals der älteste noch lebende männliche Nachkomme des Julius Bassianus. Als er im folgenden Jahr die römische Kaiserwürde erlangte und nach Rom übersiedelte, behielt er das Priesteramt, das ihm sehr wichtig war, bei. Er überführte den heiligen Stein von Emesa nach Rom und erhob den Elagabal-Kult zur römischen Staatsreligion. Dadurch erhielt die Elagabal-Verehrung kurzzeitig welthistorische Bedeutung.
Auf dem Palatin ließ der junge Kaiser einen großen, prachtvollen Elagabal-Tempel errichten. Das Gebäude befand sich auf der heute als Vigna Barberini bezeichneten Terrasse. Der Bau erhob sich auf einem Gelände von 160 mal 110 Metern. Die Vermutung, es sei nur ein bereits existierender Bau einem neuen Zweck zugeführt worden, hat sich angesichts des archäologischen Befunds als unzutreffend erwiesen. Ein weiterer Tempel wurde außerhalb der Stadt gebaut.Das mit dem Elagabal-Kult verbundene syrische Brauchtum wurde jedoch in Rom nicht heimisch. Es stieß vielmehr in der römischen Führungsschicht auf heftige und breite Ablehnung, da es mit den herkömmlichen römischen Sitten unvereinbar war. Empörung erregte insbesondere die Anordnung des Kaisers, dass Elagabal als oberster Staatsgott dem römischen Iuppiter übergeordnet sein sollte. Auch die Anbringung eines großen Bildes, das den Kaiser als Elagabalpriester beim Opfern zeigte, in der Curia Iulia, dem Versammlungsgebäude des Senats, oberhalb des Altars der Siegesgöttin Victoria musste Anstoß erregen. Die priesterlichen Aktivitäten des Kaisers im Rahmen eines fremdartigen Kults trugen wesentlich dazu bei, ihn in der Hauptstadt des Reichs verhasst zu machen, und waren einer der Gründe für seinen Sturz. Nach seiner Ermordung am 11. März 222 wurde der heilige Stein nach Emesa zurückgebracht. Damit endete die Episode des staatlichen Elagabal-Kults in Rom. Ein privater Kult in der Hauptstadt, der schon in der Zeit der frühen Severer bestanden hatte, dauerte möglicherweise fort.Auch in den Provinzen setzte unter Kaiser Elagabal mancherorts die Verehrung seines Gottes ein. Dabei handelte es sich wohl um einzelne lokale Initiativen, nicht um eine vom Kaiser betriebene systematische Missionierung. Einige Städte prägten Münzen, auf denen der Bätyl abgebildet wurde. In Altava in der Provinz Mauretania Caesariensis errichteten wohlhabende Bürger 221 einen Elagabaltempel. In der kleinasiatischen Stadt Sardes wurden Elagabalia – Wettkämpfe zu Ehren des Gottes aus Emesa – eingerichtet.
In der älteren Forschung wurde die Ansicht vertreten, Elagabal sei mit dem römischen Sonnengott Sol invictus gleichzusetzen. Die wachsende Bedeutung des Sol invictus im 3. Jahrhundert sei ein Symptom des Vordringens orientalischer Einflüsse im religiösen Denken der Römer. Tatsächlich ist Invictus Sol Elagabalus inschriftlich bezeugt. Im Sinne dieser Deutungsweise wurden alle Quellenbelege für den „unbesiegten“ Sonnengott im Westen des Reichs als Hinweise auf Ausbreitung einer orientalischen, von Emesa aus propagierten Sonnenreligion gedeutet. Manche Gelehrte sahen darin ein Zeichen kulturellen Verfalls, der die Folge der „Orientalisierung“, des Vordringens von unrömischen Sitten östlichen Ursprungs gewesen sei. Die spätere Forschung hat aber zeigen können, dass der Kult des Sol invictus im Westen in erster Linie einheimische Wurzeln hatte und nicht auf der Übernahme östlicher Vorstellungen beruhte. Hinter der staatlichen Verehrung des Sol invictus im 3. und 4. Jahrhundert stand eine religionspolitische Konzeption, die sich auf eine bereits bestehende, eigenständige römische Tradition stützen konnte. Der Elagabalkult hingegen war eine syrische Religion, die nur kurzzeitig in Rom Fuß fassen konnte, da sie von den Römern als Fremdkörper im religiösen Leben empfunden wurde.
Einzelheiten der rituellen Elagabal-Verehrung gehen aus den Berichten römischer Geschichtsschreiber über die spektakuläre Verpflanzung des Kults nach Rom unter Kaiser Elagabal hervor. Die Quellen sind die Geschichtswerke zweier Zeitgenossen des Kaisers, Cassius Dio und Herodian, sowie die spätantike Historia Augusta. Der Quellenwert dieser Darstellungen wird allerdings durch die tendenziöse Sichtweise ihrer Verfasser beeinträchtigt. Die römischen Geschichtsschreiber verurteilten das Verhalten und das Religionsverständnis Kaiser Elagabals nachdrücklich. Cassius Dio schilderte den Elagabal-Kult aus der Sicht eines konservativen Senators mit großer Empörung, nahm aber viele wichtige, wohl authentische Einzelheiten in seinen Bericht auf. Herodian schrieb distanzierter; auch er verfügte anscheinend über gute Informationen, doch ist seine Neigung zu Ausschmückungen und Übertreibungen zu berücksichtigen. Der unbekannte Verfasser der Historia Augusta hatte offenbar ebenfalls Zugang zu einer guten, heute verlorenen zeitgenössischen Quelle. Die Glaubwürdigkeit seiner Angaben ist aber umstritten. Er war pagan, als Anhänger der alten römischen Religion wollte er das Christentum diskreditieren. Daher machte er den allgemein verhassten Kaiser Elagabal zu einem Vorläufer spätantiker christlicher Kaiser, indem er ihm religiöse Intoleranz unterstellte.Die Schilderungen in den Quellen legen die Annahme nahe, dass der römische Elagabal-Kult denjenigen von Emesa getreu kopierte, also keine Anpassung an westliche Vorstellungen vorgenommen wurde. Diesen Berichten zufolge fand täglich ein Gottesdienst statt, wobei der prunkvoll gekleidete und geschmückte, zahlreiche Amulette tragende Oberpriester zusammen mit Frauen unter Musikbegleitung – es werden insbesondere Blasinstrumente, Zimbeln und Trommeln erwähnt – vor dem Altar tanzte und sang. Dazu wurde geräuchert. Die Priester waren beschnitten und durften kein Schweinefleisch essen.Zu den täglichen Opfergaben gehörte insbesondere Wein. Zahlreiche Tieropfer wurden dargebracht, das Blut der geschlachteten Opfertiere wurde mit dem Wein vermischt. Die Berichte hingegen, wonach auch rituelle Menschenopfer zur gängigen Praxis gehörten, gelten in der Forschung als unzutreffend oder werden zumindest skeptisch beurteilt. Möglich ist allerdings, dass es eine alte Sitte des Kinderopfers gab, die noch im 3. Jahrhundert von einzelnen Elagabal-Verehrern praktiziert wurde. Die Verbreitung derartiger Gerüchte zeigt jedenfalls, dass die römische Öffentlichkeit bereit war, den Elagabal-Anhängern jede Scheußlichkeit zuzutrauen.
Alljährlich fand im Hochsommer eine vom Kaiser geleitete Prozession vom Tempel auf dem Palatin zu dem anderen, außerhalb der Stadt gelegenen Elagabal-Tempel statt. Dabei wurde der Bätyl auf einem Wagen mitgeführt, der von sechs weißen Pferden gezogen wurde. Der Kaiser schritt dem Wagen voran, wobei er rückwärts gewendet auf den Bätyl blickte und die Pferde lenkte; auf dem Wagen saß niemand.Kaiser Elagabal trug kostbare seidene Gewänder, seine priesterliche Amtstracht, in der er auch öffentlich auftrat. Die römische Wollkleidung mochte er nicht. Dies wurde ihm als unangebrachter Luxus und als Ausdruck von Verweichlichung und Unmännlichkeit verübelt. Der Grund lag aber wahrscheinlich in einer religiösen Vorschrift zur Priesterkleidung; im Osten galten bestimmte Bekleidungselemente tierischer Herkunft, darunter Lederschuhe, als unrein.Anscheinend beschränkte sich die religiöse Betätigung der Elagabal-Anhänger auf den Vollzug der Riten, der ihnen das Wohlwollen des Gottes verschaffen sollte. Von einer Jenseitslehre oder ethischen Normen ist in den Quellen nicht die Rede.
In der arabischen Tradition von Emesa hatte Elagabal zwei Gefährtinnen. Die eine war die Kriegsgöttin al-Lāt, die andere vermutlich al-Uzzā. Die beiden Göttinnen wurden auch auf der Arabischen Halbinsel verehrt und sind im Koran erwähnt. Im Römischen Reich wurde al-Lāt mit der jungfräulichen Athene der Griechen – der römischen Minerva – identifiziert, in al-Uzzā sah man die Entsprechung zur griechischen Aphrodite und zur römischen Venus, der Liebesgöttin.An dieser Vorstellung einer Verbindung von Männlichem und Weiblichem in der Götterwelt orientierte sich Kaiser Elagabal, als er in Rom die Hierogamie („heilige Hochzeit“) seines Gottes feierte. Dabei handelte es sich um die Vermählung des Sonnengottes mit der karthagischen Tinnit, die lateinisch „Himmelsgöttin“ (Dea Caelestis), griechisch Urania genannt wurde. Ihr kam in dieser Hierogamie die Funktion von al-Uzzā zu. Zum Zweck der Hochzeitsfeier ließ der Kaiser das berühmte Kultbild der Himmelsgöttin, die auch als Mondgöttin galt, aus Karthago nach Rom holen. Dazu stellte er fest, es sei passend, dass der Sonnengott die Mondgöttin heirate. Die Art der Beziehung der jungfräulichen Athene zum Sonnengott im Rahmen des in Rom praktizierten Kults ist nicht genau bekannt. Unrichtig ist die Angabe Herodians, Kaiser Elagabal habe erst Athene zur Gattin seines Gottes ausersehen, sich aber schließlich für Aphrodite entschieden.Die wichtige Rolle der beiden Göttinnen lässt erkennen, dass die Elagabal-Religion nicht, wie in älterer Forschungsliteratur mitunter angenommen wurde, monotheistisch war. Die Eigenschaften und Zuständigkeiten Elagabals und seiner Gefährtinnen wurden nicht synkretistisch vermischt, sondern klar unterschieden. Elagabal war nicht der einzige, sondern der oberste Gott. Auch nach dem Tod des Kaisers Elagabal scheint der Gedanke einer Verbindung des Sonnengottes mit den beiden Göttinnen in Emesa lebendig geblieben zu sein, wie die Münzprägung des dort im Jahr 253 erhobenen Gegenkaisers Uranius Antoninus erkennen lässt.Die Promiskuität und Prostitution, die dem Kaiser Elagabal als persönliche Gewohnheit vorgeworfen wurde, ist als Bestandteil religiöser Riten zu deuten, die nicht zum Kult des Gottes Elagabal, sondern zu dem der Dea Caelestis gehörten. Religiöse Prostitution war in orientalischen religiösen Traditionen im Rahmen des Fruchtbarkeitskults verbreitet.
Elagabal war nicht anthropomorph (menschengestaltig). In den bildlichen Darstellungen seines Kults erscheint er nie in einer bestimmten Gestalt, sondern ist nur an der Beschriftung und an seinen Symbolen zu erkennen. Einen solcher Kult nennt man anikonisch („bilderlos“, ohne Abbildungen der Gottheit). Der anikonische Elagabal wurde ikonographisch nicht mit dem anthropomorphen römischen Sol invictus vermischt. Oft kommt als Symbol der Gottheit ein Adler vor, der gewöhnlich eine Krone im Schnabel trägt. Auf der Stele des 1. Jahrhunderts n. Chr. ist der Berg, von dem der Gottesname abgeleitet ist, abgebildet. Auf dem aus Felsbrocken zusammengesetzten Berg sitzt der Adler.
Das wichtigste Element der Darstellungen ist der Bätyl. Er wird oft auf einer Quadriga (Viergespann) gezeigt, also während der Prozession. Ein Figurenkapitell, das von einer römischen Elagabal-Kultstätte stammt, zeigt den Bätyl mit dem sitzenden Adler und den beiden Gefährtinnen des Gottes, die jeweils eine Hand auf die Spitze des Steins legen. Auf Münzen bedeckt der Adler den Bätyl mit seinen ausgebreiteten Flügeln.
Eine Münze aus der Mitte des 3. Jahrhunderts zeigt den Bätyl im Tempel von Emesa und gestattet damit einen Blick ins Innere des Heiligtums. Neben dem Stein befinden sich zwei verzierte, pilzförmige Objekte, die in der Numismatik meist als Sonnenschirme gedeutet werden. In den Schalen zweier Kandelaber sind Flammen sichtbar. Vor der Basis, auf welcher der Stein ruht, steht eine große Amphore. Vor dem Bätyl ist ein nach links blickender Adler zu sehen, bei dem es sich vielleicht um ein Relief auf dem Stein handelt.
Christian Augé, Pascale Linant de Bellefonds: Elagabalos. In: Lexicon Iconographicum Mythologiae Classicae (LIMC). Band 3, Artemis, Zürich 1986, ISBN 3-7608-8751-1, S. 705–708 (Tafelband S. 542)
Edward Lipiński: Elaha Gabal d’Émèse dans son contexte historique. In: Latomus 70, 2011, S. 1081–1101
Henri Seyrig: Antiquités syriennes. Le culte du Soleil en Syrie à l’époque romaine. In: Syria. Revue d’art oriental et d’archéologie 48, 1971, S. 337–373
Jean Starcky: Stèle d’Elahagabal. In: Mélanges de l’Université Saint-Joseph 49/2, 1975–1976, S. 501–520Verpflanzung des Elagabal-Kults nach Rom
Martin Frey: Untersuchungen zur Religion und zur Religionspolitik des Kaisers Elagabal. Franz Steiner, Stuttgart 1989, ISBN 3-515-05370-0
Michael Pietrzykowski: Die Religionspolitik des Kaisers Elagabal. In: Aufstieg und Niedergang der römischen Welt, Band II 16.3, hrsg. von Wolfgang Haase. De Gruyter, Berlin 1986, ISBN 3-11-008289-6, S. 1806–1825
Ruprecht Ziegler: Der Burgberg von Anazarbos in Kilikien und der Kult des Elagabal in den Jahren 218 bis 222 n. Chr. In: Chiron 34, 2004, S. 59–85

Der Elch (Alces alces) ist die größte heute vorkommende Art der Hirsche. Sein Lebensraum erstreckt sich über Nordeuropa, Nordasien und Nordamerika. Der Elch wird von der IUCN als „nicht gefährdet“ eingestuft.
Der Elch hat eine Kopf-Rumpf-Länge bis 3 Meter, eine maximale Schulterhöhe von 2,30 Meter; er wiegt bis 800 Kilogramm. Die Körpergröße und das Gewicht sind allerdings je nach Unterart, Lebensraum und Lebensbedingungen unterschiedlich. So erreichten männliche Elche, die in den 1950er Jahren am Oberlauf der Petschora im nördlichen, europäischen Teil Russlands geschossen wurden, maximal ein Gewicht von 518 Kilogramm. Elchkühe wogen maximale 423 Kilogramm. Elchhirsche sind ab dem dritten Lebensjahr durchschnittlich schwerer als die Weibchen. Die Widerristhöhe der Elche aus der Petschora-Taiga betrug maximal 190 Zentimeter.
Charakteristisch für den Körperbau des Elches ist der kurze massige Rumpf mit seinen relativ langen Gliedmaßen. Der Brustkorb ist bei erwachsenen Tieren stark entwickelt und die Schulterpartie muskulös. Die Wirbel der Brustwirbelsäule tragen verlängerte Dornfortsätze. Dort setzen die Muskeln und Bänder an, die das Gewicht des Geweihes tragen. Dadurch entsteht ein erhöhter Widerrist, der typische Elchbuckel, der mit langen, abstehenden Haaren bedeckt ist. Der schwächer ausgebildete hintere Teil des Rumpfes fällt nach hinten ab. Der mit Haaren bedeckte Schwanz ist mit acht bis zehn Zentimetern eher kurz und erreicht nur ein Drittel der Länge der Ohren; er liegt dicht am Körper an und tritt kaum aus dem Fell hervor. Ein auffälliger Geschlechtsdimorphismus besteht bezogen auf den Körperbau nicht. Elchkühe sind lediglich etwas leichter, der Widerrist tritt nicht so stark in Erscheinung und die Schulterpartie ist etwas schwächer bemuskelt.
Die Ohren sind breit, länglich oval und laufen an den Enden etwas spitz zu. Die Augen sind im Verhältnis zum Kopf sehr klein. Die Augenfarbe ist dunkel. Die Voraugendrüse, die sich bei den meisten Hirschen findet, ist beim Elch verhältnismäßig klein oder fehlt. Charakteristisch für den Elch ist die breite und überhängende Oberlippe. Sie verleiht dem Gesichtsprofil eine gekrümmte Linie. Bei beiden Geschlechtern findet sich ein Kinnbart, der am größten bei Elchen zwischen dem 3. und 5. Lebensjahr ist. Er ist dann durchschnittlich 20 bis 25 Zentimeter lang. Einzelne Individuen weisen aber auch einen deutlich längeren Kinnbart auf. Bei älteren Elchen kann dieser Kinnbart fast verschwunden sein.Je nach Alter und Geschlecht beträgt die Beinlänge bei europäischen Elchen 90 bis 110 Zentimeter, bei Alaska-Elchen sind die Beine etwa zehn Zentimeter länger. Die Vorder- und Hinterhandgelenke sind sehr beweglich, was den Elchen in unebenem Gelände eine sehr schnelle Fortbewegung ermöglicht; die langen Beine machen sie für den Aufenthalt in Sümpfen und Mooren besonders geeignet. Elche besitzen an Vorder- und Hinterbeinen Zwischenklauendrüsen, mit denen sie Duftspuren legen.
Elche sind Paarhufer und haben somit gespaltene Hufe. Ein Huf besteht jeweils aus den zwei Hauptklauen oder Schalen und einer Afterklaue. Die Hauptklauen sind bis zu 18 Zentimeter lang, laufen spitz aus und sind insbesondere an der Vorderseite hart- und scharfkantig. Die vorderen Hufe sind etwas größer und breiter als die hinteren. Eine Besonderheit ist die Schwimmhaut, eine Verbindungshaut zwischen den großen Schalen, die sich nur beim Elch findet, keine andere Hirschart weist diese Eigenart auf. Die Hufe sind immer etwas gespreizt, auf weichem Grund gehen sie besonders weit auseinander, dabei spannt sich die Schwimmhaut und vermindert das Einsinken im Schnee oder morastigem Boden. Bei weit gespreizten Hufen übernehmen auch die Afterklauen eine Stützfunktion.
Das Haar ist grob und hart. Die längsten Haare finden sich am Widerrist. Die durchschnittliche Länge der Haare beträgt an dieser Stelle 16 bis 18 Zentimeter, kann aber bei einzelnen Individuen auch eine Länge von 24 bis 25 Zentimeter erreichen. Sie stehen sehr dicht, sind etwas nach hinten gerichtet und unterstreichen die für Elche charakteristische buckelige Gestalt. Die Nackenhaare sind etwas kürzer als die Haare am Widerrist und bilden eine kurze Mähne. Am Kopf und an den Beinen sind die Haare sehr kurz.
Die Fellfarbe von Rumpf, den oberen Teilen der Läufe, dem Hals und dem Kopf variiert individuell zwischen rotbraun und schwarzbraun. Sie ist am dunkelsten im Sommer, wenn Elche die letzten Reste ihres Winterhaares verloren haben und am hellsten zu Ende des Winters, wenn sich die dunklen Haarspitzen der Winterhaare abgenutzt und die hellen Basalabschnitte der Winterhaare durchschimmern. Der Beginn des Haarwechsels vom Winter- ins Sommerfell ist abhängig vom jeweiligen Verbreitungsgebiet. In Mittelrussland beginnt er im April und dauert bis Juli.Abweichend von vielen anderen Hirschen fehlt beim Elch der Spiegel am Rumpfende. Der Spiegel hat bei vielen Hirscharten eine Sozialfunktion und hilft beispielsweise dem Kalb, der Mutter zu folgen. Bei den Elchen übernehmen die grauweißen Läufe diese Signalfunktion. Die Läufe sind ab etwa der Mitte des Unterschenkels beziehungsweise des Unterarms grauweiß bis fast reinweiß mit einem silbrigen Schimmer und kontrastieren stark mit dem dunklen Rumpf. Sie sind gut sichtbar, wenn Elche sich im Halbdunkel des Waldes bewegen, bei dem sich der dunkle Rumpf nur wenig vom Hintergrund abhebt.Frisch geborene Elchkälber weisen keine Fleckung auf, wie sie für die Jungtiere vieler Hirscharten charakteristisch ist. Sie sind einschließlich der Läufe dunkelbraun bis rötlichbraun. Einzelne Individuen weisen gelegentlich auf dem Hinterhals und dem Rücken einen Aalstrich auf.
Die männlichen Tiere zeichnen sich durch ein Geweih mit einer maximalen Spannweite von zwei Metern aus. Besonders große Schaufelgeweihe weisen die Alaska-Elche auf. Schaufelgeweihe der europäischen Unterart bleiben etwas kleiner und haben eine Spannweite von bis zu 1,35 Meter und wiegen bis zu 20 Kilogramm. Das Geweih wird jedes Jahr im Zeitraum Januar bis Februar abgeworfen. Es ist in Größe und Gestalt sehr veränderlich und kann aus verzweigten Stangen oder aus breiten, flächigen Schaufeln sowie einer Mischung dieser zwei Typen bestehen. In der Regel weist es eine horizontal zum Schädel stehende Stange und eine breite, abgeflachte Schaufel auf, deren Fläche seitwärts und etwas nach hinten gerichtet ist. An der Schaufel sitzen Fortsätze, die nach vorn außen und nach hinten gerichtet sind.Junge Elchhirsche entwickeln in ihrem zweiten Lebensjahr erstmals einen kurzen, ungegabelten Spieß. Im folgenden Jahr weisen sie eine Gabel mit zwei Enden auf, dann folgt in der Regel ein kleines Geweih mit jeweils drei Enden je Geweihseite. Die weitere Entwicklung unterliegt keiner Gesetzmäßigkeit, so dass eine Altersbestimmung der Elche anhand der Zahl der Geweihenden nicht möglich ist. Meist bilden sich jedoch in den folgenden Jahren zunehmend größer werdende Schaufeln aus. Männchen im Alter zwischen fünf und zehn Jahren, dem Zeitraum, in denen sie physisch voll entwickelt sind, haben gewöhnlich die größten Geweihe; bei älteren Elchen geht die Geweihentwicklung wieder zurück.
Als Bewohner des nördlichen borealen Waldes und der Taigagebiete kommt der Elch in Europa, Asien und Nordamerika vor. Besiedelt werden in Asien unter anderem die Mongolei und die Mandschurei. Er fehlt auf Sachalin und auf den Kurilen, ansonsten stellt der Pazifik die Ostgrenze des asiatischen Verbreitungsgebietes dar.
In Nordamerika kommt der Elch vor allem in Kanada vor, im zentralen und westlichen Alaska, in großen Teilen von Neuengland und New York, in den oberen Rocky Mountains, Nordost-Minnesota, Michigan auf der Oberen Halbinsel und der Isle Royale im Lake Superior. Isolierte Elch-Populationen wurden auch weiter südlich, in den Bergen von Utah und Colorado gesichtet.
Größere europäische Elchpopulationen finden sich in Norwegen, Schweden, Finnland und den baltischen Staaten; weit verbreitet sind sie auch in Russland, kleine Ansiedlungen gibt es in Polen, Weißrussland und Tschechien. In Schweden etwa werden pro Jahr rund 80.000 Elche erlegt, was die hiesige Population jedoch nicht gefährdet. In historischer Zeit kam der Elch auch in Westeuropa mit Ausnahme des Südens, des Südostens und Westens vor. Um die Zeitenwende war der Elch in ganz Germanien, das damals ein sehr dünn besiedeltes Waldland war, verbreitet. Überreste der ältesten Jagdfallen für Elche wurden in Nordeuropa bereits auf 3.700 vor Christus datiert. Mit dem Verschwinden der Wälder und Ausweitung des Kulturraumes ging der Elchbestand zurück. Bis zum Zweiten Weltkrieg kam der Elch in Deutschland in Mecklenburg, Teilen Ost-Brandenburgs und Schlesiens und vor allem aber in Ostpreußen auf der Kurischen Nehrung und in den Niederungen am Ostufer des Kurischen Haffs vor. Der kleine Bestand in Mecklenburg und Neuvorpommern verschwand mit den Kriegswirren. Der Bestand im ehemaligen Ostpreußen konnte sich jedoch bis heute halten. In jüngster Zeit kehrt neben dem Wolf auch der Elch zurück und breitet sich in Deutschland unter anderem in Brandenburg aus.Das Verbreitungsgebiet des Elches ist sehr dynamisch. Im europäischen Russland kam es beispielsweise in der ersten Hälfte des 19. Jahrhunderts zu einer drastischen Arealverminderung, bei der sich die südliche Verbreitungsgrenze um fast 1000 Kilometer nach Norden verschob. Die Ursachen dafür sind unklar, da es in diesem Zeitraum zu keinem starken Rückgang der Waldzone kam. Eine starke Bejagung ist aber möglicherweise einer der Einflussfaktoren, da ab Anfang des 18. Jahrhunderts Teile der russischen Armee mit Uniformen aus Elchleder ausgerüstet wurden. In der zweiten Hälfte des 19. Jahrhunderts, als die Verwendung von Elchleder für die russische Uniformenschneiderei nahezu vollständig eingestellt wurde, kam es zu einer weitgehenden Wiederbesiedelung des verlorengegangenen Areals. Dabei verschob sich die südliche Verbreitungsgrenze in wenigen Jahrzehnten wieder um 500 bis 600 Kilometer nach Süden. Im Verlauf des 20. Jahrhunderts besiedelte der Elch auch den Kaukasus wieder, wo er seit Beginn des 19. Jahrhunderts ausgestorben war. Dies ist aus zoogeographischem Gesichtspunkt interessant, weil Elche dabei die Steppen des Kaukasusvorlands überwanden, die vollständig kultiviert und dicht besiedelt sind. Offenbar sind Elche in der Lage, für ihre Lebensraumanforderungen ungeeignete Gebiete schnell zu durchqueren, um geeignete Lebensräume zu erreichen.
Die Ausbreitungsdynamik des Elchs zeigt sich auch in Mitteleuropa. In den letzten Jahren wurden Einzeltiere über längere Zeiträume in Sachsen-Anhalt,Brandenburg, Hessen und Thüringen gesichtet, fallweise (zuletzt im August 2014 im Stadtgebiet von Dresden) auch in Sachsen. In Bayern wurde wegen der zunehmenden Einwanderung der Tiere aus Tschechien sogar ein „Elchplan“ entwickelt. Elch, wie auch Wolf, werden somit als wieder in Deutschland heimisch gewordene Wildtiere bezeichnet. Auch im österreichischen Thayatal, Böhmerwald sowie bis zum südlichen Waldviertel wie auch im Mühlviertel wurden aus Tschechien zugewanderte Elche beobachtet.
Im Jahr 1904 wurden Elche in Neufundland erfolgreich eingeführt; sie sind dort inzwischen die beherrschenden Huftiere. Zehn Elche wurden 1910 in Fiordland in Neuseeland ausgesetzt, die jedoch wieder ausstarben. Seit 2015 findet ein Auswilderungsprojekt auch in Dänemark statt (Lille Vildmose).
Der Elch ist in seinen Lebensraumansprüchen anpassungsfähig, bevorzugt aber unebenes, schwergängiges Gelände. Flache und hindernislose Steppe, Tundra oder Prärie wird von ihnen selten genutzt. Er ist relativ ortstreu und hält sich in der Regel in einem Gebiet auf, das ihm vertraut ist. Beides ist auf das Fluchtverhalten der Elche zurückzuführen. Elche fliehen vor ihren Fressfeinden wie Wölfen oder Bären, da sie mit ihren langen Beinen Hindernisse im Trott überwinden können, die von ihren Verfolgern mit größerem Körpereinsatz übersprungen werden müssen. Dieses Verhalten setzt jedoch auch voraus, dass der Elch sich in einem Gebiet aufhält, das ihm vertraut ist. Elche nutzen ganzjährig ein Territorium von bis zu 1500 Hektar. Sie halten sich saisonal jedoch in einem deutlich kleineren Gebiet auf. Nach nordamerikanischen Untersuchungen betragen diese saisonalen Territorien zwischen 200 und 400 Hektar.Elche sind in baumloser Arktis, alpinen Matten, Prärie und Sumpfwäldern zu finden. Regionen mit hohen Schneelagen werden von ihnen gemieden. In Regionen, in denen viel Schnee fällt, halten sie sich meist an Stellen mit einem Bestand an Nadelbäumen und immergrünen Sträuchern auf, die verhindern, dass sich am Boden hoher Schnee bilden kann. In Schweden verlassen Elche ihre Sommerreviere und suchen niedrigere Höhenlagen auf, sobald die Schneehöhe mehr als 42 Zentimeter beträgt.
Elche sind Selektierer und fressen überwiegend sehr energiereiche Nahrung, wie junge Baumtriebe und Wasserpflanzen, da frisches Laub wesentlich protein- und mineralreicher als Gras ist. Sie bevorzugen dabei Pappeln, Birken und Weiden. Wasserpflanzen werden möglicherweise auch wegen ihres hohen Natriumgehalts von Elchen gerne gefressen. Elche sind die einzigen Hirsche, die auch unter Wasser äsen können. Im Herbst und Winter fressen sie auch Blaubeerreisig, Besenheide und junge Kieferntriebe. Ähnlich wie bei anderen Selektierern ist der Pansen verhältnismäßig klein, da die energiereiche Nahrung schnell verdaut wird.
Elche halten sich normalerweise an den Stellen auf, die ihnen ein großes Angebot an Nahrung bieten. Sie ziehen erst weiter, wenn dieses Nahrungsangebot erschöpft ist. Anders als Rentiere sind sie während ihrer Nahrungssuche einzelgängerisch und durchstreifen dabei ein wesentlich kleineres Gebiet. Die aufgenommene Nahrungsmenge schwankt jahreszeitlich. Im Sommer und Herbst fressen sie sich einen Fettvorrat an, mit dem sie die während des Winters geringere Nahrungsaufnahme kompensieren. In den Wintermonaten verlieren sie etwa 12 bis 20 Prozent ihres Herbstgewichtes. Bullen, die während der Brunft gleichfalls erheblich an Gewicht verlieren, sind einem größeren Risiko als Elchkühe ausgesetzt, in den Wintermonaten zu verhungern.
Elche sind tagaktive Einzelgänger. Im Winter finden sie sich manchmal zu losen Gemeinschaften zusammen. Temperaturen von minus 50 °C sind für sie kein Problem. Bei Temperaturen von plus 10 °C bis minus 20 °C fühlen sie sich am wohlsten; wird es zu warm, leiden sie an Hitzestress. Dabei sind die Bullen durch ihre Körpergröße anfälliger als die Kühe und Kälber. Wird es den Tieren zu warm, ziehen sie in die kühleren Gebirge, steiles Gelände versuchen sie zu vermeiden.
Das Elchgeweih ist im Herbst, zum Beginn der Brunft ausgewachsen. Dann streifen die Bullen die Basthaut des Geweihs an Bäumen und Sträuchern ab. Mit gefegtem Geweih stellen die Bullen in Übungskämpfen eine Rangordnung fest. Diese Kämpfe werden noch nicht mit aller Kraft geführt. Wenn der Platzhalter Mitte September die Kühe gegen seine Rivalen verteidigt, werden aus Drohgebärden und leichtem Drücken und Schieben wütende Zweikämpfe. Doch selbst in dieser Phase versuchen die Bullen Kraft zu sparen und ihre Gegner einzuschüchtern, indem sie mit ihrem Geweih Sträucher und Büsche bearbeiten. Während der Brunft nehmen die Bullen kaum Nahrung auf und verlieren stark an Gewicht.
An den Brunftplätzen finden sich oft weibliche Rudel von bis zu 15 Tieren ein. Elchkühe sind in der Paarungszeit alle 28 Tage für nur 30 Stunden empfängnisbereit. Die Kuh zeigt sich dem Bullen zuerst desinteressiert bis ablehnend. Je näher jedoch der Zeitpunkt ihrer Empfängnisbereitschaft kommt, desto eher reagiert sie auf seine Annäherungsversuche. Die Paarung dauert nur zwei bis drei Sekunden. Sie erfolgt mehrmals am Tage, meistens in den frühen Morgenstunden oder spät am Abend. Da Elche meistens solitär leben, verlassen die Weibchen die Bullen nach der Paarung wieder. Sind alle Weibchen gedeckt, verlassen auch die Bullen den Brunftplatz.
Viele Elchkälber sterben durch eine Infektion mit einem auch bei Rindern vorkommenden Betacoronavirus. Die Erkrankung ist für sie tödlich. Eine Behandlung
der Erkrankung ist nicht möglich. Einzige Hilfe verspricht eine Impfung der Kühe im letzten Drittel der Trächtigkeit.Die Tragzeit dauert 226 bis 264 Tage (etwa acht Monate). Meistens wird ein einziges Tier geboren, aber auch Zwillinge sind keine Seltenheit. Wenige Tage vor der Geburt vertreibt die Elchkuh das letztjährige Kalb. Für die Geburt sucht sich die Elchkuh eine einsame, geschützte Stelle im Wald. Nach der Geburt gelten Elchkühe als sehr gefährlich. Menschen, die ihnen zu nahe kommen, attackieren sie mit ihren Hufen. Dabei kam es schon zu tödlichen Unfällen. Bereits wenige Minuten nach der Geburt versucht das Kalb aufzustehen; nach etwa 20 Minuten folgt es der Mutter. Das Kalb ist kurz nach der Geburt etwa 80 Zentimeter groß und wiegt 10 bis 15 Kilogramm. Zwillinge sind meistens etwas kleiner und leichter. Die Mutter säugt das Kalb an ihren vier Zitzen bis zu achtmal am Tag. Das Jungtier trinkt in den ersten Tagen täglich bis zu 1,5 Liter Milch, mit zunehmender Größe bis zu 3 Liter.
Das Fell der Jungtiere ist sehr weich, dicht und meist von gleichmäßiger rötlicher bis brauner Farbe. Der erste Fellwechsel findet schon nach drei Monaten statt. Das Kalb bleibt mindestens ein Jahr bei seiner Mutter und wird vertrieben, sobald eine neue Geburt ansteht. Junge Elche werden bereits nach 16 bis 17 Monaten geschlechtsreif, in diesem Alter können sie sich jedoch noch nicht gegen die Altbullen durchsetzen. Zwischen dem sechsten und elften Lebensjahr haben Elchkühe ihre größte Fruchtbarkeit. Die maximale Lebensdauer liegt bei 27 Jahren, in Freiheit dürften aber selten 15 Jahre überschritten werden.
Elchkälber sind in ihren ersten Lebensmonaten zu klein, um ihren Müttern in deren Geschwindigkeit über Hindernisse zu folgen. Die enge Bindung zwischen Kalb und Mutter führt dazu, dass Elchkühe diese sehr entschieden verteidigen. Kälber erreichen in der Regel vor ihrem ersten Winter eine Körpergröße, die es ihnen möglich macht, ihren Müttern zu folgen. Sie sind jedoch noch zu schwach, um sich erfolgreich zu verteidigen. Selbst verteidigen können sie sich mit sechzehn bis achtzehn Monaten, Alaska-Elche wiegen dann etwa 280 Kilogramm.Natürliche Feinde des Elches sind Braunbären und Wölfe (in Europa, Asien und Nordamerika) sowie Schwarzbären und Pumas (nur in Nordamerika). Doch auch Luchse und Vielfraße können sehr junge Kälber schlagen. Ausgewachsene und gesunde Elche müssen auf Grund ihrer Körpergröße kaum eine andere Tierart fürchten. Ihr Trott ist außerdem sehr schnell. Bei Elchen in Schweden ist schon eine Geschwindigkeit von 60 Kilometern pro Stunde gemessen worden.
Zu den Fressfeinden zählen in erster Linie Braunbären und Schwarzbären. Während Schwarzbären eher kleinere Elche jagen, halten sich die größeren Braunbären, wie der Grizzlybär, an die großen Elche. Sie können sogar erwachsene Elchbullen überwältigen. Die Bären packen ihre Opfer mit den Fangzähnen an der Kehle und drücken ihnen die Luft ab.
In Ostasien zählt der Elch zu den Beutetieren des Sibirischen Tigers. Sibirische Tiger sind die größten ihrer Art; für diese Beutegreifer sind selbst die ausgewachsenen Elchbullen kein Problem. Anders als Bären töten Tiger ihre Beute mit einem Nackenbiss und brechen ihr dabei mit ihren starken Kiefern das Genick.
Wölfe reißen häufig Elchkälber und Jährlinge. Erwachsene Elche greifen sie nur an, wenn diese alt, krank oder verletzt sind. Von sehr großen Wolfsrudeln werden jedoch auch erwachsene, gesunde Bullen überwältigt. Besonders in tiefem Schnee oder auf dünnem Eis sind die Wölfe dem Elch deutlich überlegen.
Elche werden häufig von Parasiten wie Zecken, Milben oder Leberegeln befallen. Diese können Krankheiten übertragen, die die Tiere schwächen oder sogar ihren Tod verursachen.
Innerhalb der Paarhufer gehört der Elch zur Familie der Hirsche, die besonders artenreich in der Neuen Welt vertreten ist. Innerhalb dieser Familie zählt der Elch zu den sogenannten Trughirschen. Diese Unterfamilie ist unter anderem dadurch charakterisiert, dass an den Vorderbeinen von den stark reduzierten Mittelknochen der 2. und 5. Finger nur die distalen Abschnitte als dünne, stäbchenförmige Knöchelchen erhalten geblieben sind. Der Elch zählt wie das Rentier zu den Hirscharten, die sowohl in der Neuen als auch der Alten Welt vertreten sind.
In Europa traten die ersten Hirsche vor 25 bis 30 Millionen Jahren auf. Diese ursprünglichen Arten trugen allerdings noch kein Geweih. Erst aus dem Jungtertiär gibt es Funde von Hirschen mit Geweih, aber noch mit Eckzähnen. Die Eckzähne bildeten sich im Laufe der Evolution zurück, während sich die Geweihe immer stärker ausbildeten. Elche der Gattung Alces sind seit dem Pleistozän bekannt. Die ausgestorbene nordamerikanische Gattung Cervalces und der ausgestorbene Breitstirnelch (Alces latifrons) des Pleistozäns mit einem über zwei Meter breiten Geweih, gelten als enge Verwandte.
Der heutige Elch ist eine relativ junge Art, wahrscheinlich nicht älter als 60.000 Jahre, als sein Ursprungsgebiet wird Zentralasien angenommen. Die Vorfahren der amerikanischen Elche sind am Ende des Pleistozäns über die in der letzten Eiszeit trocken liegende Beringbrücke nach Alaska gezogen. Dies war möglich, als sich die Borealen Wälder, die Lebensräume des Elchs am Ende des Pleistozäns vor rund 12.000 Jahren nach Norden schoben und die Mammutsteppen verdrängten. So lange die Beringbrücke trocken lag, konnte der Elch über diese Landbrücke nach Nordamerika gelangen, wo bis zu seiner Ankunft noch die ähnliche Gattung Cervalces verbreitet war. Diese verschwand bald darauf. Als es wärmer wurde und der Meeresspiegel wieder stieg, wurde die Verbindung nach Eurasien unterbrochen und die Population der amerikanischen Elche von der eurasischen isoliert.
Europäischer Elch (Alces alces alces), Skandinavien, Polen, baltische Staaten, Nordrussland westlich des Ural
Alaska-Elch (Alces alces gigas), Alaska, YukonEine weitere Unterart, der Kaukasus-Elch (Alces alces caucasicus), ist im frühen 19. Jahrhundert ausgerottet worden.
Wie bei den meisten Tierarten gibt es über die genaue Zahl der Unterarten keine Einigkeit unter Zoologen. So gibt es beispielsweise Auffassungen, nach denen alle nordamerikanischen Elche in Wirklichkeit zu einer einzigen Unterart zusammengefasst werden müssten.
Einige Systematiken teilen die Elche überhaupt nur in zwei Arten auf, den Eurasischen Elch (Alces alces) und den Amerikanischen Elch (Alces americanus).
Dass Elche seit der Steinzeit von Menschen gejagt werden, schließt man aus entsprechenden Darstellungen in Höhlenzeichnungen. Die früheste Darstellung eines Elches findet sich im etwa 14.000 Jahre alten Bernsteinelch von Weitsche, die früheste Beschreibung im sechsten Buch Caesars De bello Gallico in einem Exkurs über den Herkynischen Wald in Germanien. Caesar stützt sich in seinen Angaben auf heute verlorene Darstellungen griechischer Ethnographen wie Eratosthenes. Dort beschreibt er die Elche als Tiere ohne Kniegelenke, die sich zum Schlafen gewöhnlich an Bäume anlehnen würden. Die Germanen würden diese Schwäche zur Jagd auf Elche nutzen, indem sie Bäume ansägten, so dass diese umfielen, sobald sich ein Elch daran lehnt.
Auch Plinius der Ältere beschrieb in seiner Naturalis historia den Elch in gleicher Weise und bereicherte die Darstellungen um weitere Falschbehauptungen: Wegen seiner großen Oberlippe, steht bei ihm zu lesen, könne der Elch nur rückwärts gehend grasen.
Darstellungen von Elchen, Elchköpfen oder Elchschaufeln galten und gelten seit langer Zeit als volkstümliche Symbole für Ostpreußen, meist in den Preußenfarben schwarz und weiß gehalten. Seit 1957 ist die schwarze Elchschaufel im weißen Feld ein eingetragenes Warenzeichen des Vertriebenenverbandes Landsmannschaft Ostpreußen. Das Brandzeichen des ostpreußischen Gestüts Trakehnen zeigt zwei Elchschaufeln.
Durch Bejagung wurde der noch im Mittelalter in Deutschland weit verbreitete Elch hier ganz ausgerottet. Nur gelegentlich wandern Elche aus Polen nach Deutschland ein. Auch dort waren sie zwischenzeitlich fast ausgestorben, nur im Nationalpark Białowieża hatten sie überlebt. Inzwischen umfasst der landesweite Bestand in Polen wieder 4000 Tiere. Eine kontrollierte Wiedereinführung in Deutschland ist wegen erwarteter Konflikte mit der Forst- und Landwirtschaft nicht geplant. Ein Auswilderungsprojekt im Biosphärenreservat Oberlausitzer Heide- und Teichlandschaft prüft die Auswirkungen und Anforderungen; die Elche sollen helfen, die Heide vor dem Zuwachsen zu bewahren. Eine dauerhafte Wiederansiedlung soll dies jedoch nicht sein. Im Jahr 2007 wurden in Südbrandenburg neun Elche, darunter zwei Kühe, in freier Wildbahn beobachtet.Das Bayerische Staatsministerium für Ernährung, Landwirtschaft und Forsten hat am 10. Oktober 2007 gemeinsam mit dem Obersten Jagdbeirat ein 14-seitiges Informationsmaterial zum Umgang mit Elchen herausgegeben. Den Auftrag dazu erteilte der Bayerische Landtag. Anlass war die zunehmende Einwanderung von Elchen aus Tschechien nach Bayern. In Österreich dürfte es sich nur um ziehendes Wild handeln, eine echte Wiederansiedlung wird aufgrund des eingeschränkten Lebensraums als unwahrscheinlich eingeschätzt, selbst die beiden benachbarten südböhmischen Elchpopulationen sind als „nicht gesichert“ angesehen.
Mit ihrem dunklen Fell sind sie im langen, dunklen nördlichen Winter sehr gut getarnt. Das wird Autofahrern und Elchen heute zum Verhängnis, so sterben in Alaska jährlich etwa 500 bis 1000 Elche im Straßenverkehr, in Finnland sind es jährlich ca. 3500 und in Schweden 4000 bis zu 5000 Tiere. Die meisten Unfälle geschehen im Frühjahr, wenn die unerfahrenen Jährlingskälber ihre Mütter verlassen; auch in der Brunftzeit werden viele Bullen von Autos erfasst. In Schweden versucht man, die Unfallgefahr mit Wildzäunen und Straßenunterführungen zu vermindern. 
Gelegentlich kommt es auch in Deutschland zu Unfällen mit freilebenden Elchen. Die Fallzahlen steigen an, seit Polen im Jahr 2001 die Jagd auf Elche verboten hat. Die Population verdoppelte sich binnen zehn Jahren und die Tiere eroberten den westlichen Teil Polens, die Slowakei und Tschechien. Vermehrt überqueren sie die Grenzen zu Deutschland. In den deutschen Ländern ist der Straßenverkehr in Brandenburg, Sachsen und Bayern am stärksten betroffen. Die Tierart nutzt zwar vorhandene Grünbrücken, ist aber auch dafür bekannt, nicht vor Autos auszuweichen, wenn es zu einer Begegnung kommt.
Die Jagd auf den Elch hat in vielen Regionen eine lange Tradition. Vor der Verwendung von Gewehren war die Jagd keineswegs ungefährlich. Wladimir Heptner und Andrej Nasimowitsch berichten in ihrer Elchmonographie, dass in einigen sibirischen Regionen vor der Einführung von Schusswaffen die Jagd auf den Elch als gefährlicher als die Bärenjagd galt. Sie führen dies darauf zurück, dass die traditionelle Jagdzeit in die Brunftzeit fiel, während der erregte Elche eher als sonst auch Menschen angriffen. Während sich ein Jäger gegenüber einem Bärenangriff im Notfall noch mit dem Dolch wehren kann, ist diese Waffe gegenüber einem Elch, der mit Hufschlägen angreift, wirkungslos. Es sind mehrere Todesfälle bekannt, da die Hufschläge mit großer Schnelligkeit und Wucht ausgeführt werden und die Hufe sehr scharfe Vorderkanten haben.Nach Zählungen des Ministeriums für Jagd und Landwirtschaft leben in Alaska derzeit etwa 160.000 Elche. Jährlich werden etwa 8.000 bis 11.000 Elche erlegt. In Europa werden Elche in den baltischen Staaten, im europäischen Teil Russlands, in Polen und vor allem in Skandinavien gejagt. Da den Elchen in Schweden die natürlichen Fressfeinde wie Wolf und Bär zum großen Teil fehlen, richten die 300.000 Tiere schwere Schäden im Wald an. Deshalb werden allein in Schweden jedes Jahr bis zu 90.000 Tiere geschossen.In Österreich ist der Elch prinzipiell jagdbar, aber ganzjährig geschont.
Der Elch zählt nicht zu den Tierarten, die vom Menschen domestiziert wurden. Handaufgezogene Elche werden sehr zahm. Der Zoologe Valerius Geist vergleicht solche Elche in ihrem Verhalten eher mit Hunden als mit anderen Hirscharten. Elche haben jedoch sehr spezifische Ernährungsanforderungen und fallen verschiedenen Wildtierkrankheiten zum Opfer. Aus diesem Grund ist die Domestikation (weitgehend) unterblieben.
Weißwedelhirsche (Odocoileus virginianus) übertragen über ihren Kot einen Parasiten, der für Elche tödlich ist. Die Weißwedelhirsche selbst werden vom Parasiten nur merklich beeinträchtigt, wenn sie alt, erkrankt oder anderweitig geschwächt sind. Dies ist für den Elch so lange unbedenklich, wie sich sein Lebensraum nicht oder nur wenig mit dem vieler Weißwedelhirsche überschneidet. Weißwedelhirsche leben meist südlich des Verbreitungsgebietes der Elche und bevorzugen junge Wälder mit viel Unterholz als nährstoffreiche Nahrung. Der Hirsch braucht vor allem im kalten Winter viele energiereiche Jungpflanzen, um seinen Kalorienbedarf zu decken. Elche sind besser an die Kälte und die Verwertung energiearmer Nahrung angepasst und leben weiter nördlich in älteren Wäldern mit wenig Unterholz.
Durch die großflächige Abholzung in Ontario in den 1930er- und 1940er-Jahren und der anschließenden Aufforstung in der Nachkriegszeit wurde der dortige Waldbestand stark verjüngt. Zusammen mit vielen milden Wintern bis in die 1960er-Jahre hinein führte dies bei den Weißwedelhirschen zu einer starken Bestandsvergrößerung. Durch die oben erwähnte Parasitenübertragung erkrankten und starben sehr viele Elche und wurden in diesen Gebieten selten. In den Schutzgebieten wurde kein Holz mehr geschlagen, und der Waldbestand wurde wieder älter. Zusammen mit dem kälteren Wetter in den letzten 30 Jahren hat dies die Hirschbestände stark dezimiert; der Bestand an Elchen nahm im gleichen Zeitraum stark zu.
Historisch wurden unter anderem die Namen Elend, Elentier, Elenhirsch, Elen und Elk verwendet. In der Volksmedizin wurden Elendsklauen zum Beispiel gegen Epilepsie, Gicht oder Kopfschmerz verwendet. Der Huf des Elchs wurde zur Abwehr des Bösen Blicks getragen.
Valerius Geist: Deer of the World: Their Evolution, Behaviour, and Ecology. Stackpole Books, Mechanisburg PA 1998, ISBN 0-8117-0496-3.
Valerius Geist, Michael H. Francis (Fotograf): Moose: Behavior, Ecology, Conservation. Voyager Press Inc. U.S. 1999 ISBN 978-1-55059-332-7
Hans Kramer: Elchwald. Land - Leute - Jagd. Der Elchwald als Quell und Hort ostpreußischer Jagd. 3. Auflage. (= Ostpreußen-Trilogie. Teil 3). Jagd- und Kulturverlag, Sulzberg im Allgäu 1990, ISBN 3-925456-00-7.
Wladimir G. Heptner, Andrej A. Nasimowitsch: Der Elch. Westarp-Wissenschaften, Hohenwarsleben 2004, ISBN 3-89432-173-3.
Don E. Wilson, DeeAnn M. Reeder (Hrsg.): Mammal Species of the World. 3. Auflage. The Johns Hopkins University Press, Baltimore 2005, ISBN 0-8018-8221-4.(*) Wladimir G. Heptner, Andrej A. Nasimowitsch: Der Elch. Westarp-Wissenschaften, Hohenwarsleben 2004, ISBN 3-89432-173-3.
(+) Valerius Geist: Deer of the World: Their Evolution, Behaviour, and Ecology. Stackpole Books, Mechanisburg PA 1998, ISBN 0-8117-0496-3.
Elchplan und detaillierte Hintergrundinformationen zum Elch - Informationen der Bayerischen Forstverwaltung
Alces alces in der Roten Liste gefährdeter Arten der IUCN 2016. Eingestellt von: K. Hundertmark, 2015. Abgerufen am 31. Dezember 2016.

Die Elde ist der längste Fluss in Mecklenburg-Vorpommern, liegt im Süden und Südwesten dieses Landes und verbindet das Gebiet um die Müritz mit der Elbe. Sie durchfließt in ihrem Oberlauf mehrere große Seen der Mecklenburgischen Seenplatte. Der letzte dieser Seen ist der Plauer See, ab dem der Fluss staugeregelt ist. 180 km der insgesamt 208 Kilometer langen Elde vom Südrand der Müritz bis zur Elbe bei Dömitz sind schiffbar. Sie bilden die als Bundeswasserstraße ausgewiesene Müritz-Elde-Wasserstraße.
Die Quelle der Elde befindet sich beim Altenhofer Ortsteil Darze, sechs Kilometer südöstlich des Plauer Sees und 20 Kilometer westlich der Müritz. Die ersten etwa 600 Meter des Quellbachs fließen in einem unter einem Feld verlegten Rohr, aus dem er knapp 400 Meter westlich von Darze auf einer Geländehöhe von etwa 90 m ü. NHN austritt.
In einigen Dokumenten werden andere Quellorte genannt. So verweisen die Karten des Amtes für Geoinformation, Vermessungs- und Katasterwesen beim Landesamt für innere Verwaltung Mecklenburg-Vorpommern auf einen beim Finckener Ortsteil Knüppeldamm entspringenden Graben. Das Landesamt für Umwelt, Naturschutz und Geologie Mecklenburg-Vorpommern (LUNG) nannte 1997 in den Beschreibungen zweier Biotope in der näheren Umgebung weitere Quellorte (nordöstlich von Knüppeldamm und in den Darzer Tannen nördlich der Bundesstraße 198), entschied sich aber für die Darzer Quelle.Auch in alten schriftlichen Texten wird die Quelle der Elde in der Nähe von Darze lokalisiert („bei der Darzer Mühle“)Nur wenige Kilometer südwestlich von der Eldequelle entspringen die nach Südosten fließende Dosse und die nach Südwesten fließende Stepenitz.
Die Elde fließt zunächst nach Südosten. Von der Quelle bis zum Müritzsee verläuft sie durch ein flaches, ehemals sumpfiges Gebiet. Sie ist meistens begradigt und von zahlreichen Entwässerungs- und Quellgräben begleitet. Ihr Oberlauf ist ein großer, gegen den Uhrzeigersinn geformter, etwa Dreiviertel eines Kreises umfassender und bis Plau reichender Bogen. Während die Eldequelle nur sechs Kilometer Luftlinie von der Südspitze des Plauer Sees entspringt, erreicht sie den See erst nach etwa 80 Flusskilometern. Alle von der Elde durchflossenen Seen sind Teile dieses Bogens und gehören zur Mecklenburgischen Seenplatte.
Die ersten dieser Seen sind der Darzer See, der Finckener See, der Massower See, nach Unterquerung der Bundesautobahn 19 der Mönchsee und der Melzer See.
Bei Priborn erreicht die Elde den Müritzsee und den sich anschließenden Müritzarm, anschließend die Kleine Müritz und schließlich von Süden her die Müritz, den größten See der Mecklenburgischen Seenplatte und den größten Binnensee Deutschlands. Der Abfluss aus der Müritz liegt an deren nördlichen Zipfel (Binnenmüritz bei der Stadt Waren). Auf dem jetzt westlichen Weg durchquert die Elde den Kölpinsee, den Fleesensee, den von der Stadt Malchow rechts gesäumten Malchower See, den Petersdorfer See und nach dem Lenzer Kanal den Plauer See. In den langen Seestrecken vom Müritzarm bis zum Plauer See ist die Elde als Fluss kaum wahrnehmbar. Zwischen Müritz und Kölpinsee wird sie auch als Reeckkanal oder Ree(c)ke und zwischen Fleesensee, Malchower See und Petersdorfer See als Recken bezeichnet. Diese Namen sind slawischen Ursprungs (vergleiche polnisch: rzeka, tschechisch: řeka, russisch: Река).
Ab Plau am See ist die Elde in fast ganzer Länge ein staugeregelter Fluss, wobei aber noch lange Altstrecken parallel existieren. Über Lübz und Parchim ist die Flussrichtung überwiegend westlich. Rund 14 Kilometer nach Parchim ist die Elde über den Störkanal und die Stör mit dem Schweriner See verbunden. An diesem Abzweig wendet sich der Fluss nach Süden und verläuft über Neustadt-Glewe, Grabow und weiter in Richtung Südwesten nach Eldena.
Vor Eldena zweigt der Elde-Seitenkanal ab, der das meiste Wasser der Elde auf kurzem Weg über Neu Kaliß zur Elbe bei Dömitz führt. Der ursprüngliche Flusslauf wendet sich hingegen über Gorlosen nach Süden und vereinigt sich bei Seedorf mit der Löcknitz. Bis etwa Ende des 19. Jahrhunderts wurde der gemeinsame Flusslauf meist als Elde oder Alte Elde bezeichnet, später dagegen wurde er wie heute als Teil der Löcknitz angesehen. Nach der Vereinigung von Alter Elde und Löcknitz verläuft der Fluss in westliche Richtung. Die ursprüngliche Mündung in die Elbe lag wenige Kilometer östlich von Dömitz bei Klein Schmölen. Seit den 1970er Jahren wird die Löcknitz nördlich an Dömitz vorbeigeführt, unterquert mittels eines Dükers den Elde-Seitenkanal und mündet erst bei Wehningen in die Elbe.
Der Name Elde stammt aus einer alten indoeuropäischen Wurzel mit der Bedeutung fließen, strömen.Die teilweise sehr langen erhalten gebliebenen Altarme neben den staugeregelten Abschnitten wie auch das Stück von Eldena bis zur Mündung in die Löcknitz werden Alte Elde genannt. Zwischen den großen Seen heißt der Fluss außer Elde gelegentlich auch Reecke (siehe oben).
Die staugeregelten Abschnitte und der Kanal von Eldena nach Dömitz wurden früher als Neue Elde bezeichnet. Diese Bezeichnung wurde langsam verdrängt. Anfang der 1960er Jahre, als in Landkarten und anderen Veröffentlichungen noch Neue Elde eingetragen war, hieß der gesamte schiffbare Teil vom Südrand der Müritz bis zur Elbe bei Dömitz auch Elde-Müritz-Wasserstraße. Dieser Name ging aus der 1936 vom Reichsverkehrsministerium benutzten Bezeichnung Müritz-Elde-Wasserstraße, hervor. Heute ist ebenfalls die Variante Müritz-Elde-Wasserstraße die offizielle Bezeichnung.
Für den Abschnitt zwischen Eldena und Dömitz findet sich in Karten außer Neue Elde auch die Kurzbezeichnung Eldekanal. Die heutige amtliche Bezeichnung ist Elde-Seitenkanal als Teil der Müritz-Elde-Wasserstraße.
Die Müritz-Elde-Wasserstraße (MEW) ist eine Verbindung für Binnenschiffe zwischen Elbe und Mecklenburgischer Seenplatte und über den von der Elde abzweigenden Störkanal auch zum Schweriner See. Sie ist eine Bundeswasserstraße der Wasserstraßenklasse I, für die das Wasserstraßen- und Schifffahrtsamt Lauenburg zuständig ist.
Die in üblicher Weise bergwärts fortlaufende Kilometrierung beginnt mit Kilometer 0,0 in Dömitz (bei Elbe-km 504,1). Bei Kilometer 56 zweigt der Störkanal ab. Das Ende der Müritz-Elde-Wasserstraße ist bei Kilometer 180 bei der Ortschaft Buchholz (12 km südlich von Röbel) am Müritzsee festgelegt. Der nicht schiffbare Teil der Elde mündet bereits vorher bei Priborn in den Müritzsee, wo sich etwa gegenüber auf der östlichen Seite bei Kilometer 171,9 der Anschluss der Müritz-Elde-Wasserstraße an die Müritz-Havel-Wasserstraße befindet.
Die Tauchtiefe beträgt zwischen Dömitz und Plauer See 1,20 Meter, auf der übrigen Strecke durch die Mecklenburger Seenplatte, in der sich keine Schleusen mehr befinden, 1,40 Meter.
Die wirtschaftliche Bedeutung der Wasserstraße ist heute aufgrund ihrer geringen Abmessungen eher gering, sie wird vorwiegend von Sportbooten und Ausflugsschiffen befahren. Im Jahr 2009 wurden in Dömitz 3481 und in Plau am See 7962 Wasserfahrzeuge geschleust.Die zur Müritz-Elde-Wasserstraße gehörende Stör-Wasserstraße (StW: Störkanal, Stör und Schweriner See) ist 44,7 km lang.
Die Schleuse mit größtem Hub (6,9 Meter) befindet sich in Bobzin bei Lübz. Die kürzeste Schleuse ist 41,50 Meter lang, und die schmalste Schleuse ist 5,20 Meter breit. Die geringste feste Durchfahrtshöhe (3,73 Meter bei normalem Wasserstand) befindet sich bei der Schleuse Eldena.
Bis 2009 waren bereits neun Schleusen zu Selbstbedienungsschleusen umgebaut worden. Deren Überwachungszentrale hat ihren Sitz in Parchim.
(*) Das Kraftwerk in Grabow ist in der Bolbrüggeschen Mühle an der Alten Elde in der Innenstadt, etwa einen Kilometer von der Schleuse entfernt.
die Alte Elde als die unterhalb des Abzweigs der Schifffahrt bei Eldena in den Elde-Seitenkanal weiter fließende Elde und
die zahlreichen neben der oberhalb Eldena staugeregelten und begradigten Elde erhalten gebliebenen, oft windungsreichen Teile des ursprünglichen Flusslaufs (Altwasser), die auch meistens Alte Elde genannt werden.Die von Eldena kommende und heute beim brandenburgischen Seedorf in der Löcknitz endende Alte Elde wurde in den Jahren 2005 und 2006 in einem länderübergreifenden (Mecklenburg-Vorpommern und Brandenburg) Projekt renaturiert.Einen längeren zusammenhängenden Abschnitt eines Altwassers gibt es nördlich der Wasserstraße östlich von Lübz bei Kuppentin. Er ist wegen des Vorkommens seltener Tier- und Pflanzenarten und des Vorhandenseins fast naturnaher Gewässerstrukturen als Naturschutzgebiet ausgewiesen.
Zwischen Lübz und Parchim ist die Alte Elde auf mehreren Teilstücken teils rechts, teils links der Wasserstraße erhalten. Zwischen der Einmündung des Störkanals und Neustadt-Glewe befindet sich der Altlauf, dort auf neueren Karten teilweise nur als Elde bezeichnet, rechts der Wasserstraße.
In Grabow fließt ein Stück Alte Elde wie früher durch die Innenstadt und weiter bis Güritz links der Wasserstraße.
Die Elde wird in einer im Mecklenburgischen Urkundenbuch veröffentlichten Urkunde aus dem Jahre 786 über die Stiftung des Bistums Verden durch Karl den Großen erwähnt. Deren Echtheit ist allerdings umstritten. 946 stellte Otto I. dem Bistum Havelberg einen Stiftungs- und Bewidmungsbrief aus, in dem dessen Nordgrenze entlang der Elde bis zu deren Mündung in die Elbe festgelegt wurde. Heinrich der Löwe nannte 1163 als Grenze des Bistums Ratzeburg die Elde bis zu ihrer Mündung.
Nachdem Lübeck 1398 über die Stecknitzfahrt auf dem Wasserweg Anschluss an die Elbe erhalten hatte, geriet das auf Salzhandel spezialisierte Wismar ins Hintertreffen. Man suchte ebenfalls einen direkten Wasserweg, auf dem Lüneburger Salz von der Elbe her nach Wismar gebracht werden konnte. Eins der Projekte sah eine Verbindung über die Boize und den Schaalsee zum Schweriner See vor. Einen Schiffsverkehr über die Elde einzurichten, hatten schon die mecklenburgischen Herzöge Magnus II. und Balthasar versucht. Aber weder die Herzöge noch die Stadt Wismar sahen sich in der Lage, das Projekt zu finanzieren. Wismar suchte unter anderem ohne Erfolg in Magdeburg um Finanzierungshilfe nach. Ein weiteres Problem waren die fehlenden Nutzungsrechte für ein kurzes Eldeteilstück auf brandenburgischem Gebiet. Das Projekt fand erst unter Herzog Albrecht VII. 1531 wieder Erwähnung. Dieser hatte Prinzessin Anna, eine Tochter des Kurfürsten Joachim I. von Brandenburg, geheiratet und hoffte so und durch diplomatische Beziehungen mit benachbarten Fürsten und Ständen, bei denen er um finanzielle Unterstützung warb, die Brandenburger vom Nutzen eines Schifffahrtsweges überzeugen zu können.
1566 ließ Herzog Johann Albrecht I. vier Schleusen an Stör und Elde errichten. Auf einer mühseligen Fahrt, auf der das Boot trotz eines Tiefgangs von nur 60 cm über mehrere flache Stellen gezogen werden musste, reiste im Mai 1567 eine Kommission, der Tilemann Stella als Vertreter Johann Albrechts angehörte, von Viecheln am Nordufer des Schweriner Sees nach Dömitz. Joachim II., Kurfürst von Brandenburg, ließ sich jedoch nicht vom Nutzen eines Schifffahrtsweges überzeugen, so dass Stella den unteren Abschnitt der Elde nicht weiter in seine Planungen einbezog. Die Arbeiten an der Schleuse in Gorlosen nahe der Grenze zu Brandenburg wurden eingestellt.
Ein Ausweg war der Bau einer direkten Wasserverbindung von Eldena nach Dömitz auf mecklenburgischem Gebiet. Der Vorteil, dass die mecklenburgische Seite alle Zolleinnahmen erhalten konnte, überwog den Nachteil möglicher Konflikte mit den brandenburgischen Nachbarn.
Das zu durchquerende Terrain wurde von April bis Mai 1568 erkundet und ausgemessen. Im August des Jahres wurde mit den Arbeiten am Graben zwischen Dömitz und dem Brantlewe, einem sumpfigen Gehölz in der Nähe der Stadt, begonnen. Auch von Eldenaer Seite arbeitete man sich voran. Zur Finanzierung wurde im April 1569 landesweit eine Steuer erhoben, von der nur die Ritterschaft ausgenommen war. Bereits im November stellte der Wallmeister Jost Spangenberg ein baldiges Ende der Arbeiten in Aussicht. Einen Rückschlag erlitt das Projekt im August 1571, als Johann Georg, Kurfürst von Brandenburg, kurz nach seinem Amtsantritt vier Schleusen und Teile des Grabens durch Adlige, Bauern und Knechte zerstören ließ. Die Kanalarbeiter bedrohte er für den Fall der bei Fortsetzung ihrer Arbeit mit dem Tod. Kurfürst August von Sachsen griff schlichtend in den Streit ein.
Im Februar 1572 war die Kanalverbindung zwischen Eldena und Dömitz, die Neue Elde, mit sieben Schleusen so gut wie fertiggestellt. Mit Ausnahme einer Steinschleuse in Dömitz waren die Schleusen aus Holz.
Am 11. August 1572 erreichte das erste Schiff auf dem Kanal Dömitz. Nach von Herzog Johann Albrecht angeordneten Vergrößerungen der Schleusen und Mängelausbesserungen wurde schließlich am 15. März 1575 den Städten Magdeburg und Hamburg vermeldet, dass die Neue Fahrt in beiden Richtungen schiffbar sei. Eine durchgehende Schifffahrt von Viecheln am Nordufer des Schweriner Sees bis an die Elbe war damit möglich.
Die seit 1480 geplante schiffbare Verbindung vom Schweriner See nach Wismar, die sogenannte Viechelnsche Fahrt oder später der Wallensteingraben, wurde jedoch nie fertiggestellt, und bereits gebaute Teile verfielen wieder.
Die wirtschaftliche Entwicklung im beginnenden 19. Jahrhundert legte einen weiteren Ausbau der Schifffahrtswege im Bereich der Elde nahe. So wurden Verbindungsstrecken zwischen den Seen, sogenannte Reeken, bei Waren und am Plauer See auf 40 Fuß verbreitert und von 1798 bis 1803 Schleusen- und Mühlenanlagen modernisiert. 1831 bildete sich die Elde-Actien-Societät, die sich die Schiffbarmachung der Elde von der Müritz bis zur Elbe, der Stör zum Schweriner See und der Verbindung von der Müritz zur Havel zur Aufgabe machte. Ein Drittel der Kosten von insgesamt 400.000 Reichstalern wollte das Land Mecklenburg übernehmen. Wichtigstes Projekt war der Bau des Friedrich-Franz-Kanals von 1832 bis 1834 oberhalb von Neustadt-Glewe, der den ursprünglichen Flusslauf der Elde im Lewitzbruch abkürzte. Er ist als Dammstrecke ausgeführt, das heißt, der Wasserspiegel befindet sich über dem Niveau der umgebenden Landschaft. Außerdem wurden Schleusen erneuert oder neu angelegt; der alte Störkanal wurde vertieft. In die gleiche Zeit fiel die Schaffung einer Verbindung der Elde zur Havel über die Müritz durch den Bau des Bolter Kanals. Der bereits zuvor durch Mühlenstaue schwankende Wasserstand der Müritz fiel während der Elderegulierungen zwischen 1798 und 1836 um insgesamt 1,30 Meter.Obwohl die Elde-Actien-Societät wegen gestiegener Baukosten zunächst Darlehen aufnehmen musste, reichten in den ersten Jahren die Einnahmen durch Schleusengebühren aus, um einen kleinen Gewinn zu erwirtschaften. Seit Ende der 1840er Jahre machte sich mehr und mehr die Konkurrenz durch Eisenbahnen, wie etwa der Berlin-Hamburger Bahn, bemerkbar. 1857 musste die Gesellschaft deswegen ihre Rechte an die Landesherren abtreten, die auch die Schulden übernahmen. Des Weiteren erwies sich die Begrenzung der Schiffsgrößen als Nachteil. Die Bauten waren ursprünglich auf 20 Last (etwa 50 Tonnen) ausgelegt, später wurde eine Beschränkung auf 25 Last vorgeschrieben. Der Bau einzelner Flussbegradigungen, wie etwa des Grabower und des Güritzer Kanals flussabwärts von Grabow, reichten nicht aus.
In einem Gutachten für die Landesregierung stellte der Berliner Baurat Adolf Wiebe von 1877 fest, dass die mecklenburgischen Wasserstraßen niemals großen durchgehenden Verkehr aufnehmen könnten. Er hielt jedoch einen Ausbau für Schiffe von 40 Meter Länge und bis zu 2500 Zentnern (125 Tonnen) Beladung für sinnvoll. Nach längeren Verhandlungen wurde auf dem Landtag von 1890 der Ausbau beschlossen; 1,5 Millionen Mark wurden aus Landesmitteln bewilligt.Zu den wichtigsten Baumaßnahmen gehörte der Ausbau der Kanäle (Elde-Seitenkanal und Schleusenkanäle) zwischen Dömitz und Grabow und nördlich von Neustadt. Weitere Kanalbauprojekte, wie etwa der erneute Ausbau des Wallensteingrabens oder eine Verbindung von Rostock nach Berlin über Warnow, Nebel, Krakower und Plauer See sowie die Müritz standen zu dieser Zeit in Aussicht oder waren bereits (etwa mit dem Bützow-Güstrow-Kanal) in Angriff genommen. In diesem Zusammenhang wurde ein acht Kilometer lange Schleusenkanalabschnitt zwischen Grabow und Neustadt bereits für 51,5 Meter lange Schiffe mit bis zu 7000 Zentnern (350 Tonnen) ausgelegt und im Herbst 1895 fertiggestellt. Restarbeiten fanden in den Jahren 1896 und 1897 statt. Die Baukosten hatten sich auf etwas über zwei Millionen Mark erhöht.
Auch ohne Verbindung zur Ostsee hatte die Elde hohe Bedeutung wegen der Beförderung landwirtschaftlicher Produkte nach Hamburg. 1921 begannen erneut umfangreiche Regulierungen am Flusslauf, die in den 1930er Jahren fortgeführt wurden. Sie dienten unter anderem dem Ziel, über die Elde eine schnelle und sichere Verbindung von Hamburg nach Berlin herzustellen. Der Wasserpegel auf der Elbe oberhalb von Dömitz war im Vergleich zu denen der Elde und der Havel zu wenig stabil. Durch die Zusammenfassung von Staustufen sollten an den Schleusen höhere Fallhöhen für den einträglichen Betrieb von Wasserkraftwerken erreicht werden. Obwohl diese Pläne mehrheitlich nicht ausgeführt wurden, blieb die Elde bis einschließlich der DDR-Zeit wichtiger Transportweg, an dem sich auch verarbeitende Industrien ansiedelten. Nach dem Mauerbau 1961 wurde die Zufahrt zur Elbe gesperrt.
Im Jahr 1973 wurde die Mündung der Löcknitz von Klein Schmölen elbabwärts in die Nähe von Wehningen verlegt. Die Löcknitz wird bei Dömitz durch einen Düker unter dem Elde-Seitenkanal hindurchgeführt.
Zur Ent- und Bewässerung landwirtschaftlicher Nutzflächen wurde in den 1970er Jahren eine künstliche Wasserverbindung von der Elde zur Rögnitz, der sogenannte Elde-Rögnitz-Überleiter, geschaffen.Nach 1990 war die Passage zur Elbe wieder frei passierbar. Durch den weitgehenden Zusammenbruch der Industrie in der Region hat die Elde jedoch für die Frachtschifffahrt praktisch keine Bedeutung mehr. Sie dient heute vor allem dem touristischen Verkehr.
Die Mündung der Elde wurde mehrfach verlegt. Stellas Karte aus dem Jahr 1576 (siehe oben) deutet darauf hin, dass sich die Elde bereits vor dem Bau der Neuen Elde südlich von Dömitz der Elbe näherte, jedoch erst nach östlichem Umlaufen der Stadt nordwestlich in den Strom mündete. Eine Karte des Geometers Gerhart Evert Pilooth von 1612 zeigt eine Steinschleuse an der Mündung der Neuen Elde mit der Inschrift „Hyr compt der Nye Elde durch dissen steinen Släysz in den olden Elde“. Die Schleuse wurde 1722 und 1823 renoviert und zum Schutz vor Überfällen mit einer Schanze umgeben. Sie wird nicht mehr genutzt, ist aber erhalten geblieben und steht unter Denkmalschutz.
Auch eine Karte Mecklenburgs aus dem Jahr 1645 zeigt eine Annäherung der Alten Elde mit Verbindung zur Elbe bei Klein Schmölen mit anschließendem östlichem Umfließen von Dömitz.
Später mündet die Alte Elde bei Klein Schmölen und das Dove Elde genannte Bett bei Dömitz bildet einen Seitenarm der Elbe.
Bei der Elderegulierung 1831 bis 1836 wurde die Mündung des Elde-Seitenkanals in den Elbe-Seitenarm verlegt, und von 1835 bis 1836 wurden eine Kammerschleuse mit Wänden aus massivem Ziegelmauerwerk und ein neuer Schiffsliegeplatz errichtet. Zur Anlage gehörte eine seitliche Sturzschleuse, über die das aufgestaute Eldewasser in den Seitenarm abfließen konnte. Es wurde möglich, mit bis zu 40 Meter langen Schiffen nach Schwerin zu gelangen.
In die Zeit des Baus der Dömitzer Eisenbahnbrücke von 1871 bis 1873 fiel eine erneute Verlegung der Mündung des Elde-Seitenkanals, diesmal direkt in die Elbe, wobei der Aushub des neuen Flussbettes für den zur Brücke führenden Bahndamm Verwendung fand. Bis 1890 wurde ein neuer Hafen errichtet, dieser erhielt Anschluss an die Bahnstrecke Lüneburg-Wittenberge. Eine 1934 geplante Erweiterung des Hafenbeckens wurde nicht verwirklicht.
1938 wurde ein noch erhaltener Getreidespeicher als ein gegen Luftangriffe sicheres Stahlbetonzellensilo errichtet.
Nach dem Zweiten Weltkrieg behielt der Hafen in Dömitz zunächst seine Bedeutung, da eine entsprechende Infrastruktur für den Güterverkehr per Bahn noch nicht wieder im benötigten Umfang zur Verfügung stand. Wegen der Grenzlage zur Britischen Besatzungszone gab es jedoch erste Einschränkungen. Als Ersatz für bisher über den Hamburger Hafen abgewickelte Getreidetransporte von der Sowjetunion in die Tschechoslowakei wurde in den 1950er Jahren eine Getreideumschlaganlage errichtet, die Getreideanlieferungen per Bahn aus dem Wismarer Hafen ermöglichen sollte. Zur Nutzung kam es jedoch nicht mehr, da mit der Grenzsicherung ab 1961 der Güterumschlag in Dömitz vollständig aufgegeben wurde. Der Hafen wurde Grenzbootstützpunkt. Um Dömitz aus der Fünf-Kilometer-Sperrzone herausnehmen zu können, wurde der Hafen 1973 mit hohen Zäunen umgeben. Hafen und Schleuse wurden vernachlässigt. Vor der nach 1989 erfolgten Wiederinbetriebnahme des Dömitzer Hafens und des Elbanschlusses des Elde-Seitenkanals wurde die Kammerschleuse, von der Mitte der 1980er Jahre eine Wand wegen Baufälligkeit abgebrochen werden musste, erneuert. Teile der Getreideumladeanlage wurden 1990 abgerissen. Ihr als Stahlbetonkonstruktion errichteter Teil wird nach einem Umbau gastronomisch genutzt.
Von den insgesamt 22 Wasserkraftanlagen in Mecklenburg-Vorpommern liegen neun an der Elde und am Elde-Seitenkanal. In diesem Fluss-Abschnitt beträgt das Gefälle etwa 50 Meter. Die Anlagen befinden sich meist neben oder nahe bei den Schleusen und nutzen deren Freiflut (zum Schleusen nicht benötigtes Wasser).
Es handelt sich um die Wasserkraftanlagen (siehe auch Abschnitt: Schleusen) in Barkow, im Lübzer Ortsteil Bobzin, im Dammer Ortsteil Malchow, in Neustadt-Glewe (zwei Anlagen: Lewitzschleuse und das Elektrizitätswerk Neustadt-Glewe an einem Seitenarm der Elde in der Innenstadt), in Grabow (zwei Anlagen: Hechtsforthschleuse und Bolbrüggesche Mühle an der Alten Elde in der Innenstadt) und in Neu Kaliß (zwei Anlagen: Findenwirunshier, Neu Kaliß bei der Papierfabrik).
Am Oberlauf befindet sich das Naturschutzgebiet Mönchsee, ein Flachsee mit umliegendem Verlandungsmoor, das Brut- und Rastplatz für Wasservögel ist.
An und teils in der Müritz liegen der Müritz-Nationalpark und die NSGe Müritzsteilufer bei Rechlin und Großer Schwerin und Steinhorn.
Am Kölpinsee befinden sich die NSGe Damerower Werder und das stark durch Wasserspiegelschwankungen des Kölpinsees beeinflusste Blüchersches Bruch und Mittelplan.
Mit der Unterschutzstellung des Gebiets Nordufer Plauer See wird der Erhalt, die Pflege und die Entwicklung einer Landschaft mit Seen, Mooren, Wäldern und Feuchtwiesen verfolgt.
Der ursprüngliche Verlauf der Elde steht im Gebiet Alte Elde bei Kuppentin zur Erhaltung der artenreichen Fischfauna und der angrenzenden Feuchtwiesen und Wälder unter Naturschutz.
Nördlich von Neustadt-Glewe durchlaufen die ursprüngliche und die staugeregelte Elde das Naturschutzgebiet Fischteiche in der Lewitz. Der Wasserspiegel der beidseits der staugeregelten Elde liegenden Teiche ist tiefer als der der Wasserstraße.
Bodo Müller: Von der Elbe zur Müritz. 4. Auflage, Edition Maritim, Hamburg 2005, ISBN 3-89225-256-4.
Friedrich Stuhr: Der Elbe-Ostsee-Kanal zwischen Dömitz und Wismar. In: Verein für Mecklenburgische Geschichte und Altertumskunde: Jahrbücher des Vereins für Mecklenburgische Geschichte und Altertumskunde. – Band 64 (1899), S. 193–260 Digitalisat (Memento  vom 26. September 2007 im Internet Archive).
Martin Eckoldt (Hrsg.): Flüsse und Kanäle, Die Geschichte der deutschen Wasserstraßen. DSV, Hamburg 1998, ISBN 978-3-88412-243-3.

Der Eldfell ([ˈɛltfɛtl̥], isländisch Feuerberg) ist ein Vulkan mit einem 200 m hohen Schlackenkegel auf der 13,4 km² großen isländischen Insel Heimaey. Er bildete sich ohne Vorwarnung bei vulkanischen Eruptionen wenige hundert Meter außerhalb der Stadt Heimaey am 23. Januar 1973. Seit dem Ende der Ausbrüche im Juli 1973 befindet sich der Vulkan im Ruhezustand.
Die Eruptionen verursachten eine Krise bei den damals ca. 5.000 Einwohnern und führten fast zu ihrer dauerhaften Aussiedlung von der Insel.
Vulkanische Asche ging auf Heimaey nieder und zerstörte dabei viele Häuser. Lavaströme drohten den natürlichen Hafen vom Meer abzutrennen. Damit wäre die Haupteinnahmequelle der Inselbewohner, die Fischerei, stark beeinträchtigt gewesen. Mittels großer Meerwassermengen wurde jedoch die Lava erfolgreich so weit abgekühlt, dass sie zum Stillstand kam und nicht nur die Nutzung des Hafens weiter möglich war, sondern die bis 40 m hohen Lavawälle ihn sogar ausgesprochen gut gegen die berüchtigten Winterstürme aus Osten und Südosten schützten.
Nach dem Ende der Eruptionen wurden die nur langsam abkühlenden Lavaströme zur Energie- und Warmwassergewinnung genutzt. Die niedergegangenen Pyroklastika wurden zur Erweiterung des Flughafens und zur Landgewinnung eingesetzt, wodurch Platz für 200 neue Häuser geschaffen wurde.
Die Vestmannaeyjar (isl. Westmännerinseln) wurden 874 erstmals besiedelt, ursprünglich möglicherweise von geflohenen irischen Sklaven, die den nordischen Siedlern auf Island gehörten. Diese Siedler gaben einer Theorie nach der Insel ihren Namen (Westmänner), da Irland westlich von Skandinavien liegt.
Obwohl es auf Heimaey an Frischwasser mangelt und die Siedler durch Piraterie bedrängt wurden, wurde die Insel ein wichtiges Zentrum der isländischen Fischerei, da sich dort fischreiche Gewässer befinden. Zudem gelang es um die Mitte des 20. Jahrhunderts schließlich einen eigenen Hafen auszubauen, der auch Motorschiffe und Trawler aufnehmen konnte. Ab da trieb man den Aufbau einer eigenen Fischereiindustrie erfolgreich voran.
Aufgrund seiner Lage über dem mittelatlantischen Rücken, wo sich die Eurasische und die Nordamerikanische Platte voneinander entfernen, ist Island eine Region mit starker vulkanischer Aktivität. Die inzwischen von Wissenschaftlern mehrheitlich angenommene Lage Islands über einem nach dem Land benannten Hotspot trägt zu Anzahl und Intensität der Ausbrüche maßgeblich bei.
Der Archipel der Vestmannaeyjar liegt vor der Südküste Islands und besteht aus mehreren im Holozän entstandenen Inseln. Das Vulkansystem der Vestmannaeyjar ist das jüngste und südlichste der östlichen Vulkanzone Islands (Eastern Volcanic Zone) und seit der Eiszeit aktiv.
Heimaey, die größte Insel der Gruppe und die einzige bewohnte, enthält auch im Pleistozän entstandenes Material. Bis vor dem Ausbruch des Eldfell war der Helgafell, ein 200 m hoher und etwa 6.000 Jahre alter Vulkankegel, der bekannteste Vulkan der Insel.
Seit der Besiedlung ereigneten sich die ersten größeren Eruptionen 1637 und 1896 als unterseeische Ereignisse. Zu einer weiteren Eruption kam es 1963, als sich etwa 20 km südwestlich die neue Insel Surtsey bildete.
Wissenschaftler vermuten, dass die verstärkte vulkanische Aktivität auf eine Ausdehnung der Island durchquerenden Riftzone Richtung Süden zurückzuführen ist.
Am 21. Januar 1973 begann etwa um 20 Uhr eine Serie schwacher Erdbeben um Heimaey. Sie waren zuerst zu schwach, um von den Bewohnern wahrgenommen zu werden, konnten jedoch von einem 60 km entfernten Seismographen aufgezeichnet werden. Zwischen 01:00 und 03:00 in der folgenden Nacht (22. Januar) zeichnete er über 100 Beben auf. Anschließend nahm die Anzahl der Beben wieder ab und hörte bis um 11:00 vollständig auf. Erst am späten Abend dieses Tages, um 23:00, begannen die Beben von neuem. Zwar wurden bis 01:34 am folgenden Tag nur sieben Beben gemessen, sie waren jedoch stärker als die vorigen; das stärkste erreichte einen Wert von 2,7 auf der Richterskala.
Derartige Beben sind an den Rändern von Lithosphärenplatten nicht ungewöhnlich, und so deutete noch nichts auf einen bevorstehenden Vulkanausbruch hin. Die Eruptionen begannen daher praktisch unerwartet. Um etwa 01:55 am 23. Januar öffnete sich eine Spalte auf der Ostseite der Insel, nur etwa 1 km vom Ortszentrum Heimaeys entfernt und nur 200 m östlich des nächstgelegenen Hofes, Kirkjubær (isl. Kirchenhof), wo sich eine der Kirchen der Insel befand.
Die Spalte vergrößerte sich rasch auf eine Länge von 2 km. Auch unterseeische Aktivitäten wurden registriert, jeweils knapp vor der Nord- und der Südküste. Während der ersten Stunden konnten entlang der – im Maximum 3 km langen – Spalte spektakuläre Lava-Fontänen beobachtet werden, aber die Aktivität konzentrierte sich danach auf eine Öffnung, die sich etwa 800 m nördlich des Helgafell-Vulkanes und gerade noch außerhalb der Stadt befand.
Während der ersten Tage der Eruptionen wurden pro Sekunde geschätzte 100 Kubikmeter Lava und Pyroklastika ausgestoßen. Innerhalb von zwei Tagen hatten die Lava-Fontänen einen Vulkankegel mit einer Höhe von 100 m aufgeschüttet. Ursprünglich wurde dieser neue Vulkan Kirkjufell (Kirchenberg) genannt, aufgrund seiner Nähe zur ehemaligen Kirche. Dieser Name wurde von den isländischen Behörden jedoch nicht übernommen; sie gaben dem Vulkan gegen den Widerstand der lokalen Bevölkerung den Namen Eldfell (‚Feuerberg‘).
In den ersten Stunden der Eruptionen wurden von der isländischen Zivilschutzbehörde bereits im Vorfeld ausgearbeitete Notfallpläne umgesetzt und die gesamte Bevölkerung der Insel ausgesiedelt. Die Evakuierung war notwendig, da Lavaströme bereits in das östliche Ende der Stadt zu fließen begannen, außerdem war die gesamte Insel von niedergehender Asche bedroht.
Wegen schwerer Stürme in den vorangegangenen Tagen befand sich glücklicherweise fast die gesamte Fischereiflotte im Hafen, was eine rasche Evakuierung ermöglichte. Die Bevölkerung wurde von der Feuerwehr mittels Sirenen alarmiert und versammelte sich im Hafen. Die ersten Boote verließen die Insel Richtung Þorlákshöfn um 02:30, nur eine halbe Stunde nach Beginn der Eruptionen.
Der Großteil der Bevölkerung verließ Heimaey per Schiff. Zum Glück bedrohten Lava und Asche anfänglich das Flugfeld nicht, so konnten einige Personen zusätzlich ausgeflogen werden. Dies betraf vor allem ältere Einwohner und die Patienten des örtlichen Krankenhauses, die nicht per Schiff transportiert werden konnten. Innerhalb von sechs Stunden wurden so nahezu alle 5.300 Einwohner sicher auf die isländische Hauptinsel ausgesiedelt. Nur wenige Menschen blieben zurück, für wichtige Arbeiten oder für die Rettung von Wertgegenständen in bedrohten Häusern.
Nahe an der Spalte stehende Häuser wurden bald durch Lava oder Pyroklastika zerstört. Einige Tage nach Beginn der Eruptionen drehte die Hauptwindrichtung auf West und trug so den Hauptteil der Asche auf die Insel und die Stadt. Viele Häuser wurden durch das Gewicht der niedergehenden Asche zerstört, einige konnten von freiwilligen Helfern gerettet werden. Bis Ende Januar bedeckten Pyroklastika weite Teile der Insel, an manchen Stellen bis zu 5 m hoch. Einige Häuser wurden auch durch Lavabomben in Brand gesteckt oder gerieten unter vorrückende Lavaströme.
Anfang Februar klang der Niederschlag von Pyroklastika langsam ab, Lavaströme sorgten aber weiter für Zerstörung. Unterseeische Aktivität nördlich der Insel zerstörte sowohl das Stromkabel als auch die Wasserleitung, die Heimaey von der Hauptinsel aus versorgten.
Auch drohte Lava die Hafeneinfahrt zu verschütten. Dies wäre eine Katastrophe für die isländische Wirtschaft gewesen, ist die Fischwirtschaft doch die Haupteinnahmequelle Islands und Heimaey mit etwa 25 % des isländischen Fischfangs der wichtigste Hafen des Landes.
Neben der Zerstörung von Häusern in der Stadt schuf die Lava aber auch über zwei Quadratkilometer neue Landfläche im Nordosten der Insel. Die Lavaströme bestanden aus dickflüssiger, krustiger ʻAʻā-Lava und bedeckten das Gebiet um den Vulkan mit einer durchschnittlich 40 m dicken Lavaschicht, an manchen Stellen bis zu 100 m hoch. Im weiteren Verlauf der Ausbrüche zerstörten die Lavaflüsse eine Fischverarbeitungsfabrik und beschädigten zwei andere schwer, außerdem zerstörten sie das Kraftwerk der Insel.
Obwohl die Ausbrüche so nahe bei der Stadt geschahen, war nur ein Todesfall zu beklagen. Ein Mann verstarb nach dem Einatmen giftiger Dämpfe, als er etwas aus einem Keller holen wollte. Kohlenstoffdioxid, verbunden mit einer geringen Konzentration giftiger Gase, sammelte sich in vielen von der Vulkanasche bedeckten Gebäuden. Mehrere andere Personen verletzten sich beim Einatmen dieser Gase, als sie kontaminierte Häuser betreten wollten.Es wurden Bemühungen unternommen, die giftigen Gase aus der Stadt abzuleiten. Dazu wurden Gräben ausgehoben und Wälle aufgeschüttet, um die im Vergleich zu Luft schwereren Gase aus der Stadt fließen oder sie diese erst gar nicht erreichen zu lassen. Dabei wurde angenommen, dass die Gase aus der Vulkanspalte stammten. Im Nachhinein erkannte man, dass zumindest ein Teil der Gase unterirdisch vom Vulkan durch das alte Gestein unter der Stadt in sie eingedrungen sein könnte.
Die schlimmste Bedrohung für die Stadt waren die Lavaströme; sie drohten den Hafen vom Meer abzutrennen. Es wurde schon diskutiert, durch eine Nehrung im Norden der Insel eine neue Hafeneinfahrt zu graben, sollte die alte tatsächlich verschlossen werden, doch es wurden auch Anstrengungen unternommen, es überhaupt nicht so weit kommen zu lassen. Bereits zuvor war versucht worden, Lavaströme wie etwa in Hawaii und am Ätna mittels Wasser zu kühlen und auf diese Weise zu stoppen. Diese Versuche brachten jedoch wenig ein, und es hatte sich immer nur um kleinere Einsätze gehandelt. Trotzdem schlug Prof. Þorbjörn Sigurgeirsson von der Háskóli Íslands, einer isländischen Universität in Reykjavík, vor, die Lavaströme mit einer enormen Menge an Meerwasser zu besprühen und damit ihr weiteres Vordringen in die Stadt und den Hafen zu verhindern.
Die ersten derartigen Versuche wurden am 7. Februar unternommen. Obwohl dabei nur etwa 100 Liter Wasser pro Sekunde auf die Lava gepumpt wurde, verlangsamte sich das Vordringen merklich. Die Lava kühlte zwar nur sehr langsam ab, doch die Methode erwies sich als sehr effizient, da praktisch das gesamte eingesetzte Wasser verdampfte und so der Lava die Wärmeenergie entzog. Durch die ersten Erfolge ermutigt, wurden die Bemühungen rasch weiter erhöht.
Anfang März brach ein größeres Stück vom Krater ab und floss im Lavastrom Richtung Hafen. Dieser Flakkarinn (Der Wanderer) genannte Felsblock stellte eine große Bedrohung für den Hafen dar, hätte er ihn erreicht. Vom Baggerschiff Sandey aus wurde versucht, den Lavastrom zu kühlen und zum Stillstand zu bringen. Der Flakkarinn brach schließlich in zwei Teile, die jeweils 100 m vom Hafen entfernt liegen blieben.
Die darauffolgenden Versuche, die Lava abzukühlen, waren die größte derartige Operation in der Geschichte. Von der Sandey aus wurden bis zu 400 Liter Wasser pro Sekunde auf die Lava gesprüht. Zusätzlich wurde ein Netz von Wasserleitungen direkt auf die Lava gelegt, um das Meerwasser auf eine möglichst große Fläche direkt auf der Lava zu verteilen. Hölzerne Verstärkungen der Leitungen fingen durch die Hitze bald Feuer, und auch Aluminiumteile des Leitungssystems begannen zu schmelzen. Die Leitungen selber aber überstanden dank der Kühlung durch das Meerwasser die enorme Hitze. Durch diese Methode konnte bald eine Fläche von 1,2 Hektar gekühlt werden. Manche Stellen wurden besonders intensiv gekühlt und bildeten so natürliche Barrieren, die durch nachfließende, ebenfalls wieder gekühlte Lava weiter verstärkt wurden.
Die Arbeiten, Wasserleitungen auf einen zwar oberflächlich erstarrten Lavastrom aufzubringen, der aber immer noch sehr heiß war und sich dank der flüssigen Lava im Inneren pro Tag immer noch um mehrere Meter vorwärts schob, waren extrem gefährlich. Zusätzlich erschwerte das verdampfende Wasser die Sicht und behinderte damit die Arbeiten weiter. Mit Planierraupen wurden provisorische Wege auf der Lava angelegt, die aber schnell uneben wurden und sich um mehrere Meter pro Tag bewegten. Die Leitungsleger, die sich selber Das Selbstmordkommando nannten, konnten Leitungen bis zu 130 m weit vom festen Untergrund aus über die Lava legen, direkt auf den sich immer noch bewegenden Lavastrom. Dabei erlitten zwar mehrere Arbeiter Verbrennungen, es gab jedoch keine schwerer Verletzten.
Ende März war ein Fünftel der Stadt von der Lava bedeckt. Aus den USA wurden 32 Pumpen mit einer Leistung von jeweils 1000 Litern pro Sekunde geliefert, die schließlich den weiteren Vorstoß der Lavaströme beenden konnten. Diese Pumpen waren eigentlich für das Pumpen von Öl konstruiert worden und mussten daher mit eigens dafür in Reykjavík entwickelten Ersatzteilen umgerüstet werden.
Mit dem Meerwasser wurde auch eine große Menge an Meersalz auf die Lava aufgebracht, so dass sich bald eine Salzkruste auf dem erstarrenden Gestein bildete. Schätzungen geben an, dass so bis zu 220.000 Tonnen Salz an Land abgelagert wurden.
Die Eruptionen hatten für Schlagzeilen auf der ganzen Welt gesorgt, und insbesondere in Island wurde ständig darüber berichtet. Auch in Europa fanden die Nachrichten über die Vorgänge in Island Beachtung, obwohl sie um die Plätze auf der Titelseite mit den Friedensverhandlungen zum Vietnamkrieg in Paris konkurrierten. Besonders die Versuche, die Lavaströme zu kühlen, zogen die Aufmerksamkeit auf sich und fanden Eingang in Publikationen wie dem National Geographic (Volcano overwhelms an Icelandic village, 1973). Die Aufmerksamkeit, die der Vulkanausbruch Island und im Besonderen Heimaey brachte, führte zu einem sprunghaften Anstieg des Tourismus in der Zeit nach den Eruptionen.
Nach den ersten paar Tagen nahm der Nachschub an frischer Lava wieder ab. Wurden anfänglich 100 Kubikmeter Lava pro Sekunde ausgestoßen, sank dieser Wert bis zum 8. Februar auf 60 m³. Mitte März schließlich drangen nur mehr 10 m³ pro Sekunde an die Oberfläche. Danach fiel dieser Wert nicht mehr so stark: etwa Mitte April wurden immer noch 5 m³ ausgestoßen.
Am 26. Mai wurden von einem Fischerboot aus vulkanische Aktivitäten am Meeresgrund zwischen Heimaey und Island beobachtet, die jedoch bald wieder abklangen. Gänzlich zum Erliegen kamen die Eruptionen Anfang Juli, als an der Oberfläche keine frische Lava mehr austrat, wobei jedoch unter der Oberfläche noch ein paar weitere Tage lang Lava strömte. Kurz vor Ende der Ausbrüche zeigte eine 1150 m vom Krater entfernt angebrachte Wasserwaage an, dass sich der Boden unter dem Krater nach innen wölbte, was darauf schließen ließ, dass sich die Magmakammer unter dem Vulkan geleert hatte.
Insgesamt wurden während der fünf Monate dauernden Ausbrüche 0,23 Kubikkilometer Lava und 0,02 Kubikkilometer Tephra ausgestoßen. Etwa 2,5 km² neues Land wurden der Insel hinzugefügt, was einer Zunahme um 20 % entsprach. Die Hafeneinfahrt wurde zwar beträchtlich verschmälert, blieb jedoch offen. Tatsächlich verbesserte die Lava den Hafen sogar, da die neue erstarrte Lava nun als zusätzlicher Wellenbrecher dient.
Aufgrund der schlechten Wärmeleitfähigkeit der erstarrten Lava liegt im Inneren der Lavaströme die Temperatur auch Jahre nach dem Ausbruch immer noch bei mehreren hundert Grad Celsius. Schon bald nach Ende der Eruptionen suchten Geologen nach Möglichkeiten, diese Energiequelle zu nutzen. Bald wurden entsprechende Heizsysteme entwickelt, und schon 1974 wurde ein erstes Haus an diese Energiequelle angeschlossen. Nach und nach wurden weitere Häuser mit Heizenergie versorgt. Ab 1979 wurden vier Kraftwerke zur Nutzung der Lavaenergie gebaut. Jedes dieser Kraftwerke entzog einer Fläche von 100 mal 100 Metern die Wärmeenergie, indem kaltes Wasser in das Gestein gepumpt und der zurückkehrende Dampf verwertet wurde. Bis zu 40 Megawatt an Leistung konnten so erzeugt werden, was zur Versorgung praktisch aller Häuser auf der Insel genügte.
Die in Unmengen vorhandenen Pyroklastika wurden genutzt, um die Landebahn des Flughafens der Insel zu vergrößern, außerdem wurde damit Neuland im Meer aufgeschüttet, auf dem 200 neue Häuser errichtet werden konnten. Mitte 1974 war etwa die Hälfte der Bevölkerung auf die Insel zurückgekehrt, und im März 1975 waren es bereits 80 %. Die Wiederherstellung der Infrastruktur auf Heimaey wurde über eine zweckgebundene islandweite Umsatzsteuer finanziert, wobei auch internationale Hilfe in Höhe von 2,1 Mio. US-Dollar bereitgestellt wurde, hauptsächlich vom ehemaligen Mutterland Dänemark, aber auch von den USA und mehreren internationalen Organisationen. In dem durch den neuen Wellenbrecher besser geschützten Hafen kehrte bald die alte Betriebsamkeit ein, und er blieb das wichtigste Fischereizentrum des Landes.
Am Ende der Eruptionen betrug die Höhe des Eldfell etwa 220 m über dem Meeresspiegel. Seit damals hat die Höhe um etwa 18 bis 20 m abgenommen, was sowohl auf Erosion durch den Wind und Regen als auch auf die Verdichtung des Materials zurückzuführen ist. Am Fuße des Vulkans wurde Gras angepflanzt, um die weitere Erosion zu verlangsamen. Der gesamte Eldfell soll in Zukunft möglichst von Gras bedeckt sein, wie das auch beim nahen Helgafell der Fall ist.
Das typische Verhalten eines Vulkans im Vestmannaeyjar-Archipel ist eine einzige Phase vulkanischer Aktivität. Dies macht weitere Eruptionen des Eldfell lange nach der ersten aktiven Phase sehr unwahrscheinlich, aber nicht unmöglich.
Pompeji des Nordens ist ein Projekt, das sich zum Ziel gesetzt hat, einen Teil – etwa 7 bis 10 Häuser – der fast 400 verschütteten Gebäude wieder auszugraben. Der Name spielt auf die Ausgrabungen in Pompeji an, wo durch den Vulkan Vesuv verschüttete Häuser freigelegt wurden.
Die freigelegten Gebäude sollen anschließend der Öffentlichkeit als Museum zugänglich gemacht werden, um einen realistischen Eindruck der Ereignisse zu vermitteln. Seit das Projekt im Sommer 2005 gegründet wurde, konnten bereits drei Wohnhäuser an der verschütteten Straße Suðurvegur freigelegt werden (Stand 2006).
Kristjansson L., Simon I., Cohen M.L., Björnsson S. (1975), Ground tilt measurements during the 1973 Heimaey eruption, Journal of Geophysical Research, v. 80, S. 2951–2954
Lava-Cooling Operations During the 1973 Eruption of Eldfell Volcano, Heimaey, Vestmannaeyjar, Iceland, U.S. Geological Survey Open-File Report 97–724
Williams Jr. R. S., Moore J. G. (1983), Man Against Volcano: The Eruption on Heimaey, Vestmannaeyjar, Iceland, 2. Auflage, veröffentlicht vom US Geological Survey (PDF)
John McPhee, The Control of Nature (1989), ISBN 0-374-12890-1 Das mittlere Drittel dieses Buches ist dem Eldfell-Ausbruch und seinen Folgen gewidmet.
Mattsson H., Hoskuldsson A. (2003), Geology of the Heimaey volcanic centre, south Iceland: early evolution of a central volcano in a propagating rift?, Journal of Volcanology and Geothermal Research, v. 127, S. 55–71

Die elektrische Ladung (Elektrizitätsmenge) ist eine physikalische Größe, die mit der Materie verbunden ist, wie z. B. auch die Masse. Sie bestimmt die elektromagnetische Wechselwirkung, wie also Materie auf elektrische und magnetische Felder reagiert und diese hervorruft. Ihr Formelzeichen 
   ist vom lateinischen Wort ‚quantum‘ abgeleitet. Im Internationalen Einheitensystem wird die Ladung in der abgeleiteten Einheit Coulomb (= Amperesekunde) angegeben.
Die elektrische Ladung ist eine von mehreren Arten von Ladung, die bei Elementarteilchen auftreten.
Elementarteilchen können positive, negative oder keine elektrische Ladung tragen. Die Ladung freier Teilchen ist stets ein ganzzahliges Vielfaches der Elementarladung 
  . Bei zusammengesetzten Teilchen wie Atomen addieren sich die einzelnen Ladungen. Ist die Gesamtladung null, so heißt das zusammengesetzte Teilchen neutral. In einem abgeschlossenen System ist die Gesamtladung unveränderlich (Ladungserhaltung). Die Ladung eines Teilchens ist im Rahmen der Relativitätstheorie in jedem Bezugssystem gleich, also eine Lorentz-Invariante.
Die Elektrostatik betrachtet ruhende Ladungen und rein elektrische Felder. Die Coulombkraft zwischen positiv und negativ geladenen Körpern ist anziehend, zwischen gleichnamigen Ladungen abstoßend. Ausgedehnte physikalische Systeme enthalten stets etwa gleich viele positive und negative Elementarladungen. Schon relativ kleine Überschussladungen können beträchtliche Feldstärken und Kräfte bewirken (Beispiel Gewitter).
Bewegte elektrische Ladungen bilden einen elektrischen Strom. Sie erzeugen elektromagnetische Felder und ihre Bewegung wird durch solche beeinflusst. Dieses Verhalten wird in der klassischen Elektrodynamik beschrieben.
Auf mikroskopischer Ebene ist die elektromagnetische Wechselwirkung zwischen Ladungen zusammen mit quantenmechanischen Effekten wie dem Pauli-Prinzip die Ursache für Zusammenhalt und Struktur der Materie.
Nahezu alle im Alltag beobachtbaren physikalischen Phänomene basieren auf einer von zwei fundamentalen Wechselwirkungen, der Schwerkraft oder der Wechselwirkung elektrischer Ladungen. Zur Erklärung der chemischen Prozesse und allgemein der erfahrbaren Eigenschaften der Materie sind elektrische Kräfte zwischen den Elektronenhüllen von Atomen wesentlich – auch wenn man zum vollen Verständnis oft quantenmechanische Eigenschaften wie etwa den Spin berücksichtigen muss.
Auch geladene Gegenstände können sich durch Kräfte bemerkbar machen. Wenn Verpackungsmaterial, zum Beispiel kleine Polystyrolteile, scheinbar von selbst Bewegungen ausführt, steckt die elektrostatische Anziehung oder Abstoßung von geladenen Teilchen dahinter.
Eine eindrucksvolle Folge elektrischer Aufladungen durch Reibungselektrizität sind Gewitterblitze. Luft ist normalerweise ein Isolator, aber bei hoher Spannung kommt es zu einem Durchschlag. In Blitzen kommt es zu einem schlagartigen Ladungsausgleich zwischen unterschiedlich geladenen Bereichen in der Gewitterzelle oder – seltener – zwischen einem Bereich in der Gewitterzelle und dem Erdboden. Kleine Funken, die von einem Knistern begleitet werden, können auch beim An- und Ausziehen von Kleidungsstücken oder beim Kämmen entstehen.
Der Mensch besitzt kein spezifisches Sinnesorgan für elektrische Ladung. Er kann sie lediglich indirekt wahrnehmen, wenn der Strom durch den Körper bzw. die am Hautwiderstand abfallende Spannung den Schwellenwert des Aktionspotentials der Neuronen erreicht. Ein leichter elektrischer Schlag wird, wie oben schon erwähnt, beim Ausziehen von Kleidungsstücken gespürt, oder wenn man über einen Teppichboden geht und anschließend eine Türklinke berührt. Elektrischer Strom kann ziehende Schmerzen im Zahnnerv auslösen, wenn im Mund elektrochemisch unterschiedliche Metalle (beispielsweise Aluminiumfolie und Amalgam) in Kontakt sind und sich ein Lokalelement bildet. In gleicher Weise wird ein Kribbeln in der Zunge durch Stromfluss hervorgerufen, wenn man mit feuchter Zunge beide Pole einer geeigneten Batterie berührt.
Vermutlich wurden bereits um 550 v. Chr. von Thales von Milet im antiken Griechenland Experimente durchgeführt, bei denen die von elektrischen Ladungen ausgehenden Kräfte beobachtet wurden. Es wurde beispielsweise eine von einem Stück Bernstein (griechisch ηλεκτρόν – gesprochen elektron) ausgehende anziehende Kraft auf Vogelfedern oder Haare festgestellt, nachdem der Bernstein an einem trockenen Fell gerieben worden war.
Der Hofarzt der Königin Elisabeth I., William Gilbert, setzte die Arbeiten von Petrus Peregrinus aus dem 13. Jahrhundert fort und fand heraus, dass andere Stoffe ebenfalls durch Reibung elektrisiert werden können. Er führte in seinem 1600 erschienenen Buch De Magnete, Magnetisque Corporibus, et de Magno Magnete Tellure (deutsch etwa: Über den Magneten, Magnetische Körper und den großen Magneten Erde) den dem Neulateinischen entlehnten Begriff „electrica“ für die Erscheinungen ein, die er im Zusammenhang mit dem Bernstein entdeckte. Später wurde dieser Begriff als Elektron zur Bezeichnung für den Träger der negativen Elementarladung, das 1891 von George Johnstone Stoney so bezeichnete und 1897 von Joseph John Thomson nachgewiesene Elektron (auch der geriebene Bernstein nimmt eine negative Ladung an).
William Gilbert gilt wegen seiner Arbeiten als Begründer der Elektrizitätslehre. Er unterschied als Erster zwischen elektrischer und magnetischer Anziehung. Seine Erklärung für die Anziehungskraft eines geriebenen Bernsteins auf andere Körper bestand darin, dass er ein in allen durch Reibung beeinflussbaren Körpern befindliches „imponderables“ (also unwägbar leichtes) Fluidum annahm, das durch die Wärme bei der Reibung austräte und den Körper wie eine Dunstwolke umgäbe. Andere Stoffe würden beim Eindringen in diesen Dunst angezogen, analog zur Anziehung eines Steins durch die Erde. In Gilberts Fluidumstheorie bzw. Fluidumshypothese klingt aus heutiger Sicht etwas vom modernen Begriff des Feldes an. Die Unterschiede sind jedoch beträchtlich, insbesondere weil der Dunst aus ausgetretenem Fluidum besteht. Otto von Guericke beschäftigte sich in seinen späten Arbeiten mit statischer Elektrizität, von seinen Ergebnissen ist allerdings wenig erhalten. Er erfand 1672 eine einfache Elektrisiermaschine, mit deren Hilfe er eine ganze Reihe von Phänomenen beobachten konnte, etwa die Influenz, die Leitung von elektrischer Ladung, die Leuchtwirkung (Elektrolumineszenz) und die Tatsache, dass sich zwei gleichnamig elektrisierte Körper abstoßen. Bis dahin wusste man nur von der Anziehungswirkung der Elektrizität, Gilberts Erklärungsversuch des einen Fluids reichte nun nicht mehr aus.  
Charles du Fay erkannte 1733 bei Versuchen mit der Reibungselektrizität, dass sich die beiden Arten von Elektrizität gegenseitig neutralisieren konnten. Er bezeichnete die Elektrizitätsarten als Glaselektrizität (französisch électricité vitreuse) und Harzelektrizität (französisch électricité résineuse). Dabei entspricht die Glaselektrizität in der heutigen Bezeichnungsweise der positiven Ladung. Jean-Antoine Nollet entwickelte aus diesen Versuchen die sogenannte „Zweiflüssigkeitstheorie“ oder auch dualistische Theorie, wie sich auch Robert Symmer vertrat. Demnach umgeben die beiden Elektrizitätssorten als „zwei Fluide“ (das Effluvium und das Affluvium) die elektrisierten Körper. Ein elektrisch neutraler Körper enthält nach dieser Theorie die gleiche Menge beider Fluida. Bei der innigen Berührung zweier Körper geht dann vom einen Körper positives Fluidum auf den anderen über, während der andere Körper die gleiche Menge negativen Fluidums an den ersten abgibt. Diese Sprechweise prägte das Denken über die Natur der Elektrizität im 18. Jahrhundert und lebt noch heute in den „zwei Ladungsarten“ (positive Ladung vs. negative Ladung) weiter.
Im von Benjamin Franklin – zum Thema der elektrischen Erscheinungen – verfassten Buch Experiments and Observations on Electricity prägte dieser die Bezeichnung Ladung (engl. charge). Vorher musste von „Körpern, die in einen elektrischen Zustand versetzt worden sind“ gesprochen werden, Franklin führte eine Sichtweise wie beim belasteten und unbelasteten Konto ein, wo durch Reibung Umverteilungen eintraten. William Watson kam zur selben Zeit zu einer vergleichbaren Einschätzung. Nach dieser unitarischen Theorie oder Einflüssigkeitstheorie ist also das einzige Fluidum in einer bestimmten Normalmenge auf elektrisch neutral erscheinenden Körpern vorhanden. Bei der Reibung zweier Körper aneinander gehe dann eine gewisse Menge dieses Fluidums von einem auf den anderen Körper über, sodass die Veränderung des Aufenthaltsortes des Fluidums bewirkt, dass der eine Körper positiv geladen, der andere gleich stark negativ aufgeladen ist. Franklin konnte mit seiner Sichtweise nicht erklären, weshalb zwei gleichermaßen von Ladung entleerte Körper einander abstoßen, erst Franz Ulrich Theodor Aepinus behob diesen Mangel. In heutiger Sprechweise sah er die Stoffteilchen beim Entfernen der Ladungen gewissermaßen in einem ionisierten Zustand.Die Annahme Franklins, dass die Elektrizität des Glases existent und die Harzelektrizität ein Mangel ist und dass bei der Berührung von geladenen und ungeladenen Körpern die Elektrizität immer nur in eine Richtung strömt, legte es nahe, dass – in heutiger Bezeichnungsweise – sich stets die positiven Ladungen bewegen. Vermutlich wurde Franklin zu dieser Annahme durch die Art der beobachtbaren Leuchterscheinungen bei seinen Versuchen mit geladenen Metallspitzen geleitet.
Mit dieser neuerlichen Theorie der Elektrizität als „einem Fluid“ wurde der Idee der Ladungserhaltung zum Durchbruch verholfen. Die Ladungen werden durch Reibung nicht erzeugt, sondern lediglich voneinander getrennt. Da die Kraftrichtung zwischen zwei Ladungen mit Hilfe des Zweiflüssigkeitsmodells einfach mit dem Vorzeichen der beteiligten Ladungen beschrieben werden kann, nahm Charles Augustin de Coulomb das dualistische Modell der „zwei Fluide“ an und legte die Existenz zweier Ladungsarten zugrunde. Aus heutiger Sicht kann man mit beiden Modellen das gleiche Ergebnis erhalten.Im deutschsprachigen Raum wurde die Bezeichnungsweise von Franklin vermutlich vor allem durch Leonhard Euler bzw. Georg Christoph Lichtenberg verbreitet.
Robert Boyle stellte 1675 fest, dass elektrische Anziehung bzw. Abstoßung auch durch ein Vakuum hindurch erfolgt, Francis Hauksbee vertiefte diese Untersuchungen anhand von elektrischen Leuchterscheinungen im Vakuum. Stephen Gray teilte 1729 Materialien in elektrisch leitfähig und elektrisch isolierend ein und demonstrierte, dass auch der menschliche Körper Strom leiten konnte.Im letzten Viertel des 18. Jahrhunderts verlagerte sich der Schwerpunkt der mittlerweile (nachdem mit der Leidener Flasche ein eindrucksvolles Experimentiermittel gefunden worden war) sehr populären Auseinandersetzung mit der Elektrizitätslehre hin zu quantitativen Untersuchungen zur Elektrostatik. Besondere Beiträge zur Forschung wurden von Joseph Priestley und Charles Augustin de Coulomb erbracht. Coulomb veröffentlichte 1785 das coulombsche Gesetz, das besagt, dass der Betrag dieser Kraft zwischen zwei geladenen Kugeln proportional zum Produkt der beiden Ladungsmengen und umgekehrt proportional zum Quadrat des Abstandes der Kugelmittelpunkte ist. Die Kraft wirkt je nach Vorzeichen der Ladungen anziehend oder abstoßend in Richtung der Verbindungsgeraden der Mittelpunkte.
Die 1832 von Michael Faraday formulierten faradayschen Gesetze stellen einen Zusammenhang zwischen geflossener elektrischer Ladung und Stoffumsatz (an den Elektroden abgeschiedene Stoffmenge) bei der Elektrolyse her. In einem 1833 vor der Royal Society gehaltenen Vortrag wies Faraday nach, dass die bis dahin als „verschiedene Elektrizität“ aufgefasste „statische“ (oder „gewöhnliche“), die „atmosphärische“, die „physiologische“ (oder „tierische“), die „Volta’sche“ (oder „Berührungselektrizität“) und die „Thermoelektrizität“ in Wahrheit nur verschiedene Aspekte des einen – von ihm als „Magnetelektrizität“ bezeichneten – physikalischen Prinzips darstellten. Somit war auch klar, dass die elektrische Ladung die Grundeigenschaft der Materie für alle diese Phänomene ist. Ein wichtiger Beitrag von Michael Faraday zur Theorie der Elektrizität war die systematische Einführung des Feldbegriffs zur Beschreibung elektrischer und magnetischer Phänomene.
Im Jahr 1873 entdeckte Frederick Guthrie, dass ein positiv geladenes Elektroskop entladen wird, wenn man ein geerdetes, glühendes Metallstück in die Nähe bringt. Bei negativ geladenem Elektroskop passiert nichts, woraus folgte, dass glühendes Metall nur negative Ladung abgeben und dieser elektrische Strom nur in eine Richtung fließen kann. Thomas Edison hat diese Erscheinung im Jahr 1880 bei Experimenten mit Glühlampen wiederentdeckt und meldete 1883 eine darauf beruhende Anwendung zum Patent an. Den „glühelektrischen Effekt“ nennt man nach Edison und Richardson, dem für die Erklärung der Nobelpreis 1928 verliehen wurde, Edison-Richardson-Effekt.
Im Jahre 1897 konnte Joseph John Thomson nachweisen, dass Kathodenstrahlen aus Elektronen bestehen. Durch ein stark verbessertes Vakuum konnte er für diese das Verhältnis von Ladung zu Masse bestimmen. Thomson vermutete, dass die Elektronen bereits in den Atomen der Kathode vorhanden waren, und stellte 1903 erstmals ein Atommodell auf, das den Atomen eine innere Struktur zuschreibt.
Der diskrete Charakter der elektrischen Ladung, der schon im 19. Jahrhundert von Faraday im Zuge seiner Elektrolyseversuche vorhergesagt worden war, konnte 1910 von Robert Andrews Millikan im sogenannten Millikan-Versuch bestätigt werden. In diesem Versuch wurde der Nachweis geführt, dass geladene Öltröpfchen stets mit einem ganzzahligen Vielfachen der Elementarladung geladen sind, er lieferte auch einen brauchbaren Zahlenwert für die Größe der Elementarladung.
Die elektrische Ladung kann positive oder negative Werte annehmen. Man spricht oft von zwei Arten von elektrischen Ladungen. Beispielsweise hat ein Elektron oder ein Myon die Ladung −1 e, ein Positron oder ein Proton die Ladung +1 e.
Ein Teilchen und sein Antiteilchen besitzen genau die entgegengesetzt gleiche Ladungsmenge. Beispielsweise trägt das Antiproton, Antiteilchen des Protons, die Ladung −1 e.
Die absolute Ladung eines Körpers oder einer Stoffmenge ist die Summe aller enthaltenen Elementarladungen. Dafür werden auch die Bezeichnungen Gesamtladung, Nettoladung oder Überschussladung verwendet. Die Bedeutung dieses Begriffs beruht darauf, dass sich die elektrischen Wirkungen positiver und negativer Ladungen aufheben, wenn ihr gegenseitiger Abstand vernachlässigbar klein ist gegenüber dem Abstand zum Wirkort. So wirkt das abgebildete Lithium-Ion in Abständen von einigen Nanometern wie ein einziger Ladungsträger mit einfach positiver Ladung und wird auch so geschrieben, Li+. Die Aufhebung funktioniert auch mit Hunderten Milliarden Ladungsträgern exakt, wie etwa bei den bereits beschriebenen Öltröpfchen Millikans.
Als elektrisch neutral wird einerseits ein Teilchen bezeichnet, das keine Ladung trägt (zum Beispiel ein Neutron, im Lithium-Atom-Bild grau). Andererseits wird auch ein Körper neutral genannt, der gleich viele positive und negative Elementarladungen trägt, etwa ein Heliumatom mit zwei Protonen und zwei Elektronen.
Von einer Ladungstrennung spricht man, wenn in bestimmten Raumbereichen Ladungen eines Vorzeichens überwiegen, dort die absolute Ladung also nicht null ist. Bei Ladungstrennungen innerhalb eines Körpers bzw. Bauteils ist also die Angabe der Gesamtladung nicht ausreichend. Beispielsweise kann die Gesamtladung sowohl des geladenen wie des ungeladenen Kondensators null sein. Während aber die Platten des ungeladenen Kondensators jede für sich elektrisch neutral ist, tragen die Platten des geladenen Kondensator entgegengesetzt gleiche Überschussladungen, die vor allem zwischen den Platten ein elektrisches Feld erzeugen. Dort etwa vorhandene Öltröpfchen werden polarisiert.
Unter Ladungserhaltung versteht man das Phänomen, dass in jedem abgeschlossenen System die vorhandene Menge an elektrischer Ladung zeitlich konstant bleibt. Dieses Phänomen hat Konsequenzen: Wenn aus elektromagnetischer Strahlung bzw. Photonen Materie entsteht, dann muss dies so geschehen, dass keine Ladung erzeugt wird. Es entsteht deswegen bei der Paarbildung beispielsweise gleichzeitig ein Elektron und dessen Antiteilchen, das Positron. Damit ist die erzeugte Gesamtladung null, die Ladungsmenge bleibt erhalten. Ebenso verhält es sich bei der Umkehrung dieses Vorgangs, der Paarvernichtung eines Teilchen-Antiteilchen-Paares, bei der die vernichtete Gesamtladung ebenfalls null ist.
Wie bei jedem grundlegenden physikalischen Erhaltungssatz beruht der Satz von der Erhaltung der elektrischen Ladung auf Beobachtungen und Experimenten. Bisher haben alle diesbezüglich relevanten Experimente die elektrische Ladungserhaltung bestätigt – zum Teil mit sehr hoher Genauigkeit. In der formalen theoretischen Beschreibung der Elektrodynamik wird die Ladungserhaltung durch eine Kontinuitätsgleichung ausgedrückt, die eine Folgerung aus den maxwellschen Gleichungen ist (siehe Abschnitt Ladung und elektrischer Strom). Eine abstraktere Eigenschaft der Elektrodynamik ist ihre Invarianz (oft auch Symmetrie genannt) unter Eichtransformationen, aus der sich die Quantenelektrodynamik als Eichtheorie ergibt. Nach dem noetherschen Theorem ist mit der Invarianz der Elektrodynamik unter Eichtransformationen ebenfalls die elektrische Ladung als Erhaltungsgröße verknüpft.
Im scheinbaren Widerspruch zur Ladungserhaltung steht die Redeweise von einer Ladungserzeugung oder Aufladung. Damit ist aber eine lokale Anhäufung von Ladungen eines Vorzeichens gemeint, also eigentlich eine Ladungstrennung (und keine Erzeugung).
Zur Aufladung (im Sinne einer Überschussladung) eines zuvor neutralen Körpers muss er Ladungsträger aufnehmen oder abgeben. Aber auch bei einer ungleichmäßigen Ladungsverteilung in einem insgesamt neutralen Körper spricht man von „Aufladung“. Dies geschieht etwa aufgrund eines anliegenden elektrischen Feldes oder durch Bewegungen in molekularem Maßstab. Bei einem polarisierten Material liegt die Ladung gebunden vor, bei der Influenz werden „frei bewegliche“ Ladungsträger in einem Leiter verschoben.
Ein aus dem Alltag bekannter Mechanismus zur Trennung von Ladungen ist die Reibung. Wenn man beispielsweise einen Luftballon an einem Pullover reibt, dann werden Elektronen von einem Material auf das andere übertragen, sodass Elektronen und der zurückbleibende Atomrumpf getrennt werden. Solche Reibungselektrizität ist ein Spezialfall der Berührungselektrizität. Der Bandgenerator nutzt sowohl Berührungselektrizität als auch Influenz.
In Batterien und Akkumulatoren werden chemische Reaktionen ausgenutzt, um eine große Menge von Ladungsträgern (Elektronen bzw. Ionen) umzuverteilen. Wie beim Kondensator bleibt die Gesamtladung null. Anders als bei diesem steigt jedoch die Spannung dabei nicht nahezu linear an, sondern bleibt etwa konstant. Deshalb wird die Kapazität als Energiespeicher beim Kondensator in Farad (= Coulomb pro Volt) angegeben, während die Kapazität einer Batterie als Ladungsmenge charakterisiert wird – in Amperestunden, wobei 1 Amperestunde gleich 3600 Coulomb ist.
Ladungstrennung kann auch durch elektromagnetische Wellen, zum Beispiel Licht, hervorgerufen werden: Lässt man Licht ausreichend hoher Frequenz auf eine Metalloberfläche treffen und platziert im Vakuum eine zweite Metallplatte in der Nähe, entsteht eine Ladungsdifferenz zwischen ihnen, weil durch das Licht Elektronen aus der ersten Platte herausgelöst werden, die sich teilweise zur zweiten Platte bewegen (äußerer photoelektrischer Effekt).
   eines Körpers ist nicht nur eine Erhaltungsgröße, sondern auch unabhängig von seiner Geschwindigkeit. Das heißt, die elektrische Ladung ist eine relativistische Invariante, die Gesamtladung eines Gegenstandes wird nicht durch die Längenkontraktion verändert. Diese Eigenschaft hat die Ladung mit der invarianten Masse eines Systems gemeinsam, unterscheidet sie aber beispielsweise von der Energie. Aus diesem Beispiel kann man erkennen, dass relativistische Invarianz selbst für Erhaltungsgrößen nicht selbstverständlich, sondern eine zusätzliche Eigenschaft ist.
  Unter einer Lorentz-Transformation transformiert sich die Ladungsdichte wie die Zeitkomponente eines Vierervektors, erfährt also eine Veränderung analog der Zeitdilatation; das Volumenelement 
   erfährt dagegen eine Lorentz-Kontraktion. Diese beiden Effekte heben sich genau auf, sodass die Ladung selbst unverändert bleibt.
Interferenzversuche (beispielsweise von Claus Jönsson) mit Elektronen verschiedener Geschwindigkeiten zeigen direkt, dass ihre Ladung unabhängig von der Geschwindigkeit ist. Außerdem müsste sich sonst bei Temperaturänderung die Ladung eines Festkörpers ändern, weil die Geschwindigkeit seiner Bestandteile aufgrund der gestiegenen thermischen Energie zugenommen hat, die Elektronen aber im Mittel eine viel größere Geschwindigkeit erhalten als die massereicheren positiven Atomkerne. Auch sind Wasserstoffmoleküle und Heliumatome (beide enthalten zwei Protonen und zwei Elektronen) elektrisch neutral, obwohl sich die relativen Geschwindigkeiten ihrer Bestandteile deutlich unterscheiden.
Elektrisch geladene Materie kann keine beliebigen Ladungsmengen tragen. Die Ladungen aller bekannten Elementarteilchen sind experimentell vermessen worden mit dem Ergebnis, dass alle Leptonen und ihre Antiteilchen immer ganzzahlige Vielfache der Elementarladung 
  , das Neutron keine (elektrische) Ladung. Der aktuell (2015) genaueste Wert dieser Naturkonstanten beträgt 
  , aber Quarks treten niemals frei auf (siehe Confinement), sondern immer nur in gebundenen Zuständen, den Hadronen, die wiederum immer ganzzahlige Vielfache der Elementarladung tragen. Somit tragen alle frei auftretenden Teilchen ganzzahlige Vielfache der Elementarladung.
Dies wird theoretisch im elektroschwachen Modell begründet, indem die elektrische Ladung auf die schwache Hyperladung und den schwachen Isospin zurückgeführt wird. Warum jedoch die schwache Hyperladung und der schwache Isospin nur bestimmte Werte annehmen, kann durch das Modell nicht erklärt werden. Daher ist bislang auch die „Ursache“ der beobachteten Quantisierung der Ladung ungeklärt; sie gehört nach Meinung von John David Jackson zu den größten Geheimnissen der Physik. Nach Paul Diracs Überlegung zu einem magnetischen Monopol würde die Existenz eines solchen Teilchens – und damit magnetischer Ladungen – die Ladungsquantisierung zwanglos auf die Quantisierung des Drehimpulses zurückführen. Überlegungen aus der Quantenfeldtheorie führen die Ladungsquantisierung auf die Forderung nach Anomaliefreiheit des Standardmodells zurück.Außerhalb atomarer Strukturen ist es in der Regel zulässig, die Ladung als kontinuierliche Größe anzusehen. Selbst eine winzige Stromstärke von 1 Nanoampere bedeutet einen gerichteten Ladungstransport von rund sechs Milliarden Elektronen pro Sekunde. Damit sind einzelne Elementarladungen in den meisten Aspekten der Elektrotechnik nicht erkennbar. Eine Ausnahme ist das „Schrotrauschen“.
Im Rahmen der Quantenfeldtheorie ist die Elementarladung die Kopplungskonstante der elektromagnetischen Wechselwirkung. Aus dem Blickwinkel der Renormierungsgruppe sind allerdings die Kopplungskonstanten von Quantenfeldtheorien keine Konstanten, sondern von der Energieskala abhängig. Auch die Elementarladung ist abhängig von der Energieskala, wobei sie mit steigender Energie größer wird. Das bedeutet, dass bei sehr hohen Energien die Wechselwirkung zwischen geladenen Teilchen stärker ist. Als Folge davon sind bei hohen Energien Teilchenreaktionen durch die elektromagnetische Wechselwirkung wahrscheinlicher. Die Wahrscheinlichkeit, dass beispielsweise beim Zusammenprall zweier Elektronen ein Elektron-Positron-Paar gebildet wird, steigt mit der Energie des Zusammenpralls.
Das elektroschwache Modell besagt, dass sowohl der Elektromagnetismus als auch die schwache Wechselwirkung nur effektive Wechselwirkungen bei niedrigen Energien sind, die durch eine spontane Symmetriebrechung mittels des Higgs-Mechanismus entstehen. Die elektromagnetische Wechselwirkung wird dabei durch den ungebrochenen Anteil der Symmetrie beschrieben, sodass sich nach dem Fabri-Picasso-Theorem eine elektrische Ladung definieren lässt. Bei höheren Energien treten nach dem Modell zwei andere Wechselwirkungen an die Stelle des Elektromagnetismus sowie der schwachen Wechselwirkung und die elektrische Ladung wird durch die schwache Hyperladung und den schwachen Isospin ersetzt. Demnach kann die elektrische Ladung in gewissem Sinne als aus diesen beiden Ladungstypen zusammengesetzt betrachtet werden.
Die Symmetrie positiver und negativer Ladung ist für die Quantenfeldtheorie von Bedeutung. Die Transformation, die in einem Teilchensystem alle Vorzeichen der elektrischen Ladungen umkehrt, wird C genannt. Weitere wichtige Transformationen im Folgenden sind P, die Punktspiegelung des Raumes am Nullpunkt, und T die Umkehr der Zeitrichtung. Das CPT-Theorem, eine fundamentale Aussage über alle Quantenfeldtheorien, besagt, dass Streuprozesse genau gleichartig ablaufen, wenn man alle diese drei Transformationen auf das System anwendet. Dies gilt nicht für die einzelnen Transformationen. Es gibt paritätsverletzende Prozesse, die anders ablaufen, wenn nur P angewandt wird, und von CP-Verletzung spricht man, wenn ein Prozess anders abläuft als seine raum- und ladungsgespiegelte Entsprechung.
Elektrisch geladene Körper erzeugen elektrische Felder und werden selbst von solchen Feldern beeinflusst. Zwischen den Ladungen wirkt die Coulombkraft, deren Stärke – verglichen mit der Gravitationskraft zwischen den Ladungsträgern – sehr groß ist. Sie wirkt zwischen einer positiven und einer negativen Ladung anziehend, zwischen zwei gleichnamigen Ladungen abstoßend. Dabei spielt im coulombschen Gesetz auch der Abstand der Ladungen eine Rolle. Mit ruhenden elektrischen Ladungen, Ladungsverteilungen und den elektrischen Feldern geladener Körper beschäftigt sich die Elektrostatik.
Bei der Aufladung von Körpern muss man Energie aufwenden, um entgegengesetzte Ladungen, die sich gegenseitig anziehen, zu trennen. Diese Energie liegt nach der Ladungstrennung als elektrische Feldenergie vor. Die elektrische Spannung gibt an, wie viel Arbeit bzw. Energie nötig ist, um ein Objekt mit einer bestimmten elektrischen Ladung im elektrischen Feld zu bewegen.
Wenn sich elektrische Ladungen bewegen, spricht man von elektrischem Strom. Die Bewegung von elektrischen Ladungen führt zu magnetischen Kräften und elektromagnetischen Feldern; dies wird durch die maxwellschen Gleichungen und die spezielle Relativitätstheorie beschrieben. Mit bewegten Ladungen in allgemeinerer Form beschäftigt sich dabei die Elektrodynamik. Die Wechselwirkung geladener Teilchen, die mittels Photonen erfolgt, ist wiederum Gegenstand der Quantenelektrodynamik.
Diese Beschreibung von elektrischen Wechselwirkungen zwischen Elementarteilchen ist praktisch nur bei Systemen mit wenigen Teilchen durchführbar. Für viele Betrachtungen reicht es jedoch völlig aus, mit räumlich und zeitlich geeignet gemittelten Größen zu arbeiten, weil die nicht beachteten Details für diese makroskopische Sichtweise vernachlässigbar sind. In diesem Sinne wurden die Gleichungen der Elektrodynamik aufgestellt, ohne den submikroskopischen Aufbau der Materie kennen zu müssen. Durch den Vorgang der Mittelwertbildung werden die Grundgleichungen der Elektrodynamik formal nicht verändert. Ob gemittelte oder exakte Gleichungen gemeint sind, ergibt sich aus dem Kontext.
   die elektrische Feldkonstante. Anschaulich bedeutet das gaußsche Gesetz, dass elektrische Feldlinien von positiven Ladungen (Quellen) ausgehen und in negativen Ladungen (Senken) enden.
In der Relativitätstheorie wird das elektrische Feld mit dem Magnetfeld im Feldstärketensor zusammengefasst. Die Raumladungsdichte 
Wenn ein elektrischer Strom fließt, dann wird die durch eine Fläche (beispielsweise die Querschnittsfläche eines elektrischen Leiters) hindurchfließende Ladungsmenge (hier auch Strommenge genannt) – bezogen auf die dazu benötigte Zeitspanne – als elektrische Stromstärke 
  Für einen zeitlich konstanten Strom vereinfacht sich der Zusammenhang zwischen Ladung und Strom zu:
   darstellen lässt. Durch diese Beziehung der Basiseinheiten Ampere und Sekunde ist das Coulomb im Internationalen Einheitensystem festgelegt.
Wegen der Ladungserhaltung ändert sich die Ladungsmenge in einem bestimmten Raumbereich nur genau in dem Maße, wie Ladungen in diesen Raumbereich hinein- bzw. herausfließen. Die Ladungserhaltung entspricht somit der Kontinuitätsgleichung. Die betrachtete Ladung ist dabei gleich dem Volumenintegral der Ladungsdichte 
    {\displaystyle -{\frac {\mathrm {d} }{\mathrm {d} t}}\iiint _{V}\rho \,\mathrm {d} V=\oiint _{\partial V}{\vec {j}}\cdot \mathrm {d} {\vec {S}}=I}
    {\displaystyle {\frac {\partial }{\partial t}}\rho (t,{\vec {x}})+\nabla \cdot {\vec {j}}(t,{\vec {x}})=0,}
Die Ladungsmenge von 1 Coulomb entspricht etwa 6,24 · 1018 Elementarladungen. Zur Bestimmung von Gesamtladungen können deswegen in der Regel nicht einfach die Ladungsträger abgezählt werden.
Indirekt kann die ab- oder zugeflossene Ladungsmenge über die Messung der Stromstärke bestimmt werden: Fließt ein Strom konstanter Stärke 
  . Allgemein ist die Ladung, die in oder durch einen Körper geflossen ist, das Integral des elektrischen Stromes über der Zeit. Ist die Entladezeit kurz gegenüber der Schwingungsdauer eines ballistischen Galvanometers, so lässt sich die Ladung direkt als Amplitude der angestoßenen Schwingung ablesen.
   der Kraft auf einen geladenen Testkörper misst. Die Definition der Feldstärke liefert die Beziehung
  Diese Methode unterliegt starken Einschränkungen: Der Testkörper muss klein, beweglich und elektrisch sehr gut isoliert sein. Seine Ladung darf das elektrische Feld nicht merklich beeinflussen, was aber schwer überprüfbar ist. Deshalb soll die Ladung gering sein – dann ist aber auch die Kraft schwierig messbar.
Die aufgeführten Nachteile besitzt eine weitere Methode nicht, sie gelingt auch bei recht großen Ladungen. Grundlage ist die Beziehung zwischen der Kapazität 
  Mit der zu messenden Ladung wird ein Kondensator bekannter Kapazität aufgeladen und dann dessen Spannung gemessen. Diese Messung muss allerdings hochohmig erfolgen, d. h. so, dass sie dem Kondensator nur vernachlässigbar wenig von der gespeicherten Ladung entnimmt. Das geschieht mit einem Elektroskop oder besser mit einem Impedanzwandler. Allerdings muss bei dieser Methode die Kapazität der Ladungsquelle bekannt sein, da ein Teil der Ladung dort verbleibt. Die spannungslose Messung mit einem Integrierer (ohne Eingangswiderstand auch als Ladungsverstärker bezeichnet) vermeidet dieses Problem.
Richard P. Feynman: Feynman-Vorlesungen über Physik. Oldenbourg, München/Wien 2007, ISBN 978-3-486-58444-8.
Paul A. Tipler: Physik. 3. korrigierter Nachdruck der 1. Auflage. 1994, Spektrum Akademischer Verlag, Heidelberg/Berlin, 2000, ISBN 3-86025-122-8.
Ludwig Bergmann, Clemens Schaefer: Elektromagnetismus. In: Lehrbuch der Experimentalphysik. Bd. 2, 9. Auflage. Walter de Gruyter, Berlin 2006, ISBN 978-3-11-018898-1.
Wolfgang Nolting: Elektrodynamik. In: Grundkurs Theoretische Physik. Bd. 3, 8. Auflage. Springer, Berlin 2007, ISBN 978-3-540-71251-0.Zur Geschichte
Károly Simonyi: Kulturgeschichte der Physik. Harri Deutsch, Thun, Frankfurt a. M. 1995, ISBN 3-8171-1379-X, S. 320 ff. 
Hans-Peter Sang: Geschichte der Physik (Band 1). Klett, Stuttgart 1999, ISBN 3-12-770230-2, S. 47 ff.

Das Elektrizitätswerk Amstetten ist ein Laufwasserkraftwerk, das für Kleinwasserkraft genutzt wird. Es liegt am Fluss Ybbs in der österreichischen Stadt Amstetten. In Betrieb ging es 1901 und 2010 wurde es um eine Restwassernutzung erweitert.
Über einen Kanal (den „Werkskanal“) wird Wasser von der Ybbs abgezweigt und ins Kraftwerk im Stadtteil Allersdorf geleitet, wo zwei Kaplan-Turbinen Energie erzeugen. Eine weitere Restwasserturbine ist an der erneuerten Wehranlage im Ortsteil Greinsfurth in Betrieb. Über das im Besitz der Stadtwerke Amstetten befindliche Stromnetz wird rund ein Fünftel des Stadtgebietes mit Energie versorgt. Das Regelarbeitsvermögen beträgt 15 GWh.
Am 30. August 1899 fasste der Gemeinderat der Stadt Amstetten den Beschluss zum Bau eines Elektrizitätswerks. Die Stadt hatte die Konzession erworben, um die Wasserkraft der Ybbs zu nutzen, und beabsichtigte mit dem Bau, die Straßen und Plätze zu beleuchten und die Abgabe von Energie an private Haushalte. Bei der Projektierung des Wasserbaues musste besonderes Augenmerk auf die Hochwassersituation gelegt werden. Bereits 1899 wurde im Amstettner Wochenblatt darauf hingewiesen, dass die Hausbesitzer der Stadt sich in der Gemeindekanzlei melden sollen, wenn bei ihnen elektrisches Licht eingeleitet werden soll.Der Bau konnte nicht begonnen werden, bevor die Kraftversorgung der Siechenanstalt Mauer-Öhling (heutiges Landesklinikum Mostviertel) durch den niederösterreichischen Landesausschuss geklärt war. Am 2. Jänner 1900 begann der Bau und Einbau zweier Francis-Turbinensätze mit einer Leistung von je 250 kW. Bei der Eröffnung durch den damaligen Bürgermeister Anton Schmidl am 26. Jänner 1901 war das Elektrizitätswerk Amstetten das größte Wasserkraftwerk Niederösterreichs und es wurden bereits der Hauptplatz sowie der Sitzungssaal des Gemeinderates elektrisch beleuchtet.Der Einbau des dritten und vierten Francis-Turbinensatzes erfolgte im Jahr 1901 und 1905, ebenfalls mit einer Leistung von je 250 kW.
Im Jahr 1911 wurde ein Dieselaggregat mit einer Leistung von 300 kW eingebaut. 1931 erfolgte der Ausbau zweier Francis-Turbinensätze und der Einbau der Kaplan-Turbine I mit einer Leistung von 1160 kW.
Das am 26. März 1947 im Nationalrat beschlossene 2. Verstaatlichungsgesetz erlaubte, dass private und kommunale E-Werke in den Besitz der NEWAG (heute Energieversorgung Niederösterreich) übergehen, so auch das Kraftwerke in Amstetten. Der provisorische Gemeinderat beschloss in einer Resolution vom 23. Mai 1947 jedoch, dies abzulehnen. Daraufhin folgten mehrjährige Proteste gegen die drohende Übernahme. Diese Proteste fanden ihren Höhepunkt im Juli 1952. Das Kraftwerk sollte in einer nächtlichen Aktion gewaltsam übernommen werden, was durch das Eingreifen der Belegschaft, durch Luftschutzsirenen alarmiert, verhindern werden konnte. Die Übernahme ließ sich jedoch nicht verhindern und so erfolgte die Abtretung im Jahr 1956. Die Stadtwerke Amstetten blieben nach wie vor der Betreiber des Kraftwerkes, die NEWAG erhielt den Grundbesitz.Im Jahr 1960 wurden die beiden verbliebenen Francis-Turbinensätze ausgebaut und die Kaplan-Turbine II mit einer Leistung von 1780 kW eingebaut. Die alte Kaplan-Turbine I wurde 1988 ausgebaut und durch eine neue Kaplan-Rohrturbine mit einer Leistung von 1710 kW ersetzt.Im Jahr 1991 wurde der verstaatlichte Grundbesitz wieder an die Stadtwerke Amstetten zurückübertragen, wodurch offene Rechtsfragen zwischen EVN und den Stadtwerken bereinigt werden konnten.
In den Jahren 2004 und 2005 wurde die Wehranlage in Greinsfurth saniert und für den Einbau der Restwassernutzung vorbereitet. Ebenfalls erfolgte der Einbau einer Umwälzanlage beim Kraftwerk, um Eisdruck im Winter zu verhindern. Mit ihr wird Luft eingeblasen, um das Entstehen einer Eisschicht zu verhindern und dünne Eisschichten aufzubrechen.
Im Jahr 2010 wurde ein Restwasserkraftwerk mit einer Leistung von 479 kW bei der Wehranlage in Greinsfurth mit einer Fischaufstiegshilfe errichtet.
Das Einzugsgebiet der Ybbs bis zur Wehranlage Greinsfurth beträgt etwa 833 km². Die durchschnittliche Wassermenge sind 26 m³/s. (MQ) Die größten Hochwässer führten bis dato ungefähr 1000 m³/s und ereignen sich statistisch alle 50–60 Jahre. Die minimale Wasserführung in Trockenperioden beträgt weniger als 4 m³/s.
Das Stauziel beträgt 278,81 müA (Meter über der Adria). Der Werkskanal hat eine Länge von 1300 m und eine Tiefe von durchschnittlich 3 m. Die Fischbauchklappe wurde 57 m breit und 1,54 m hoch dimensioniert und dient zur Hochwasserregulierung. Der Tauchbalken verhindert, dass größere schwimmende Teile (Äste, Bäume usw.) in den Werkskanal gelangen. Die Einlaufschleusen regulieren den Wasserpegel bei Hochwasser oder dienen zum Absperren des Werkskanals in Richtung Kraftwerk bei Revisionen. Die Hochwasserschleuse (Grundablass) wird bei großen Hochwässern (ab 150–200 m³/s) zur Hochwasserregulierung und zum Ablassen des Staubereiches benötigt. Die Schotterschleuse dient zur Spülung der Schotterrinne im Einlaufbereich. Ein Fallschütz in Richtung der Restwasserturbine dient als Absperrorgan für die Restwasserturbine und schließt mit Eigengewicht.
Das Elektrizitätswerk in Allersdorf ist ein Ausleitungskraftwerk. Mit zwei Kaplan-Turbinensätzen werden pro Jahr rund 15 Millionen Kilowattstunden Strom erzeugt. Das entspricht etwa dem durchschnittlichen Bedarf von ungefähr 4300 Haushalten im Netz der Stadtwerke Amstetten.
Durch den Bau des Ausleitungskanals (dem „Werkskanal“), der bei der Wehr in Greinsfurth beginnt, wird das Wasser auf direktem Wege, mit sehr geringem Gefälle in das Kraftwerk Allersdorf geleitet. Durch diese Abkürzung wird im Kraftwerk Allersdorf eine größere Fallhöhe (12 m statt 8 m) des Wassers erreicht, was zu einer höheren Stromerzeugung führt.
Bisher war bei Niederwasser die 2,5 km lange Restwasserstrecke zumindest bis zur Urlmündung trocken. Bis zur Mündung des Unterwasserkanals war eine ökologisch ungünstige Restwassersituation gegeben und der Fischzug aus der Donau war durch die Wehranlage unterbrochen.
Durch die Abgabe von Wasser in den ursprünglichen Verlauf der Ybbs und nicht mehr nur in den Werkskanal, ist es wieder möglich, den Fluss im Bereich von der Wehr Greinsfurth bis zur Rückleitung vom Kraftwerk Allersdorf ganzjährig mit Wasser zu dotieren. Bisher setzte die Dotation der Ybbs in den Sommermonaten meistens aus. Der Grund hierfür war, dass das Wasser durch den Werkskanal aus der Ybbs auf direktem Weg zum Kraftwerk Allersdorf umgeleitet wurde, um Strom zu erzeugen.
Das Wasser fließt durch das Einlaufbauwerk dem Druckrohr zu. Die Druckrohrleitung verläuft bis zum Krafthaus – dort erfolgt der Übergang auf das Einlaufrohr aus Stahl und die Kaplan-S-Rohrturbine. Die Stromerzeugung erfolgt durch eine Drehstrom-Synchronmaschine welche aufgrund der geringen Leistung direkt in das lokale Niederspannungsnetz einspeist.
Für natürliche Fischbestände ist die Möglichkeit einer freien Passierbarkeit von großer Bedeutung. Die größenbestimmende Fischart für die Planung der Fischwanderhilfe in Greinsfurth ist der Huchen, welcher eine Länge von über einen Meter erreichen kann. Die Fischwanderhilfe Greinsfurth ist als Schlitzpass, ein so genannter „Vertical Slot“, gebaut. Durch die Fischwanderhilfe wird eine Fallhöhe von über 8 m zwischen Ober- und Unterwasser überwunden. Die Fischwanderhilfe ist in ihrer Bauart eine der größten in Österreich. Die Dotation des Fischaufstieges schwankt je nach Jahreszeit zwischen 290 und 500 l/s. Diese Wassermengen sind unter anderem nötig, um eine entsprechende Lockströmung im Bereich des Einstieges der Fischwanderhilfe zu erreichen.
Stromerzeugungsverlust im bestehenden Kraftwerk: 200 MWh/Jahr (das entspricht etwa dem Energiebedarf von 60 Haushalten)
Christian Mitterlehner: Abschlussbericht Monitoring FAH Greinsfurth, Stadtwerke Amstetten. (PDF; 1,9 MB)  Amstetten 2012.

Die Elektrofotografie, Xerografie oder das Elektrofaksimileverfahren ist ein foto-elektrisches Druckverfahren zum Vervielfältigen von Dokumenten. Dazu wird ein Fotoleiter mit dem optischen Abbild einer Vorlage belichtet, wodurch ein latentes Bild aus elektrischen Ladungen entsteht. An den geladenen Stellen bleibt Farbe in Form eines Toners haften, mit der anschließend eine Kopie der Vorlage gedruckt werden kann.
In der Alltagssprache wird der Begriff Kopie (bzw. Fotokopie) gleichbedeutend verwendet, obwohl die Elektrofotografie nicht das einzige fotografische Kopierverfahren ist.
Man unterscheidet direkte und indirekte sowie nasse und trockene Elektrofotografie. Die Nassverfahren nutzen als Entwickler eine Suspension aus einem aliphatischen Lösungsmittel mit geringer Dielektrizitätskonstante und dem Toner (siehe auch Nassabzugverfahren), während das Trockenverfahren ein Pulver verwendet.
Das direkte Verfahren nutzt eine Fotoleiterschicht auf dem Träger selbst (zum Beispiel Zinkoxid auf Papier); die Entwicklung erfolgt nass mit einer Suspension aus Toner in einer isolierenden Flüssigkeit (z. B. Leichtpetroleum) oder auch mit Trockentinte.
Das indirekte Nassverfahren nutzt wie die Xerografiegeräte eine fotoempfindliche Trommel; diese wird jedoch mit einer Toner-Suspension benetzt. Der haftende Toner wird direkt von dieser auf das Papier gebracht und muss zum Fixieren nur trocknen.
Das heute ausschließlich gebräuchliche indirekte, trockene Verfahren benutzt fotoempfindliche Trommeln oder Bänder, deren Tonerbild nach der Entwicklung in Pulverform auf den Träger (Papier, Kopierfolie) übertragen und dort thermisch fixiert wird. Das Verfahren arbeitet mit trockenem Toner; es wird daher auch Xerografie (griechisch für „trocken schreiben“) genannt.
Bei der Risografie wird zuerst eine Matrize elektrofotografisch belichtet. Durch die dabei entstandenen feinen Löcher wird in der Folge die Druckfarbe aufs Papier übertragen. Das Verfahren erlaubt kostengünstigen Massendruck mit großen Auflagen, hoher Geschwindigkeit mit bis zu 180 Seiten pro Minute im Format DIN A3, im Vollfarbdruck oder mit 16 monochromen Farben und Papiergewichten im Bereich 40–400 g/m². Die Kopien bzw. Drucke sind mit differenzierten, durch Rasterung erzeugten Grauwerten herstellbar. Risografie zeichnet sich dadurch aus, dass die Farbe ohne Anwendung von Chemikalien oder Hitze aufs Papier gebracht wird. Der ökologische Vorteil wird von günstigen Verbrauchskosten begleitet.
Die Xerografie ist ein Verfahren zur Trockenkopie von meist einfarbigen Papiervorlagen (z. B. Akten), das in allen heute gängigen Kopiergeräten und Laserdruckern eingesetzt wird. Die Ergebnisse sind denen der Tintenstrahldrucker oder Verfahren mit Thermopapier (Thermokopierer, Thermodrucker) hinsichtlich Auflösung, Lichtechtheit und Beständigkeit überlegen.
Die Elektrofotografie ist ein von dem Amerikaner Chester F. Carlson zusammen mit seinem Assistenten Otto Kornei erfundenes Kopierverfahren.
Das Patent wurde am 27. Oktober 1937 angemeldet. Der erste erfolgreiche Versuch fand am 22. Oktober 1938 unter Zuhilfenahme einer mit einem Tuch elektrisch aufgeladenen Metallplatte, Schwefelpuder, staubfeinen Bärlappsporen und einer Wachsplatte statt. Auf der ersten Fotokopie (Trockenkopie) war „10.–22.-38 ASTORIA“ zu lesen. Hierbei handelte es sich um das Datum der ersten Fotokopie, den 22. Oktober 1938, und den Ort, Astoria (New York).
Die Haloid Company kaufte das Patent 1947 und brachte 1949 den ersten kommerziellen Kopierer auf den Markt. 1961 wurde die Haloid Company auf den Namen Xerox umbenannt. In Deutschland wurde die Lizenz an die englische Rank Group gegeben, aus der die Firma Rank Xerox entstand. Der Name Xerox Machine wird in englischsprachigen Ländern auch für das Kopiergerät an sich verwendet.
Das zentrale Element bei der Xerografie ist die Trommel, die mit einer lichtempfindlichen Beschichtung versehen ist, im Folgenden aktive Schicht oder Fotoleiter genannt. Sie besitzt die Eigenschaft, im Dunkeln elektrisch nichtleitend zu sein, bei Lichteinfall dagegen Stromleitung zuzulassen. Bis etwa 1975 verwendete man amorphes Selen; heute werden amorphe organische Halbleiter, amorphes Silicium oder Arsentriselenid (As2Se3) verwendet.
Eine Serie von dünnen Edelstahl- oder Wolframdrähten wird mittels einer Spannung von in der Regel 5 kV positiv gegenüber der aktiven Schicht aufgeladen. Durch die hohe Spannung wird die Umgebungsluft ionisiert, negative Ionen werden zur positiv geladenen aktiven Schicht gezogen, setzen sich dort ab und laden diese negativ auf, da sie im Dunkeln nicht leitfähig ist. So wird Schicht für Schicht die Walze negativ geladen.
Beim Fotokopierer (bzw. Analogkopierer) wird die Vorlage mittels einer starken Lichtquelle (z. B. Halogenlampe) beleuchtet. Die Vorlage wird über ein Objektiv auf die aktive Schicht abgebildet.
Beim Laserdrucker bzw. digitalen Kopierer wird das reflektierte Licht zunächst durch einen Zeilensensor aufgenommen, vergleichbar mit einem Scanner. Gegebenenfalls nach einer Bildbearbeitung wird das digitalisierte Druckbild mit einem Laser oder einer LED-Zeile auf den Fotohalbleiter geschrieben (siehe Laserdrucker).Durch den Lichteinfall werden in der aktiven Halbleiterschicht Ladungsträger erzeugt (innerer fotoelektrischer Effekt). Die Ladungsträger entladen an den belichteten Stellen die positiven Oberflächenladungen zur elektrisch leitenden Rückseite (Aluminiumtrommel) – das latente Bild besteht aus ladungsfreien Zonen.
Der Toner wird möglichst gleichmäßig an die belichtete Walze herangebracht, und zwar überall hin, sowohl an die belichteten als auch an die unbelichteten, geladen gebliebenen Bereiche. Das geschieht mittels einer weiteren, „Bürste“ genannten Walze, die magnetisch ist, wodurch der Toner (Zweikomponenten-Toner enthalten Eisenpartikel, Einkomponententoner sind selbst magnetisch) an ihr haftet, wobei sich die Partikel infolge der magnetischen Feldlinienrichtung wie Borsten aufstellen. Die Tonerpartikel (Durchmesser 3–15 µm) werden an der Kontaktstelle zwischen den beiden Walzen aufgrund elektrostatischer Anziehung übertragen und bleiben an den unbelichteten, also geladenen Stellen der Fotoschicht haften (Schwarzschreiben oder Charged Area Development). Bei gleichnamiger Ladung des Toners können sie auch an den zuvor belichteten, also entladenen Stellen haftenbleiben (Weißschreiben oder Discharged Area Development).
Bei der sogenannten Jumpentwicklung wird der Toner mit Hilfe einer Walze nur in die Nähe des Fotoleiters transportiert. Den verbleibenden Luftspalt überspringt (engl. to jump) der Toner dann aufgrund der elektrostatischen Anziehung. Der Vorteil dieser Methode gegenüber der Bürstenentwicklung ist, dass die Bürste den schon entwickelten Toner nicht wieder verwischt und dass die Bildtrommel weniger verschleißt.
Einkomponententoner werden bei nahezu allen günstigen Kartuschensystemen angewendet; die Partikel landen komplett auf der Trommel und dem Träger.
Bei Zweikomponententoner bleibt die magnetische Komponente zurück; nur die Tonerpartikel werden verbraucht.
Das Tonerbild muss von der Trommel auf das zu bedruckende Medium (Papier oder Kopierfolien) übertragen werden. Dazu wird eine zweite Ladungsquelle (Trommel oder Band) verwendet, die stärker (i. d. R. mit 15 kV) geladen ist als die Trommel und entsprechend den Toner anzieht. Wird der Bedruckstoff an dieser Stelle zwischen beiden hindurchgeführt, geht der Toner auf diesen über.
Um das Bild haltbar zu machen, muss es fixiert werden, d. h. üblicherweise durch zwei geheizte Walzen (bei manchen Geräten auch durch eine Heizkammer ohne Druck) geführt, wodurch die Tonerteilchen schmelzen und sich fest mit dem Bedruckstoff verbinden.
Um zu verhindern, dass der Toner an den Fixierwalzen haften bleibt, sind diese entweder aus einem speziellen Material (z. B. Teflon) oder werden mit einer hauchdünnen Ölschicht aus Fixieröl (i. d. R. Silikonöl) überzogen. Letzteres Verfahren wurde vor allem bei Vollfarbsystemen eingesetzt, da es bei diesen Geräten zu einem dicken Farbauftrag kommen kann und man elastische Walzen (Gummi) verwenden muss. Zudem war der Glanz, den das Fixieröl hinterlässt, bei einigen Druckerzeugnissen durchaus erwünscht. Bei neueren Geräten wird ein elastischer Kunststoff verwendet, der das Fixieröl überflüssig macht. Das bei älteren SW-Geräten eingesetzte Fixieren mittels starker Lichtblitze ist zwar ein ideales berührungsloses Verfahren, wird jedoch wegen der fehlenden Eignung für Farbgeräte heute nicht mehr verwendet. Das geläufigste Verfahren benutzt eine Heizwalze und eine Presswalze. Die Fixiertemperatur beträgt zwischen 165 und 190 °C. Die Fixierung ist bestimmend für die Haltbarkeit der Kopie. Die Lebensdauer der Heizquelle (z. B. eine Halogenglühlampe in Stabform im Inneren einer Walze) kann 50.000 bis 500.000 Kopien betragen.
Nachdem der Toner auf das Medium übertragen ist, muss die verbliebene Ladung der Trommel vor dem nächsten Laden und Belichten entfernt werden. Das geschieht durch Vollbelichtung (stabförmige Lichtquelle) und elektrisches Abstreifen der Ladungen.
Zum Schluss wird die Trommel mit einem Abstreifer oder einer Bürste von etwaigen Tonerrückständen befreit. Der Resttoner wird in einen im Gerät eingebauten Behälter entsorgt. Bei einigen Geräten wird der Toner auch recycelt und dem Entwicklungsprozess wieder zugeführt.
Die Anforderungen an die aktive Schicht der Trommel sind recht hoch: Sie muss eine geringe Dunkelleitfähigkeit zusammen mit einer hohen Lichtempfindlichkeit aufweisen. Bei der Belichtung muss sie kurzzeitig im Bereich geringer lateraler Abstände eine hohe Leitfähigkeit aufweisen, sonst ginge die Auflösung bzw. Schärfe verloren. Sie muss mechanisch stabil sein und die Einflüsse von Ultraviolettstrahlung und Ionen bzw. Radikalen ertragen.
Trommeln können aus verschiedenen Materialien hergestellt werden; üblich sind OPC (Organic Photo Conductor, engl. für organischer Fotoleiter) oder a-Si (amorphes Silizium). Die Lebensdauer dieser Trommeln ist jedoch begrenzt. Die Hersteller geben eine ungefähre Anzahl der möglichen Abzüge an. Während OPC-Trommeln, die hauptsächlich in Bürogeräten verwendet werden, eine Lebensdauer von 25.000 bis 60.000 Seiten aufweisen, sind für a-Si-Trommeln Lebensdauern von einer Million bis zu fünf Millionen Seiten möglich, wodurch sie sich für große Anlagen eignen, wie sie zum Beispiel Telekommunikationsfirmen zum Drucken ihrer Rechnungen verwenden.
Die Anzahl der Abzüge ist jedoch nur ein Idealwert – das Alter und vor allem die Nutzungsart sind ebenfalls entscheidend: Wird ein Kopierer oder Laserdrucker nur bei Bedarf eingeschaltet und werden dabei nur wenige Drucke getätigt, so sinkt die Anzahl der möglichen Kopien. Abrasive Füllstoffe im Papier oder andere mechanische Beschädigungen (z. B. beim Herausziehen verklemmter Seiten) können die fotoempfindliche Schicht dauerhaft aufrauen oder zerkratzen.
Xerografien sind im Gegensatz zu Tintenstrahl- oder Thermodrucken sehr dauerhaft und vor allem lichtecht. Der Toner haftet jedoch nur oberflächlich und kann wieder vom Träger entfernt werden. Manchmal geschieht das im Laufe der Jahre von selbst. Der Toner kann sich auch an Knickstellen ablösen. Inzwischen gibt es Polymertoner mit feineren und gleichmäßiger geformten Partikeln. Dieser platzt an den Falzkanten nicht mehr ab.
Beidseitig bedruckte Papiere können aneinander haften; in Klarsichthüllen bleibt manchmal ein Teil des Toners haften.
Verschiedene Gutachten bescheinigen den Xerografien bzw. Laserdruckern eine Archivfestigkeit von über fünfzig Jahren. Nasskopien halten angeblich länger als Xerografien – bei diesen inzwischen nicht mehr gebräuchlichen Erzeugnissen haben sich keine Lebensdauerbeschränkungen feststellen lassen.
Aufgrund der optischen Abtastung der Vorlage sind die Abbildungsqualität und die Auflösung des Zeilensensors bzw. der Trommel entscheidend für die Auflösung.
Im Bereich von Grauwerten mit unter zehn Prozent Farbdeckung zeigen selbst hochwertige Geräte Schwächen in Form von Rauschen oder sogenannten Schmutzeffekten.
Die Homogenität, die Graduierung sowie die Farbtreue sind in den letzten Jahren verbessert worden, anderen Reproduktionsverfahren jedoch unterlegen.
Vor allem bei Farbsystemen setzen die verwendeten Farbpigmente Grenzen, da die verschiedenfarbigen Toner alle den gleichen hohen Anforderungen genügen müssen, die nicht unbedingt mit einem guten Druckergebnis vereinbar sind.
Bei Digitalisierung ist es möglich, Halbtonbilder so aufzubereiten, dass wie bei anderen Druckverfahren feinstrukturierte Flächen gedruckt werden. So können auf Kosten der Auflösung Flächen mit geringer Farbdeckung sicherer gelingen.
Xerografische Kopiergeräte benutzen ebenso wie Laserdrucker Trockentoner, der ein schwarzes Farbpigment Ruß und bei bestimmten Sorten Schwermetalle wie Blei und Cadmium enthält, mithin also gesundheitsschädlich sein kann.
Das Problem dabei ist nicht nur die Tonerzusammensetzung, sondern auch seine (erwünschte) Feinheit. Das Tonerpulver hat zwar Partikelgrößen oberhalb des lungengängigen Feinstaubes, lagert sich aber dennoch in den Bronchien ab, da es nicht einfach durch Abhusten wieder entfernt werden kann: Toner verändert bereits bei Körpertemperatur seinen Zustand und verklebt unter Umständen mit den Schleimhäuten. Tonerschadstoffe können damit dauerhaft und direkt auf die Schleimhäute, insbesondere der Atemwege oder auf die Haut wirken.
Toner wird nicht nur eingeatmet, sondern durch Kontaminationen auch unbeabsichtigt geschluckt. Besonders Beschäftigte im Bereich des Service, Refill und Recycling sind naturgemäß über lange Zeit den Schadstoffen ausgesetzt.
Die zur Ladungserzeugung eingesetzten Koronaentladungen (Korona-Draht) erzeugen Ozon: Im Bereich der hohen Feldstärken wird die Umgebungsluft ionisiert, wobei u. a. Ozon entsteht. Ozon ist gesundheitsschädlich und erzeugt seinerseits ebenfalls schädliche freie Radikale aus anderen Stoffen. Die meisten solcher Geräte besitzen jedoch Ozonfilter aus Aktivkohle, welche einen großen Teil der Schadstoffe entfernen.
Komplett ozonfrei arbeiten Druckwerke, die die Bildtrommel mit Hilfe einer Ladungsrolle laden. Diese steht in direktem Kontakt mit der Bildtrommel; es ist keine Koronaentladung erforderlich und somit entsteht kein Ozon.
Das Anfertigen von Kopien bestimmter Urkunden oder gültiger Geldscheine ist bei Strafandrohung verboten. Die Hersteller haben teilweise Funktionen implementiert, die solche Kopien unterbinden oder erschweren.
Nachdem die Bilddaten für den Druck aufbereitet wurden (RIP), werden sie noch einmal auf bestimmte Muster hin untersucht, wie sie nur auf Geldscheinen oder bestimmten Urkunden verwendet werden. Wird ein solches Muster entdeckt, gibt es verschiedene Möglichkeiten zu reagieren. Viele Geräte drucken anstatt der Kopie eine schwarze Fläche, verfälschen die Farben oder überziehen das Dokument mit dem deutlichen Aufdruck „Kopie“. Andere Geräte täuschen einen Gerätefehler vor und verlangen nach dem Kundendienst.
Einige Hersteller von Kopiergeräten hinterlegen elektronische Kennungen (z. B. den Machine Identification Code) auf den Kopien. Das geschieht beispielsweise, indem ein definiertes Bitmuster weiträumig verteilt in der Farbe Gelb bei Farbgeräten und bei Schwarz-Weiß-Geräten als schwache Tönung auf den Träger aufgebracht wird.
Bei einem Hersteller ist die Seriennummer des Gerätes auf der Rückseite der Glasplatte nahezu unsichtbar eingeätzt und wird bei jedem Kopiervorgang mit erfasst.
Diese Maßnahmen ermöglichen Herstellern und Ermittlungsbehörden, auf das Kopiergerät selbst, den Standort und evtl. sogar auf die die Kopie anfertigende Person zu schließen. Datenschützer sehen darin verfassungsmäßig garantierte Grundrechte gefährdet (z. B. durch die einfache Möglichkeit zur Aufdeckung von Presseinformanten).
Fotokopiergeräte sind sehr zuverlässig, jedoch nicht wartungsfrei. Aufgrund des feinen Tonerpulvers ist ein Großteil der Ausfälle auch heute noch auf Verschmutzungen zurückzuführen. Technisch bedingt sind die Geräte nicht vollständig hermetisch abgeschlossen, so dass sich oft Tonerpulver auf der Belichtereinheit niederschlägt.
Das Transportsystem eines Kopierers besteht aus Gummi-Walzen, welche altern können und dann entweder regeneriert oder ausgetauscht werden.
Kopierer können in analoge und digitale Kopierer eingeteilt werden. Bis etwa Mitte der 1980er Jahre wurden ausschließlich analoge Kopierer hergestellt. Seit dieser Zeit werden immer mehr digitale Kopierer entwickelt. Der analoge Kopierer ist etwa seit dem Jahr 2000 von digitalen Kopierern verdrängt worden; analoge Kopierer werden inzwischen nicht mehr hergestellt. Ausnahmen hiervon sind kleine A4-Kopierer für den persönlichen Bedarf mit einer Geschwindigkeit von ca. vier A4-Kopien pro Minute. Diese werden von einigen Herstellern weiterhin produziert (Stand: April 2011).
In analogen Kopierern erfolgt die Entwicklung der Trommel direkt vom Original über ein Objektiv und Spiegel; das Abbild der Vorlage wird optisch auf der Trommel abgebildet. Belichtung und Entwicklung laufen synchron in einem Gerät. Der digitale Kopierer besteht dagegen aus zwei getrennten Einheiten, dem Scanner und dem Druckwerk. In der Regel werden diese Einheiten jedoch wie bei einem analogen Kopierer in einem Gerät untergebracht. Bei einem digitalen Kopierer wird die Vorlage mit dem Scanner digitalisiert und in einem Speicher (RAM oder auch Festplatte) zwischengespeichert. Das hier gespeicherte Bild der Vorlage wird anschließend elektronisch an das Druckwerk (Laserdruckwerk) übertragen und ausgedruckt.
Ein Vorteil der Digitaltechnik liegt darin, dass Seiten aus dem Zwischenspeicher mehrfach kopiert werden können, ohne dass die Vorlage erneut belichtet werden muss. Zudem können neben dem reinen Kopieren zusätzliche Funktionen wie Drucken, Faxen, Scannen und das elektronische Versenden der Vorlagen per E-Mail oder in Netzwerkverzeichnisse angeboten werden. Ein weiterer Vorteil ist die Möglichkeit der Zwischenbearbeitung einer Kopie im Gerät. Die hier am häufigsten eingesetzte Funktion ist die Kantenschärfung für Schriften, die das bei analogen Systemen bekannte Problem der Randunschärfen eliminiert und insbesondere bei Schriftstücken eine erhebliche Verbesserung der Qualität bedeutet.
Der wichtigste Vorteil ist jedoch die kompaktere und preisgünstigere Bauweise, da keine Optiken, Blenden und Spiegel im verschmutzungsgefährdeten Bereich zwischen Belichtereinheit und Trommel sein müssen. Die Abtastung muss überdies nicht zeitsynchron zur Entwicklung laufen.
Die digitale Kopiertechnik kann Halbtonwiedergaben sicherer machen, indem statt einer Fläche mikroskopisch kleine Strukturen gedruckt werden. Das sowie die möglicherweise nicht ausreichend genaue Digitalisierung und die Kantenschärfung kann bei Halbtonvorlagen unerwünscht bzw. störend sein.
Insbesondere für den professionellen Einsatz gedachte Kopierer verfügen in der Regel über eine Vielzahl von Zusatzfunktionen, meist in Form von Anbauten:
Der automatische Vorlageneinzug (ADF, Automatic Document Feeder) ermöglicht das automatische Kopieren von Vorlagen mit mehreren Seiten. Der Originaleinzug positioniert eine Seite auf dem Vorlagenglas, wo sie belichtet wird. Anschließend wird die Seite vom Vorlagenglas entfernt und die nächste Seite der Vorlage vom Originaleinzug auf dem Vorlagenglas positioniert. Originaleinzüge mit Originalwendung (RADF, Recirculating Automatic Document Feeder) können auch die Rückseite einer Seite der Vorlage automatisch auf dem Vorlagenglas positionieren. Ein alternatives Verfahren führt die vom ADF eingezogenen Seiten an einer feststehenden Scanzeile vorbei, wo das Original im Vorbeiziehen eingelesen wird. Etwa seit 2005 werden sogenannte Dual-Scan-Vorlagenwechsler hergestellt, die eine separate integrierte Scanzeile für die Rückseite des Blattes haben und somit zusammen mit der feststehenden Scanzeile sowohl die Vorder- als auch die Rückseite eines Blattes in einem Durchgang (ohne mechanische Wendung) einlesen können. Für eine zuverlässige Funktion ist eine regelmäßige Wartung des ADF notwendig. So besteht das Pad zum Separieren der Seiten meist aus Gummi und Kork. Dieses Trenngummi altert. Wird es nicht gereinigt und regeneriert oder ausgetauscht, sind Einzugsfehler die Folge.
Die Duplexeinheit ermöglicht das automatische Bedrucken der Rückseite der Kopien. Damit kann der Papierverbrauch gegenüber dem einseitigen Kopieren halbiert werden.
In Kassetten können normalerweise Papiergrößen von DIN A5 bis DIN A3 oder A3+ bzw. SRA3 (Überformat) vorgehalten werden. Die Kassetten werden als Universalkassetten bezeichnet, wenn sie sich auf die verschiedenen Papierformate einstellen lassen. Die Kapazität einer Kassette liegt bei ca. 500 bis 1000 Blatt Papier.
Papiermagazine sind normalerweise für das A4-Format vorgesehen. Bei Produktionssystemen sind auch Magazine für DIN A3 verfügbar. In der Regel lassen sich Magazine nicht auf ein anderes Papierformat einstellen. Die Kapazität eines Magazins liegt bei ca. 2.500 bis zu 4.000 Seiten.
Finisher und Sorter dienen zur Aufnahme der fertigen Kopien oder Drucke. Bei digitalen Kopierern wird die Ausgabeeinheit als Finisher bezeichnet, bei analogen Kopierern als Sorter. In Finishern und Sortern werden die Kopien automatisch nach Dokumenten und Seiten sortiert abgelegt. Bei vielen Finishern und Sortern können die sortierten Stapel auch geheftet und/oder gelocht werden; die Kopiensätze dürfen dabei bis zu 50 oder 100 Seiten umfassen. Des Weiteren können Finisher mehrseitige Broschüren erstellen. Dafür werden die fertig gedruckten Seiten in der Mitte gefalzt und zweifach mit Draht geheftet. Mögliche Formate sind DIN-A5-Broschüren (halbe DIN-A4-Seite) und DIN-A4-Broschüren (halbe DIN-A3-Seite).
Die Lochereinheit ermöglicht das Lochen der Kopien. Die Kopien werden einzeln gelocht, so dass es keine Beschränkung bei der Seitenzahl (bzw. Stärke) eines Kopiensatzes gibt. Die Lochereinheit kann bei vielen Kopierermodellen zwischen den verschiedenen Standards umgeschaltet werden, so dass die Lochung nach ISO-Standard 838 und nach der 4-Loch-Erweiterung des ISO-Standards 838 möglich ist. Die schwedische Lochung wird in der Regel in einer separaten Lochereinheit angeboten, die kein Umschalten zu den vorab genannten Standards ermöglicht.
Die Druckfunktion ermöglicht das Drucken von Dokumenten von einem Computerarbeitsplatz im Netzwerk oder von einem Datenträger aus.
Die Scanfunktion ermöglicht das Speichern der abgetasteten Vorlage im Netzwerk, auf einem Datenträger oder das direkte Versenden per E-Mail.
Die Faxfunktion arbeitet wie ein herkömmliches Faxgerät. Dokumente werden über das Vorlagenglas eingelesen und über einen Telefonanschluss an die Gegenstelle übertragen. Es können auch Faxe empfangen und ausgedruckt werden. Weiterhin können empfangene Faxe direkt an verschiedene Ziele wie z. B. E-Mail-Adressen oder Verzeichnisse in Netzwerken weitergeleitet werden.
R. Hoffmann: Modeling and Simulation of an Electrostatic Image Transfer. Shaker-Verlag, 2004, ISBN 3-8322-3427-6.
Detaillierte Beschreibung, mathematische Modellierung und numerische Simulation des elektrofotografischen Druckprozesses (Memento  vom 22. Oktober 2005 im Internet Archive) (englisch, PDF, 7,3 MB)

Elektronische Archivierung steht allgemein für die Aufbewahrung von Informationen in elektronischer Form. Ein spezieller Fall der elektronischen Archivierung ist die Revisionssichere Archivierung Handels- und Steuerrechtlich relevanter Dokumente, für die besondere Anforderungen, insbesondere die Unveränderbarkeit und langfristige Verfügbarkeit gemäß der geltenden Aufbewahrungsfristen, gelten.
Für die elektronische Archivierung werden in der Regel spezielle Archivsysteme eingesetzt. Der Begriff Elektronische Archivierung fasst unterschiedliche Komponenten eines Enterprise-Content-Management-Systems zusammen, die im angloamerikanischen Sprachgebrauch separat als „Records Management“, „Storage“ und „Preservation“ bezeichnet werden. Der wissenschaftliche Begriff eines Archivs im Sinne der Langzeitarchivierung ist inhaltlich nicht identisch mit dem Begriff, der von der Dokumentenmanagement-Branche verwendet wird.
Der Begriff der elektronischen Archivierung wird sehr unterschiedlich benutzt. Während heute Unternehmen schon Aufbewahrungsfristen von zehn Jahren für handelsrechtlich und steuerlich relevante Daten und Dokumente als nur sehr schwierig umsetzbar sehen, wird in historischen Archiven von einer sicheren, geordneten und jederzeit zugreifbaren Aufbewahrung von Informationen mit Speicherzeiträumen von mehreren Jahrhunderten gesprochen. Angesichts der sich ständig verändernden Techniken, immer neuer Software, Formate und Standards, ist dies eine große Herausforderung für die Informationsgesellschaft.
Die Aufbewahrung, Erschließung und Bereitstellung von Information ist eine Voraussetzung für die Arbeitsfähigkeit moderner Unternehmen und Verwaltungen. Mit dem exponentiellen Wachstum elektronischer Information wachsen die Probleme der langzeitigen Aufbewahrung, obwohl moderne Softwaretechniken wesentlich besser geeignet sind, Informationen zu verwalten, als dies herkömmlich mit Papier, Aktenordnern und Regalen möglich war. Immer mehr Information entsteht digital und die Ausgabe als Papier ist nur noch eine mögliche Repräsentation des ursprünglichen elektronischen Dokuments. Durch den Einsatz elektronischer Signaturen erhalten elektronische Dokumente den gleichen Rechtscharakter wie ursprünglich manuell unterzeichnete Schriftstücke. Solche digitalen Dokumente existieren rechtskräftig nur noch in elektronischer Form. Diese Entwicklungen zwingen inzwischen jedes Unternehmen, sich verstärkt mit dem Thema elektronische Archivierung auseinanderzusetzen.
Elektronische Archivierung ist die datenbankgestützte, langzeitige, sichere und unveränderbare Aufbewahrung von jederzeit wieder reproduzierbaren elektronischen Informationsobjekten.
Elektronische Langzeitarchivierung ist die Aufbewahrung elektronischer Informationen für mehr als 10 Jahre.Der Begriff Langzeitarchivierung ist im Prinzip ein Pleonasmus, da Archivierung den Langzeitaspekt bereits impliziert, er hilft aber den Unterschied zur Kurzzeitarchivierung, bzw. Backup hervorzuheben.
Revisionssichere Archivierung ist die Aufbewahrung von elektronischen, geschäftsrelevanten Informationsobjekten, die den Anforderungen des Handelsgesetzbuches § 239, § 257 HGB sowie der Abgabenordnung § 146, § 147, § 200 und den GoBD an die sichere, ordnungsgemäße Aufbewahrung von kaufmännischen Dokumenten entspricht und die Aufbewahrungsfristen von sechs bis zehn Jahren erfüllt.Das Handelsgesetzbuch (HGB) und die Abgabenordnung (AO) geben hier die Grundlagen für die Speicherung, unabhängig ob in herkömmlichen Papierarchiven oder elektronischen Systemen, vor (siehe Revisionssicherheit). Die Anforderungen sind in dem Code of Practice „Grundsätze der elektronischen Archivierung“ des VOI Verband Organisations- und Informationssysteme e. V. 1996 zusammengefasst worden. Die Definition für revisionssichere Archivierung stammt ursprünglich von Ulrich Kampffmeyer bereits aus dem Jahr 1992. International ist die Funktionalität und der Umfang von elektronischen Archiven in der Norm ISO 14721 OAIS Open Archival Information System und der von Records-Management-Systemen in der ISO 15489-1/-2 Information und Dokumentation – Schriftgutverwaltung definiert. In Deutschland können unter dem Aspekt der Sicherheit und Prüfung von Archivsystemen die IT-Grundschutz-Kataloge des BSI herangezogen werden. Werden E-Mails nicht revisionssicher abgespeichert, müssen geschäftsführende IT-Verantwortliche u. U. persönlich haften, was Freiheitsstrafen von bis zu zwei Jahren oder Geldstrafen mit sich bringen kann.
Der wichtigste Standard für die elektronische Archivierung ist das OAIS „Reference Model for an Open Archive Information System“. Das Referenzmodell beschreibt die Funktionen und Komponenten, die für eine langzeitige elektronische Archivierung notwendig sind. OAIS wurde von den Weltraumbehörden entwickelt und 2003 als ISO Standard 14721 übernommen. Die Version 2 von OAIS, das sogenannte „Magenta Book“ wurde im August 2012 als ISO Norm 14721:2012 übernommen.
Die folgenden zehn Merksätze zur revisionssicheren elektronischen Archivierung stammen vom Verband Organisations- und Informationssysteme e. V.:
Jedes Dokument muss nach Maßgabe der rechtlichen und organisationsinternen Anforderungen ordnungsgemäß aufbewahrt werden
Die Archivierung hat vollständig zu erfolgen – kein Dokument darf auf dem Weg ins Archiv oder im Archiv selbst verloren gehen
Jedes Dokument darf frühestens nach Ablauf seiner Aufbewahrungsfrist vernichtet, d. h. aus dem Archiv gelöscht werden
Jede ändernde Aktion im elektronischen Archivsystem muss für Berechtigte nachvollziehbar protokolliert werden
Das gesamte organisatorische und technische Verfahren der Archivierung kann von einem sachverständigen Dritten jederzeit geprüft werden
Bei allen Migrationen und Änderungen am Archivsystem muss die Einhaltung aller zuvor aufgeführten Grundsätze sichergestellt sein
Zur Erfüllung dieser Vorgaben wurden Archivsysteme bestehend aus Datenbanken, Archivsoftware und Speichersystemen geschaffen, die in Deutschland von zahlreichen Herstellern und Systemintegratoren angeboten werden. Diese Systeme basieren meistens auf dem Ansatz, über eine Referenzdatenbank mit den Verwaltungs- und Indexkriterien auf einen externen Speicher zu verweisen, in dem die Informationsobjekte gehalten werden. Diese sogenannte Referenz-Datenbank-Architektur war notwendig, um große Mengen von Informationen von den zwar schnellen, aber kostenintensiven Online-Speichern in separate Archivspeicher auszulagern. Die Datenbank erlaubt über den Index dabei jederzeit, das Dokument wiederzufinden und mit einem entsprechenden Anzeigeprogramm dem Anwender bereitzustellen. In den Frühzeiten dieser Technologie handelte es sich meistens um sehr geschlossene, eigenständige Systeme, die praktisch zu „Inseln“ in der IT-Landschaft führten. Heute gliedern sich Archivsysteme als nachgeordnete Dienste (→ Serviceorientierte Architektur) in die IT-Infrastruktur ein, werden direkt von Bürokommunikations- und Fachanwendungen bedient und stellen diesen Anwendungen auch die benötigten Informationen zur Verarbeitung und Anzeige wieder zur Verfügung. Für den Anwender ist es dabei unerheblich, wo die benötigte Information gespeichert ist. Die Diskussion um das „richtige“ Speichermedium für die elektronische Archivierung führen meistens nur die IT-Fachleute, Projektmitarbeiter und Rechtsabteilungen, wenn es um die Auswahl und Einführung eines elektronischen Archivsystems geht.
programmgestützter, direkter Zugriff auf einzelne Informationsobjekte, landläufig auch Dokumente genannt, oder Informationskollektionen, z. B. Listen, Container mit mehreren Objekten etc.
datenbankgestützte Verwaltung der Informationsobjekte auf Basis von Metadaten und gegebenenfalls Volltexterschließung der Inhalte der archivierten Informationsobjekte
Unterstützung verschiedener Indizierungs- und Recherchestrategien, um auf die gesuchte Information direkt zugreifen zu können
Einheitliche und gemeinsame Speicherung beliebiger Informationsobjekte, vom gescannten Faksimile über Dokumentenformat-Dateien und E-Mails bis hin zu komplexen XML-Strukturen, Listen, COLD-Dokumenten oder ganzen Datenbankinhalten
Verwaltung von Speichersystemen mit nur einmal beschreibbaren Medien einschließlich des Zugriffs auf Medien die sich nicht mehr im Speichersystem direkt befinden
Sicherstellung der Verfügbarkeit der gespeicherten Informationen über einen längeren Zeitraum, der Jahrzehnte betragen kann
Bereitstellung von Informationsobjekten unabhängig von der sie ursprünglich erzeugenden Anwendung auf verschiedenen Klienten und mit Übergabe an andere Programme
Unterstützung von „Klassen-Konzepten“ zur Vereinfachung der Erfassung durch Vererbung von Merkmalen und Strukturierung der Informationsbasis
Konverter zur Erzeugung von langfristig stabilen Archivformaten und Betrachter (englisch Viewer) zur Anzeige von Informationsobjekten, für die die ursprünglich erzeugende Anwendung nicht mehr zur Verfügung steht
Absicherung der gespeicherten Informationsobjekte gegen unberechtigten Zugriff und gegen Veränderbarkeit der gespeicherten Information
Übergreifende Verwaltung unterschiedlicher Speichersysteme, um z. B. durch Zwischenspeicher (Caches) schnellen Zugriff und zügige Bereitstellung der Informationen zu gewährleisten
Standardisierte Schnittstellen, um elektronische Archive als Dienste in beliebige Anwendungen integrieren zu können
Eigenständige Wiederherstellungsfunktionalität (Recovery), um inkonsistent gewordene oder gestörte Systeme aus sich heraus verlustfrei wieder aufbauen zu können
Sichere Protokollierung von allen Veränderungen an Strukturen und Informationsobjekten, die die Konsistenz und Wiederauffindbarkeit gefährden können und dokumentieren, wie die Informationen im Archivsystem verarbeitet wurden
Unterstützung von Standards für die spezielle Aufzeichnung von Informationen auf Speichern mit WORM-Verfahren, für gespeicherte Dokumente und für die Informationsobjekte beschreibende Metadaten um eine langfristige Verfügbarkeit und die Migrationssicherheit zu gewährleisten
Unterstützung von automatisierten, nachvollziehbaren und verlustfreien MigrationsverfahrenAll diese Eigenschaften sollten deutlich machen, dass es nicht um hierarchisches Speichermanagement oder herkömmliche Datensicherung geht. Elektronische Archivsysteme sind eine Klasse für sich, die als nachgeordnete Dienste in jede IT-Infrastruktur gehören.
Bei den elektronischen Speichertechnologien ist eine Trennung zwischen der Verwaltungs- und Ansteuerungssoftware einerseits und den eigentlichen Speichermedien andererseits notwendig. Herkömmliche magnetische Speichermedien galten in der Vergangenheit als nicht geeignet für die revisionssichere elektronische Archivierung, da die gespeicherten Informationen jederzeit geändert und überschrieben werden können. Dies betrifft in besonderem Maße Festplatten, die von Betriebssystemen dynamisch verwaltet werden. Magnetische Einflüsse, „Head-Crashs“ und andere Risiken wiesen den Festplatten die Rolle der reinen Onlinespeicher zu. Bei Magnetbändern kam neben der Löschbarkeit hinzu, dass diese Medien hohen Belastungen und Abnutzungen, sowie magnetischen Überlagerungen bei zu langer Aufbewahrung unterliegen.
In den 80er Jahren wurden spezielle digital-optische Speichermedien entwickelt, die in ihrem Laufwerk mit einem Laser berührungsfrei nur einmal beschrieben werden können. Diese Speichertechnologie bezeichnet man als WORM „Write Once, Read Multiple Times“. Die Speichermedien selbst sind durch ihre physikalischen Eigenschaften gegen Veränderungen geschützt und bieten eine wesentliche höhere Lebensdauer, als die bis dahin bekannten magnetischen Medien.
Bei der nur einmal beschreibbaren Compact Disc mit etwa 650 Megabyte Speicherkapazität wird die Speicheroberfläche des Mediums beim Schreiben irreversibel verändert. CD-Medien sind durch ISO 9660 standardisiert und kostengünstig. Die Qualität mancher billiger Medien ist aber für eine Langzeitarchivierung als nicht ausreichend zu erachten. Für Laufwerke und Medien gibt es zahlreiche Anbieter. Die Ansteuerung der Laufwerke wird von den Betriebssystemen direkt unterstützt.
Ähnlich wie die CD wird bei der DVD-ROM die Speicheroberfläche irreversibel im Medium verändert. DVDs bieten unterschiedliche Speicherkapazitäten zwischen 4 und 24 Gigabyte. Beim Einsatz für die Archivierung ist darauf zu achten, dass Laufwerk und Medien den Anforderungen der langzeitigen Verfügbarkeit gerecht werden. Es gibt auch hier zahlreiche Anbieter und die meisten Laufwerke werden direkt von den gängigen Betriebssystemen unterstützt.
Bei diesen Medien und Laufwerken handelt es sich um die traditionelle Technologie, die speziell für die elektronische Archivierung entwickelt wurde. Die Medien befinden sich in einer Schutzhülle und sind daher gegen Umwelteinflüsse besser gesichert als CD und DVD, die für den Endverbraucher-Markt entwickelt wurden. Die Medien werden mit einem Laser beschrieben und bieten eine äußerst hohe Verfälschungssicherheit. Der derzeitige Stand der Technik sind sogenannte UDO-Medien, die einen blauen Laser verwenden und eine Speicherkapazität von 60 Gigabyte bieten. Zukünftig ist mit noch deutlich höheren Kapazitäten je Medium zu rechnen. Nachteilig ist, dass Medien jeweils vorangehender Generationen der 5¼″-Medien in neuen Laufwerken nicht verwendet werden können. Für den Anschluss von 5¼″-Laufwerken ist spezielle Treibersoftware notwendig.
Für die Verwaltung und Nutzung der Medien sind sogenannte Jukeboxen, also Plattenwechselautomaten gebräuchlich. Diese stellen softwaregestützt die benötigten Informationen von den Medien bereit. Die Software ermöglicht es in der Regel auch, diejenigen Medien mit zu verwalten, die sich nicht mehr in der Jukebox befinden und auf Anforderung manuell zugeführt werden müssen. Die Software zur Ansteuerung von Jukeboxen wird direkt in die Archivsoftware integriert, aber auch als unabhängige Ansteuerungssoftware, angeboten. Zum Anschluss von Jukeboxen bedient man sich in der Regel eigener Server, die auch die Verwaltung und das Caching übernehmen. Inzwischen können solche Systeme aber auch als NAS Network attached Storage oder integriert in SAN Storage Area Networks genutzt werden.
Neben den klassischen Archivspeichern, die auf rotierenden, digital-optischen Wechselmedien basieren, treten inzwischen zwei weitere Technologien auf:
Hierbei handelt es sich um Festplattensysteme, die durch spezielle Software die gleichen Eigenschaften wie ein herkömmliches WORM-Medium erreichen. Ein Überschreiben oder Ändern der Information auf dem Speichersystem wird durch die Kodierung bei der Speicherung und die spezielle Adressierung verhindert. Bei diesen CAS-Speichersystemen handelt es sich um abgeschlossene Subsysteme, die allerdings nahezu wie herkömmliche Festplattensysteme direkt in die IT-Umgebung integriert werden können. Sie bieten Speicherkapazitäten mit hoher Performance im Terabytebereich.
WORM-Bänder sind Magnetbänder, die durch mehrere kombinierte Eigenschaften ebenfalls die Anforderungen an ein herkömmliches WORM-Medium erfüllen. Hierzu gehören spezielle Bandmedien sowie geschützte Kassetten und besondere Laufwerke, die die Einmalbeschreibbarkeit sicherstellen. Besonders in Rechenzentren, in denen Bandroboter und Librarysysteme bereits vorhanden sind, stellen die WORM-Bänder eine einfach zu integrierende Komponente für die Langzeitarchivierung dar. Die vorhandene Steuersoftware kann mit den Medien umgehen und auch entsprechendes Umkopieren und Sichern automatisieren.
Besonders für größere Unternehmen und Verwaltungen mit Rechenzentren stellen Festplatten- oder WORM-Bandarchive eine Option dar, da sie sich einfach in den laufenden Betrieb integrieren lassen.
Der Einsatz von WORM-Bändern für den Online-Zugriff ist jedoch zweifelhaft, da Wartezeiten sowohl für das Einlegen des Bandes per Roboter als auch Umspulzeiten anfallen. Sind die Daten in Containern organisiert, kann es zusätzlich innerhalb des Containers zu mehreren Umspulvorgängen für ein einzelnes Datenobjekt kommen (Lesen des Inhaltsverzeichnisses, Lesen des Datenobjektes, Lesen einer Checksumme). Damit verbunden ist eine entsprechende Beanspruchung der Hardware und der Bänder selbst.
Wesentliche Voraussetzung für die langfristige Verfügbarkeit elektronischer Information ist die Einhaltung von Standards. Zu berücksichtigen sind Aufzeichnungsformate, Metadaten, Medien und die Dateiformate der Informationsobjekte selbst. Schon bei der Erzeugung von Daten sollte die langfristige Speicherung berücksichtigt werden. Langzeitig stabile Formate sollten bevorzugt verwendet werden. Eigenschaften eines solchen Formats sollten eine weite Verbreitung, eine offene Spezifikation (Norm) oder die spezielle Entwicklung als Format zur langfristigen Datenspeicherung sein. Beispiele für standardisierte Formate sind XML-Dateien, TIFF und PDF/A-Archive. Für Metadaten gibt es verschiedene standardisierte Metadatenformate. Die Architektur von Archivsystemen und der Aufbau von Informationsobjekten ist durch die ISO 14721 OAIS Open Archival Information System Spezifikation standardisiert worden. Für den Anschluss vom Archivspeichern wird durch die SNIA, den Dachverband der Speicherhersteller, eine XML-basierte Schnittstelle (XAM) bereitgestellt.
Eine Methode zur Sicherstellung der Verfügbarkeit ist die Migration von Information in eine neue Systemumgebung. Sie stellt unter Umständen ein Risiko dar, wenn die Informationen nicht nachweislich unverändert, vollständig und weiterhin uneingeschränkt wieder findbar von einer Systemlösung auf eine andere migriert werden. Originalität und Authentizität können durch eine Migration in Frage gestellt werden. Anderseits zwingt der technologische Wandel die Anwender auf neue Speicher- und Verwaltungskomponenten rechtzeitig zu wechseln, um die Information verfügbar zu halten. Die Migration ist daher bereits bei der Ersteinrichtung eines Archiv- und Speichersystems zu planen, um ohne Risiko und Aufwand den Wechsel vollziehen zu können. Kontrollierte, verlustfreie, „kontinuierliche Migration“ ist zurzeit die wichtigste Lösung, Information über Jahrzehnte und Jahrhunderte verfügbar zu halten. Das Thema Migration wurde durch die Veränderungen und die Konsolidierung des Dokumentenmanagement-Marktes mit dem Verschwinden von zahlreichen Anbietern häufig diskutiert. Der Wegfall einzelner Produkte zwingt zur Migration auf andere Formate, manchmal mit Hilfe eines eigenen Migrationsprogramms. Wer ein Archivsystem einführt, muss sich daher von Anbeginn an mit dem Thema Migrationsplanung beschäftigen.
In der wissenschaftlichen Welt wird noch ein zweites Modell ähnlich stark diskutiert: Emulation. Emulation heißt, die Eigenschaften eines älteren Systems so zu simulieren, dass damit auch Daten dieses Systems mit neueren Computern und Betriebssystemen wieder genutzt werden können. Beispiele gibt es einige, zum Beispiel bei Computerspielen oder Apple-Computern. Diese Lösungsstrategie wird im Bereich der langfristigen Datenspeicherung aber noch nicht in größerem Ausmaß eingesetzt. Nachteile sind, dass der Aufwand künftiger Emulationsschritte nicht planbar ist und bei einem zu großen Paradigmenwechsel eines Tages vielleicht gar nicht mehr durchführbar ist. Diese Nachteile gelten in ähnlicher Form auch für nicht rechtzeitig durchgeführte Migrationen.
Als Vorbereitung für Emulation eignet sich insbesondere das Kapselung-Verfahren. Dabei werden zusätzlich mit der zu bewahrenden Datei oder dem Informationsobjekt auch noch die Software, mit der man es visualisieren und reproduzieren kann, sowie die zugehörigen Metadaten in einer „Kapsel“ gespeichert. Damit sind alle für die Nutzung notwendigen Informationen in Zukunft sofort zusammenhängend gespeichert. Durch diese Methode können die zu speichernden Objekte sehr groß werden, ohne dass jedoch vollständig sichergestellt ist, dass die mitarchivierte Software auch in zukünftigen Betriebssystemumgebungen lauffähig ist.
Lassen sich die Formate der zu speichernden Informationsobjekte nicht kontrollieren und nicht auf wenige Langzeitformate einschränken, sind Konverter und Betrachter systemseitig ständig vorzuhalten, die ältere Formate in anzeigbare Formate beim Aufruf der Objekte wandeln. Dies führt mittelfristig zu einer Vielzahl von bereitzuhaltenden Konvertern und Betrachtern, für die eine eigenständige Verwaltung erforderlich ist, um zu einem älteren Informationsobjekt den jeweils passenden, aktuellen Konverter aufrufen zu können. Die Konversion zur Laufzeit unterscheidet sich von der Emulation dadurch, dass nicht eine ältere Umgebung aufgerufen, sondern das Objekt für die aktuelle Umgebung gewandelt wird. Spezielle Eigenschaften von Formaten, elektronische Signaturen und Komponenten digitaler Rechteverwaltung können hierbei, ebenso wie bei den anderen Verfahren, zu Problemen führen.
Das Thema Archivierung und Langzeitspeicherung hat in den letzten Jahren besonders durch rechtliche und regulative Vorgaben an Bedeutung gewonnen. Die Gleichbehandlung von digitalen Dokumenten mit elektronischer Signatur wie herkömmlichen Papierdokumente, der Sarbanes-Oxley Act und andere Compliance-Anforderungen in den USA, die Diskussion um die Archivierung steuerrelevanter Daten entsprechend den GDPdU in Deutschland machen revisionssichere Archiv- und Speichersysteme erforderlich. Im Rahmen der Diskussion der gesetzlichen Anforderungen stellte sich häufig die Frage nach dem „richtigen“ Speichermedium. Traditionelle WORM-Medien, die physisch nur einmal beschreibbar sind, erhoben den Anspruch, die einzig richtigen Speichermedien zu sein. Die Hersteller von Festplattensystemen und WORM-Tapes konterten. Grundsätzlich gilt jedoch, dass Gesetze und Verordnungen medienneutral sind (oder sein sollten), da angesichts der langfristigen Aufbewahrungszeiträume auch Technologiewechsel berücksichtigt werden müssen. Das „richtige“ Speichermedium gibt es daher nicht. Das gesamte Verfahren der Archivierung muss geschlossen und sicher sein. Dies geht über die Frage der Speicherlaufwerke und -medien hinaus und bezieht auch die organisatorischen Prozesse mit ein.
Entscheidend für den Einsatz von Archiv-Speichertechnologien ist inzwischen die Software geworden. Sie sichert unabhängig vom Medium die Unveränderbarkeit der Information, sie ermöglicht den schnellen Zugriff und sie verwaltet gigantische Speichermengen. Bisher waren elektronische Archive eine spezielle Domäne der Archivsystemanbieter. Nunmehr wird aber die Speichertechnologie selbst immer intelligenter. Systemmanagement- und Speicherverwaltungssoftware verwalten inzwischen auch die elektronischen Archive. Zusätzlich kann immer noch ein herkömmliches Archiv-, Records-Management- oder Content-Management-System für die inhaltliche Strukturierung, die Ordnung, Erschließung und Bereitstellung der Informationen eingesetzt werden. Die Speichersystemanbieter rüsten jedoch auf. Ihr Ziel ist es, Archivspeicher als Infrastruktur betriebssystemnah und für alle Anwendungen gleich bereitzustellen: Dieser Trend im Jahr 2003 wird Informationslebenszyklusmanagement (englisch information lifecycle management, ILM) genannt und soll die elektronische Archivierung einschließen. Besonders das Versprechen, das ILM Migrationen unnötig macht oder automatisiert, weckt bei vielen Anwendern Interesse. Der Anspruch an ILM ist dabei deutlich jenseits des herkömmlichen, Hierarchisches Speichermanagement (HSM), angesiedelt. Es geht zunehmend um die Software zur Verwaltung des gesamten Lebenszyklus von Information anstelle von reiner Speicherhardware. Elektronische Archivierung wird als nachgeordneter Dienst eingesetzt, der in Enterprise-Content-Management-Lösungen (ECM) integriert wird, aber als Archivierungskomponente allen Anwendungen zur Verfügung steht, deren Informationen langfristig und sicher aufbewahrt werden müssen.
Uwe M. Borghoff, Peter Rödig, Jan Scheffczyk, Lothar Schmitz: Langzeitarchivierung. dPunkt-Verlag, 2003, ISBN 3-8986-4245-3.
Ulrich Kampffmeyer, Jörg Rogalla: Grundsätze der elektronischen Archivierung. VOI-Kompendium Band 3. VOI Verband Organisations- und Informationssysteme e. V., Darmstadt 1997, ISBN 3-932898-03-6.
Ulrich Kampffmeyer: Dokumenten-Technologien: Wohin geht die Reise?. Hamburg 2003, 411 Seiten, ISBN 3-9806756-4-5.
ISO 14721:2012 OAIS Open Archive Information System http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=57284, basierend auf dem CCSDS Standard OAIS (Magenta Book), freier Download http://www.project-consult.de/files/ISO_14721_OAIS_V2.pdf(PDF, 1,4 MB)

Die Elektronische Zeitschriftenbibliothek (EZB) ist eine der umfassendsten kostenlos zugänglichen bibliografischen Datenbanken zu wissenschaftlichen elektronischen Zeitschriften. Im Jahr 2012 hatte sie über 16 Millionen Titelzugriffe und wird von fast allen größeren Bibliotheken des deutschen Sprachraums verwendet und bearbeitet.
Die EZB bot ihren Benutzern im Jahr 2014 die Metadaten (Titel, Verlag, etc.) von mehr als 76.000 Zeitschriften in ebenso vielen Datensätzen. In jedem dieser Zeitschriften-Datensätze findet sich auch jeweils ein Hyperlink, der direkt zur Internetseite der gewünschten Zeitschrift führt, wo die Volltexte der einzelnen Bände gelesen werden können. Über 45.000 der 76.000 Zeitschriften sind online und kostenlos lesbare Zeitschriften. Die Volltexte der übrigen 31.000 sind gegen eine von den Verlagen verlangte Gebühr zugänglich – aber auch Benutzern einer Bibliothek, die die gewünschten Zeitschriften für ihre Benutzer lizenziert hat. Ende 2012 wurde die EZB von rund 600 Anwenderbibliotheken als Verbindung ihrer Webseiten mit den Websites der Zeitschriften genutzt.
Die EZB wird mit Steuergeldern finanziert und ist nicht kommerziell ausgerichtet. Entwickelt wurde die EZB 1997 von der Universitätsbibliothek Regensburg, von der sie auch heute noch betrieben und weiterentwickelt wird. Die bibliographischen Datensätze der verfügbaren Zeitschriften und die Links zu ihnen werden arbeitsteilig von allen Teilnehmerbibliotheken erstellt und ständig verbessert.
Anträge für die Aufnahme einer Zeitschrift in die EZB können über die Homepage eingebracht werden. Kriterien für eine Aufnahme sind, dass es sich um eine wissenschaftliche Zeitschrift handelt (im Gegensatz zu populärwissenschaftlichen), die Abrufbarkeit im Volltext (im Unterschied zu Zeitschriften, die nur Inhaltsverzeichnisse oder Abstracts anbieten) und dass die Volltexte über das World Wide Web zugänglich sind.Insgesamt bietet die EZB im Jahr 2014 Titeldatensätze zu über 76.000 wissenschaftlichen elektronischen Volltextzeitschriften, die Zahlen steigen seit der Gründung im Jahr 1997 ständig stark an. Das gilt insbesondere auch für den Anteil der „freien Zeitschriften“ (Open-Access-Zeitschriften). So sank der Anteil der kostenpflichtigen Zeitschriften von 82 % im Jahr 2000 kontinuierlich bis zu nur noch 43 % im Jahr 2012. Im selben Jahr waren über 10.500 der Zeitschriften (16 %) reine Onlinezeitschriften, die also parallel zur elektronischen Ausgabe nicht mehr im Druck erscheinen. Auch deren Anteil steigt, im Jahr 2003 waren es lediglich 11 % (2.012). Spitzenreiter unter den Fachgebieten waren 2012 mit rund 14.400 Zeitschriften die Wirtschaftswissenschaften und mit rund 11.700 Zeitschriften die Medizin. Zusätzlich zum EZB-Angebot werden in der EZB auch noch etwa 60.000 Zeitschriftentitel aus Aggregatoren (wie EBSCO oder ProQuest) verzeichnet (Stand: 2010), die als „lokale Zeitschriften“ geführt werden. Es handelt sich dabei um Titel, für die die Bibliotheken die Zugriffsrechte über Aggregatoren erworben haben. Die EZB listet 2014 rund 45.000 freie wissenschaftliche Zeitschriften, das schwedische Directory of Open Access Journals hingegen nur rund 10.000. Mit ein Grund ist, dass die Anforderungen an die Qualität der Zeitschriften in der EZB niedriger sind.
Der Großteil der in der EZB gelisteten kostenpflichtigen Zeitschriften wird von mindestens einer Teilnehmerbibliothek lizenziert und somit im Volltext angeboten. Einem Bericht aus dem Jahr 2002 ist aber zu entnehmen, dass damals immerhin 9 % der kostenpflichtigen Zeitschriften von keiner einzigen der teilnehmenden Bibliotheken lizenziert waren.Die EZB listet ganze Zeitschriften. Deren Artikel sind nur teilweise erfasst, nach ihren Titeln kann also nur in Ausnahmefällen gesucht werden. Nicht in der EZB verzeichnet sind E-Books, Zeitungen, Datenbanken und Graue Literatur.
In den Jahresberichten der EZB finden sich Daten dazu, wie oft in einem Jahr einer der verfügbaren Zeitschriftentitel angeklickt wird. Von 2000 bis 2007 stieg die Anzahl der Klicks kontinuierlich von 2,5 auf 16,9 Millionen, jährlich waren es um etwa 2 Millionen mehr. Im Jahr 2008 stiegen die Zugriffe ein letztes Mal stark an und kletterten auf den bisherigen Höchststand von 22,4 Millionen. In den beiden Folgejahren sanken die Zahlen stark um jeweils 2 Millionen pro Jahr und pendelten sich danach im Bereich von 16 bis 17 Millionen Titelklicks ein.
Den Grund für die sinkenden Nutzungszahlen ab 2009 sehen die EZB-Verantwortlichen unter anderem darin, dass die Volltexte elektronischer Zeitschriften immer öfter auch durch Linkingdienste (wie SFX oder auch den EZB-Linkingdienst) zugänglich wurden.
Genutzt wird die EZB vor allem von wissenschaftlichen Bibliotheken, aber auch von Konsortien, Forschungs- und anderen wissenschaftlichen Einrichtungen, wie etwa Krankenhäusern. Ebenfalls zum Einsatz kommt die EZB in den deutschen virtuellen Fachbibliotheken sowie anderen Fachinformationsdiensten. Über die Zulassung ausländischer Institutionen wird jeweils im Einzelfall entschieden.Die EZB ging im Oktober 1997 in Betrieb und war das Ergebnis eines im April 1997 gestarteten Projekts der Universitätsbibliothek Regensburg; Projektpartner bei der Umsetzung war die Universitätsbibliothek der Technischen Universität München. Seit 1998 traten zunächst vor allem bayerische Teilnehmerbibliotheken bei, bald jedoch auch nicht-bayerische und ausländische Bibliotheken. Im Oktober 1999 beteiligte sich mit der Thüringer Universitäts- und Landesbibliothek die 50. Einrichtung, bereits im Mai mit der Universitätsbibliothek Wien die 100. Während im Jahr 1997 noch zwei Bibliotheken an dem Dienst beteiligt waren, sind es 2014 bereits über 600 Bibliotheken und Forschungseinrichtungen. Innerhalb des deutschen Sprachraums nehmen heute so gut wie alle wichtigen Universitäts- und staatlichen Bibliotheken teil, ein prominentes ausländisches Mitglied ist seit 2003 die amerikanische Library of Congress.Ab 2003 war die EZB in die Fachportale und Virtuellen Fachbibliotheken, die im 2011 eingestellten Vascoda-Verbund entwickelt wurden, integriert.
Die elektronischen Zeitschriften können von Bibliotheksbenutzern meist nicht im OPAC der Bibliothek gefunden werden. Deshalb führen von den Homepages Links auf die EZB, wo nach den lizenzierten Zeitschriften gesucht werden kann. Eine Ausnahme ist beispielsweise der Verbundkatalog Gateway Bayern, der dem Benutzer für jede elektronische Zeitschrift einen Link aus dem Katalogisat auf den EZB-Datensatz bietet. Außer allgemeinen Links auf die Startseite der EZB gibt es auch spezielle Links auf bestimmte Datensätze der EZB. So werden über Linkresolver Links angeboten, die direkt von Katalogisaten in Katalogen und Datenbanken zum gewünschten EZB-Datensatz führen. Auch der Link im EZB-Katalogisat zur Website der Zeitschrift funktioniert über Linkresolver.
Grundsätzlich sind alle in der EZB geführten Zeitschriften online im Volltext abrufbar. Die EZB zeigt ihrem Benutzer optisch an, ob er auch die Möglichkeit eines für ihn kostenlosen Zugriffs auf die Volltexte hat. Sämtlichen Zeitschriften ist dazu eines von vier möglichen Ampelsymbolen zugeordnet. Ist die Ampel auf grün, ist die betreffende Zeitschrift für jeden, ganz ohne Bedingungen abrufbar. Ist die Ampel auf gelb, so ist die jeweilige Zeitschrift für den Benutzer lesbar, weil die Bibliothek oder die Institution, über die er die EZB nutzt, eine kostenpflichtige Lizenz dieser Zeitschrift erworben hat. Ist die Ampel gleichzeitig gelb und rot, bedeutet das, dass die Zeitschrift nur in Teilen lizenziert wurde und nicht sämtliche Jahrgänge zugänglich sind (so etwa bei Moving Walls und Embargos). Leuchtet die Ampel rot, dann ist die Zeitschrift für den betreffenden Nutzer nicht im Volltext zugänglich (die Bibliothek des Nutzers verfügt nicht über die nötige Lizenz). Der Zugriff der Bibliotheksbenutzer auf die von ihrer Bibliothek lizenzierten Zeitschriften (gelbe und gelbrote Ampel) kann entweder auf Rechnern innerhalb der Bibliothek erfolgen oder mittels Virtual Private Network (VPN). Für einen Zugriff über VPN benötigt der Benutzer nur einen Internetzugang und seine von der Bibliothek bereitgestellten Zugangsdaten (Benutzername und Passwort).
Angezeigt werden dem Benutzer die Zeitschriften in alphabetischer Reihenfolge, wobei er die Anzeige aber auch auf eines von etwa 40 Themengebieten (z. B. auf biologische Zeitschriften) einschränken kann. Neben einer Schnellsuche und einer erweiterten Suchmöglichkeit, kann sich der Benutzer auch die neu in die EZB aufgenommenen Zeitschriften anzeigen lassen. Mittels weiterer Filter können auch nur die von einer bestimmten Bibliothek lizenzierten Zeitschriften oder die Open-Access-Zeitschriften aufgerufen werden.
Ein Datensatz zu einer Zeitschrift beinhaltet vor allem bibliographische Informationen, wie etwa den Titel, die URL der Zeitschrift, die ISSN, eine Fachzuordnung, die verlinkte ZDB-Nummer, den Preistyp (kostenlos oder kostenpflichtig) und den Verlag. Schlagwörter kommen zwar in manchen, aber nicht allen Datensätzen vor.
Seit Juni 2013 steht dem Benutzer auch eine von der Universitätsbibliothek Regensburg entwickelte Android-App für Tablets und Smartphones zur Verfügung. Seit April 2002 ging neben der deutschen auch eine englischsprachige Benutzeroberfläche in Betrieb, nicht zuletzt im Hinblick auf fremdsprachige Teilnehmerbibliotheken.Im Rahmen eines Projektes mit Studenten der Universität Regensburg wurde die EZB Ende 2014 hinsichtlich ihrer Barrierefreiheit überarbeitet. Dabei wurde die Seite so umgestaltet, dass sie für Menschen mit körperlichen Einschränkungen zugänglich und für alternative Interaktionsformen geeignet ist. Außerdem wurde die Kompatibilität mit gängiger Assistenzsoftware (z. B. Screenreader) deutlich verbessert. Die Überarbeitungen wurden unter dem Paradigma Design für Alle durchgeführt und im Sinne der Inklusion können nun alle Nutzer die gleiche Internetseite verwenden, unabhängig davon ob sie eingeschränkt sind oder nicht.
Eine für die Bibliotheken bedeutende Aufwandsersparnis ergibt sich daraus, dass alle Teilnehmerbibliotheken Titeldatensätze erstellen, verbessern und aktualisieren können. Wechselt etwa eine Zeitschrift ihre Internetadresse, genügt es, wenn eine der 600 Bibliotheken die neue Adresse im EZB-Katalogisat einträgt. Bei vielen zehntausend Zeitschriften fällt eine gemeinsame Verwaltung deutlich ins Gewicht. Obwohl also Arbeitsteilung praktiziert wird, ist es in der Praxis aber doch so, dass die großen Bibliotheken aufgrund höherer Kapazitäten auch einen größeren Beitrag zum Aufbau und der Pflege des Titelbestands leisten. Nach eigener Aussage führt die gemeinsame Pflege zu einer hohen Qualität und Aktualität der Daten.Die Ampelsymbole grün, gelb, gelbrot und rot werden nicht automatisch generiert, sie müssen von Mitarbeitern der jeweiligen Bibliothek auf eine der Varianten eingestellt werden. Bezahlt die Bibliothek beispielsweise Lizenzgebühren für sämtliche Jahrgänge einer kostenpflichtigen E-Zeitschrift, dann stellt sie deren Ampel auf gelb. Verfügt eine andere Bibliothek nicht über die nötige Lizenz, stellt sie für ihre Benutzer auf rot. Erwerben mehrere Bibliotheken im Rahmen einer Erwerbungskooperation gemeinsam Lizenzen, so gibt es auch eine Möglichkeit, die lizenzierten Titel zentral zu verwalten und das Ampelsymbol für alle Kooperationsteilnehmer auf einmal einzustellen (Konsortialverwaltung). Im Jahr 2008 wurden in der EZB 14
große Konsortien mit zahlreichen Unterkonsortien verwaltet. Seit 2007 stehen auch Funktionen zur Verwaltung von Nationallizenzen bereit. Dabei verwaltet die Bibliothek die Lizenzeinträge, die die Verhandlungen führt; die Lizenzdaten werden dann automatisch auch für alle anderen angemeldeten Bibliotheken eingespielt. Auch für verschiedene Lizenzarten wurde eine einfache Verwaltungsmöglichkeit geschaffen.Da keine umfangreichen standardisierten Regelwerke bestehen, ist die Erstellung eines Datensatzes weit unkomplizierter als beispielsweise die Erstellung eines bibliographischen Datensatzes anhand der RAK-WB oder der RDA.
Die von der EZB und anderen Diensten vorangetriebene Vernetzung dient in erster Linie der Informationsweitergabe. So gibt die EZB beispielsweise Informationen darüber weiter, welche Bibliothek welche Zeitschriften lizenziert hat und bekommt von anderen Anbietern beispielsweise Informationen über die Open Access Policies der Zeitschriftenverlage.XML-Ausgabeformate
Portale und Bibliotheken können dank XML-Ausgabeformaten auf ihren Webseiten EZB-Angebote integrieren. Beispielsweise bietet die Virtuelle Fachbibliothek Germanistik auf ihren Seiten eine solche Oberfläche an. Dort ist eine Liste von germanistischen Zeitschriften abrufbar, von denen Links zu den Katalogisaten der Zeitschriften führen, wo sich wiederum ein Link zur Zeitschrift sowie durch die Ampelsignale angezeigte mögliche Zugriffsrechte finden. Die Zeitschriftenkatalogisate befinden sich zwar auf den Seiten der Virtuellen Fachbibliothek Germanistik, deren Daten stammen jedoch aus der EZB.Der EZB-Linkingdienst
Zur besseren Vernetzung der Online-Dienstleistungen von Bibliotheken ging 2004 der EZB-Linkingdienst in Betrieb, der eine weitere Möglichkeit der Verlinkung bietet. Bieten die Webseiten von Portalen oder Bibliotheken den EZB-Linkingdienst an, so können deren Benutzer von der Trefferliste einer durchgeführten Suche oder den Katalogisaten über einen Link direkt zum EZB-Katalogisat gelangen, von wo aus die Zeitschrift erreichbar ist. Dabei zeigen bereits die Links per Ampelsymbol an, ob der Benutzer Zugriffsrechte für den Volltext der Zeitschrift besitzt oder nicht. Im Jahr 2010 war der EZB-Linkingdienst in rund 40 Dienste eingebunden (wie etwa ReDI, DigiBib und Medpilot).JOP
Seit 2009 besteht ein weiterer Linkingdienst, der von der EZB und der ZDB gemeinsam angeboten wird. Der „Journals Online & Print“ (JOP) bietet seinen Anwenderbibliotheken Links an, die bereits anzeigen, ob der spezifische Benutzer über Zugriffsrechte auf eine Zeitschrift verfügt. Dabei wird nicht nur der mögliche Zugriff auf Online-Volltexte geprüft (über die EZB), sondern auch, ob die Bibliothek des Benutzers über gedruckte Bestände der Zeitschriften verfügt (über die ZDB). Über den Link kann man zur Homepage der Zeitschrift oder die Bestandsangabe der gedruckten Versionen gelangen. Der Linkingdienst JOP wird auch von der EZB selbst verwendet, um lokale Bestandsinformationen anzuzeigen.Datenlieferungen für andere Dienste
Für einen gemeinsamen Datenlieferdienst der ZDB und der EZB liefert die EZB Lizenzdaten von Zeitschriften an die ZDB, welche diese samt den Katalogdaten der ZDB für Online-Kataloge zur Verfügung stellt. Aber auch an andere Dienstleister werden Daten geliefert. Anwenderbibliotheken können Listen der von ihnen lizenzierten Zeitschriften generieren und diese in anderen Diensten im Hintergrund für die Steuerung der Anzeige von EZB-Links und Verlinkungen auf Zeitschrifteninhalte nutzen. Verwendet wird dieses Angebot bei Google Scholar und dem Linkresolver SFX.Schnittstelle zu Sherpa RoMEO
Die Sherpa-RoMEO-Datenbank erfasst die Open Access Policies verschiedener Verlage. Um diese den EZB-Nutzern einfach zugänglich zu machen, gibt es in den EZB-Katalogisaten der Zeitschriften Links zu diesen Policies bei Sherpa RoMEO. Finanziert wurde die Vernetzung mit Sherpa RoMEO durch ein DFG-Projekt, in Betrieb ging die neue Funktion 2009.Pay-per-View
Auf den Seiten der EZB findet sich ein Verzeichnis von Zeitschriften, die gegen Entgelt gelesen werden können (Pay-per-View). Seit 2008 schreibt der § 53a des deutschen Urheberrechts vor, den Endkunden den Kauf der Zeitschriften offensichtlich anzubieten. Über die Liste können die Verlage seit 2009 ihre Zeitschriften zum Kauf anbieten. Der Dokumentenlieferdienst subito nutzt diese Online-Schnittstelle, um die Pay-per-View-Angebote der Verlage zu prüfen und gleich auch die Lieferbedingungen festzulegen. Im Jahr 2010 waren über 5.000 Zeitschriften von 25 Verlagen in dem Verzeichnis abrufbar.
Die technische Betreuung und Weiterentwicklung liegt bei der Universitätsbibliothek Regensburg. Auch die Daten werden zentral in einer Datenbank in Regensburg gespeichert, bearbeitet werden können sie hingegen von sämtlichen Teilnehmerbibliotheken (zentrale Datenbank, aber dezentrale Dateneingabe).Bei der Datenbank handelt es sich um eine relationale Datenbank. Eingegeben und verändert werden die bibliographischen Datensätze über ein WWW-Interface zur Datenbank. Für die Praxis heißt das, die Teilnehmerinstitutionen geben die Metadaten der Zeitschriften in ein Webformular ein und speichern ihre Eingaben danach ab.
Die dem Benutzer angezeigten Seiten werden on-the-fly aus der Datenbank erzeugt. Die Hardware auf der die EZB läuft, besteht aus einem Datenbankserver (ein Dell PowerEdge 2950 mit zwei 3-GHz-Xeon-Vierkernprozessoren) sowie einer virtuellen Maschine als Webfrontend (mit zwei virtuellen CPUs). Betriebssystem und Software sind mit verschiedenen Open-Source-Programmpaketen realisiert, beispielsweise mit Debian GNU/Linux, MySQL, Apache 2 und PHP.
Die Organisation der EZB liegt bei der Universitätsbibliothek Regensburg. Die EZB ist nicht kommerziell ausgerichtet und ihrem öffentlichen Auftrag verpflichtet. Laut diesem ist es ihre Aufgabe, die Literatur- und Informationsversorgung im akademischen Bereich zu verbessern und an neue Anforderungen anzupassen.
Die finanzielle Hauptlast für die technische Realisierung und Entwicklung der EZB haben seit jeher Förderorganisationen getragen. Das ursprüngliche Projekt wurde ab 1997 für zwei Jahre vom Bayerischen Staatsministerium für Wissenschaft, Forschung und Kunst finanziell gefördert. Weitere Geldgeber waren oder sind die Deutsche Forschungsgemeinschaft und das Bundesministerium für Bildung und Forschung. Bis August 2002 lief beispielsweise das DFG-Projekt „IBS – EZB, Integrierter Benutzer-Service in der Elektronischen Zeitschriftenbibliothek“, bis Ende 2004 war die EZB Teilprojektnehmerin des Projekts „Arbeitsgruppe Informationsverbünde“, das vom deutschen Bundesministerium für Bildung und Forschung gefördert wurde.Seit dem 5. Mai 2011 gibt es einen wissenschaftlichen Beirat, der die Universitätsbibliothek Regensburg bei der weiteren Entwicklung der EZB berät.Zur Verbesserung der EZB wird von der UB Regensburg ein Informationsaustausch mit den Teilnehmerbibliotheken betrieben. Zu diesem Zweck existieren beispielsweise eine nur für die Anwenderbibliotheken zugängliche Webseite sowie eine geschlossene Mailingliste, über die auch praktische Probleme in der täglichen Anwendung der EZB diskutiert werden. Darüber hinaus finden einmal jährlich ein EZB-Anwendertreffen und mehrmals im Jahr Anwenderschulungen an verschiedenen Orten statt.
Bruno Bauer: EZB, Elektronische Zeitschriftenbibliothek. 10 Fragen von Bruno Bauer an Evelinde Hutzler, Projektverantwortliche für die EZB an der Universitätsbibliothek Regensburg. In: Medizin – Bibliothek – Information, Band 2, Heft 3, 2002, S. 26–30 (PDF; 72 KB)
Evelinde Hutzler: 10 Jahre Elektronische Zeitschriftenbibliothek. Kontinuität und Wandel einer kooperativen Dienstleistung. In: Bibliotheksdienst, Band 42, Heft 2, 2008, S. 169–181 (online)
Evelinde Hutzler: Verbesserter Service durch Vernetzung von Dienstleistungen am Beispiel von elektronischen Zeitschriften und Datenbanken. In: Ute Bergner, Erhard Göbel (Hrsg.): The ne(x)t generation. Das Angebot der Bibliotheken, Neugebauer, Graz-Feldkirch 2010, S. 188–195
Evelinde Hutzler: Elektronische Zeitschriftenbibliothek. Entwicklung, Stand und Perspektiven. In: Zeitschrift für Bibliothekswesen und Bibliographie, Band 47, Heft 6, 2000, S. 571–587
Evelinde Hutzler: Elektronische Zeitschriften als Service von Bibliotheken. Erfahrungen aus der Praxis der Elektronischen Zeitschriftenbibliothek. In: BuB, Band 53, Heft 3, 2001, S. 150–155
Evelinde Hutzler: Elektronische Zeitschriften in wissenschaftlichen Bibliotheken. In: Beate Tröger (Hrsg.): Wissenschaft Online, Vittorio Klostermann, Frankfurt am Main 2000, S. 37–66 (= Zeitschrift für Bibliothekswesen und Bibliographie, Sonderheft 80)
Evelinde Hutzler, Albert Schröder, Gabriele Schweikl: Auf dem Weg zur Digitalen Bibliothek. Lokale, regionale und überregionale digitale Dienste der Universitätsbibliothek Regensburg. In: Evelinde Hutzler, Albert Schröder, Gabriele Schweikl (Hrsg.): Bibliotheken gestalten Zukunft. Kooperative Wege zur digitalen Bibliothek, Universitätsverlag Göttingen 2008, S. 83–100
Evelinde Hutzler, Martin Scheuplein, Petra Schröder: Der schnelle Weg zum Volltext. Einsatz und Nutzung des Verlinkungsdienstes der Elektronischen Zeitschriftenbibliothek. In: Bibliotheksdienst, Band 40, Heft 3, 2006, S. 306–313
