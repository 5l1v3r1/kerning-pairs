
Die Glanzstoff Austria (früher Erste österreichische Glanzstoff-Fabriken, Glanzstoff-Fabrik St. Pölten oder Enka Austria) war ein chemisches Unternehmen im niederösterreichischen St. Pölten. Das 1906 eröffnete Werk produzierte bis 2008 bis zu 12.000 Tonnen Viskosefasern pro Jahr und war zeitweise deren zweitgrößter Produzent weltweit. Zuletzt erwirtschaftete es einen Umsatz von 50 Mio. Euro.Nach einem Brand in der Abluftreinigungsanlage im Jänner 2008 wurde die Produktion gestoppt, konnte jedoch im April des Jahres teilweise wiederaufgenommen werden. Im Juli 2008 wurde überraschend bekannt gegeben, die Viskosegarnproduktion in St. Pölten zu beenden. Ende 2008 wurde die Produktion endgültig eingestellt, Anfang 2009 wurde die Glanzstoff Austria aufgelöst und die Liegenschaften von der Glanzstoff-Gruppe, die sich im Eigentum der CAG Holding befindet, übernommen. Studien zur neuen Nutzung wurden angelegt, ein Masterplan vom Wiener Architekturbüro F + P ARCHITEKTEN ZT GMBH / Sepp Frank ausgearbeitet. 2011 entstand die Studie „design now“ von Peter Noever. Am ehemaligen Fabrikstandort sind heute etwa 15 Verwaltungsmitarbeiter der Glanzstoff-Gruppe angestellt. 2012–2015 hatte die New Design University unter der Leitung von Rektor Stephan Schmidt-Wulffen ihren Hauptsitz in der 2.500 m² großen Glanzstoff-Halle, die dafür eigens renoviert wurde. 1.300 neue Wohnungen und über 1.000 neue Arbeitsplätze zu schaffen wurde als Ziel deklariert. 2015 wurde Felix Mitterers Theaterstück „Glanzstoff“ in der Glanzstoff uraufgeführt. Im Herbst 2017 wurde die permanente Lichtskulptur der Künstlerin Brigitte Kowanz eingeweiht.Seit 2015 steht die Anlage unter Denkmalschutz.
1903 wurde ein neuer Zollvertrag zwischen Österreich-Ungarn und dem Deutschen Kaiserreich abgeschlossen, der die Einfuhr deutscher Waren nach Österreich erschwerte. Die Eigentümer des deutschen Unternehmens Vereinigte Glanzstoff-Fabriken AG, allen voran der Österreicher und Mitbegründer Johann Urban, entschieden, ein eigenes Werk in Österreich zu eröffnen, um trotzdem den Markt in Österreich-Ungarn ohne Schwierigkeiten bedienen zu können. Urban wurde 1904 zum Leiter des Tochterunternehmens unter der Firma Erste Österreichische Glanzstoff-Fabrik AG mit Sitz in Wien ernannt. Als die Aktiengesellschaft im Dezember 1904 gegründet wurde, waren außer der Muttergesellschaft noch die Oberrheinische Bank und die Niederösterreichische Escompte-Gesellschaft beteiligt. Das Aktienkapital sollte ursprünglich drei Millionen Kronen betragen, bei der Gründung am 17. Dezember wurde es jedoch auf nur 2,5 Millionen Kronen festgesetzt.Urban machte sich auf die Suche nach einem Standort für eine Betriebsstätte in Österreich, Wien kam aufgrund der hohen Grundstückspreise nicht in Frage. Die Wahl fiel vor allem aufgrund der verkehrsgünstigen Lage an der Westbahn, den ausreichenden Grundwasserreserven und der günstigen Energieversorgung aus den städtischen Elektrizitätswerken auf Viehofen. Der damalige St. Pöltner Bürgermeister Wilhelm Voelkl hatte Urban zudem den günstigen Baugrund vom Niederösterreichischen Religionsfonds knapp außerhalb der Stadtgrenzen verschafft, ließ auf Stadtkosten die Zufahrtsstraße errichten und veranlasste einen vorgezogenen Kanalbau. Zudem wurden von der Stadt die Kanalgebühren um 25 % reduziert und die Verpflichtung eingegangen, Arbeiterwohnungen zu errichten. Schon 1903 hatte Hermann Ofner einen Verein zur Erbauung billiger Wohnungen gegründet, der in den Folgejahren zahlreiche Arbeiterwohnhäuser errichtete. Das mit Abstand größte war der Komplex der Zehn-Häuser-Gruppe am Mühlweg, bei seiner Fertigstellung 1908 beherbergte er über 100 Arbeiterwohnungen.
Die Fabrik wurde am 4. April 1906 feierlich in Betrieb genommen. Zu Produktionsbeginn erzeugten 306 Arbeiter täglich 125 Kilogramm Kupferseide, später wurde die Tagesproduktion auf 600 Kilogramm gesteigert. Bald darauf erfolgten die ersten Erweiterungsarbeiten, unter anderem ließ Urban 1908 das Verwaltungsgebäude aufstocken.
Der Verwaltungsrat entschied, 1911 die Fabrik auf Viskosefaser-Erzeugung umzustellen, was bauliche Änderungen bedingte und durch eine Kapitalerhöhung auf 4 Millionen Kronen finanziert wurde. Die mehrere Neubauten umfassende Umrüstung war 1913 abgeschlossen.Bis zum Beginn des Ersten Weltkriegs stieg die Mitarbeiterzahl auf 1.700 an. Schon kurz nach Kriegsausbruch musste ein Teil des Werksareals an ein Zweigwerk der Whitehead Torpedofabrik abgegeben werden. 1917 wurde die Produktion von Kartuschbeuteln für den Sprengstoff in Granaten unter Heeresaufsicht gestellt, was zu einer kurzfristigen Produktionssteigerung führte. 1918 stand das Werk nahezu still, das Schwefelsäurelager und ein Magazin waren zerstört, 1919 kam die Produktion mangels Kohle komplett zum erliegen. Nach sechs Monaten konnte der Betrieb reduziert wiederaufgenommen werden, die Anlagen waren jedoch erst 1922 wieder voll ausgelastet.
1926 wurde damit begonnen, das Werk auszubauen. Grundlage dieser Erweiterung war eine Erhöhung des Aktienkapitals und der Verkauf der Aktienmehrheit an die niederländische Algemene Kunstzijde Unie N.V. Bis 1929 wurden das Kesselhaus, das Turbinenhaus und der 100 m hohe Schornstein errichtet, weiters wurde das angrenzende Areal der geschlossenen Whitehead Torpedofabrik erworben. Es konnten jedoch nicht alle zu Kriegsbeginn abgegebenen Flächen zurückgekauft werden, ein Teil der Grundstücke wurde der Stadt St. Pölten zur Schuldenabdeckung übergeben. Nach den Ausbauten beschäftigte die Glanzstoff 3.000 Mitarbeiter, die pro Tag sechs Tonnen Viskose produzierten. Als der Ort Viehofen 1923 eingemeindet wurde, war der Betrieb der größte St. Pöltens. Die Ende der 1920er Jahre einsetzende Weltwirtschaftskrise traf das Unternehmen ungleich härter als die meisten anderen der Stadt. Millionenverluste zwangen die Eigentümer, das Werk für 18 Monate zu schließen, 1932 wurde mit 800 Mitarbeitern der Betrieb wiederaufgenommen. 1933 wurde eine bis zuletzt bestehende eigene Betriebsfeuerwehr gegründet, die Feuerwehrausrüstung wurde von der geschlossenen Torpedofabrik übernommen.
Der Anschluss an das Deutsche Reich bewirkte zunächst einen Aufschwung durch die Integration in einen größeren Wirtschaftsraum. Schon bald wurde die Fabrik ein wichtiger Betrieb für die Rüstungsindustrie. Die produzierten Garne fanden in Rüstungsprodukten wie Fallschirmen, Reifenkörben und Kartuschbeuteln für den Sprengstoff in Granaten Verwendung. Nachdem das Unternehmen 1941 in Glanzstoff-Fabrik St. Pölten AG umbenannt worden war, wurde es als kriegswichtig eingestuft. Die durch die Umstellung auf Cordkunstseide, eine mehrfach gezwirnte Viskosefaser, notwendigen Umbauarbeiten konnten dadurch rasch vollendet werden. Neben der Errichtung eines Ätznatronlagers wurden Anbauten an die Kuchendruckwäsche errichtet und die Heizanlage von Dampf auf Heißwasser umgestellt.Ab 1943 wurde die Produktion weiter ausgebaut. Unter anderem wurden ein 35 m hoher Wasserturm, der zu Kriegsende auch als Flakturm diente, eine neue Transformatorenstation und eine Schwefel-Kohlenstoff-Rückgewinnungsanlage neu errichtet. Mit diesen Ausbauten wurde die Produktion zwischen 1938 und 1944 von 2.100 auf 9.500 Jahrestonnen mehr als vervierfacht. In der Fabrik wurden zahlreiche Zwangsarbeiter beschäftigt, für die ein eigenes Barackenlager auf dem Werksgelände unterhalten wurde.Bei der Einnahme St. Pöltens durch die sowjetischen Truppen wurde das Werk stark beschädigt, immerhin konnte die Sprengung des Betriebs verhindert werden. Am 14. April um 4 Uhr früh informierte ein italienischer Arbeiter den Produktionsleiter Franz Laimer über die geplante Sprengung, der im letzten Augenblick die Zündschnur durchschneiden konnte. Die Glanzstoff wurde von den Sowjets als Deutsches Eigentum beschlagnahmt und in die USIA eingegliedert, 45 % der Maschinen des Betriebs wurden noch 1945 in die Sowjetunion abtransportiert. Zu Kriegsende beschäftigte der Betrieb nur mehr 600 Mitarbeiter, bis 1955 stieg die Zahl der Arbeitnehmer auf 1.400 an. 1955, nach Abschluss des österreichischen Staatsvertrags und dem Ende der USIA, wurde das Werk unter öffentlicher Verwaltung weitergeführt. Erst 1956 konnte die Algemene Kunstzijde Unie die Aktienmehrheit wieder übernehmen, musste jedoch in den nächsten sechs Jahren 10.000 Tonnen Seide an die Sowjetunion als Ablöse liefern. Unter Auswertung der während des Kriegs entwickelten Herstellungsverfahren konnte die Produktion von Kunstseide für Autoreifenkarkassen bald wieder aufgenommen werden, 1957 waren 1.400 Personen angestellt.
In den folgenden Jahren wurde die Fabrik erweitert. Gegen Ende der 1950er wurden eine Spinnbad-Kristallisationsanlage zur Wiederaufbereitung der Spinnbäder sowie eine Schärabteilung eingerichtet, zwischen 1963 und 65 wurde neben zwei neuen Werkshallen vor allem die Energieversorgung infolge eines einwöchigen Kohlebunkerbrandes von Kohle auf Erdgas umgestellt. Seit 1960 produzierte das Werk auch das Bautenschutzmittel Kenitex. 1969 reorganisierte der Hauptaktionär der Glanzstoff seine Chemiefaserunternehmen. Die der Algemene Kunstzijde Unie gehörenden Werke in den Niederlanden sollten mit jenen der Glanzstoff AG in Deutschland wirtschaftlich und organisatorisch als ein Unternehmen geführt werden, der Name der neuen Gruppe war AKZO N.V., die Glanzstoff wurde der Tochtergesellschaft Enka-Glanzstoff untergeordnet.Nachdem 1975 vorübergehend auf Kurzarbeit umgestellt wurde, entschloss sich die Konzernleitung Ende 1977, das Werk aufgrund pessimistischer Absatzprognosen im Laufe des Jahres 1978 zu schließen. Nach einem Konjunkturaufschwung im Herbst 1978 eröffneten sich neue Absatzmärkte und der Stilllegungsbeschluss wurde wieder aufgehoben. Zudem gewährte die Bundesregierung einen Kredit in Millionenhöhe. Im August 1978 brach ein Brand aus und löste einen Großeinsatz aller Stadtfeuerwehren aus. Es entstand ein Schaden in Höhe von mehr als zwei Millionen Schilling (inflationsbereinigt heute etwa 400.000 Euro), und die Fabrik stand 20 Tage still. 1982 wurde die Firma in Enka Austria AG geändert.Trotz der Aufhebung des Stilllegungsbeschlusses deponierten die Konzernvertreter 1979 bei Bundeskanzler Bruno Kreisky die Absicht das Werk mittelfristig zu schließen. Es wurde vereinbart, dass bei zukünftigen Problemen die Bundesregierung frühzeitig informiert und ihr das Werk zum Kauf angeboten wird. Im September 1981 trat der Fall ein, die Werksleitung informierte den Bundeskanzler über finanzielle Probleme. Nach Zuschuss von je 20 Mio. Schilling für 1982 und 1983 wurde versucht, die Fabrik an die Chemiefaser Lenzing zu verkaufen, die jedoch nach eingehender Prüfung ablehnte. Im November 1982 wurden 110 Mitarbeiter zur Kündigung angemeldet und die Produktion um 70 % gesenkt, im darauffolgenden Monat wurde bekannt, dass der Betrieb mit April 1983 verstaatlicht werden sollte. Das neue Unternehmen mit dem Namen Glanzstoff Austria Ges.m.b.H sollte ursprünglich zu einem Drittel vom Land Niederösterreich übernommen werden, was am damaligen Landeshauptmann Siegfried Ludwig scheiterte.Nach einer Sanierung und Umstrukturierung erfolgte 1988 die Rückumwandlung in eine Aktiengesellschaft mit einem Grundkapital von 50 Millionen Schilling (heute etwa 6.645.000 Euro), 99,6 Prozent der  Aktien wurden an die Lenzing AG verkauft, der Kaufpreis lag bei rund 111 Millionen Schilling (heute etwa 14.752.000 Euro). Im Dezember 1989 erhielt das Werk die Staatliche Auszeichnung und durfte seither das Bundeswappen im Geschäftsverkehr verwenden. Zwei Jahre später fanden 900 Personen Arbeit in der Glanzstoff, das Unternehmen ging an die Börse. 1990 wurde die Tochtergesellschaft Domus Liegenschaftsverwaltung gegründet, die alle Wohnhäuser der Glanzstoff und, seit 1993, die nicht mehr betriebsnotwendigen Gebäude der CAG-Gruppe verwaltet. 1991 ging eine biologische Abwasser- und Recyclinganlage in Betrieb. Zwei Jahre später stürzten Absatzrückgänge und Zahlungsrückstände von Kunden das Unternehmen in eine Krise. Der Textilbereich wurde geschlossen, einzig die Reifenindustrie wurde weitergeführt. Die Mitarbeiterzahl reduzierte sich auf 250. Nachdem das Unternehmen Konkurs anmelden musste, wurde es 1994 von der CAG Holding des Industriellen Cornelius Grupp übernommen. Im Jahr darauf wurde die Produktion textiler Garne wiederaufgenommen. Nach der Inbetriebnahme einer neuen Abgasanlage 1998 wurde das Werk 2001 mit neuen Spinnmaschinen ausgestattet. Ab 1997 wurde innerhalb der CAG Holding die Glanzstoff-Gruppe gegründet, die neben der Glanzstoff Austria auch die Werke Glanzstoff Bohemia und Textilcord Steinfort umfasste, 2007 kam Sicrem hinzu.
Am Abend des 10. Jänner 2008 brach in der Abluftanlage ein Brand aus, die Löscharbeiten dauerten bis in die Morgenstunden. Im Juli wurde bekanntgegeben, dass das Werk mit Jahresende geschlossen werden sollte. Die 327 Beschäftigten wurden beim AMS zur Kündigung angemeldet. Als Grund für die Schließung wurde das langwierige Genehmigungsverfahren für eine neue Abluftanlage genannt, ohne die eine wirtschaftliche Produktion nicht sinnvoll war. Im Dezember 2008 wurde der Betrieb eingestellt, einige Mitarbeiter blieben allerdings in der Energiezentrale sowie in der Abwasserentsorgung eingesetzt. Die gekündigten Mitarbeiter wurden von einer Outplacementstiftung betreut. Diese wurde gemeinsam vom Land Niederösterreich, dem Arbeitsmarktservice und der CAG-Holding finanziert, alleine das Land Niederösterreich investierte über 280.000 Euro. Die Glanzstoff Austria wurde Anfang 2009 aufgelöst, die Glanzstoff-Gruppe übernahm die Liegenschaften. Einzig die Holding der Glanzstoff-Gruppe blieb mit 15 Mitarbeitern in St. Pölten, die verbliebenen Angestellten erledigen Verwaltungsaufgaben für die weitergeführten Werke Glanzstoff Bohemia, Textilcord Steinfort und Sicrem der Glanzstoff-Gruppe.
Am 17. Juli 2009 wurde vom Sprengdienst der Feuerwehr St. Pölten der 1929 erbaute Ziegelschornstein gesprengt. Der ursprünglich mit 100 m Höhe erbaute Schornstein musste 1978 infolge eines Blitzschlags bereits bis auf 86 m Höhe abgetragen werden, war jedoch noch immer der höchste in Österreich. Die Sprengung war notwendig geworden, weil das Mauerwerk des außer Betrieb gestellten Schornsteins brüchig zu werden drohte.Am 17. Februar 2012 wurde von der Feuerwehr auch der 45 m hohe und 1941 errichtete Wasserturm gesprengt. Er war nie als solcher in Betrieb, jedoch als Flak-Turm und zuletzt als Montageort für Funkantennen.Im Herbst 2012 zog ein Teil der New Design University in eine eigens adaptierte Halle der Glanzstoff ein.Seit 2015 steht die ehemalige Fabriksanlage unter Denkmalschutz.
Die Glanzstoff Austria hatte während ihres 104-jährigen Bestehens mehrere Eigentümer. In der Bevölkerung einfach Glanzstoff genannt, änderte sich die offizielle Firma und das verwendete Logo im Laufe der Zeit mehrmals, meist aufgrund Eigentümerwechsels. In der folgenden Tabelle werden die Logos, Eigentümer und Firmen aufgeführt.
Kurz nach der Eröffnung der Ersten österreichischen Glanzstoffabrik wurde von der Kupferseide- auf die Viskosegarnerzeugung umgestellt. Bis zuletzt produzierte das Werk vorwiegend Viskosefilamentgarne. Die technischen Garne, vertrieben unter dem Namen Viscord, wurden vor allem für die Reifenproduktion verwendet, die textilen Garne Viscofil und Viscont wurden in der Kleidungsindustrie angewandt. Der Unterschied zwischen technischen und textilen Garnen lag vor allem in der Anzahl der verzwirnten Filamente. Während beim textilen Garn zwischen 33 und 330 Fasern verzwirnt wurden, betrug die Anzahl bei den technischen Garnen zwischen 660 und 2.640.Die Produktion verlief in beiden Fällen gleich. Zuerst wurde langfasriger Zellstoff in 15-prozentiger Natronlauge alkalisiert. Die Lauge wurde danach abgepresst und in den Prozess zurückgeführt. Nachdem der Zellstoff zerfasert war, wurde er vorgereift und anschließend in Schwefelkohlenstoff xanthogeniert. Durch den Zusatz von wässriger Natronlauge entstand eine zähe Flüssigkeit, genannt Viskose, die in die Spinnabteilung gelangte. Dort wurde die Viskose, je nach gewünschter Faserqualität, durch Spinndüsen unterschiedlicher Lochzahl in ein Spinnbad gedrückt. Die schwefelsauren Spinnbäder enthielten hohe Konzentrationen an Natriumsulfat und Zinksulfat, bei textilen Garnen zusätzlich Farbpigmente. Darin flockte die gelöste Cellulose unter Kohlenstoffdisulfid-Abgabe. Danach wurden die Fäden aufgespult, mehrfach zur Entfernung von Spinnbadrückständen gewaschen, getrocknet und bei Bedarf verzwirnt. Im Wesentlichen lief die Produktion schon 1961 auf gleiche Weise.
Bei der Spinnbadaufbereitung entstand als Nebenprodukt Natriumsulfat. Die jährlich etwa 12.000 produzierten Tonnen wurden weiterverkauft.In den 1960ern und beginnenden 1970ern produzierte das Werk zudem das Bautenschutzmittel Kenitex. Das Mittel bestand aus einem Kunststoff-Bindemittel mit darin enthaltenen Mineralstoffen wie Asbest, Titanoxid und Zinkoxid, zusätzlich wurden verschiedene Farben beigemengt. Dieses Mittel wurde auf Fassaden aufgetragen und machte das Gebäude wetterfest und beständig gegen Säuren und Laugen. Mit dem schrittweisen Verbot von Asbest wurde die Produktion in den 1970ern eingestellt.
In ihrer Geschichte hat die Glanzstoff-Fabrik die Umwelt unterschiedlich stark belastet. Neben der Luftverschmutzung durch Abgase aus der Produktion wurden vor allem in den Anfangszeiten der Boden und das Grundwasser massiv beeinträchtigt.
Zwischen 1904 und 1983 wurde der Nordteil des Betriebsgeländes, etwa 15.000 m², als Abfalldeponie benutzt. Vorwiegend wurden bei der Produktion anfallende Abfälle wie Kohlenasche, Schlacke, Laugenschlamm, Viskoseabfälle und Kalkschlamm sowie hausmüllähnliche Abfälle, Bauschutt und Kies abgelagert. Die Deponie umfasste etwa 38.000 m³ bis 50.000 m³ oder 57.100 Tonnen abgelagertes Material. Feststoffuntersuchungen ergaben hohe Konzentrationen an Sulfaten, Zink, FCKW und Schwefelwasserstoff. Auch im Grundwasser wurde eine ständige Überschreitung der für diese Substanzen geltenden Trinkwassergrenzwerte gemessen. Im Jahr 2000 wurde die Deponie Nord als Altlast eingestuft und der Glanzstoff eine Beihilfe von 2,056 Millionen Euro zugestanden, um sie zu sanieren. Die Deponie wurde 2002 teilweise saniert, der Nordteil der Deponie wurde ausgehoben und entsorgt. Auf dem Südteil der Deponie war eine Werkshalle errichtet worden, weshalb das Ausheben nicht mehr möglich war. Trotzdem sanken die Konzentrationen im Grundwasser unter die gesetzlichen Grenzwerte. Die Deponie Nord wird heute noch immer zu den Altlasten gezählt. Im April 2009 wurden am gesamten Areal Bohrungen durchgeführt. Die Bodenproben zeigten eine Bodenbelastung über den Grenzwerten, blieben jedoch unter der Maßnahmenschwelle.Durch die direkte Nachbarschaft zur Traisen und die Ableitung der geklärten Abwässer in diese kam es im Fluss zu Belastungen. Zwischen 1993 und 2002 wurden Messungen flussauf und flussab der Glanzstoff durchgeführt, teilweise wurden die Grenzwerte nicht eingehalten. Vor allem der Sulfatwert wurde, trotz der geringen Konzentration vor dem Werk, teilweise um über 300 % überschritten.Die Glanzstoff und mit ihr St. Pölten war bekannt für den schwefeligen Geruch, der an faule Eier erinnerte. Dieser wurde vor allem durch Kohlenstoffdisulfid und Schwefelwasserstoff hervorgerufen. Neben diesen beiden Stoffen wurde auch Schwefeldioxid in hohen Mengen ausgestoßen. Während der Ausstoß von Schwefeldioxid sich immer innerhalb des gesetzlichen Rahmens befand, gab es für Schwefelwasserstoff in Niederösterreich keine Grenzwerte. Wenn man die oberösterreichischen Grenzwerte oder jene der WHO anwendete, so wurden diese mehrfach überschritten.
Thomas Karl u. a.: Die Kunstdenkmäler der Stadt St. Pölten und ihrer eingemeindeten Ortschaften. 1999, ISBN 3-85028-310-0, Eintrag Erste Österreichische Glanzstoffabrik, S. 333–336. 
Franz Mathis: Big Business in Österreich: österreichische Grossunternehmen in Kurzdarstellungen. 1987, ISBN 3-486-53771-7, Eintrag Glanzstoff, S. 122–123. 
Gerhard Stadler: Das industrielle Erbe Niederösterreichs. 2006, ISBN 3-205-77460-4, Kapitel Gemeinde St. Pölten – Glanzstoff, S. 603–606. 
Franz Fiedler: 75 Jahre Erste österreichische Glanzstoff-Fabrik Aktiengesellschaft, Sondernummer der Werkszeitung reyon post, 1979
Frühe Dokumente und Zeitungsartikel zu  Glanzstoff Austria in der Pressemappe 20. Jahrhundert der ZBW – Leibniz-Informationszentrum Wirtschaft.

Die Glasenbachklamm  ist eine Klamm des Klausbachs in Glasenbach, einem Ortsteil der Gemeinde Elsbethen, im Süden der Stadt Salzburg in Österreich.
Sie ist ein beliebtes Wandergebiet für die Bewohner der Stadt und des Umlands. Die Glasenbachklamm ist bekannt für fossile Funde wie zum Beispiel des Fischsauriers, der im Haus der Natur ausgestellt ist. Eine weitere Besonderheit sind die durch den Gebirgsbach freigelegten, 200 Millionen Jahre alten Felsformationen aus der Jurazeit. Damit kann die Entstehungsgeschichte der Alpen vom einstigen Meeresboden bis zum heutigen Gebirge ausschnittsweise betrachtet werden.
Die Glasenbachklamm befindet sich in den nördlichen Ausläufern der Alpen in der Osterhorngruppe. Das Kerbtal der Klamm verläuft etwa drei Kilometer von Osten Richtung Westen.
Die Glasenbachklamm befindet sich etwa 500 Meter südlich der Stadtgrenze von Salzburg. Der Klausbach ist ein rechter Zufluss der Salzach, der seinen Ursprung in etwa 650 Meter Seehöhe vom inzwischen verlandeten Egelsee hat. Der Klausbach entwässert Teile der Osterhorngruppe, wie Gaisberg, Schwarzerberg und Mühlstein Erentrudisalm. Die Erentrudisalm wird bis ins Frühjahr 2019 renoviert, man sucht seit eineinhalb Jahren (Stand Oktober 2018) nach einem Pächter, da es aus wirtschaftlicher Sicht, nicht mehr möglich gewesen ist Sie zu bewirtschaften. Die Glasenbachklamm endet etwa zwei Kilometer bachaufwärts oberhalb der Mündung. Der Eingang zur Glasenbachklamm liegt etwa auf 450 Meter Höhe, deren Ende auf etwa 650 Meter Seehöhe im Weiler Höhenwald Gfalls.
Der Bach, der durch die Glasenbachklamm fließt, heißt Klausbach und war somit nicht namensbestimmend für die Klamm – außerdem ist der Glasbach der Bach nördlich, bei Glas. Glasenbach bezeichnete stattdessen ursprünglich eine kleine Gruppe von Weilern zwischen dem uralten Ort Glas (glasa) mit seinen römischen Siedlungsresten, und dem Klausbach. Aigen – Glas (heute Gemeinde Salzburg Stadt) und Elsbethen – Glasenbach gehörten ursprünglich beide zu Elsbethen, und sind heute weitgehend verwachsen. 
Der Name Klausbach ist wiederum darauf zurückzuführen, dass der durch die Klamm fließende Bach früher wegen der Holztrift mit Klausen versehen wurde. Bis zur Eröffnung der Giselabahn 1871 (Bahnabschnitt zwischen Salzburg und Hallein) wurden jährlich mindestens 12.000 Klafter (= 46.680 Festmeter) Holz getriftet. Dieses Holz wurde zur Befeuerung der Siedeanlage der Saline in Hallein benötigt. Um 1880 wurde jedoch das Holztriften eingestellt, da wegen der neuen Bahnverbindung zur Saline die Holzfeuerung eingestellt und auf Kohlefeuerung umgestellt worden war. Dies war für die Mayr-Melnhof’sche Forstverwaltung, die zu dieser Zeit zahlreiche Grundstücke besaß, der Anlass, im Jahre 1882 einen Saumweg entlang des Klausbachs durch die Glasenbachklamm anzulegen.
Der Klausbach hat seinen Ursprung in etwa 650 Meter Seehöhe und überwindet bis zur Mündung in die Salzach rund 150 Meter Höhenunterschied. Auf der Südseite der Glasenbachklamm gibt es zahlreiche Gräben, während die Nordseite einen steilen Abhang bildet. So münden vom Norden her nur wenige Gerinne in den Klausbach. Von Süden her ist der größte Zufluss der Lettenbach.
Die Klamm ist durch erodierende Eintiefung des Baches nach Ende der Eiszeit ab vor 12000 Jahren entstanden. Der Gestaltungsprozess ist unaufhörlich im Gang.
Der Klausbach war ein Anziehungspunkt für Gewerbebetriebe, die die Wasserkraft zum Antrieb ihrer Maschinen benötigten. Dies hatte zur Folge, dass der Bach von mehreren Betrieben genutzt wurde und dadurch ergaben sich besondere Schwierigkeiten, denn das Wasser musste gerecht zwischen den Nutzern aufgeteilt werden.
Der erste bekannte Betrieb im Klausbach war eine Kugelmühle, die im Jahre 1770 errichtet wurde, aber 1798 durch ein Hochwasser wieder weggerissen wurde. Die späteren Betriebe des Glasenbacher Gewerbeviertels lagen nicht mehr direkt am Klausbach, sondern bezogen das Wasser von einem vom Klausbach abgeleiteten Werkskanal, der auch Pulvermühlbach genannt wurde. Dieser Werkskanal war ca. 1200 m lang, zweigte am Ausgang der Glasenbachklamm vom Klausbach ab und mündete kurz vor der Einmündung des Klausbachs in die Salzach wieder in den Bach zurück. Aus dem Jahr 1800 ist bekannt, dass die Wasserkraft des Werkskanals von zwei Mühlen (Höllmühle und Glasenbachmühle), einer Huf- und Waffenschmiede, einem Sägewerk und zwei Pulvermühlen genutzt wurden. Im Jahre 1830 betrieben die Pulvermacher Löhner und Sinder am Werkskanal je zwei Pulverstampfen, die bis zum Jahr 1883 auf insgesamt sechs Stampfen erweitert wurden. Die erste Holztrift fand dann 1860 statt, da einem Holzknecht bewilligt wurde, Schnittholz, jedoch kein Rundholz (auch Bloche genannt), von Höhenwald bis unterhalb der Höllmühle zu triften. Er musste aber für mögliche Beschädigungen der Glasenbachbrücke, der Salzburg-Halleiner Straße und der Brücke an der Mündung des Klausbachs in die Salzach geradestehen.
Die wirtschaftliche Nutzung des Klausbachs und dessen Nebenkanals wurde um ca. 1954 eingestellt. Gründe dafür waren einerseits die Zerstörung der Wehr, die das Wasser des Klausbachs zurückgestaut hatte, durch ein Hochwasser und andererseits die Elektrifizierung des Sägewerks, welches zu diesem Zeitpunkt der letzte Betrieb war, der noch den Werkskanal nutzte (der Betrieb im Pulvermacherhaus wurde bereits 1918 eingestellt).
Die Glasenbachklamm bietet anschaulich eine Vielzahl an Gesteinen aus den einzelnen Epochen der Erdgeschichte. Diese Gesteine stammen wesentlich aus der Jura- und der Kreidezeit aber auch aus dem Pleistozän (Eiszeit). Die exogenen Kräfte, die diese Klamm geformt haben, wirken bis zum heutigen Tag.
Aber nicht nur die Zeit von 213 Millionen Jahren, sondern auch die Sedimentation hatte seit dem Beginn der Entstehung der Gesteine einen Einfluss. Denn die Ablagerungen fanden nicht im heutigen Gebiet von Salzburg statt, sondern in einem warmen, tropischen Meer, weit im Süden – etwa auf der geographischen Breite des heutigen Nordafrika. Die fossile Flora und Fauna der Gesteinsschichten in der Klamm weisen nämlich auf diese Verhältnisse hin. Zwar war die weltweite Durchschnittstemperatur zur Zeit des Erdmittelalters (Mesozoikum), als die Gesteine in der Glasenbachklamm zur Ablagerung kamen, wesentlich höher als heute, doch der tropische Klimabereich reichte keineswegs bis zu den heutigen Breiten der Klamm (ca. 47,5° N). Konkrete Fakten für einen weit im Süden gelegenen Entstehungsbereich liefern vor allem plattentektonische und paläomagnetische Überlegungen.
Erste Überlegungen über einen südlichen Entstehungsraum begannen, als Alfred Wegener 1912 behauptete, dass die Kontinente nicht starre, unbewegliche Landmassen sind, sondern ähnlich wie Eisschollen auf einem zähflüssigen Untergrund im oberen Teil des Erdmantels driften. Später stellte sich heraus, dass nicht nur die Kontinente ihre Lage verändern konnten, sondern auch ihre Unterlagen aufeinander zukommen und sich voneinander wegbewegen. Heute beträgt diese Geschwindigkeit etwa 3–15 cm pro Jahr, während der Kreidezeit dürften die Bewegungen sogar 25 cm pro Jahr betragen haben. Diese von Harry Hammond Hess aufgestellte Theorie wurde als „Theorie der Plattentektonik“ bekannt. Sie beruht auf jener von Alfred Wegener und der Unterströmungstheorie des österreichischen Geologen Otto Ampferer.
Es gibt noch weitere Belege dafür, dass die Sedimente der Glasenbachklamm weiter im Süden zur Ablagerung kamen, und zwar durch ihren Paläomagnetismus. Die magnetisierbaren Minerale im Sediment oder im flüssigen Magma verhalten sich wie Magnete und stellen sich je nach Richtung des herrschenden Magnetfelds ein. Durch die Verfestigung der Sedimente beziehungsweise der Erstarrung des Magmas wird die Inklination der magnetisierten Minerale „eingefroren“. Da diese Materialien ihre Neigung beibehalten, kann man aus den Ergebnissen der Inklinations-Messung Rückschlüsse auf ihr Entstehungsgebiet ziehen.
Der Großteil der Gesteine bildete sich zu unterschiedlichsten Zeiten in unterschiedlichsten Regionen wie auch unter verschiedenen Klimabedingungen. In der Glasenbachklamm ist es nicht so einfach das zeitliche Hintereinander der Gesteinsentstehung von damals mit dem räumlichen Nebeneinander der heutigen Lage in Einklang zu bringen. Die ältesten aufgeschlossenen Gesteine stammen aus dem ältesten Zeitabschnitt des Mesozoikums, der Trias.
In der Triaszeit, vor ca. 240 Millionen Jahren, breitete sich im Nordteil jenes Ablagerungsbereiches, in dem die Kalkalpen entstanden, ein extrem seichtes Flachmeer aus. Es lag um die 25° bis 30° nördlicher Breite, was der heutigen Lagen von Las Palmas, Kairo und Kuwait entspricht. Ähnliche Verhältnisse wie in diesem einstigen Flachmeer herrschen heute im Golf von Mexiko. In den Wattenflächen dieses Flachmeers bildete sich aus abgelagerten Kalkschichten Dolomit, der sogenannte Hauptdolomit. Gegen Ende der Triaszeit senkte sich der Flachwasserbereich und damit auch die Wassertemperatur, was schließlich zum Absterben der Riffe führte.
Im Jura herrschten niedrigere Temperaturen als im Trias, jedoch noch deutlich wärmer als heute. In Folge änderten sich nicht nur die Ablagerungsverhältnisse gegenüber dem Trias, sonders es kam auch zunehmend zu Bodenunruhen, welche zu Sedimentationsunterbrechungen führten. Weiters führte die tektonische Unruhe zu Sedimenteingleitungen. Beim Zerfall dieser Gleitmassen kam es zur Brekzienbildung (vgl. Knollenbrekzie). In der Glasenbachklamm erreichen diese Gesteine eine Höhe von bis zu 15 m.
Der Übergang vom Jura zur Kreide vor 145 Millionen Jahren ging meist ohne gesteinsmäßigen Änderungen vorüber. In der höheren Unterkreide kam es zu den ersten Aufwölbungen und Faltungen im Zuge der ersten großen alpidischen Gebirgsbildungsphase, die sich in der Mittelkreide vor etwa 120 Millionen Jahren ereignete und vor 88 Millionen Jahren ihren Höhenpunkt erreichte. Mit Beginn der Oberkreide setzte die Bildung der Gosauablagerungen ein. Tektonisch gesehen war die Zeit der Gosauablagerungen durch starke Bodenunruhen gekennzeichnet, die im Auftreten mächtiger Konglomerate zum Ausdruck kommen. Mit jeder Phase der Alpenauffaltung kam es zur verstärkten Abtragung. Wildbäche transportierten den Schutt in die Flachmeere und bauten mächtige Schotterkörper auf, die heute als Konglomerat bestehen.
In der Glasenbachklamm sind keine Ablagerungen mit Sicherheit aus dem Tertiär bekannt. Somit ist ein Zeitraum von 63 Millionen Jahren gesteinsmäßig nicht belegt.
Das Quartär ist mit 1,64 Millionen Jahren der jüngste Abschnitt der Erdgeschichte. Das Känozoikum begann vor 1,64 Millionen Jahren mit der Eiszeit (Pleistozän). Der jüngste Abschnitt der Erdneuzeit, das Holozän, begann mit dem Abschmelzen der großen Gletschermassen. Die Bildung des Kerbtals des Klausbachs ist auf das Eiszeitalter mit seinen wechselnden Kalt- und Warmzeiten zurückzuführen, die vor 2,6 Millionen Jahren begann. In den von den Gletschermassen vertieften Haupttälern der Alpen schütteten die Seitenbäche mächtige Schwemmkegel auf. Am Fuß der steilen Trogwände bildeten sich Schutthalden, die im Laufe der Zeit erodierten, so auch im früher längeren Tal des Klausbachs.
Die Glasenbachklamm ist nicht nur in geologischer Hinsicht ein beliebtes Wanderziel der Salzburger und den Bewohnern aus dem Umland, sondern auch von der Vielfalt der Pflanzen die in der Klamm wachsen. Wegen der geologischen und biologischen Besonderheit wurde dieses Gebiet 1987 zum „Geschützten Landschaftsteil“ erklärt.
Das tief eingeschnittene Tal mit einer gering entwickelten Talsohle bietet für die Pflanzen andere ökologische Bedingungen als für jene der Umgebung. Die Schlucht birgt im Gegensatz zu den darüber liegenden Hängen eine Vielfalt an Laubbäumen. Die Buche (Fagus sylvatica) dringt eigentlich nur untergeordnet in tiefere Schluchten ein, hier gedeihen vor allem Berg-Ahorn (Acer pseudoplatanus), Spitzahorn (Acer platanoides), Esche (Fraxinus excelsior), Grau-Erle (Alnus incana) und Bergulme (Ulmus glabra). Dieser Baumbestand wird wesentlich auch durch die gegebenen Klima-Faktoren geprägt. Die Ost-West-Lage lässt wenig Sonnenschein in die Klamm, wodurch Schattenpflanzen gefördert werden. Die hohe Luftfeuchtigkeit, die durch die Schattenlage und die Verdunstung des Klausbachs entsteht, führt auch an heißen Tagen zu einer kühlen Temperatur.
In den flacheren Bereichen des Unterhanges reichern sich mit den Oberflächenwässern Nährstoffe und Humus an. Dadurch entsteht eine üppige Auenvegetation mit Sträuchern und eine Vielzahl von Farnen, vor allem der dornige Schildfarn (Polystichum aculeatum), der Frauenfarn (Athyrium filix-femina) und die sonst seltene, hier aber üppig vorkommende Hirschzunge (Phyllitis scolopendrium). Der häufigste Vertreter der Sträucher ist die Heckenkirsche (Lonicera xylosteum). Epiphyten wie der Efeu (Hedera helix) und Hochstauden wie der Geißbart (Arnuncus dioicus), das Christophskraut (Actaea spicata), der gelb blühende Wolfs-Eisenhut (Aconitum vulparia) sind ebenfalls in diesem Gebiet anzufinden. Auch für die Brennnessel (Urtica dioica), den Giersch (Aegopodium podagraria) und das einjährige Kleinblütige Springkraut (Impatiens noli-tangere) ist das gute Nährstoffangebot mit gleich bleibend hoher Feuchtigkeit wichtig.
Besonders üppig sind die zahlreichen Moosarten, die auf der Borke von Bäumen, auf Felsen und auch auf dem Waldboden gedeihen. Auf den Wegmauern und Steinen entlang des Baches kommt neben den Moosen auch der Braune Streifenfarn (Asplenium trichomanes) vor.
Der feuchte Schluchtwald bietet auch für eine Vielzahl von Pilzarten einen Lebensraum. Die eingeschränkte Holzbewirtschaftung, ein teilweise hohes Baumalter, das zahlreich vorhandene Totholz, die naturnahe bis natürliche Baumartenzusammensetzung und ein außerordentlich feuchtes, kühles Schluchtwaldklima werden von holzbewohnenden Pilzarten bevorzugt.
Die feuchten Felswände und die am Boden liegenden Steine sind mit verschiedenen gefärbten Krustenflechten überzogen, die nur wenig Licht benötigen. Auch auf der Borke von Bäumen kommen vereinzelt hell-bräunlich grüne Flecken vor, dieses sind epiphytische Krustenflechten die ihre Lagerzellen direkt in der Borke ausbilden.
Neben der großen Vielfalt der Flora ist die Glasenbachklamm Rückzugsgebiet und ökologisch vielgliedriger Lebensraum für eine bemerkenswerte Tierwelt. Insekten (wie Steinfliegen, Eintagsfliegen, Köcherfliegen, Kriebelmücken), deren Larven im Bach zu finden sind, bilden die Nahrungsbasis für Vögel und Fische.
Die Sauberkeit des Wassers ist ausschlaggebend für das Vorkommen der Bachforelle (Salmo trutta). Auch die Koppe (Cottus gobio) lebt im Bach.
Auch verschiedene Käfer, Schmetterlinge (Aurorafalter, Zitronenfalter), Schnecken oder der feuchtigkeitsliebende Feuersalamander (Salamandra salamandra) sind sehr zahlreich in der Klamm vertreten.
An Säugetieren im Gebiet der Glasenbachklamm sind Dachs (Meles meles), Fuchs (Vulpes vulpes), Baummarder (Martes martes) und Alpenspitzmaus (Sorex alpinus) zu erwähnen.
Die Wasseramsel (Cinclus cinclus) und die Gebirgsstelze (Motacilla cinerea) liebt den hier vorhandenen Lebensraum Gebirgsbach. Auch eine Vielfalt an Singvögeln kann die Glasenbachklamm vorweisen, wie z. B. die Singdrossel, Blau-, Kohl-, Tannen-, Hauben- und Sumpfmeise, Rotkehlchen, Kleiber, Buchfink u. a.
Bereits 1897 wurden erstmals Überreste von einem Ichthyosaurus beschrieben. Spätere Grabungen von 1960 bis 1978 erbrachten noch eine Reihe weiterer Zähne, Wirbelkörper und Rippen, welche heute im Haus der Natur in Salzburg ausgestellt sind.
Eingleitungen in jüngeren Schichten werden dadurch offensichtlich, dass sowohl im Liegenden (untere Schichten) wie auch im Hangenden (obere Schichten) Ammoniten gefunden wurden. Weiters wurde auch unterhalb der Saurierfundstelle eine Reihe von Exemplaren von Ammoniten (Echioceras raricostatum) aus dem Lias gefunden, die jetzt im Haus der Natur aufbewahrt werden.
Die relativ hohe Planktonproduktivität im Meerwasser und der mangelnde Sauerstoffgehalt am Meeresgrund bedingten auch den hohen Nährstoffgehalt der Scheibelbergschichten.
Heute können immer noch viele Fossilien geborgen werden, meist nach der Winterschneeschmelze oder nach einem Hangrutsch in der Klamm. Die dadurch freigelegten Schotterhänge bringen eine große Anzahl von Fossilien ans Tageslicht. So wurde 2002 bei einem gewaltigen Hangrutsch (200 m breit und 100 m lang) in der Nähe der Saurierfundstelle eine Vielzahl von Fossilien freigelegt, wie z. B. Ammoniten, Muscheln und Schnecken.
Der Eingang befindet sich in Glasenbach, etwas südlich der Stadt Salzburg. Die Glasenbachklamm kann problemlos mit der Oberleitungsbuslinie 7 von der Stadt her erreicht werden. Nach etwa zehn Minuten Gehzeit erreicht man den Eingang der Glasenbachklamm und damit auch die erste von mehreren Schautafeln in der Klamm. Nach etwa 3 km erreicht man das Ende der Klamm. Von dort kann man sich entweder zum Gasthaus Ramsau begeben oder man geht Richtung Gaisberg weiter.
Gottfried Tichy, Judith Herbst: Glasenbachklamm. Naturkundlich-geologischer Führer. Herausgeber: ÖNB & OeAV, 1997, ISBN 3-901866-00-0

Der Glatte Hammerhai (Sphyrna zygaena) gehört zur Familie der Hammerhaie (Sphyrnidae). Er ist nach dem Großen Hammerhai (Sphyrna mokarran) mit einer maximalen Länge zwischen 3,70 und 4,00 Metern der zweitgrößte Hai innerhalb dieser Familie. Anders als andere Hammerhaie bevorzugt diese Art vor allem gemäßigte Wassertemperaturen, wodurch er deutlich weiter nördlich anzutreffen ist als seine Verwandten. Im Sommer wandern die Tiere polwärts in kältere Wassergebiete, wobei sie Schulen von Hunderten oder sogar Tausenden von Tieren bilden können.
Der Glatte Hammerhai ist ein aktiver Jäger und ernährt sich von einer Vielzahl von Knochen- und Knorpelfischen einschließlich kleiner Haie und Vertretern der eigenen Art sowie wirbellosen Tieren wie Kopffüßern oder Krebstieren. Wie alle anderen großen Hammerhaie wird diese Art als dem Menschen potenziell gefährlich eingestuft. Haiunfälle mit Hammerhaien sind allerdings sehr selten dokumentiert und aufgrund seiner Vorliebe für kühlere Gewässer ist dieser Hai wahrscheinlich für sehr wenige Unfälle verantwortlich. Wegen seiner großen Flossen und seiner Haut wird er kommerziell bejagt und ist von der International Union for Conservation of Nature (IUCN) als „gefährdet“ (vulnerable) eingestuft.
Der Glatte Hammerhai stellt nach dem Großen Hammerhai die größte Art der Hammerhaie dar. Die durchschnittliche Größe liegt zwischen 2,50 und 3,50 Meter, es gibt aber auch größere Exemplare mit einer Körperlänge bis zu 3,70 und 4,00 Meter (nach anderen Quellen bis 5,00 Meter) und einem Maximalgewicht von 400 Kilogramm. Die Rückenfärbung ist dunkel-braungrau bis olivgrün und wird an den Flanken heller. Der Bauch ist weiß und die Brustflossen können auf der Unterseite schwarze Ränder oder Spitzen haben.
Von anderen Hammerhaien lässt er sich durch die Form seines verbreiterten Kopfes (Cephalofoil) unterscheiden, der eine gerundete Vorderseite ohne Einbuchtung in der Kopfmitte hat. Das Cephalofoil hat eine Breite von 26 bis 29 % der Körperlänge. Die Nasenlöcher befinden sich nahe den Enden des Cephalofoil und besitzen lange Gruben, die zur Mitte des Kopfes reichen. Im Oberkiefer befinden sich 26 bis 32 und im Unterkiefer 25 bis 30 Zähne. Wie bei anderen Haien liegen hinter diesen Zähnen weitere Zahnreihen. Die Zähne sind dreieckig mit glatten oder leicht gezähnten Schneidkanten.Der Körper ist stromlinienförmig mit einem Interdorsalkamm zwischen den beiden Rückenflossen. Wie alle Arten der Familie besitzt auch dieser Hammerhai 5 Kiemenspalten, ein Saugloch fehlt. Er hat eine vergleichsweise große und sichelförmige erste Rückenflosse mit abgerundeter Spitze, die auf Höhe des Innenrandes der Brustflossen ansetzt. Die Afterflosse ist größer als die zweite Rückenflosse und besitzt ein langes, freies Ende. Die Brust- und Bauchflossen sind nicht sichelförmig und haben stattdessen eine gerade ausgebildete Flossenkante, wodurch sie sich von denen des Großen Hammerhais unterscheiden. Die Hautzähne (Placoidschuppen) sind sehr eng beieinanderstehend und besitzen bei ausgewachsenen Exemplaren 5 bis 7 (bei Jungtieren 3) horizontale Leisten und eine W-förmige Hinterkante.
Im Gegensatz zu allen anderen Hammerhaien ist der Glatte Hammerhai verhältnismäßig tolerant gegenüber kühlerem Wasser der gemäßigten Zonen, wodurch er nördlicher als alle anderen Arten anzutreffen ist. Er findet sich in gemäßigten und subtropischen Gewässern des Atlantischen Ozeans, des Pazifischen Ozeans und des Indischen Ozeans vorwiegend in Küstennähe der Festlandsockel. Dabei ist er im westlichen Atlantik von Nova Scotia, Kanada, bis zu den Virgin Islands und von Brasilien bis zum südlichen Argentinien anzutreffen, im östlichen Atlantik reicht das Gebiet von den Britischen Inseln bis Côte d'Ivoire, einschließlich des Mittelmeeres. Im Indischen Ozean kommt er an den Küsten Südafrikas, Indiens und Sri Lankas vor. Im westlichen Pazifik reicht das Verbreitungsgebiet vom Golf von Tonkin bis Süd-Japan und Sibirien, außerdem ist er für die Küstengebiete des südlichen Australiens und Neuseelands nachgewiesen. Im Zentral- und Ost-Pazifik lebt er um Hawaii sowie von Kalifornien über Panama, die Galapagosinseln und Ecuador bis Chile. Gemeinhin wird angenommen, dass die Art die tropischen Gewässer meidet, allerdings gibt es einige Sichtungen aus tropischen Gewässern im Golf von Mannar vor Indien sowie vor dem südlichen Mosambik. Diese Vorkommen sind schwer zu bestätigen und es könnte sich um Verwechslungen mit anderen Arten handeln.Verglichen mit dem Großen und dem Bogenstirn-Hammerhai lebt der Glatte Hammerhai näher an der Wasseroberfläche in Tiefen, die in der Regel 20 Meter nicht überschreiten. Allerdings wurden Einzeltiere in Tiefen bis 200 Meter registriert. Er bevorzugt Küstengewässer im Bereich von Buchten und Ästuaren, kommt allerdings auch weiter im offenen Ozean über dem Kontinentalschelf und im Bereich von Inselgruppen vor. Der Hai verträgt Brackwasser und wurde auch bereits beobachtet, wie er in Süßwasserflüsse wie den Indian River in Florida eindringt. Im Sommer wandern die Tiere polwärts in kältere Gewässer und kehren im Winter zurück in Richtung des Äquators.
Geschlechtsreife Glatte Hammerhaie sind meistens Einzelgänger oder bilden kleine Gruppen. Während ihrer jährlichen Wanderungen kommen sie teilweise in sehr großen Individuenzahlen zusammen, wobei Schulen mit mehr als Hundert Individuen vor dem Ostkap Südafrikas sowie Schulen mit mehreren Tausend Individuen vor der Küste Kaliforniens beobachtet wurden. Vor allem im Sommer und bei warmem Wetter können sie als Oberflächenschwimmer mit aus dem Wasser ragender Rückenflosse beobachtet werden.Die jungen Haie können von anderen, größeren Haien wie dem Schwarzhai (Carcharhinus obscurus) sowie von Individuen der eigenen Art erbeutet werden. Große Hammerhaie können Beutetiere des Großen Schwertwals (Orcinus orca) sein, wie Beobachtungen aus Neuseeland belegen. Zu den bekannten Parasiten des Glatten Hammerhais gehören die Fadenwürmer Parascarophis sphyrnae und Contracaecum spp.
Der Glatte Hammerhai ist ein aktiver Jäger, der sich von Knochenfischen, Rochen, anderen Haien einschließlich kleinen Vertretern der eigenen Art, Kopffüßern sowie zu geringeren Anteilen von Krebstieren wie Garnelen, Krabben und Seepocken ernährt. Außerdem fressen sie Aas und auch Köder von Fischerleinen. In einigen Regionen sind wie bei dem Großen Hammerhai Stachelrochen die bevorzugte Beute und stellen einen großen Anteil der Beutetiere. Die Giftstacheln dieser Rochen finden sich entsprechend häufig im und um das Maul der Haie, ein gefangenes Exemplar des Großen Hammerhais hatte insgesamt 95 Stachel im Mundraum stecken. In Nordeuropa ernährt sich der Glatte Hammerhai vor allem von Heringen und Wolfsbarschen, während er in Nordamerika vor allem Makrelen und Menhaden erbeutet. Vor Südafrika lebt er vor allem von Kopffüßern wie Loligo vulgaris sowie von kleinen Schwarmfischen wie Sardinen im Bereich der Korallenriffe am Rand des Kontinentalschelfs, wobei größere Individuen zunehmend kleine Haie und Rochen erbeuten. Auch vor Australien sind Kopffüßer die wichtigste Nahrungsquelle, gefolgt von kleinen Knochenfischen.
Wie alle Hammerhaie ist diese Art lebendgebärend (ovovivipar), wobei die ungeborenen Junghaie über eine Dottersack-Plazenta ernährt werden. Dabei wird der Dottersack, nachdem der Dottervorrat von den Junghaien verbraucht wurde, in eine Plazenta umgebildet, die der der Säugetiere analog ist und im Laufe der weiteren Entwicklung die Ernährung über den mütterlichen Blutkreislauf sicherstellt. Die Weibchen bekommen nach einer Tragzeit von 10 bis 11 Monaten zwischen 29 und 37 Jungtiere mit einer Größe von ungefähr 50 bis 61 Zentimetern. Die Geburt findet in flachen Buchten wie der Bulls Bay in North Carolina statt, die als „Geburtsstationen“ dienen.Die Weibchen erreichen ihre Geschlechtsreife mit einer Körperlänge von etwa 2,70 Metern, die Männchen regional unterschiedlich mit 2,10 bis 2,50 Metern. Vor Südafrika wurden gerade befruchtete Weibchen im Februar und Weibchen mit fast ausgewachsenen Embryonen im November gefangen; vor Australien bekommen die Weibchen ihre Jungtiere zwischen Januar und März, zur gleichen Zeit findet dort auch der Eisprung statt. Die Lebensdauer der Haie beträgt mehr als 20 Jahre.
Der Glatte Hammerhai wurde erstmals 1758 von dem schwedischen Naturforscher Carl von Linné in dessen bekannter 10. Auflage der Systema naturae als Squalus zygaena beschrieben. Dabei ordnete Linné dieser Art keinen Typus als Referenzindividuum zu. Der Name wurde später in den heute gültigen Namen Sphyrna zygaena geändert. Die Artbezeichnung zygaena leitet sich dabei vom griechischen Wort zygon (griechisch ζυγόν) ab, dass das Joch eines Zugtiers bezeichnet und sich auf die Kopfform des Hais bezieht.Insgesamt werden in der Gattung Sphyrna acht Arten geführt, die gemeinsam mit der nur durch den Flügelkopf-Hammerhai (Eusphyra blochii) gebildeten Gattung Eusphyrna die Familie der Hammerhaie (Spyrnidae) bilden. Aufgrund phylogenetischer Untersuchungen auf der Basis von morphologischen sowie molekularbiologischen Merkmalen (Isoenzyme und mitochondriale DNA) konnte nachgewiesen werden, dass der Glatte Hammerhai die Schwesterart des Großen Hammerhais (Sphyrna mokarran) und des Bogenstirn-Hammerhais (Sphyrna lewini) darstellt und mit diesen ein Taxon bildet, das den anderen Arten der Gattung Spyrna gegenübergestellt wird. Die Position der großen Arten mit besonders ausladendem Cephalofoil konnte auch durch weitere Untersuchungen im Jahr 2010 bestätigt werden, wobei sich die Ergebnisse bezüglich der Verwandtschaftsverhältnisse der Arten untereinander leicht von den Ergebnissen von 2007 unterscheiden: Hier bilden nur der Große und der Glatte Hammerhai ein gemeinsames Taxon, während der Bogenstirn-Hammerhai als basale Art der Gruppe aller anderen Hammerhaie zugeordnet wird.Die phylogenetische Position der großen Arten zwischen dem Flügelkopf-Hammerhai (Eusphyrna blochii) und den kleineren Hammerhaien mit einem deutlich schmaleren Kopf deutet darauf hin, dass innerhalb der Hammerhaie ein großes Cephalofoil den ursprünglichen Zustand darstellt und der schmalere Kopf von diesem abgeleitet ist. Damit verbunden ist eine Funktionsveränderung des Cephalofoil, der sich in der Lebensweise der Hammerhaie widerspiegelt: Während ein breites Cephalofoil vor allem bei freischwimmenden Arten des Pelagials vorkommt und hier vor allem die Rolle als Tragflügel wahrnimmt, leben die kleineren Arten mit kleinerem Cephalofoil vor allem in Bodennähe sowie in schlammigen Küstengebieten und nutzen die Ausstattung der Sinnesorgane, insbesondere der Lorenzinischen Ampullen, zur Lokalisierung von Beutetieren. Bezüglich der Körpergröße schließen Lim et al. 2010 aufgrund ihrer Verwandtschaftshypothese und der Verbreitung der Arten, dass die ursprünglichsten Hammerhaie große Arten waren, von denen sich sowohl der kleine Flügelkopf-Hammerhai als auch die kleineren Sphyrna-Arten ableiten.
Aufgrund seiner Größe wird der Glatte Hammerhai − wie alle anderen großen Hammerhaie, insbesondere der Große und der Bogenstirn-Hammerhai − als dem Menschen potenziell gefährlich eingestuft. Bis 2009 verzeichnete das International Shark Attack File 34 Angriffe von Hammerhaien der Gattung Sphyrna auf Menschen, 17 davon unprovoziert, von denen einer tödlich war. Aufgrund der Schwierigkeit, die Haie zu unterscheiden, ist unklar, wie viele dieser Angriffe durch Glatte Hammerhaie erfolgten, die sichere Zuordnung erfolgte nur bei einem unprovozierten Unfall. Da der Glatte Hammerhai allerdings vor allem in den Gewässern der gemäßigten Zonen lebt, in denen sich Menschen seltener im Meer aufhalten, sind wahrscheinlich nur wenige Angriffe auf ihn zurückzuführen. Vor der südlichen Küste Kaliforniens wurde dokumentiert, dass der Hai Fänge von Sportfischern und Tauchern stiehlt.Wie der Große Hammerhai wird auch der Glatte Hammerhai weltweit befischt, vor allem vor den Küsten der Vereinigten Staaten (Ost- und Westküste), Brasiliens, Spaniens, Taiwans, den Philippinen, des südwestlichen Australiens und West-Afrikas. Dabei werden vor allem Fangnetze und Langleinen eingesetzt. Die Fangquoten lassen sich nur schwer erfassen, da in der Regel keine Unterscheidung zwischen den einzelnen Arten der Hammerhaie stattfindet. Das Fleisch wird frisch getrocknet und gesalzen sowie geräuchert, ist jedoch in der Regel wenig beliebt und wird entsprechend selten genutzt. Vor allem die Flossen haben einen hohen Wert für den asiatischen Markt, wo sie gemeinsam mit denen anderer großer Haie zu Haifischflossensuppe verarbeitet werden; dies führt dazu, dass den gefangenen Haien häufig nur die Flossen abgeschnitten und die verletzten Tiere dann wieder ins Meer geworfen werden (Shark-Finning). Zudem wird ihre Haut zu Haileder verarbeitet, das aus ihrer Leber gewonnene Leberöl dient der Gewinnung von Vitaminen und ihre Kadaver werden der Fischmehlproduktion zugeführt. Auch in der Traditionellen Chinesischen Medizin wird der Hai genutzt.Daneben wird der Glatte Hammerhai wie andere Haie auch unabsichtlich als Beifang gefangen und getötet. Eine weitere vom Menschen verursachte Todesursache stellen Hainetze dar, die zum Schutz von Stränden gespannt werden und in denen sich viele Haie verfangen. Dabei verfingen sich in den Netzen vor KwaZulu-Natal, Südafrika, zwischen 1978 und 1990 jährlich weniger als 10 Glatte Hammerhai während sie in den Netzen vor New South Wales, Australien, 50 % der gefangenen 4.715 Haie ausmachen, die zwischen 1972 und 1990 gefangen wurden. Aktuell wird der Hai von der International Union for Conservation of Nature and Natural Resources (IUCN) global als „gefährdet“ („vulnerable“) eingestuft. In den Gewässern Neuseelands gehört die Art zu den verbotenen Zielarten der Fischerei und sie ist der häufigste Hai an der Nordwestküste. Darüber hinaus scheint sich die Fischerei vor der Küste von Süd-Australien nicht negativ auf die Bestände ausgewirkt zu haben. Vor der Ostküste der Vereinigten Staaten sind die Fänge durch den Atlantic shark Fishery Management Plan (FMP) des National Marine Fisheries Service (NMFS) reglementiert, wo diese Art als „Large Coastal Shark“ (LCS) klassifiziert ist.Im März 2013 wurde auf der Artenschutzkonferenz der CITES in Bangkok eine Regulierung des Handels mit Glatten Hammerhaien beschlossen, die Regelung trat am 14. September 2014 in Kraft.
Leonard J. V. Compagno: Sharks of the world. An annotated and illustrated catalogue of shark species known to date. Part 2. Carcharhiniformes. FAO Species Catalogue for Fishery Purposes. Band 4, FAO, Rom 1984; S. 545–554 ISBN 92-5-101383-7. (Familie Sphyrnidae Gill, 1872, Artporträt)
Leonard Compagno, Marc Dando, Sarah Fowler: Sharks of the World. Princeton University Press, Princeton 2005; S. 326. ISBN 0-691-12072-2.
Alessandro de Maddalena, Harald Bänsch: Haie im Mittelmeer. Franckh-Kosmos, Stuttgart 2005; S. 221–223. ISBN 3-440-10458-3.

Die Glattwale (Balaenidae) sind eine Familie aus der Ordnung der Wale (Cetacea) mit derzeit vier Arten. Sie leben in den nördlichen und südlichen Meeren und ernähren sich von Plankton. Aufgrund verschiedener Merkmale grenzen sie sich deutlich von den anderen Bartenwalen (Mysticeti) ab. Die etwa 15 bis 20 Meter großen Tiere wurden von allen Walen durch die Bejagung am stärksten dezimiert. Früher wurden sie im Deutschen häufig als Walfisch oder auch gemeiner Walfisch bezeichnet.
Glattwale sind eher plump gebaut. Ihnen fehlt eine Rückenfinne. Die Flipper sind recht kurz, jedoch kräftig ausgebildet.
Wesentliche Unterschiede zu den nahe verwandten Furchenwalen finden sich beim Bau des Kopfes. Der Oberkiefer (Rostrum) ist stark gewölbt, der Kieferknochen ist sehr viel schmaler als der von Furchenwalen. Der Kopf ist proportional deutlich größer als bei Furchenwalen. Beim Grönlandwal kann der Kopf 40 % der Körperlänge ausmachen, also etwa acht Meter bei zwanzig Metern Körperlänge. Kennzeichnend sind auch die langen, dünnen, schwarzen Barten, die bei den Nordkapern und dem Südkaper etwa 2,5 Meter und beim Grönlandwal etwa 4 Meter lang sind. Glattwalen fehlen die Furchen an der Kehle, die es den Furchenwalen erlauben, ihr Maul weit auszudehnen. Alle sieben Halswirbel sind miteinander verschmolzen. Des Weiteren kennzeichnet Glattwale eine extrem dicke Speckschicht, welche in der Dicke die anderer Wale deutlich übertrifft. Als Bewohner meist sehr kalter Meere bis hin zum Südpolarmeer bildeten die Glattwale den dicken Blubber als Isolation gegen das kalte Wasser.
Nordkaper und Südkaper unterscheiden sich von anderen Walarten und auch vom verwandtschaftlich nahestehenden Grönlandwal durch ihre auffälligen Hautwucherungen im Kopfbereich. Vor allem Ober- und Unterkiefer sowie der Augenbereich sind davon betroffen. Sie werden von Seepocken und Walläusen (Cyamus) besiedelt. Zur Bezeichnung des Bewuchses an verschiedenen Hautwucherungen sind einige Begriffe gebräuchlich:
Auffällig ist, dass die „Mützen“ bei Bullen ausgeprägter sind als bei Weibchen. Warum dies so ist, ist unklar. Eventuell spielen die Mützen beim Balz- und Paarungsverhalten eine Rolle. Für Wissenschaftler hilfreich ist die Ausbildung der Mütze zur Identifizierung von Einzelindividuen. Durch die individuelle Ausbildung der Mützen konnte in der Zwischenzeit fast der gesamte Nord- und Südkaperbestand katalogisiert werden. Dies soll zur Erforschung des Alters, des Lebenszyklus und der Wanderrouten dienen.
Glattwale gehören zu den größten Säugetieren; im Vergleich zu anderen Bartenwalen sind sie jedoch nur mittelgroß. Nordkaper und Südkaper sind etwa gleich groß und gleich schwer; sie werden meist etwa 15 Meter lang, maximal kann die Körperlänge 18 Meter erreichen. Ihr Gewicht liegt zwischen 50 und 56 Tonnen. Mit im Mittel 17 Metern wird der Grönlandwal noch größer. Die Länge äußerst großer Exemplare dieser Art kann 20 Meter überschreiten. Solche Exemplare, die jedoch nur mehr ganz selten festgestellt werden, wiegen dann an die 80 Tonnen. Normalerweise beträgt das Durchschnittsgewicht dieser Wale rund 65 Tonnen.
Glattwale leben bevorzugt in kalten Meeren, auf ihren Wanderungen erreichen sie jedoch auch warme Meeresteile in den Subtropen. Fast alle Gewässer um die Arktis werden von Glattwalen bewohnt, ebenso wie Großteile des Nordatlantiks und des Nordpazifiks. Hier leben der Atlantische Nordkaper, der Pazifische Nordkaper und der Grönlandwal. Die drei genannten Walarten leben auch im Ochotskischen Meer und an vielen Küsten Ostasiens, etwa um ganz Japan. Sie fehlen jedoch fast vollständig an der Nordküste Russlands. Die südliche Hemisphäre wird vom Südkaper bewohnt, der alle südlichen Meere bis auf die Küsten der Antarktis bewohnt. Die Südküste Australiens, Teile der Küste Südamerikas und die Südküste Afrikas gehören zum von ihm bewohnten Areal. Trotz großer Verbreitung ist die Dichte des Bestandes in diesem Gebiet gering.
Die Sozialstruktur der Glattwale ist weitgehend unerforscht. Vermutlich leben Glattwale einzelgängerisch oder in Kleingruppen von drei bis vier Exemplaren. Es wurde beobachtet, dass Glattwale häufig aus dem Wasser springen und mit der Fluke auf die Wasseroberfläche schlagen. Diese lauten Geräusche dienen wohl dazu, anderen Glattwalen die eigene Position mitzuteilen.
Am häufigsten werden von Glattwalen verschiedene Arten der Ruderfußkrebse verzehrt, doch je nach Art scheint das Nahrungsspektrum zu variieren. Die beiden Nordkaperarten fressen neben Ruderfußkrebsen auch jungen Krill und die schwarmbildenden Larven vieler Arten von Zooplankton. Der Südkaper verzehrt neben seiner gewöhnlichen Nahrung auch Krill aller Entwicklungsstadien. Der Nahrungsbedarf beträgt für einen einzelnen adulten Glattwal etwa 1000 bis 2500 Kilogramm pro Tag.
Zum Nahrungserwerb schwimmen Glattwale einfach mit geöffnetem Maul durch Schwärme von Zooplankton. Im Gegensatz dazu fressen Furchenwale „schluckweise“; sie ziehen das Wasser in ihr Maul und drücken es dann wieder heraus, wobei das Zooplankton an den Barten hängenbleibt. Die Tauchgänge der Glattwale zur Nahrungsaufnahme dauern meist ungefähr 8 bis 12 Minuten, seltener fangen sie ihre Beute an der Oberfläche. Dies tun sie nur, wenn dort eine höhere Ausbeute zu erwarten ist. Die beiden Nordkaperarten halten gelegentlich mit einigen Mitgliedern ihrer Gruppe bei der Nahrungssuche Körperkontakt, der Grund hierfür ist noch nicht sicher festgestellt.
Bis jetzt ist der Walgesang von Glattwalen nur unzureichend erforscht; das Repertoire aller Rufe ist wohl wesentlich größer als bisher bekannt. Es gibt mindestens zwei verschiedene Laute: Kontaktrufe, mit denen die Tiere über weite Entfernungen kommunizieren, und Rufe, mit denen die Kühe Bullen anlocken. Buckelwale erzeugen zusammenhängende, wiederholte Lautfolgen, die man als „Lieder“ bezeichnen könnte, wohingegen Glattwale schnell aufeinander folgende tiefe Einzeltöne erzeugen, deren Frequenz sich im Bereich von 50 bis 500 Hertz bewegt. Ein Ruf, dessen Zweck noch nicht bekannt ist, wird beim Beutefang ausgestoßen: Dieser hört sich wie ein Gluckern an und liegt im Frequenzbereich von 2 bis 4 Kilohertz. Grönlandwale produzieren einfachere Laute als der Südkaper und die beiden Arten der Nordkaper.
Da beim langen Reproduktionszyklus der Glattwale meist zwei bis drei Jahre zwischen zwei Würfen liegen, sind oft weniger als ein Drittel aller Weibchen in einer Region empfängnisbereit. Dieses Drittel lockt die Bullen mittels akustischer Signale an. Die Paarung findet meist im Zeitraum Spätherbst bis Frühling der jeweiligen Hemisphäre statt. Wenn der Bulle ankommt, versucht das Weibchen die Begattung hinauszuzögern, indem es wegschwimmt oder auf dem Rücken schwimmt und seine Flipper ausstreckt. Die Werbung der Bullen besteht darin, die Nebenbuhler wegzudrängen. Eventuell spielt auch die Mütze eine Rolle, hierzu ist jedoch nichts Näheres bekannt. Die Weibchen werden von verschiedenen Männchen befruchtet, diese führen wohl eine Spermakonkurrenz. Hierauf deutet auch hin, dass die Hoden von Glattwalen, speziell dem Südkaper, mit teils über 800 Kilogramm die schwersten weltweit sind. Auch haben sie unter den Bartenwalen die im Verhältnis zur Körpergröße größten Hoden.
Nach einer Tragzeit von 10 bis 13 Monaten, maximal 17 Monaten, kalbt das Weibchen und bringt ein einzelnes, 4 bis 5 Meter langes Jungtier zur Welt. Selten entfernt sich das Kalb etwas vom Weibchen, so gut wie nie mehr als eine Körperlänge. Die schnellwachsenden Kälber werden nach etwa einem Jahr entwöhnt und sind dann meist bereits 8 bis 9 Meter lang. Die Geschlechtsreife tritt im Alter von etwa 9 Jahren ein, doch sehr frühreife Weibchen kalben manchmal im Alter von sechs Jahren. Glattwale können sehr alt werden, ein Weibchen wird seit mindestens 1935 regelmäßig gesichtet und ist wohl weit über 80 Jahre alt. Anhand der Jahresringe der knöchernen Ohrkapsel konnte das Alter des ältesten bekannt gewordenen Exemplars, eines Männchens der Grönlandwale, auf 211 Jahre zum Zeitpunkt seines Todes bestimmt werden. Damit wäre es älter geworden als viele der für ihr Alter bekannten Riesenschildkröten wie Harriet (176 Jahre) und Tu'i Malila (188 oder 192). Die genaue Bestimmung des Alters der Schildkröte Adwaita steht noch aus, falls das Alter von 256 Jahren nicht bestätigt wird, ist dieser Grönlandwal das wohl am ältesten gewordene bekannte Wirbeltier.
Die Sterblichkeitsrate von Kälbern der Glattwale liegt wohl bei 17 % im ersten Lebensjahr, 3 % der Kälber sterben in den folgenden 3 Jahren. Danach ist die Sterblichkeitsrate extrem niedrig.
Momentan sind die Kenntnisse über die Wanderwege von Glattwalen noch recht lückenhaft. Nach Angaben von Eskimos ziehen sie nach Alter und Geschlecht getrennt.
Die für den Winter wichtigsten Wurfgebiete für Glattwale im Atlantik der nördlichen Hemisphäre liegen an den Küsten der US-Bundesstaaten Georgia und Florida. Wo die restlichen Exemplare kalben, ist nicht klar. Zwei Drittel des gesamten Bestandes von Nordkapern wird im Frühling, Sommer und Herbst oft am Golf von Maine gesichtet. Analysen von verschiedenen Daten und Beobachtungen lassen vermuten, dass es ein zweites, noch nicht identifiziertes Sommergebiet gibt.
Am besten erforscht ist bis jetzt der Zug der Grönlandwale. Dieser richtet sich nach den Wasserströmungen von Wasserrinnen im Treibeis. Sie überwintern vorwiegend im Beringmeer und kalben dort auch. Sie übersommern in anderen Gebieten, so etwa Cape Bathurst, dem Amundsengolf, im Mackenziedelta und an der Küste von Yukon.
Südkaper kalben an den Südküsten Südafrikas, Argentiniens und Australiens sowie um einige neuseeländische Inseln wie Dog Island. Die Sommergebiete sind nur teilweise bekannt. Die bisher südlichste Sichtung erfolgte im Eismeer der Antarktis.
Aufgrund verschiedener morphologischer Differenzen zwischen den Furchenwalen und Glattwalen, welche unter Allgemeine Morphologie genannt wurden, sind Glattwale in eine eigene Familie eingeordnet. Ein interessanter Punkt ist die Einordnung des Zwergglattwales (Caperea marginata). Dieser wird trotz des Namens und einiger morphologischer Parallelen zu den Glattwalen in die eigene Familie Neobalaenidae gestellt.
Grönlandwal (Balaena mysticetus)Früher wurden Eubalaena glacialis und Eubalaena japonica als Unterarten der Art Eubalaena glacialis geführt. Jüngere DNA-Untersuchungen untermauerten aber die These, dass die beiden zwei getrennte Arten sind.
Die heutigen Glattwalbestände sind durch die Jagd drastisch reduzierte Restpopulationen. Vor mehr als tausend Jahren erkannten die Basken als erstes in den Glattwalen eine gute Jagdbeute und bejagten sie. Sie entwickelten mit ihren Waffen die Grundlagen für Walfänger des 20. Jahrhunderts. Der industrielle Walfang reduzierte die Tiere fast bis zur Ausrottung. Nie wurde eine Walfamilie stärker dezimiert. Auf der intensiver bejagten Südhalbkugel erfolgte die Jagd meist an den Wurfplätzen des Südkapers und an einigen Buchten. Zuerst wurde das Kalb getötet, damit das Muttertier leichter zu erlegen war. Die Beute wurde auf Land oder ins Flachwasser gezogen und dort verwertet. Stets wurden die Barten entfernt, da diese einen hohen Marktwert hatten. Waren die Fänger auch am Öl interessiert, lösten sie den Blubber, zerteilten ihn und stellten das Öl mit großen gusseisernen Pfannen her. 
Nord- und Südkaper wurden um 1935 auf internationaler Ebene geschützt. Trotzdem erlegten Walfänger der Sowjetunion noch bis 1960 im Südatlantik, im Indischen Ozean und im Pazifik tausende Exemplare.
Die englische Bezeichnung von Glattwalen, die heute noch „right whale“ lautet, geht darauf zurück, dass Glattwale die richtige Jagdbeute waren, da sie langsam und groß waren, durch ihren Fettgehalt nach dem Tod an der Oberfläche trieben (also nicht versanken) und viel Barten und Öl lieferten.
Viele Kenntnisse über Grönlandwale und deren Bejagung stammen aus einem Bericht von William Scoresby, der ihn unter dem Namen Bericht über die arktischen Gebiete veröffentlichte. Speziell als noch mit Handharpune gejagt wurde, wurden Grönlandwale häufig gejagt. 
„[…] weniger lebhaft, langsamer in ihren Bewegungen und weniger angriffslustig als jede andere ähnlich große Art, kurz, sie lässt sich leicht erbeuten.“ ,begründet Scoresby um 1820 die Beliebtheit der Grönlandwaljagd. Auch existieren Aufzeichnungen über verschiedene ökonomische Aspekte der Barten. Harmer berichtet, dass eine Tonne des so genannten Fischbeines zeitweise Spitzenpreise von 2250 britischen Pfund erzielte. Die eineinhalb Tonnen Barten, die ein Grönlandwal erbrachte, waren teils 3375 Pfund wert. Schnell startete wegen all diesen Vorzügen eine rege Jagd, durch die die Bestände stark abnahmen, obwohl ein Fangschiff pro Saison meist nicht mehr als 7 bis 10 Wale erbeutete. Da die Küstenbestände immer kleiner wurden, mussten die Walfänger auf die Hochsee, um noch ausreichend Beute zu machen. Ab 1850 waren die Bestände so klein, dass sich die Jagd nicht mehr lohnte. Im europäischen Eismeer waren sie vollständig ausgerottet.
Nach dem Verbot wurde sofort intensiv geschützt, vor Südafrika, Argentinien, Neuseeland und Australien wurden inzwischen für den Südkaper Meeresparks eingerichtet, und spezielle Vorschriften regeln den Schutz und den Umgang mit Glattwalen. Der Bestand des Südkapers scheint sich wieder zu erholen, der Bestand umfasst dort etwa 10.000 Tiere mit einer Wachstumsrate von 6 bis 7 % (Stand 2008). Vor allem mit Fokus auf die schwindenden Bestände der nördlichen Hemisphäre prognostizieren jüngste Schätzungen jedoch das Aussterben der Glattwale in 200 Jahren, da sich Bedrohungsfaktoren wie Schifffahrt in Zukunft höchstwahrscheinlich maximieren werden. Tatsächlich ist jedoch speziell über die Bestände im Nordpazifik wenig bekannt. Da sich alle Glattwale mit Vorliebe in Küstennähe aufhalten, sind sie besonders durch den expandierenden Schiffsverkehr betroffen. Weitere Bedrohungen sind große Fischernetze. Ungefähr 38 % aller Todesfälle sind auf Kollisionen mit Schiffen zurückzuführen. Seit Dezember 2008 müssen in den USA größere Schiffe vor der Ostküste ihre Geschwindigkeit auf zehn Knoten (19 Kilometer pro Stunde) drosseln, um die gefährdeten Glattwale vor Zusammenstößen und Verletzungen durch Schiffsschrauben zu bewahren.     8 % aller zu Tode gekommenen Glattwale ertrinken in Fischernetzen. Studien wiesen an 60 % aller Exemplare der beiden Nordkaperarten Narben nach, welche durch die indirekte Einwirkung der Fischereiindustrie entstanden, speziell durch Reusen, welche für den Fang von Krebstieren ausgelegt sind. Die US-amerikanische Fischereiindustrie versucht inzwischen, alternative Methoden zur Fischerei anzuwenden, um die Zahl der Opfer zu senken.
Der Bestand des Grönlandwales beläuft sich auf aktuell 10000 Exemplare, scheint nicht zuletzt wegen seiner Abgelegenheit in nördlichen Meeren stabil zu sein und ist im Vergleich zu dem 8200 Tiere umfassenden Bestand im Jahre 1993 um fast 30 % gewachsen. Der Atlantische Nordkaper bewohnt weniger abgelegene Lebensräume mit höherer Bejagungs- und Gefahrenquote, sein Bestand hat sich von einigen wenigen Tieren nach Beendigung der Jagd auf Glattwale daher nur auf etwa 300 bis 350 Einzeltiere vergrößert. Über den vermutlich geringen Bestand des Pazifischen Nordkapers sind keine Details bekannt. Der Südkaper hat einen stabilen Bestand von ungefähr 7000 Individuen aufzuweisen
Die Eskimos der Arktis betreiben seit langer Zeit die Jagd auf den Grönlandwal. Früher wurden Harpunen mit Elfenbein- oder Steinspitzen eingesetzt. Die vor der kommerziellen Jagd existierenden 10000 bis 20000 Individuen waren trotz der Bejagung durch die Eskimos nicht gefährdet und der Bestand war stabil. Etliche amerikanische und europäische Jagdschiffe dezimierten im 19. Jahrhundert jedoch die Bestände im Beringmeer auf ein paar Tausend Exemplare, außerhalb des Beringmeeres lebten noch ein paar hundert Tiere. 1915 wurde die Jagd weitgehend eingestellt, doch die Eskimos jagten weiter. Trotz der eingestellten Jagd von Europäern und Amerikanern erholten sich speziell die östlichen Populationen nicht vollständig. Dies ist wohl auch auf verschiedene Faktoren wie Inzucht wegen kleiner Populationen zurückzuführen, doch die wahrscheinlich wichtigste Ursache war die Veränderung der Jagdmethoden der Eskimos. Diese setzten seit 1880 Harpunen mit Sprenggranaten ein, daher empfahl 1977 der wissenschaftliche Ausschuss der internationalen Walfangkommission (IWC) eine Beendigung der Jagd. Nach massiven Protesten der Eskimos, die ihre kulturelle und ökonomische Abhängigkeit vom Walfang betonten, wurde ihnen eine beschränkte Jagderlaubnis verliehen, welche von der Alaska Eskimo Whaling Commission zugeteilt und durchgesetzt werden sollte. Die westlichen Populationen wachsen inzwischen trotz des Fangs der Eskimos um jährlich 3 %. Daraus wird laut einer Rechnung mit dem Bestand von 1993 (8200 Tiere) geschlossen, dass jährlich problemlos 56 Wale erlegt werden können.
Scott D. Kraus, David E. Gaskin und John Craighead George: Glattwale in: David MacDonald (Hrsg.): Die große Enzyklopädie der Säugetiere, Könemann Verlag, Königswinter 2005, ISBN 3-8331-1006-6 (deutsche Übersetzung der englischen Originalausgabe von 2001)
Bernhard Grzimek (Hrsg.): Grzimeks Tierleben Säugetiere 2, Bechtermünz Verlag, Augsburg 2000, ISBN 3-8289-1603-1 (unveränderter Nachdruck der Originalausgabe von 1980)
Balaena mysticetus in der Roten Liste gefährdeter Arten der IUCN 2006. Eingestellt von: Cetacean Specialist Group, 1996. Abgerufen am 3. November 2006.
Eubalaena glacialis in der Roten Liste gefährdeter Arten der IUCN 2006. Eingestellt von: Cetacean Specialist Group, 1996. Abgerufen am 3. November 2006.
Eubalaena japonica in der Roten Liste gefährdeter Arten der IUCN 2006. Eingestellt von: Cetacean Specialist Group, 1996. Abgerufen am 3. November 2006.
Eubalaena australis in der Roten Liste gefährdeter Arten der IUCN 2006. Eingestellt von: Cetacean Specialist Group, 1996. Abgerufen am 3. November 2006.

Gleichrichter werden in der Elektrotechnik und Elektronik zur Umwandlung von Wechselspannung in Gleichspannung verwendet. Sie bilden, neben Wechselrichtern und Umrichtern, eine Untergruppe der Stromrichter. Um Wechselanteile (verbleibende Halbwellen) zu bedämpfen, wird eine gleichgerichtete Spannung üblicherweise geglättet.
zur Verbindung weit entfernter Stromnetze oder der Kopplung nicht synchroner Stromnetze über Hochspannungs-Gleichstrom-Übertragung (HGÜ),
für Anwendungen in der Nachrichtentechnik, zum Beispiel zur Hüllkurvendemodulation.Die Gleichrichtung erfolgt meist ungesteuert durch Halbleiterdioden. Aktive elektronische Bauteile, wie Thyristoren, erlauben durch Phasenanschnittsteuerung eine gesteuerte Gleichrichtung. Feldeffekttransistoren (MOSFETs) werden bei Synchrongleichrichtern verwendet – insbesondere bei der Gleichrichtung kleiner Spannungen und großer Ströme – und gestatten aufgrund der geringeren Durchlassspannung eine höhere Effizienz, als es mit Halbleiterdioden möglich wäre.
Im Jahr 1873 entdeckte Frederick Guthrie, dass ein positiv geladenes Elektroskop entladen wird, wenn man ein geerdetes, glühendes Metallstück in die Nähe brachte. Bei negativ geladenem Elektroskop passiert nichts, woraus folgte, dass der elektrische Strom nur in eine Richtung fließen konnte.
1874 entdeckte Karl Ferdinand Braun die richtungsabhängige elektrische Leitung in bestimmten Kristallen. Er ließ sich den Kristallgleichrichter 1899 patentieren. Aus etwa derselben Zeit stammt der Kohärer als eine frühe Form der Diode.
Der indische Wissenschaftler Jagadish Chandra Bose benutzte 1894 als erster Kristalle, um elektromagnetische Wellen nachzuweisen. Der erste praktisch verwendbare Kristalldetektor aus Silizium wurde 1903 für funktechnische Anwendungen durch Greenleaf Whittier Pickard entwickelt, der sich diesen 1906 patentieren ließ. In der Nachfolgezeit wurde allerdings häufiger Bleisulfid verwendet, weil es billiger und einfacher zu verwenden war.
Thomas Edison entdeckte Guthries Beobachtung im Jahr 1880 bei Experimenten mit Glühlampen wieder und ließ sich den Effekt 1884 patentieren, ohne eine Anwendungsmöglichkeit zu kennen. Owen Willans Richardson beschrieb später den Effekt wissenschaftlich, daher nennt man ihn heute Edison-Richardson-Effekt. Etwa zwanzig Jahre später erkannte John Ambrose Fleming, der zuerst Angestellter von Edison und später wissenschaftlicher Berater der Marconi Wireless Telegraph Company war, dass der Edison-Richardson-Effekt benutzt werden konnte, um schwache Radiosignale nachzuweisen. Er ließ sich die erste brauchbare Anwendung, die Röhrendiode („Fleming valve“) im Jahr 1904 patentieren.
Alle bisher beschriebenen Effekte eigneten sich nur für sehr geringe Ströme. Die zunehmende Verbreitung elektrischer Energie benötigte aber leistungsstarke Gleichrichter, weil jene vorzugsweise durch Wechselstromgeneratoren erzeugt wird. Da die elektrischen Vorgänge in Halbleitern erst nach etwa 1950 (nach der Erfindung des Bipolartransistors) geklärt wurden, kamen verschiedene andere Gleichrichterprinzipien zur Anwendung.
Zu Beginn des 20. Jahrhunderts gab es zur Umwandlung von Wechselspannung in Gleichspannung nur elektromechanische Gleichrichter:
Bei entsprechenden Umformern sitzen ein Wechselstrommotor und ein Gleichstromgenerator auf einer gemeinsamen Welle. Diese Umformer dienten oft gleichzeitig zur Spannungstransformation und zur galvanischen Trennung vom Netz.
Sogenannte Zerhacker wurden als Wechselrichter von Gleichspannung zur nachfolgenden Transformation mittels eines Transformators verwendet. Sie vereinten einen selbstschwingenden, mit Schaltkontakten arbeitenden Wechselrichter (z. B. mit einem Wagnerschen Hammer) und einen damit gekoppelten zweiten Kontaktsatz zur Synchrongleichrichtung in sich. Diese waren aufgrund des Kontakt-Verschleißes auswechselbar gestaltet (Stecksockel).
Rotierende mechanische Gleichrichter besaßen ein Rad mit elektrischen Kontakten, das die zeitgenaue Umschaltung der Wechselspannung ermöglicht. Das Rad wurde von einem Synchronmotor angetrieben, der die Synchronisation zwischen der Drehbewegung und der Polaritätsänderung der Wechselspannung sicherstellte. Die Konstruktion wurde beispielsweise aus einem Hochspannungstransformator mit Wechselspannung versorgt, um die hohe Gleichspannung für Elektrofilter zu erzeugen. Eine Weiterentwicklung dieses Prinzips war der Kontaktumformer.
Bis etwa 1970 wurde in Kraftfahrzeugen der Ladestrom für die Akkumulatoren durch Gleichstromlichtmaschinen mit mechanischem Gleichrichter (Kommutator) erzeugt. Durch die Erfindung leistungsstarker Halbleiterdioden war der Weg zum Einsatz leistungsfähigerer Drehstromlichtmaschinen frei.Nachteile der mechanischen Gleichrichtung sind der Kontaktabbrand vor allem bei höheren Strömen, Synchronisationsprobleme und die Begrenzung auf Frequenzen unter etwa 500 Hz. Der große Vorteil der verschwindend niedrigen Durchlassspannung und entsprechend sehr geringen Verlustleistung konnte erst in jüngster Zeit durch gesteuerte MOSFET-Gleichrichter wieder erreicht werden.
Eine Möglichkeit der Gleichrichtung eröffnete das Prinzip der anodischen Oxidation, die in den Anfängen der elektrischen Telegrafie und Telefonie eine Rolle spielte. Zwei in einen Elektrolyten getauchte Elektroden können gleichrichterähnliche Eigenschaften aufweisen. Eine Elektrode muss dazu aus einem Edelmetall, zum Beispiel Platin, bestehen, die andere aus einem Metall, das durch anodische Oxidation eine dicke Oxidschicht bildet, wie Niob. Bei dieser Anordnung kann nur Strom fließen, wenn das anodisch oxidierbare Metall als Kathode fungiert. Diese Gleichrichter werden auch als Nassgleichrichter oder als elektrolytische Gleichrichter bezeichnet und konnten bis zu Spannungen um 300 V eingesetzt werden. Die Hauptnachteile – Lageempfindlichkeit und giftige bzw. korrosionsfördernde Dämpfe – beschränkten die Zahl der Anwendungen.
Eine weitere Entwicklung war der Quecksilberdampfgleichrichter, der auch bei größeren Leistungen, etwa zur Speisung einer Oberleitung einer Straßenbahn, eingesetzt werden konnte. Er besteht aus einem Glaskolben, an dessen unterem Ende sich eine Kathode mit einem Quecksilbervorrat (Teichkathode) befindet. Darüber wölbt sich der Glaskolben, an dem das Quecksilber wieder kondensiert. Seitlich sind Arme mit Graphitelektroden als Anoden angeschmolzen. Elektronen können nur von der Teichkathode zu den Graphitelektroden fließen. Dazu muss die Zündspannung der Gasentladung erreicht werden, und es wird als Nebeneffekt UV-Licht erzeugt.
Einige Jahrzehnte später wurden die ersten Halbleitergleichrichter wie Selen-Gleichrichter und Kupferoxydul-Gleichrichter erfunden. Sie wurden, da dabei keine Flüssigkeiten zum Einsatz kamen, auch als Trockengleichrichter bezeichnet. Sie bestehen aus einer Metallplatte, auf der eine oberseitig mit Zinn und einer Kontaktfeder versehene Schicht aus Selen bzw. Kupferoxid aufgebracht ist. Die Plattengröße eines Selengleichrichters beträgt je nach Stromstärke zwischen einem Quadratmillimeter und über 100 Quadratzentimetern. Die maximal erlaubte Sperrspannung einer solchen Selen-Gleichrichterplatte beträgt nur 15 bis 50 V, die Durchlassspannung ist mit 0,7 bis 1,5 V relativ hoch; der Kupferoxydul-Gleichrichter hat zwar eine geringere Durchlassspannung, aber auch eine geringere Sperrspannung von nur ca. 10 V. Um höhere Spannungen gleichzurichten, wurden die Platten gestapelt, also in Reihe geschaltet. Eine Symmetrierung ist nicht erforderlich. Die Plattenanzahl bestimmt die maximale Sperrspannung. Sogenannte Selenstäbe enthielten eine große Anzahl kleiner Selengleichrichterscheiben und dienten bis in die 1970er-Jahre unter anderem zur Gleichrichtung der Anodenspannung von Bildröhren in Schwarzweiß-Fernsehgeräten. Sie hatten Sperrspannungen bis über 20 kV; durch die hohe Anzahl an Einzelelementen war allerdings auch die Durchlassspannung entsprechend hoch.
Wegen des typischen meerrettich- oder knoblauchartig zu beschreibenden Geruchs bei Überlast eines Selen-Gleichrichters sprach man im Technikerjargon das Wort „Gleichrichter“ gerne auch als „gleich riecht er“ aus.
Zu Beginn des 20. Jahrhunderts wurden vor allem in Detektorempfängern Detektorkristalle aus Bleiglanz oder Pyrit benutzt – ein Halbleiter-Metall-Übergang, der aus einem Halbleiterkristall und einer tastenden Metallspitze bestand, mit der geeignete Punkte zur Demodulation von AM-Rundfunksendungen gesucht wurden. Diese mechanisch sehr empfindliche und wenig effektive Versuchsanordnung wurde sehr schnell durch die rasch voranschreitende Entwicklung der Elektronenröhre verdrängt, die Schaltungen ermöglichte, die verstärken und gleichzeitig gleichrichten konnten (Audion).
Nachteilig sind die geringen Sperrspannungen von etwa 15 V und vor allem das geringe Verhältnis Sperrwiderstand zu Durchlasswiderstand, das knapp über 1 liegt und heutigen Maßstäben nicht mehr genügt.
Später wurden in großem Maßstab Spitzendioden auf der Basis von Germanium gefertigt, der Einsatzzweck war weiterhin die Gleichrichtung von Hochfrequenz bis in den Zentimeterwellenbereich.
Der Durchbruch in der Entwicklung der Gleichrichter gelang erst nach der Erforschung des p-n-Übergangs im Anschluss an die Erfindung des Bipolartransistors im Jahre 1947 und die Fertigung von Flächendioden mittels Diffusion oder Epitaxie.
Über einen längeren Zeitraum verwendete man Germanium-Dioden, bis später Silizium-Dioden entwickelt wurden, mit denen eine höhere Temperaturbeständigkeit einher geht.
Man entwickelte die bereits länger bekannten Schottkydioden weiter, um sie bei großen Strömen und kleinen Spannungen als Gleichrichter einsetzen zu können. Ihre Merkmale sind die geringe Flussspannung, die geringe Sperrspannung und der relativ hohe Reststrom. Sie sind diesbezüglich mit Germaniumdioden vergleichbar, haben jedoch keinen Speichereffekt und können daher bei sehr hohen Frequenzen eingesetzt werden.
Um hohe Sperrspannungen bei geringen Speicherzeiten zu erreichen, verwendet man zunehmend Siliciumcarbid-Dioden zur Gleichrichtung in Schaltnetzteilen höherer Spannung.
Merkmale der heutigen Halbleiterdioden sind unter anderem die geringe Schwellenspannung von deutlich unter einem Volt (Germanium- und Schottkydioden typisch 0,3–0,4 V, Siliziumdioden 0,6 V), das große Verhältnis von Durchfluss- zu Sperrstrom und die sehr kleine Bauweise.
Schaltnetzteile, Gleichspannungswandler und Frequenzumrichter wurden erst nach der Erfindung der Halbleiterdioden betriebssicher und wartungsfrei.
Die Sperrspannungen von Diodengleichrichtern liegen zwischen 50 und etwa 1500 V. Höhere Spannungen erreicht man durch Serienschaltung.
Weitverbreitete Gleichrichterdioden für kleine Ströme im Niederfrequenzbereich sind die Gleichrichterdioden der Typen 1N4001 bis 1N4007 (Sperrspannung von 50 bis 1000 V) für Ströme bis 1 A sowie die Gleichrichterdioden der Typen 1N5400 bis 1N5408 für Ströme bis 3 A.
Nachteilig im Vergleich zu historischen Gleichrichterarten ist die Empfindlichkeit gegenüber Überlastung. Ursache ist die geringe Masse des Kristalls. Deshalb steigt die Kristalltemperatur bei Überstrom nach sehr kurzer Zeit (einige Millisekunden) so weit an, dass die PN-Schicht irreversibel zerstört wird. Bei Überschreitung der Sperrspannung sind nur wenige Diodentypen in der Lage, lokale Überhitzung durch einen kontrollierten Durchbruch zu vermeiden (Avalanche-Dioden).
Eine Röhrendiode oder auch Vakuumdiode ist eine Elektronenröhre mit beheizter Kathode und einer oder zwei (selten mehreren) Anode(n). Röhrendioden waren in der Anfangszeit der Rundfunktechnik in Röhrenempfängern das Standardbauteil zur Gleichrichtung der Versorgungsspannung und Demodulation des amplitudenmodulierten Signals. Nachteilig sind das voluminöse und zerbrechliche Glasgehäuse, die notwendige Heizleistung der Kathode und die hohe Durchlassspannung von etwa 40 V bei Strömen um 100 mA. Unerreicht sind dagegen die hohen erlaubten Sperrspannungen von bis zu über 100 kV und der extrem hohe Sperrwiderstand.
Ende der 1920er-Jahre wurde auch mit Gleichrichtern auf Basis von Glimmentladungen und speziellen Bauformen der Glimmlampe experimentiert. Das Verfahren ähnelt dem der Röhrendiode, es wird die Röhre aber mit einem Gas gefüllt und die Kathode nicht beheizt. Die Gleichrichterwirkung basiert auf einer unsymmetrischen Formung der beiden Entladungselektroden oder auch auf Elektrodenbeschichtungen zur Reduktion des Kathodenfalles. Der Glimmgleichrichter konnte sich wegen seines recht schlechten Verhältnisses von Durchlass- zu Sperrstrom (< 100:1), des geringen Maximalstromes und der unvergleichlich hohen Durchlassspannung von etwa 70 V nicht durchsetzen; der Einsatz von Kondensatoren zur Spannungsglättung erhöhte den Sperrstrom noch zusätzlich.
In diesen Schaltungen werden nur Dioden verwendet, deren Leitfähigkeit von der Polarität der angelegten Spannung abhängt:
Wenn die Kathode negativer als die Anode ist und die notwendige Schleusenspannung (bei Silizium etwa 0,6 V) überschritten wird, leitet die Diode.
Bei umgedrehtem Vorzeichen sperrt die Diode, solange die Durchbruchsspannung nicht überschritten ist.In beiden Fällen muss der Strom begrenzt werden, sonst wird die Diode zerstört. Im Folgenden sind einige typische Gleichrichterschaltungen skizziert, in Klammern ist jeweils die technische Kurzbezeichnung angegeben.
Bei einem ungesteuerten Einweggleichrichter (auch Einzweigschaltung) wird nur eine Halbschwingung der Wechselspannung gleichgerichtet, die andere wird nicht verwendet. Während der Halbperiode, in der die Diode in Durchlassrichtung betrieben wird, steht am Ausgang Spannung an, in der zweiten Halbperiode sperrt die Diode. Nachteilig sind die vergleichsweise große Restwelligkeit auf der Gleichspannungsseite, der schlechte Wirkungsgrad und die unsymmetrische Belastung der Wechselspannungsquelle. Dadurch wird der speisende Transformator magnetisiert, da er nur in einer Richtung vom Strom durchflossen wird, weshalb dieser für Einweggleichrichtung ausgelegt sein muss (Luftspalt). Dafür benötigt ein solcher Gleichrichter nur eine einzige Diode. Die pulsierende Gleichspannung muss im Regelfall noch geglättet werden. Die Welligkeit hat die Frequenz der Eingangsspannung.
Einweggleichrichtung entstammt einer Zeit, in der Gleichrichter noch sehr teuer waren. Man findet sie heute nur noch in Sperrwandlern oder zur Erzeugung von Hilfsspannungen, wenn nur eine sehr geringe Leistung benötigt wird. Ansonsten gilt die Einweggleichrichtung als veraltet. Schwarzweiß-Fernsehgeräte hatten einen Einweg-Hochspannungsgleichrichter zur Erzeugung der Bildröhren-Anodenspannung aus den Zeilen-Rückschlagimpulsen des Zeilentransformators. Allstrom-Röhrenradios und -Fernseher hatten zur Netzgleichrichtung und zur Gewinnung der Anodenspannung einen Einweggleichrichter aus Selen, später aus Silizium. Ein Netzpol wurde als Massepotential verwendet; die direkt am Netz betriebenen Röhrenheizungen wurden in Reihe geschaltet.
Standardgleichrichter für Einphasenwechselstrom ist der Brückengleichrichter, auch Graetzschaltung, Graetzbrücke oder Zweipuls-Brückenschaltung genannt. Namensgeber ist der deutsche Physiker Leo Graetz. Die Schaltung wird von vier Dioden gebildet: Die links anliegende Wechselspannung, die beispielsweise direkt von einem Transformator kommt, wird in eine pulsierende Gleichspannung (rechts dargestellt) umgewandelt.
Da es sich dabei um eine Zweiweggleichrichtung handelt, erscheinen die Halbschwingungen der Wechselspannung im Gleichstromkreis am Verbraucher R gleich gepolt. Ohne Glättungskondensator bleibt der Effektivwert der Spannung dabei näherungsweise gleich. Im Gegensatz zu anderen Gleichrichtertypen muss bei dieser Gleichrichterschaltung die Sperrspannung der Gleichrichterdioden nur so groß wie die Spitzenspannung der Wechselspannung sein. Man wählt sie aus Sicherheitsgründen jedoch etwas höher (bei Netzgleichrichtern am 230-Volt-Netz beispielsweise über 400 V).
Die Welligkeit hat die doppelte Frequenz der Eingangsspannung; durch die halbierte Periodendauer verringert sich der nachfolgende Filteraufwand.
Brückengleichrichter für Wechsel- und Drehstrom werden oft als bereits miteinander verschaltete Dioden im gemeinsamen Gehäuse angeboten. Ausführungen für höhere Ströme enthalten eine Kühlfläche sowie eine Bohrung zur Befestigung auf einem Kühlkörper.
Beim Mittelpunktgleichrichter werden ebenfalls beide Halbschwingungen der Wechselspannung gleichgerichtet. Allerdings ist dazu ein Transformator mit einer Mittelpunktanzapfung notwendig, die gleichzeitig einen Pol der gleichgerichteten Ausgangsspannung bildet.
Die Vorteile dieser Mittelpunktschaltung liegen darin, mit nur zwei Dioden D1 und D2 auszukommen und dass die Spannung nur um eine Diodenflussspannung reduziert wird. Nachteilig ist, dass sie einen speziellen stärker dimensionierten Transformator erfordert, da immer nur die Hälfte des Kupfers zum Stromfluss beiträgt. Bei gegebenem Kupfervolumen hat jede Hälfte der Sekundärwicklung wegen des dünneren Drahtes (doppelte Windungszahl muss Platz finden) in etwa den doppelten Innenwiderstand, der in die Verlustleistung (
Weiter ist zu beachten, dass die Sperrspannung der Dioden mindestens die doppelte Ausgangsspannung sein muss. Während eine Diode sperrt und die andere leitet, liegt auf der gesperrten die volle Trafospannung beider Wicklungshälften an.
Die Welligkeit hat die doppelte Frequenz der Eingangsspannung, was analog zur Brückenschaltung den nachfolgenden Filteraufwand gegenüber der Einweggleichrichtung verringert. Die Schaltung wird hauptsächlich bei geringen Spannungen (unter 10 V) sowie bei Schaltnetzteilen verwendet, da hier die Vorteile die Nachteile überwiegen. Die Mittelpunktschaltung wurde in früherer Zeit häufig in Röhrengeräten zur Erzeugung der Anodenspannung angewandt, da man bei ihr unter Einsatz von Duodioden, Gleichrichterröhren mit zwei Anoden und gemeinsamer Kathode oder bei mehranodigen Quecksilberdampfgleichrichtern nur eine teure Gleichrichtereinheit benötigte.
Heute verwendet man häufig Doppeldioden (Silizium- oder Schottkydioden) mit gemeinsamer Kathode. Sie bestehen aus einem Chip, der zwei Dioden enthält und dessen Rückseite, als gemeinsame Kathode, auf eine Kühlfahne gelötet ist.
Für mittlere Leistungen von einigen Kilowatt aufwärts wird die Dreiphasenwechselspannung aus dem Stromnetz gleichgerichtet, weil dann die Brummspannung auf der Gleichspannungsseite kleiner ist und nur geringer Aufwand zur Glättung der Gleichspannung entsteht.
Anwendung finden Dreiphasengleichrichter in der elektrischen Energietechnik wie beispielsweise bei Straßenbahnen, die meist mit Gleichspannungen von 500–750 V betrieben werden; dabei ist keine Glättung erforderlich. Auch bei den heute üblichen Drehstrom-Lichtmaschinen von Kraftfahrzeugen wird sie eingesetzt. Hierbei erfolgt die Glättung durch die Starterbatterie.
Vor der Zeit der Halbleitergleichrichter fertigte man auch mehrphasige Quecksilberdampfgleichrichter, die durch eine gemeinsame Teichkathode und mehrere nach oben ragende Anoden gekennzeichnet waren.
Spezielle Gleichrichterschaltungen dienen zur Spannungsvervielfachung. Dabei werden Kombinationen von Dioden und Kondensatoren so verschaltet, dass eine angelegte Wechselspannung eine vervielfachte Gleichspannung ergibt. Typische Schaltungen sind der Spannungsverdoppler, die Hochspannungskaskade und die Greinacher-Schaltung. Anwendung finden diese Schaltungen unter anderem in Fernsehempfängern mit Bildröhren zur Erzeugung der Anodenspannung im Bereich von 18 bis 27 kV.
Alle bisher beschriebenen Gleichrichter sind ungesteuert. Der Umschaltvorgang erfolgt ohne eine zusätzliche Steuerelektronik, nur aufgrund des Vorzeichens der anliegenden elektrischen Spannungen an den Dioden. Steuerbare Gleichrichter verwendet man im Bereich der Energie- und Antriebstechnik. Mit ihnen ist nicht nur eine Gleichrichtung möglich, sondern durch Phasenanschnittsteuerung auch eine Leistungssteuerung, weil man bei diesen Bauelementen den Zeitpunkt festlegen kann, ab dem der Gleichrichter elektrisch leitfähig wird – vorher isoliert er.
Im nebenstehenden Bild ist der Zündimpuls der Thyristoren unten als blaues Rechteck eingezeichnet. Nach Erlöschen des Steuerpulses bleibt der Stromfluss (rot eingezeichnet) bis zum folgenden Nulldurchgang bestehen. Durch Verschiebung des Einschaltzeitpunktes lässt sich die Energie (graue Fläche) ändern, die zum Verbraucher fließt. Einsatzbereiche sind beispielsweise die Drehzahlsteuerung von Gleichstrom- und Universalmotoren in Industrieanlagen oder Kleingeräten wie Bohrmaschinen, in modernen Elektrolokomotiven zur Beaufschlagung des Gleichspannungs-Zwischenkreises und in Anlagen der Hochspannungs-Gleichstrom-Übertragung. Gesteuerte Gleichrichter in Umrichtern ermöglichen eine sehr effektive Frequenz- und Leistungssteuerung von Drehstrommotoren in Walzstraßen, Elektrolokomotiven und Waschmaschinen.
Früher wurden für diesen Zweck gittergesteuerte Quecksilberdampfgleichrichter wie Thyratrons und Ignitrons eingesetzt, die groß, teuer und verlustreich sind. Heute werden Thyristoren, Insulated Gate Bipolar Transistors (IGBT) und teilweise Leistungs-MOSFETs verwendet, die wesentlich geringeren Kühlaufwand erfordern.
Es gibt auch sogenannte GTO-Thyristoren, die das Sperren eines Ventils durch einen Impuls erlauben. Allerdings weisen diese schlechtere elektrische Werte auf und werden zunehmend durch IGBT ersetzt.
Hat ein Brückengleichrichter voll steuerbare Zweige, ist mit ihm Vier-Quadrantenbetrieb möglich, d. h., er kann bei entsprechender Steuerung sowohl Energie aus der Wechselstromseite in die Gleichstromseite liefern als auch umgekehrt. Die entsprechende Schaltung in Form einer H-Brücke wird als Vierquadrantensteller bezeichnet und unter anderem in Wechselrichtern zur Erzeugung einer Wechselspannung aus einer Gleichspannung eingesetzt.
Die Einteilung der steuerbaren Gleichrichter erfolgt in mehrere Schaltungstopologien, dazu zählen B2HZ-, B2HK-, B2C- und B6C-Gleichrichter. Eine steuerbare Gleichrichterschaltung mit besonders geringem Oberschwingungsanteil stellt der Vienna-Gleichrichter dar.Darüber hinaus hat ein gesteuerter Gleichrichter Bedeutung in der Messtechnik, zum Beispiel bei der Wechselspannungsbrücke.
Die Gleichrichtung sehr geringer Spannungen ist problematisch, weil es keine ungesteuerten Dioden mit Schleusenspannungen unter 0,2 V gibt (Schottky-Diode). Niedrige Spannungen lassen sich so überhaupt nicht gleichrichten, bei höheren gibt es Nebeneffekte:
Bei niedrigen Spannungen sinkt der Wirkungsgrad erheblich, der Gleichrichter wird bei größeren Strömen heiß.Deshalb werden unter anderem in Schaltnetzteilen mit niedriger Ausgangsspannung leistungsstarke Synchrongleichrichter eingesetzt.
Bei dieser Art von Gleichrichtern werden keine Bauteile verwendet, die von sich aus einen Stromfluss nur in eine Richtung zulassen; stattdessen verwendet man MOSFETs, die durch eine Ansteuerelektronik so gesteuert werden, dass sie wie Halbleiterdioden mit sehr kleiner Durchlassspannung wirken.
Beispiel: Prozessoren seit etwa dem Jahr 2000 benötigen Betriebsspannungen von weniger 2 V, Mobilprozessoren mittlerweile von unter 1 V. Diese Leistung wird im Laptop aus 19 V durch Synchronwandler erzeugt, die MOSFET-Gleichrichter verwenden. Weil der Durchlass-Spannungsabfall bei diesen Bauelementen nur wenige zehn Millivolt beträgt, besitzen diese Gleichrichter einen Wirkungsgrad von deutlich über 90 Prozent und können ohne aktive Kühlung realisiert werden.
In der rechts dargestellten Schaltskizze als Teil eines Schaltreglers wird links die Wechselspannung über einen Transformator auf das benötigte Spannungsniveau transformiert, über die beiden MOSFETs gleichgerichtet und die Ausgangsspannung U0 mittels Spule L und Kondensator C geglättet. Die Ansteuerung der MOS-FETs erfolgt durch die gleiche Controllerschaltung, die auch die Eingangsfrequenz erzeugt; somit ergibt sich das Timing der MOS-FET-Ansteuerung zwanglos. Die Controllerschaltung fehlt der Übersichtlichkeit wegen in der Schaltskizze.
Synchrongleichrichter für geringe Ströme findet man in Chopper-Verstärkern, Auto-Zero-Verstärkern und Lock-in-Verstärkern.
Jeder Gleichrichter macht aus der ursprünglichen Wellenform der Spannung, die oft sinusförmig ist, eine periodisch schwankende Spannung. Die meisten Verbraucher wie Computer benötigen aber sehr konstante Gleichspannung, weshalb die Wellentäler ausgeglichen werden müssen. Diesen Vorgang bezeichnet man als Glättung, die im Regelfall durch einen parallel zum Verbraucher geschalteten Kondensator ausreichender Kapazität erfolgt. Dieser Kondensator wird durch kurze Strompulse aufgeladen, die einsetzen, wenn der Gleichrichterausgang die Ladespannung des Kondensators übersteigt, und erhebliche Werte annehmen können. Alternativ kann diese Glättung auch durch Induktivitäten in Reihe zum Verbraucher erfolgen, wodurch der Stromfluss durch den Gleichrichter gleichförmiger wird. Der nach der Glättung übrigbleibende Wechselanteil, auch Brummspannung oder Restwelligkeit genannt, kann durch nachgeschaltete Siebglieder weiter reduziert werden. Filter zur Glättung sind Tiefpassfilter.
Die Glättung vereinfacht sich mit steigender Frequenz der Wechselspannung, weshalb immer häufiger Schaltnetzteile mit Frequenzen über 40 kHz eingesetzt werden. Sehr leistungsstarke Gleichrichter werden immer mit Dreiphasenwechselstrom betrieben, weil die Gleichspannung bei Verwendung eines Zwölfpulsgleichrichters auch ohne Glättung oft ausreichend geringe Brummspannung besitzt.
In kleinen Netzteilen für Elektronenröhren und in Netzteilen mit mehreren Ausgangsspannungen aus demselben Transformator werden Pi-Filter (zwei Querkondensatoren, dazwischen eine Längsinduktivität) eingesetzt. Der Pi-Filter wird benutzt, da die anderen Spannungen auf dem Filter abfallen und das gemeinsame Potential verschieben würden. Bei sehr hohen Ansprüchen an die Qualität der Gleichspannung wird eine elektronische Stabilisierung durch Spannungsregler wie in Labornetzteilen eingesetzt. Dabei können eingebaute Verstärker die Restwelligkeit der Ausgangsspannung fast vollständig beseitigen, da die Schaltung wie eine Gegenkopplung wirkt.
In der elektrischen Messtechnik und in der analogen Audiotechnik sind präzise Gleichrichter für kleine Spannungen und kleine Stromstärken notwendig. Sie dienen nicht der Energieversorgung von elektronischen Baugruppen, sondern zur Verarbeitung von Messsignalen.
Im Gegensatz zur einfachen ungesteuerten Gleichrichtung kann sich für die Wechselspannungsmesstechnik bei einer Gleichrichtung, welche synchron zu einem äußeren Vorgang abläuft, das Vorzeichen der Ausgangsspannung umkehren.
Gleichrichter werden in der Nachrichtentechnik zum Nachweis oder zur Demodulation von Hochfrequenzsignalen eingesetzt. Ein einfaches Beispiel ist die Hüllkurvengleichrichtung amplitudenmodulierter Signale bei Detektor-Empfängern. Sie ist in nebenstehender Schaltskizze abgebildet.
Die Spule und der Drehkondensator stellen dabei einen Schwingkreis dar, der auf die gewünschte hochfrequente Trägerfrequenz abgestimmt ist. Die empfangene Spannung, deren Amplitude in Abhängigkeit vom Nutzsignal schwankt, wird über die Diode gleichgerichtet, wodurch am Kopfhörer als Spannungsverlauf die Hüllkurve des Nutzsignals anliegt – die höheren Frequenzanteile der Sendefrequenz werden durch die Induktivität des Kopfhörers unterdrückt.
Diese Form eines Empfängers ist nicht besonders empfindlich und nur für den Empfang naher und starker Sender geeignet. Die Materialien für die Diode bestanden früher unter anderem aus Bleiglanz oder Pyrit und wurden durch eine feine Metallspitze kontaktiert (was eher als Schottky-Diode anzusehen ist), wodurch eine kapazitätsarme gleichrichtende Wirkung erzielt wurde. Später wurden dafür Germaniumdioden verwendet.
Das zugrundeliegende Prinzip wird auch in heutigen Rundfunkempfängern beim Empfang von amplitudenmodulierten Signalen verwendet.
Alle Gleichrichter sind nichtlineare Schaltungen, die bei sinusförmigem Spannungsverlauf in Stromversorgungen einen nicht sinusförmigen Stromverlauf auf der Gleichspannungsseite verursachen. Die Nichtlinearität ist bedingt durch die nichtlineare Betragsfunktion und die Effekte infolge der Glättung auf der Gleichspannungsseite.
Der Strom auf der Wechselspannungsseite setzt sich aus mehreren Frequenzkomponenten zusammen, sogenannten Oberschwingungen, die in Wechselspannungsnetzen Störungen verursachen können. Um diese Oberschwingungen klein zu halten, müssen Netzteile mit Gleichrichtern von bestimmten Leistungen an über eine Leistungsfaktorkorrektur verfügen, um die Verzerrungsblindleistung zu minimieren. Das ist eine spezielle Form der Filterung, welche den erwünschten sinusförmigen Stromverlauf auf der Wechselstromseite nachbildet.
Außerdem tritt bei Gleichrichtern, wie bei allen nichtlinearen Schaltungen, eine spezielle Form der Blindleistung auf, die in der Literatur uneinheitlich als Verzerrungsblindleistung oder Verzerrungsleistung bezeichnet wird und sich ähnlich wie die Blindleistung auswirkt. Dabei handelt es sich im Gegensatz zu der Blindleistung, welche auch Verschiebungsblindleistung genannt wird und sich durch eine bestimmte Phasenverschiebung zwischen Spannung und Strom in der Grundschwingung auszeichnet, um eine Form der Blindleistung, die durch starke Oberschwingungen hervorgerufen wird, die durch den meist kleinen Stromflusswinkel entstehen. Diese Verzerrungsblindleistung belastet ebenso wie die Verschiebungsblindleistung die Leitungen und ist unerwünscht, da damit keine Arbeit am Verbraucher verrichtet wird.
Am 1. Januar 2001 trat eine EMV-Norm in Kraft, die Vorschriften über das zulässige niederfrequente Störspektrum (Oberwellen) für elektronische Verbraucher ab 75 Watt festlegt.
Die größten jemals zum Einsatz gekommenen Quecksilberdampfgleichrichter befanden sich in der Hochspannungs-Gleichstrom-Übertragungsanlage Nelson-River-Bipol 1. Sie wiesen eine Sperrspannung von 150 kV und einen maximalen Durchlassstrom von 1800 A auf. Mitte der 1990er-Jahre wurden sie durch gesteuerte Gleichrichter auf Thyristorbasis ersetzt.
Taucht man in verdünnte Schwefelsäure eine Platinelektrode und eine Niobelektrode ein und legt an diese eine Spannung, so kann nur Strom fließen, wenn die Niobelektrode die Kathode ist. Man spricht hier von einem elektrolytischen Gleichrichter. Solche Gleichrichter können auch mit anderen Elektrolyten und Metallen realisiert werden. Wichtig ist, dass eine Elektrode aus Metall mit hoher Neigung zu Passivierung wie Niob, Tantal oder Aluminium besteht.
Mit Schottky-Dioden können Gleichrichter mit niedrigerer Flussspannung als mit herkömmlichen Dioden gebaut werden. Die Flussspannung oder Vorwärtsspannung beschreibt den Spannungsabfall an der Diode im leitfähigen Zustand und ist bei Gleichrichtern unerwünscht. Schottky-Dioden kommen vor allem in Schaltnetzteilen zur Anwendung.
Hochspannungsgleichrichter, wie sie in Hochspannungslabors, Röhrenfernsehern, bei Laserdruckern zum Auftragen des Toners oder bei Hochspannungs-Gleichstromübertragungen eingesetzt werden, bestehen aus einer Reihenschaltung von herkömmlichen Dioden. Das ist notwendig, weil Siliziumdioden eine maximale Sperrspannung von nur wenigen Kilovolt haben und es bei Überschreitung dieser Spannung zum Durchbruch kommt. Die Herstellung von Halbleiterbauteilen mit Sperrspannungen von mehr als einigen Kilovolt ist nicht möglich.
Ulrich Tietze, Christoph Schenk: Halbleiter-Schaltungstechnik. 12. Auflage, Springer, Berlin 2002, ISBN 3-540-42849-6.
Otmar Kilgenstein: Schaltnetzteile in der Praxis. 3. Auflage, Vogel, Würzburg 1992, ISBN 3-8023-1436-0.
Ulrich Nicolai, Tobias Reimann, Jürgen Petzoldt, Josef Lutz: Applikationshandbuch IGBT- und MOSFET-Leistungsmodule. Isle, Ilmenau 1998, ISBN 3-932633-24-5.

Die Gleithörnchen (Pteromyini, aus griechisch πτερο- ptero- ‚Flügel-‘, und μῦς mys ‚Maus‘) sind eine Tribus der Hörnchen (Sciuridae). Zwischen ihren Vorder- und Hinterbeinen spannt sich eine Gleithaut, die wie ein Gleitschirm wirkt, wenn sie von einem Ast springen. Obwohl sie nicht fliegen können, werden sie auch Flughörnchen genannt.
Die Gleithaut wird an der Handwurzel von einem sichelförmigen Knochen gespannt; von hier reicht sie zum Fußgelenk des Hinterbeins. Der Schwanz ist immer lang, breit und buschig und dient als Steuer. Auf diese Weise können Gleithörnchen Strecken von bis zu 50 m zurücklegen. Bei den Riesengleithörnchen wurden im Einzelfall und unter günstigen Bedingungen sogar 450 m gemessen. Vor der Landung drehen Gleithörnchen ihre Körperachse, so dass sie senkrecht zum Boden stehen und mit Hilfe der weit gespreizten Gleithaut abbremsen. Mit weit gekrümmtem Körper und abstehendem Schwanz landen die Gleithörnchen mit allen vieren auf der Unterlage, die ihr Ziel ist. Gleithörnchen können mit Hilfe ihres Schwanzes sogar im Flug die Richtung ändern.
Die Gleitfähigkeit hat sich unter den Säugetieren mehrmals unabhängig voneinander entwickelt. Es gibt weitere Gruppen gleitfähiger Tiere, die mit den Gleithörnchen nicht verwandt sind. Zu diesen Tieren, die in konvergenter Evolution ganz ähnliche Lebensweisen wie die Gleithörnchen entwickelt haben, zählen die Gleitbeutler, die Zwerggleitbeutler, die Riesengleitbeutler, die Riesengleiter und die Dornschwanzhörnchen.
Alle Gleithörnchen haben kräftige Krallen, die sie zum sicheren Klettern befähigen. Sie haben an den Vorderbeinen vier und an den Hinterbeinen fünf Zehen. Der Kopf ist rund und niemals spitz zulaufend. Die großen Augen zeugen von der nachtaktiven Lebensweise.
Gleithörnchen sind Waldbewohner. Man trifft sie niemals in offenen Landschaften, da sie hier ihre Gleitfähigkeit nicht ausnutzen könnten. Meistens sind sie dämmerungs- und nachtaktiv. Sie klettern rasch in den Bäumen, können aber keine weiten Sprünge wie die Baumhörnchen vollführen; auch am Boden sind sie sehr ungelenk, da die Gleithaut bei den Bewegungen behindert. Die Nahrung besteht wie auch bei anderen Hörnchen aus Nüssen und Früchten, nebenbei werden auch Insekten gefressen.
Ein Nest wird meistens in einer Baumhöhle, gelegentlich auch im Geäst gebaut. In diesem Nest schlafen sie und ziehen ihre Jungen auf. Die Lebensdauer kann bei manchen Arten 13 Jahre betragen, ist aber meistens kürzer. Durch ihre Gleitfähigkeit gelingt es Gleithörnchen oft, baumbewohnenden Räubern wie Mardern zu entkommen, allerdings sind sie gegenüber Greifvögeln und Eulen im Nachteil. In Südostasien hat sich die Maskeneule regelrecht auf die Jagd auf Gleithörnchen spezialisiert.
Obwohl Gleithörnchen auch in Nordamerika, Europa und Nordasien leben, liegt der Schwerpunkt ihres Verbreitungsgebiets eindeutig auf Ost- und Südostasien. Vor allem in der indonesischen Inselwelt gibt es einen bemerkenswerten Artenreichtum; viele dieser Arten sind kaum erforscht.
Die Zugehörigkeit der Gleithörnchen zur Familie der Hörnchen ist unbestritten. In älteren Systematiken wurden sie als Unterfamilie geführt, in der sie allen anderen Hörnchen gegenübergestellt wurden, die als Sammelgruppe der Erd- und Baumhörnchen (Sciurinae) vereint werden. Während selbst manche Kladistiker es lange für möglich hielten, dass beide Taxa monophyletische Schwestergruppen seien, wurde dies von anderen bezweifelt. Steppan, Storz und Hoffmann kamen 2003 in ihren DNA-Analysen zu dem Schluss, dass die Erd- und Baumhörnchen ein paraphyletisches Taxon seien und die Gleithörnchen aus diesen hervorgegangen sein müssten. Gleithörnchen und Baumhörnchen (einschließlich der Rothörnchen) bilden ihren Untersuchungen zufolge eine gemeinsame Klade; hierin sind die Gleithörnchen die Schwestergruppe der Baumhörnchen. Die Monophylie der Gleithörnchen wurde in der Studie belegt.
IomysDie Beziehungen der Gattungen untereinander waren weitgehend unbekannt, bis Thorington, Pitassy und Jansa 2002 ihre umfangreichen phylogenetischen Analysen veröffentlichten. Nach dieser Untersuchung lassen sich die Gleithörnchen in vier Kladen gruppieren, die als Trogopterus-Gruppe, Petaurista-Gruppe, Hylopetes-Petinomys-Gruppe und Glaucomys-Gruppe benannt wurden. Das folgende Kladogramm stellt die Beziehungen zwischen den Gruppen und Gattungen dar:
Die Gattung Biswamoyopterus wurde in die Untersuchungen nicht einbezogen, sodass ihre Stellung in diesem System unklar ist.
Die älteste fossile Gleithörnchen-Gattung Oligopetes lebte bereits im frühen Oligozän in Europa. Allerdings wurde dieser Fund nur aufgrund dentaler Merkmale den Gleithörnchen zugeordnet, so dass manche Paläontologen anzweifeln, ob es sich tatsächlich um ein echtes Gleithörnchen gehandelt hat.
Zweifelsfrei belegt sind Gleithörnchen seit dem Miozän aus Eurasien und Nordamerika. Ausgestorbene Gattungen der Gleithörnchen sind:
Petauria, Pleistozän, EuropaDaneben sind auch manche der rezenten Gattungen fossil nachgewiesen. Die älteste der heute lebenden Gattungen scheint Hylopetes zu sein, die seit dem Miozän in Asien nachgewiesen ist. Die Gattung der Echten Gleithörnchen ist fossil seit dem Pliozän bekannt.
Der gültige wissenschaftliche Name der Gleithörnchen ist Pteromyini. Gelegentlich trifft man auch auf die Bezeichnung Petauristinae. Diese wurde in den 1940ern geprägt, als man Pteromys für ein Synonym von Petaurista hielt und so folgerichtig die ganze Gruppe umbenannte. Inzwischen besteht kein Zweifel mehr an der Gültigkeit des Namens Pteromys, so dass der von Johann Friedrich von Brandt geprägte Name Pteromyini verwendet werden sollte.
Michael D. Carleton, Guy G. Musser: Order Rodentia. In: Don E. Wilson, DeeAnn M. Reeder (Hrsg.): Mammal Species of the World. A taxonomic and geographic Reference. Band 2. 3. Auflage. Johns Hopkins University Press, Baltimore MD 2005, ISBN 0-8018-8221-4, S. 745–1600.
Bernhard Grzimek: Grzimeks Tierleben. Band 11: Säugetiere. Teil 2. Bechtermünz, Augsburg 2000, ISBN 3-8289-1603-1.
Malcolm C. McKenna, Susan K. Bell: Classification of Mammals. Above the Species Level. Columbia University Press, New York NY 1997, ISBN 0-231-11013-8 (Revised Edition. ebenda 2000, ISBN 1-82575-570-1).
Ronald M. Nowak: Walker's Mammals of the World. 2 Bände. 6. Auflage. Johns Hopkins University Press, Baltimore MD u. a. 1999, ISBN 0-8018-5789-9.
Scott J. Steppan, Brian L. Storz, Robert S. Hoffmann: Nuclear DNA phylogeny of the squirrels (Mammalia: Rodentia) and the evolution of arboreality from c-myc and RAG1. In: Molecular Phylogenetics and Evolution. Bd. 30, Nr. 3, 2004, ISSN 1095-9513, S. 703–719, doi:10.1016/S1055-7903(03)00204-5.
Richard W. Thorington Jr, Dian Pitassy, Sharon A. Jansa: Phylogenies of Flying Squirrels (Pteromyinae). In: Journal of Mammalian Evolution. Bd. 9, Nr. 1/2, 2002, S. 99–135, doi:10.1023/A:1021335912016.

Gleitschirme, auch Paragleiter oder Gleitsegel sind Luftsportgeräte zum Gleitsegeln oder Gleitschirmfliegen, beziehungsweise Paragleiten.
In Deutschland gehören Gleitschirme luftrechtlich zur Luftfahrzeugklasse der Luftsportgeräte und stellen dort die eigene Ordnung der Gleitsegel. Gleitsegel mit Motor (siehe Motorschirme) sind ebenfalls Luftsportgeräte, fallen aber in die Ordnung und unter die Bestimmungen der Ultraleichtflugzeuge.
Die ersten Ideen zu einem Fluggerät, das komplett aus Textilien besteht, wurden bereits 1948 vom späteren NASA-Ingenieur Francis Rogallo in einem Patent skizziert. Dieses beschreibt „nach vorne offene Stoffröhren, parallel nebeneinander angeordnet und durch den Fahrtwind aufgeblasen, eine Tragfläche bildend“. Konkrete Umsetzungen dieser Idee durch Rogallo sind jedoch nicht bekannt. Erst in den Jahren 1991–1996 wurde im Projekt Spacewedge der Einsatz von Gleitschirmen zur gesteuerten Landung der Rückkehrkapseln von Raumfahrzeugen experimentell untersucht.
Heutige Gleitschirme beruhen jedoch aufgrund der Geschichte des Gleitsegelfliegens und der dabei eingesetzten Schirmarten ebenso wie die heute im Fallschirmsport üblichen Fallschirme auf dem zweiflächigen mehrzelligen Parafoil-Fallschirmkonzept von Domina Jalbert. Fallschirme und Gleitschirme haben sich mittlerweile aufgrund aerodynamischer und technischer Anpassungen an die speziellen Anforderungen der jeweiligen Sportart so weit auseinanderentwickelt, dass ein Fallschirm für Bergstarts heute grundsätzlich ebenso ungeeignet ist wie ein Gleitschirm für Fallschirmabsprünge.
Die jüngste Entwicklung im Gleitschirm-Bereich repräsentiert das Speed Flying, bei dem die Fläche der Schirme stark verkleinert wurde, um dadurch eine größere Geschwindigkeit erreichen zu können.
Ein Gleitschirm besteht aus einer Schirmkappe, Leinen und Tragegurten. Zusätzlich wird zum Gleitschirmsegeln ein Gurtzeug benötigt.
Die Kappe, auch Kalotte genannt, ist eine näherungsweise elliptische Tragfläche aus Nylon-Stoff, der durch die sogenannte Ripstop-Technik besonders reißfest gewebt ist. Um die Luftdurchlässigkeit zu verringern, ist das Gewebe zusätzlich beschichtet. Die Beschichtung schützt zudem das Material gegen mechanische Beanspruchung und die durch UV-Licht bedingte Alterung.
Die Kappe besteht aus einem Ober- und einem Untersegel und ist in zahlreiche Kammern in Längsrichtung unterteilt. Die hintere Seite einer solchen Kammerzelle ist zugenäht, an der Vorderseite befindet sich die Eintrittskante, durch die die einzelnen Kammern beim Aufziehen des Gleitschirms mit Luft gefüllt werden. Durch den entstehenden Staudruck wird die Kappe versteift, so dass ein möglichst optimales Flügelprofil entsteht, an dem eine Luftströmung anliegt und dynamischen Auftrieb erzeugt. Nebeneinanderliegende Kammerzellen sind durch Öffnungen, die Cross-Ports, miteinander verbunden und gleichen den Staudruck innerhalb der gesamten Kappe aus. Je breiter die Kappe bei abnehmender Tiefe wird (zunehmende Streckung), desto aggressiver ist ihr Flugverhalten und desto anspruchsvoller ist sie zu fliegen. Eine hohe Streckung bringt eine erhöhte Leistungsfähigkeit mit sich, was vor allem im Wettkampfsport genutzt wird.
Die von der Segelunterseite in mehreren Ebenen herablaufenden Galerieleinen werden zu Stammleinen zusammengeführt, die wiederum in Leinenschlösser eingehängt und mit dem jeweiligen linken und rechten Gurtband verbunden sind. Über diese Gurtbänder, auch Tragegurte genannt, wird der Gleitschirm mittels Karabinern mit dem Gurtzeug des Piloten verbunden. Die hinterste Leinenebene ist nicht fest mit den Tragegurten verbunden, sondern wird gesondert auf beiden Seiten zu je einer Bremsleine zusammengeführt, die der Steuerung dient.
Als Fangleinen werden zumeist Aramid- oder Dyneema-Leinen mit einem Kerndurchmesser von ca. 0,6 bis 2,0 mm verwendet. Leinen aus diesen Kunstfasern haben trotz geringem Durchmesser eine hohe Reißfestigkeit. Bei den meisten Seriengeräten sind diese Leinenkerne durch eine weitere Textilummantelung vor UV-Licht und mechanischer Beanspruchung geschützt. Bei Wettkampfschirmen wird zugunsten des geringeren Luftwiderstandes häufig auf die Ummantelung verzichtet.
Das Gurtzeug ist ein Sitz, der den Gleitschirm mit dem Piloten verbindet und damit kein eigentlicher Bestandteil des Gleitschirms. Über Karabiner werden die Tragegurte, die die Stammleinen des Gleitschirms bündeln, in das Gurtzeug eingehakt. Der Pilot ist mit Bein- und Brustgurten am Gurtzeug angeschnallt. Mit dem Gurtzeug verbunden bzw. darin integriert sind der Rettungsfallschirm und das Beschleunigersystem. Ebenso sind bei aktuellen Gurtzeugen Protektoren zum Schutz der Wirbelsäule und des Beckens eingearbeitet, die den Piloten im Falle eines Unfalls schützen.
Gesteuert wird der Gleitschirm durch je eine Bremsleine auf der linken und rechten Seite, deren Galerieleinen (Bremsspinne) die Hinterkante des Gleitschirms herunterziehen und so das Profil ähnlich einem nach unten ausschlagenden Querruder verändern: eine Erhöhung sowohl des Auftriebs als auch des Widerstands ist die Folge. Im Gegensatz zu einem Querruder sind hier aber die beiden Bremsleinen nicht gekoppelt; sie können sowohl gleichsinnig als auch gegensinnig betätigt werden und dienen so zur Steuerung um zwei Achsen.
Die Geschwindigkeit des Gleitschirms wird durch gleichsinnige Betätigung der Bremsleinen im Bereich zwischen Trimmgeschwindigkeit und Stall gesteuert. Ergänzt wird dies durch den Beschleuniger: ein mit dem Fuß betätigter Mechanismus zur Verkürzung der vorderen Leinenebenen, der den Anstellwinkel des ganzen Schirms verringert und so eine Erhöhung der Geschwindigkeit über die Trimmgeschwindigkeit hinaus ermöglicht.
Für den Kurvenflug werden die Bremsleinen gegensinnig betätigt: Die Erhöhung des Widerstands bewirkt eine Drehung des Schirms um die Hochachse nach der stärker gebremsten Seite und somit die Einleitung der Kurve. Die "Ohren" (die senkrecht stehenden Enden) des Schirms verhindern dabei weitgehend ein seitliches Schieben. Der tief hängende Schwerpunkt sorgt durch seine Fliehkraft passiv (trotz der eigentlich gegensinnigen Querruderwirkung) für die passende Seitenneigung zum ausgeglichenen Kurvenflug. Zusätzlich kann der Pilot auch durch seitliche Gewichtsverlagerung im Gurtzeug den Kurvenflug einleiten bzw. unterstützen.
Im Gegensatz zu Starrflügel-Flugzeugen wird der Gleitschirm um Längs- und Querachse nicht aktiv dynamisch, sondern statisch durch den tief hängenden Schwerpunkt stabilisiert. Das vereinfacht einerseits das Steuern in ruhiger Luft, andererseits neigt ein solches System in Turbulenzen zum gefährlichen Aufschaukeln um Längs- und Querachse, dem der Gleitschirmpilot aktiv entgegensteuern muss.
Die Kappe von einsitzigen Gleitschirmen hat eine ausgelegte Fläche von ca. 20 bis 35 m² und eine Spannweite von 10 bis 13 m. Gleitschirme für den Tandemflug haben eine Fläche von bis zu 43 m².
Gleitschirmmodelle werden in mehreren Größen für verschiedene Gewichtsbereiche angeboten. Das minimal und maximal zulässige Fluggewicht ist bei zugelassenen Geräten vom Hersteller vorgegeben. Es liegt zwischen 55 kg bei Schirmen mit kleiner Fläche und endet bei einsitzigen Gleitschirmen bei ca. 130 kg, Tandemschirme können bis zu 250 kg in die Luft bringen. Das Fluggewicht berücksichtigt das Gewicht des Piloten (inkl. Bekleidung), Gurtzeug, Gleitschirmkappe, Rettungsgerät, Packsack und sonstige mitgeführten Dinge. Für die Ausrüstung ist je nach Einsatzgebiet mit etwa 5–25 kg zu rechnen.
Bei maximaler Zuladung eines Gleitschirms wird die höhere Flächenbelastung (kg/m²) in Vorwärtsgeschwindigkeit umgesetzt bei gleichzeitig zunehmender Sinkgeschwindigkeit. Der Gleitschirm zeigt nun ein dynamischeres Flugverhalten, das sich unter anderem in einem besseren Steuerverhalten zeigt. Steuerimpulse werden schneller in Richtungsänderungen umgesetzt. Vor allem beim Gleitschirm-Akro ist diese Eigenschaft sehr erwünscht, so dass diese Piloten vielfach am oberen Ende des Gewichtsbereichs oder gar über dem Gewichtslimit fliegen.
Beim Minimalgewicht folgt daraus eine kleine Flächenbelastung des Schirms. Der Gleitschirm hat zwar eine geringere Sinkgeschwindigkeit aber auch eine geringere Vorwärtsgeschwindigkeit und zeigt eher eine instabile Flugbahn, die ständig mittels Steuerimpulsen stabilisiert werden muss.
Gleitschirme müssen in Deutschland und Österreich mustergeprüft sein, ihre Lufttüchtigkeit muss von zuständiger Stelle nachgewiesen worden sein. In der Schweiz ist das nur für Gleitschirme, die während der Ausbildung und an Prüfungen verwendet werden, nötig. In vielen anderen Ländern besteht diese Verpflichtung nicht.
Gleitschirme gehören in Deutschland üblicherweise zu den nichtzulassungspflichtigen Luftsportgeräten nach § 11 LuftGerPV, für die statt der Zulassung eine Musterprüfung durch eine zuständige Stelle erfolgt. Für (nur) musterprüfungspflichtige Luftsportgeräte nach § 2 Abs. 2 LuftGerPV ist für die Lufttüchtigkeitsprüfung der Hersteller zuständig. Der jeweilige Halter des Luftsportgeräts hat gem. § 14 Abs. 5 LuftGerPV Mängel unverzüglich zu melden. Nach § 2 Nr. 1 LFBAG trifft das Luftfahrtbundesamt die Aufgabe zur Prüfung und Überwachung der Prüfungen zur Feststellung der Verkehrssicherheit (Lufttüchtigkeit) des Luftfahrtgerätes nach der Prüfordnung für Luftfahrtgerät. Kommt der Hersteller seiner Aufgabe gem. § 2 Abs. 2 LuftGerPV nicht ordnungsgemäß nach, ist deshalb das Luftfahrtbundesamt für Lufttüchtigkeitsanweisung nach § 14 LuftBO zuständig. Der Hersteller führt vor Auslieferung des Luftsportgerätes eine Stückprüfung aus. In dieser wird vom Hersteller bestätigt, dass das ausgelieferte Muster dem geprüften Muster entspricht.
Im Rahmen der Musterprüfung werden Gleitschirme verschiedenen Belastungstests ausgesetzt. Weiter werden die Schirme in Klassen eingeteilt, die unterschiedliche Ansprüche an das Pilotenkönnen stellen. Dabei werden von speziell ausgebildeten Testpiloten verschiedene vordefinierte Flugsituationen forciert und die Reaktion des Schirmes darauf geprüft. Diese Klassifizierung der Fluggeräte ist zwar nur in wenigen Ländern Pflicht, hat sich aber als Geräteeinstufung in vielen Ländern durchgesetzt.
Ähnlich wie Automobile müssen in einigen Ländern (z. B. Deutschland und Österreich) Gleitschirme in regelmäßigen Abständen überprüft werden. Das Nachprüfintervall beträgt üblicherweise zwei Jahre und wird vom Hersteller individuell festgelegt. Hierbei wird das Fluggerät auf Beschädigungen, erforderliche Luftundurchlässigkeit des Tuchs sowie Länge und Festigkeit der Leinen überprüft. Auch Gurtzeuge und Rettungsgeräte unterliegen dieser Nachprüfpflicht.
In Deutschland ist nach den vom Hersteller vorgegebenen Anweisungen durch den Halter oder in dessen Auftrag die Lufttüchtigkeit nachzuprüfen oder nachprüfen zu lassen. Der Halter ist für die rechtzeitige und vollständige Durchführung der Prüfungen verantwortlich (§ 14 LuftGerPV).
In der Schweiz ist ein solcher Test nicht vorgeschrieben und es liegt alleine in der Verantwortung des Piloten, sein Fluggerät in Stand zu halten. Auch das Mitführen eines Notschirms und das Tragen eines Helms ist dort gesetzlich nicht vorgeschrieben.
Zur Klassifizierung von Gleitschirmen werden von Testpiloten verschiedene Flugmanöver erflogen und mit Noten von A (einfach) bis D (anspruchsvoll) bewertet. Dies erfolgt jeweils im beschleunigten und unbeschleunigten Flugzustand sowie mit minimaler und maximaler Anhängelast des Schirms. Die jeweils höchste Note bestimmt die Gesamtklassifizierung. Für den Schulungsbetrieb in Deutschland und Österreich sind seit 2015 lediglich Schirme der Kategorie A zulässig.
Die neuen Klassen, die im Jahre 2010 eingeführt wurden, entsprechen der europäische Norm EN 926-2E an.
EN/LTF A Gleitsegel mit einem Maximum an passiver Sicherheit und einem extrem verzeihenden Flugverhalten. Gute Widerstandsfähigkeit gegen abnormale Flugzustände. Geeignet für Piloten aller Ausbildungsstufen.EN/LTF B Gleitsegel mit guter passiver Sicherheit und verzeihendem Flugverhalten. Einigermaßen widerstandsfähig gegen abnormale Flugzustände. Geeignet für Piloten aller Ausbildungsstufen. In der Praxis ist diese Klasse in "Low B" und "High B" unterteilt. Während Schirme am unteren Ende dieser Klasse durchaus als erstes Gerät nach der Ausbildung taugen, reizen "High B" die Begrenzungen dieser Klasse aus und erkaufen sich die höhere Leistung mit einem Extremflugverhalten, das von dem der C-Klasse nur noch unwesentlich abweicht.EN/LTF C Gleitsegel mit mäßiger passiver Sicherheit und mit potenziell dynamischen Reaktionen auf Turbulenzen und Pilotenfehler. Die Rückkehr in den Normalflug kann präzisen Piloteneingriff erfordern. Für Piloten, die das Ausleiten abnormaler Flugzustände beherrschen, die aktiv und regelmäßig fliegen und die die möglichen Konsequenzen des Fliegens mit einem Gleitsegel mit reduzierter passiver Sicherheit verstehen.EN/LTF D Gleitsegel mit anspruchsvollem Flugverhalten und potenziell heftigen Reaktionen auf Turbulenzen und Pilotenfehler. Die Rückkehr in den Normalflug erfordert präzisen Piloteneingriff. Für Piloten, die über viel Übung im Ausleiten abnormaler Flugzustände verfügen, die sehr aktiv fliegen, die signifikante Erfahrungen in turbulenten Bedingungen gesammelt haben und die die möglichen Konsequenzen des Fliegens mit einem solchen Gleitsegel akzeptieren.Die Lufttüchtigkeitsforderungen vor 2010 sind immer noch relevant, da diese Geräte weiterhin verwendet werden. Zum Vergleich:
LTF 1 Für Anfänger geeignete Schirme, Gleitschirme mit einfachem, weitgehend fehlerverzeihendem Flugverhalten.
LTF 1-2  Gleitschirme mit gutmütigem Flugverhalten. Die meisten Gleitschirme werden von den Herstellern für diese Klasse konzipiert. Wobei diese größte Klasse zweigeteilt ist in Schulungstaugliche und die eher sportlichen.
LTF 2 Gleitschirme mit anspruchsvollem Flugverhalten und dynamischen Reaktionen auf Störungen und Pilotenfehler.
LTF 2-3 Gleitschirme mit sehr anspruchsvollem Flugverhalten und heftigen Reaktionen auf Störungen und geringem Spielraum für Pilotenfehler, setzt längere Erfahrung und regelmäßige Flugpraxis voraus.
LTF 3 Gleitschirme mit sehr anspruchsvollem Flugverhalten und sehr heftigen Reaktionen auf Störungen und geringem Spielraum für Pilotenfehler. Für Piloten mit überdurchschnittlich hohem Pilotenkönnen.Bis zum Jahr 2009 war für die Lufttüchtigkeitsforderungen LTF umgangssprachlich die Kategoriebezeichnung „DHV“ üblich, da es nur eine gleichnamige Prüfstelle für den deutschsprachigen Raum gab. Ab 2008 kam zunächst die Prüfstelle der deutschen EAPR GmbH und ab 2011 der Schweizer Firma Air Turquoise hinzu.
Geräte der oberen Klassen (EN/LTF C bis D, vormals LTF 2-3, 3) sind nur besonders routinierten Piloten zu empfehlen. Wettkampfschirme fliegen in der außerhalb dieses Systems stehenden CCC-Klasse (CIVL Competition Class), die bei den Testmanövern von angemessenem Pilotenkönnen ausgeht und die maximal erreichbare Luftgeschwindigkeit auf 65 km/h limitiert.
Die Klassifizierung richtet sich ausschließlich nach der Flugsicherheit und nicht nach Leistungsmerkmalen. Es ist durchaus normal, dass sogenannte Einsteigerschirme Turbulenzen in der Luft durch Dämpfung „vernichten“, während Gleitschirme einer höheren Klasse hierbei kaum Höhenverlust zeigen, aber dafür anfälliger für Einklapper sind. Höher eingestufte Gleitschirme verfügen in der Regel über bessere Leistungsmerkmale wie besseres Gleiten und höhere Maximalgeschwindigkeit.
Ein alternatives Prüfungsverfahren für Gleitschirme ist die Prüfung nach AFNOR (Association française de Normalisation). Hier werden Gleitschirme unterteilt in die Klassen Standard, Performance und Competition. Diese Zertifizierung ist ähnlich der oben beschriebenen nach EN/LTF. Sie ist vor allem in der französisch- und englischsprachigen Fliegerwelt verbreitet.
Die AFNOR-Prüfung sollte im Laufe des Jahres 2006 durch die europäische CEN-Norm ersetzt werden. Verantwortliche Ausbildungsleiter von registrierten Flugschulen nach § 33 LuftVZO in Deutschland akzeptieren gelegentlich keine Musterprüfungen von nicht akkreditierten Musterprüfstellen, wenn nicht in jedem Fall eine Herstellererklärung über die Lufttüchtigkeit vorliegt. Das Fliegen einer offenen Klasse wäre sonst auch zu keinem Zeitpunkt in Deutschland legal möglich.
Ein moderner Gleitschirm hat einen Geschwindigkeitsbereich von ca. 22 bis 55 km/h, wobei die Trimmgeschwindigkeit, also die Geschwindigkeit bei offener Bremse und meist auch bestem Gleiten, zwischen 32 und 40 km/h liegt. Durch gleichzeitiges Ziehen beider Steuerleinen wird das Segel an der Hinterkante nach unten gewölbt. Hierdurch lässt sich die Fluggeschwindigkeit auf etwa 22 bis 25 km/h vermindern. Die geringste Sinkrate haben Gleitschirme etwa bei 25 bis 35 km/h. Durch Treten des Fußbeschleunigers wird der Anstellwinkel des Gleitschirms verkleinert, dadurch kann der Gleitschirm um bis zu 20 km/h beschleunigt werden. Wettkampfschirme fliegen im beschleunigten Zustand sogar bis zu 75 km/h.
Diese Geschwindigkeiten beziehen sich auf die umgebende Luft, die TAS (true air speed). Die Geschwindigkeit über Grund, die GS (ground speed), ist von den Luftbewegungen wie Gegenwind oder Rückenwind abhängig.
Die Gleitleistung moderner Gleitsegel der Serienklasse liegt bei etwa 1:9,5. Das heißt, ein Meter Höhe wird in 9,5 Meter horizontale Strecke umgesetzt. Wettkampfschirme erreichen ein Gleitverhältnis von 1:10,5, liegen damit aber deutlich hinter Hängegleitern (bis zu 1:20 – Starrflügler) und Segelflugzeugen (bis zu 1:70). Die minimale Sinkgeschwindigkeit liegt bei 1,0 m/s, im Trimmflug ist diese um ca. 0,3 m/s höher. Die Länge aller Leinen zusammengezählt, spielt eine große Rolle für den Luftwiderstand. Gurtzeug und der Pilot selbst können durch ihr Volumen und ihre Haltung gegenüber dem Luftstrom die Gleitleistung stark beeinflussen.
Die Gleitleistung spielt beim Fliegen in der Thermik eine untergeordnete Rolle. In steigender Luft ist es unwesentlich, ob man nach neun oder elf Kreisen 500 Meter Höhe gewonnen hat. Für Talquerungen mit Gegenwind oder Streckenflügen mit Auf- und Abwind spielt es hingegen eine Rolle, ob man nach zwei Kilometern Flug hundert Meter Höhe mehr verloren hat oder den Anschluss an die nächste Thermik noch erreicht.
Flüge von über 100 Kilometern sind unter Ausnutzung von Thermik nicht ungewöhnlich. Es wurden schon Distanzen von über 500 Kilometern geflogen (siehe Streckenfliegen).
In einigen Ländern wie Deutschland, Österreich und der Schweiz ist es grundsätzlich vom Gesetzgeber vorgeschrieben, vor dem selbstständigen Fliegen eine Schulung zu absolvieren. In anderen Ländern wie beispielsweise Frankreich besteht für das Gleitschirmfliegen keine Scheinpflicht. Der Gleitschirmsport wird in der Regel nicht als Risikosportart eingestuft. Um Gefahrensituationen entgegenzutreten, ist es wichtig, sich über die Wettersituation und die Eigenheiten des Fluggeländes zu informieren. Je nach Wetterlage sind bestimmte Fluggebiete vorzuziehen oder sogar zu meiden. Ebenso gibt es Regeln während des Fluges zu beachten (zum Beispiel Vorflugregeln oder das Verhalten in der Thermik), außerdem sind bestimmte Abläufe aus Sicherheitsgründen vorgegeben (zum Beispiel der 5-Punkte-Check).
Bei Windgeschwindigkeiten ab ca. 30 km/h kommt man an die Eigengeschwindigkeit normaler Gleitschirme (unbeschleunigt). Ein Zunehmen des Windes hat einen Rückwärtsflug (relativ zum Erdboden) zur Folge.
Wenn Wind frontal auf einen Hang trifft, wird die Luftströmung in der Regel nach oben abgelenkt. Dieser sogenannte dynamische Aufwind kann zum Höhengewinn genutzt werden. Kräftiger Wind kann insbesondere in Bodennähe starke Turbulenzen mit sich bringen, weshalb ein Mindestabstand zum Geländerelief empfohlen wird. Im Lee von Hügeln, Anhöhen oder anderen Hindernissen können sich Rotoren bilden, die zu Turbulenzen und starkem Höhenverlust führen können. Diese Bereiche sind beim Gleitschirmfliegen zu meiden.
Neue Gleitschirme kosten zwischen 1.600 und 4.000 Euro, zusammen mit dem Rest einer Ausrüstung ist mit circa 2.500 bis 5.000 Euro zu rechnen. Gebrauchte noch lufttüchtige Gleitschirme werden je nach Alter und Zustand ab etwa 500 Euro gehandelt.
In Deutschland und Österreich müssen Gleitschirme alle zwei Jahre nach strikten Vorgaben durch den Hersteller oder eine autorisierte Prüfungsstelle geprüft werden. Dabei werden die Leinenlängen, die Luftdurchlässigkeit des Tuchs, alle Nähte etc. auf Verschleiß oder Alterung geprüft.
In anderen Ländern (CH/FR) ist die Prüfung freiwillig. Eine Korrelation zwischen Prüfungspflicht und Unfallzahlen ist nicht veröffentlicht.
Gleitschirme werden aus leichten Stoffen genäht, ihnen fehlt somit die Widerstandskraft unflexibler Fluggeräte. Schäden durch Berührungen mit der Vegetation, zum Beispiel einem Dornenbusch oder Stacheldrahtzaun, sind leicht möglich.
Üblicherweise werden kleinere Risse nach Herstelleranleitung mit Ripstop-Aufklebern behoben. Bei Leinenrissen oder Leinenbeschädigungen hingegen müssen die entsprechenden Leinen ersetzt werden, da die millimetergenaue Leinenlänge die Trimmung und damit unmittelbar die Flugsicherheit beeinflusst.
Toni Schlager, Gleitschirmfliegen, Das Praxisbuch für Anfänger und Profis, Bruckmann Verlag, München 2006, ISBN 3-7654-4503-7 (Flugpraxis, Theorie, Streckenfliegen, Groundhandling, Windenschlepp, Luftrecht D,A,CH)
Peter Janssen, Karl Slezak, Klaus Tänzler: Gleitschirmfliegen, Theorie und Praxis, 15. aktualisierte Auflage, Nymphenburger Verlag, München 2007, ISBN 978-3-485-01111-2
Thomas Ulrich, Rasso Knoller, Claudia Frühwirth: Gleitschirmfliegen, Steiger Verlag, Augsburg 1999, ISBN 3-89652-166-7
Carsten Peter, Toni Schlager: Gleitschirmfliegen, vom Anfänger zum Profi Bruckmann Verlag, München 2003, ISBN 3-7654-3834-0 (mit Flugpraxis und Theorie)
Burkhard Martens: Das Thermikbuch für Gleitschirm- und Drachenflieger, 1. Auflage 2005, Eigenverlag, ISBN 3-00-015761-1
Burkhard Martens: Das Streckenflugbuch für Gleitschirm- und Drachenflieger, 1. Auflage 2007, Eigenverlag, ISBN 978-3-00-020067-0
free.aero, Digitales Magazin für Gleitschirm- und Motorschirmpiloten, Verlag voler.info, F-St.Pierre
P@r@2000 – Überblick über die meisten aktuellen und historischen Gleitschirmflügel (in franz., engl.)

Als Gletscherdynamik bezeichnet man das Bewegungsverhalten von Gletschern, Eiskappen und Eisschilden sowie deren physikalische Beschreibung. Verantwortlich für die beobachteten Bewegungen ist das Eigengewicht des Gletschers. Zum einen sorgt es für eine Verformung des Eises, das sich wie eine sehr viskose Flüssigkeit verhält (interne Deformation). Zum anderen kann sich der Gletscher als Ganzes auf seinem Felsbett bewegen (basales Gleiten) oder das Felsbett selbst durch das hohe aufliegende Gewicht deformieren, was wiederum zur Beschleunigung des Gleitprozesses führt (interne Deformation).
Die Geschwindigkeit, mit der sich Gletscher bewegen, reicht von wenigen Metern bis zu einigen Kilometern pro Jahr und wird von einer Vielzahl von Faktoren beeinflusst, unter anderem der Hangneigung, der Beschaffenheit des Felsbettes und der Temperatur. Auch innerhalb eines Gletschers ist die Geschwindigkeit nicht homogen. Im Akkumulationsgebiet (Nährgebiet) des Gletschers nimmt sie im Allgemeinen zu, im Ablationsgebiet (Zehrgebiet) dagegen wieder ab, die oberen Schichten des Eises bewegen sich zudem schneller als das Eis nahe dem Felsbett.
Mathematisch kann das Fließen der Gletscher mit Methoden der Kontinuumsmechanik beschrieben werden, indem durch ein Fließgesetz die Verformungsrate des Eises mit der Spannung in Beziehung gesetzt wird. Historisch entwickelte sich das Studium der Gletscherdynamik im achtzehnten und neunzehnten Jahrhundert durch Beobachtung der alpinen Gletscher. Erst im zwanzigsten Jahrhundert rückten die großen arktischen und antarktischen Eisschilde in den Mittelpunkt der Forschung, nicht zuletzt wegen ihrer zentralen Rolle für das Klimasystem der Erde, zum Beispiel aufgrund ihrer Albedowirkung oder ihres Einflusses auf den Meeresspiegel.
Gletscher wurden schon in Strabons Geographie behandelt, in neuzeitlichen geographischen Werken erstmals bei Sebastian Münster in seiner Cosmographia. Erwähnung fanden Gletscher auch in mittelalterlichen Urkunden, etwa zur Bezeichnung von Grenzen. In Tirol geschah dies beispielsweise erstmals in einer Schenkungsurkunde von 1260. Ein wissenschaftlicher Diskurs über die Bewegungen der Gletscher entwickelte sich jedoch erst ab dem 18. Jahrhundert. Noch im 17. Jahrhundert glaubte die einheimische Bevölkerung der Schweizer Alpen, die Gletscher wüchsen von unten nach oben den Berg hinauf. Diese Ansicht vertraten auch mehrere zeitgenössische Wissenschaftler wie zum Beispiel Johann Gottfried Gregorius in seiner Spezialenzyklopädie Beschreibung der berühmtesten Berge in alphabetischer Ordnung von 1715.Bernhard Friedrich Kuhn war 1787 einer der Ersten, der die Bewegungen der Gletscher physikalisch zu erklären versuchte. Er nahm an, dass durch die Sonne erwärmtes Geröll unter den Gletscher gelangt, Eis zum Schmelzen bringt und die Stabilität des Gletschers vermindert. Sobald so viel Wasser geschmolzen sei, dass das Eis keinen Kontakt mehr zum Felsbett hat, beginne sich der Gletscher anschließend als Ganzes talwärts zu bewegen. Auch wenn seine Theorie der Gletscherbewegungen eher zu den schwächeren Teilen seiner Arbeit gezählt wird, leistete er dennoch einen bemerkenswerten Beitrag zur Glaziologie, denn er stellte einen Zusammenhang zwischen Moränen und Änderungen der Massenbilanz her und postulierte, dass den Bewegungen alpiner Gletscher und arktischer Eiskappen der gleiche Mechanismus zu Grunde läge. Andere frühe gletscherdynamische Theorien versuchten, die talwärtige Bewegung der Gletscher durch Schmelzen und Wiedergefrieren von Wasser zu erklären. So erklärte Johann von Charpentier in seinem 1841 erschienenen Werk Essai sur les glaciers et sur le terrain erratique du bassin du Rhône die Gletscherbewegungen durch die Schneeschmelze an der Gletscheroberfläche. Das geschmolzene Wasser dringe in das Innere des Gletschers ein und verursache beim nächtlichen Wiedergefrieren Risse und Verformungen, die zu einer Bewegung des Gletschers führten. Andere Naturforscher wie Johann Jakob Scheuchzer oder Ignaz Venetz vertraten ähnliche Theorien. Schon früh wurde jedoch erkannt, dass die Temperaturen im Gletscherinneren normalerweise zu hoch sind, um eine Verformung durch derartige Vorgänge zu bewirken.Schon 1751 führte dagegen Johann Georg Altmann die Bewegungen der Gletscher auf die Gravitation zurück. Diese führe dazu, dass das Eis des Gletschers talwärts gedrückt werde. Allerdings bewegten sich nach seinen Vorstellungen die Gletscher als Ganzes, vom Fließverhalten des Eises selbst als viskose Flüssigkeit hatte er noch keine Vorstellung. Auch andere seiner Ideen wirken für heutige Vorstellungen eher kurios. So nahm er beispielsweise an, dass sich unter der Gletscheroberfläche ein Meer aus flüssigem Wasser bis hinab in die Talgründe erstrecke, von dem die Gletscher nur die oberste Schicht darstellen. Horace-Bénédict de Saussure brachte 1779 im ersten Band seiner Voyages dans les Alpes die Theorie der Bewegung durch Gravitation auf eine aus heutiger Sicht wissenschaftlich etwas solidere Grundlage. Ausgehend von der Beobachtung, dass sich häufig am Fuße des Gletschers Hohlräume und abfließende Gletscherbäche befinden, erklärte er, dass das Eis am Felsbett abschmilzt und dadurch eine Bewegung des Gletschers zulässt, der sich wegen der von oben aufdrückenden Last des Eises talwärts bewegt. Auch seine Theorie berücksichtigt die viskosen Eigenschaften des Eises nicht, erst James David Forbes erkannte diese richtig als eine der Ursachen für die Bewegung der Gletscher. Seine Beobachtungen am Mer de Glace widersprachen der Theorie Saussures, da die Temperaturen zu niedrig waren, um nennenswert Eis zu schmelzen. Stattdessen nahm er in seinem 1842 erschienenen Werk eine viskose Verformung des Eises als Ursache der Gletscherbewegungen an. Auch wenn sich diese Theorie schließlich durchsetzen konnte, blieb sie anfangs nicht unwidersprochen: John Tyndall hielt es für unmöglich, dass Eis viskose Eigenschaften habe. In diesem Fall müsste ein Gletscher in der Lage sein – so Tyndall – über steile Kanten hinwegzufließen, anstatt zu brechen. Seine Erklärung für die Bewegung der Gletscher war eine kontinuierliche Bildung und anschließendes Wiederverschließen von kleinen Rissen. Diese Risse bildeten sich, sobald Sonnenlicht das Eis an verschiedenen Stellen im Gletscher zum Schmelzen bringe und das im Vergleich zum Eis geringere Volumen des Wassers den entstandenen Hohlraum nicht vollständig ausfüllen könne. Die Luftblasen in Gletschereis sah er dementsprechend als Überreste dieser Risse an.In die Zeit von Forbes und Tyndall fällt auch der Beginn erster systematischer Messungen der Gletscherbewegungen. Louis Agassiz zeigte, dass ein Gletscher in der Mitte schneller fließt als an seinen seitlichen Rändern. Zudem fand er heraus, dass die Geschwindigkeit am Beginn und Ende eines Gletschers niedriger ist als in den Bereichen dazwischen. Harry Fielding Reid zeigte 1896 schließlich, dass die Fließlinien eines Gletschers nicht parallel zum Felsbett verlaufen, sondern im Akkumulationsgebiet nach unten geneigt sind (Submergenz) und im Ablationsgebiet nach oben (Emergenz). Dies kann als experimentelle Bestätigung von Forbes’ Theorie des Gletschers als viskose Flüssigkeit angesehen werden.
Große Fortschritte wurden in den fünfziger Jahren des 20. Jahrhunderts erzielt. Dank der Arbeiten von Glen und Nye konnte erstmals ein allgemeines Fließgesetz für Eis formuliert werden (glensches Fließgesetz, siehe unten). Zusätzlich formulierte Weertman 1957 seine Theorie des basalen Gleitens eines Gletschers als Ganzes über das darunterliegende Felsbett. Die Beschreibung des basalen Gleitens wurde in den nachfolgenden Jahrzehnten noch weiter verfeinert. Namentlich wurde die Rolle des Schmelzwassers am Felsbett sowie die Tatsache, dass das Felsbett selbst durch Druck des aufliegenden Gletschers deformationsfähig ist, stärker in den Modellen berücksichtigt.Die Relevanz der arktischen und antarktischen Eisschilde auf das globale Klimasystem und die Variation des Meeresspiegels führte in den letzten Jahrzehnten dazu, dass die Eisschilde mehr in den Fokus der Forschung gerieten, wohingegen die frühen Arbeiten zur Gletscherdynamik sich fast ausschließlich mit den alpinen Gletschern beschäftigten. Nachdem schon bei Alfred Wegeners letzter Grönlandexpedition seismische Messungen der Eisdicke durchgeführt worden waren, begannen detaillierte Studien des Antarktischen Eisschildes erst mit der Norwegisch-Britisch-Schwedischen Antarktisexpedition 1949 bis 1952.Neben der Entwicklung besserer experimenteller Methoden wie z. B. des Remote Sensing stellt die Einführung numerischer Simulation eine einschneidende Veränderung in der wissenschaftlichen Arbeit zur Gletscherdynamik dar. Erste numerische Modelle wurden Ende der 1960er-Jahre entwickelt. Das erste dreidimensionale Modell eines Gletschers wurde 1976 auf die Barnes-Eiskappe der Baffininsel angewandt, vorher wurde nur mit zweidimensionalen Vereinfachungen gearbeitet. 1977 konnte erstmals die Thermodynamik in den Modellen berücksichtigt werden. Inzwischen sind die Modelle in der Lage, Temperatur, Fließgeschwindigkeit sowie Felsbett- und Oberflächentopographie zumindest größenordnungsmäßig zu reproduzieren. Dank Computersimulationen ist es daher heute möglich, den Einfluss einzelner Parameter auf das Fließverhalten als Ganzes zu simulieren, ohne auf komplizierte Labormessungen zurückgreifen zu müssen. Auch wenn die Modelle in neuerer Zeit dank immer leistungsfähigerer Computer zunehmend mächtiger werden, ist bei der Interpretation ihrer Vorhersagen Vorsicht geboten. Der derzeitige Anstieg des Eisflusses der polaren Eisschilde wurde zum Beispiel in keinem Modell vorhergesagt. Diese neuzeitlichen dramatischen Änderungen der Gletscher und deren Auswirkungen auf das globale Klimasystem stehen derzeit im Mittelpunkt der Forschung.
Als Eis wird im Allgemeinen der feste Aggregatzustand des Wassers bezeichnet, der in verschiedenen Erscheinungsformen auftreten kann. In der Glaziologie unterscheidet man des Weiteren zwischen Neuschnee und verschiedenen Formen von Firn sowie (Gletscher-)Eis, das einen geschlossenen Porenraum aufweist und bei dem vom Eis eingeschlossene Luftblasen keinen Kontakt mit der äußeren Atmosphäre mehr haben. Für die Struktur in den Kristallen ist diese Unterscheidung jedoch zunächst irrelevant: Das Wassermolekül besteht aus einem Sauerstoffatom, das zwei Wasserstoffatome an sich gebunden hat. Im festen Aggregatzustand binden zusätzlich zwei weitere Wasserstoffatome über Wasserstoffbrückenbindungen an das Sauerstoffatom, sodass jedes Molekül vier über Wasserstoffbrücken verbundene Nachbarn hat (zwei ausgehend vom Sauerstoffatom und eine von jedem Wasserstoffatom).
Ein Molekül mit vier nächsten Nachbarn kann sich auf verschiedene Arten kristallisieren. Während unter Laborbedingungen mehrere Kristallstrukturen von Eis realisiert werden können (zurzeit sind neun stabile sowie mehrere metastabile und amorphe Strukturen bekannt), kommt in der Natur nur die hexagonale Form Eis Ih vor, in der sich jeweils sechs Wassermoleküle zu Ringen zusammenschließen, die sich in einzelnen Schichten anordnen (siehe Abbildung). Jedes Molekül gehört dabei zwei Ringen an. Der Abstand zweier benachbarter Ringschichten ist mit 0,276 nm erheblich größer als die Versetzungen innerhalb des Ringes (0,092 nm). Die Richtung senkrecht zu den Ringschichten nennt man optische oder c-Achse, die durch die Ringschichten definierte Fläche heißt basale Ebene.
Aufgrund der schichtförmigen Struktur eines einzelnen Eiskristalls findet seine Deformation normalerweise parallel zu seiner basalen Ebene statt, die benötigte Spannung zur Deformation entlang anderer Richtungen ist ungefähr 100-mal höher. Hierbei wird das Eis erst elastisch deformiert, anschließend beginnt es sich permanent zu verformen, solange die Spannung anhält. Laborexperimente zeigen, dass selbst kleine Spannungen eine Deformation verursachen. Zurückzuführen ist dies auf Defekte innerhalb der Kristallstruktur – sogenannte Versetzungen, die sich um einiges einfacher innerhalb des Kristalls bewegen können als Atome in einem perfekten Kristallgitter.
Gletschereis besteht nicht aus einem einzelnen, großen Eiskristall, sondern ist aus vielen einzelnen Einzelkristallen (Körner, englisch grains) zusammengesetzt. Ein Kubikmeter Gletschereis enthält dabei 106 bis 109 einzelne Körner. Im Gegensatz zu nur aus einem Kristall bestehendem monokristallinen Eis wird solches Eis polykristallin genannt. Es deformiert langsamer als monokristallines, da die Orientierung der einzelnen Kristalle zufällig ist und kein einheitliches Gleiten entlang der basalen Ebene zulässt. Prozesse, die zur Deformation führen, sind stattdessen Bewegung der einzelnen Kristalle relativ zueinander, Bewegung von Gitterfehlern innerhalb eines Kristalls und dynamische Rekristallisation, die Bildung neuer Kristalle die für die Deformation vorteilhaft orientiert sind.Wird ein konstanter Druck ausgeübt, folgt auf eine anfängliche elastische Deformation eine Phase, in der die Verformungsrate abnimmt (primary creep) bis ein Minimum, die secondary creep rate erreicht ist. Die Abnahme wird durch Störungen von Kristallen unterschiedlicher Orientierung verursacht, die sich gegenseitig blockieren. Dynamische Rekristallisation führt schließlich zu Kristallstrukturen, die einfacher zu deformieren sind und demzufolge zu einer Erhöhung der Verformungsrate (tertiary creep).
Auf Gletschereis wirkende Kräfte (i. A. die Gravitation) bewirken eine Deformation des Eises auf Grund der oben genannten Mechanismen. Dabei kann man für in Gletschern übliche Spannungen die Verformungsrate 
  Diese Beziehung wird Glensches Fließgesetz genannt. Das Glensche Fließgesetz ist im Wesentlichen empirisch anhand verschiedener Labor- und Felddaten gefunden worden, wobei die Werte von 
   variiert zwischen etwa 2 und 3,9, wobei für Gletschereis im Allgemeinen ein Wert von 3 angenommen wird. Während der Wert von 
   für praktische Anwendungen in der Glaziologie als konstant angenommen werden kann, ist der Wert des Ratenfaktors 
   keine Konstante, sondern hängt von Temperatur, Druck sowie der Konzentration von Verunreinigungen des Eises wie zum Beispiel Sand ab. Bezüglich der Temperatur zeigt 
   beträgt dabei etwa 60 kJ/mol für Temperaturen unter −10 °C. Dies führt dazu, dass die Verformungsrate bei −10 °C etwa fünfmal höher ist als bei −25 °C. Steigen die Temperaturen über −10 °C, verformt sich das polykristalline Gletschereis sogar noch deutlich schneller, obwohl rein monokristallines Eis dieses Verhalten nicht zeigt. Die erhöhte Verformungsrate zwischen −10 °C und 0 °C kann durch eine Aktivierungsenergie von 152 kJ/mol beschrieben werden.Die temperaturunabhängige Größe 
   ist ebenfalls keine Konstante, sondern Abhängig vom Druck, was wiederum durch eine Exponentialgleichung beschrieben werden kann:
  . Allerdings ist der Druckeffekt selbst für Drücke, wie sie an der Unterseite von Eisschilden herrschen, sehr klein und weit weniger relevant als die Temperaturabhängigkeit. Zusätzlich kann die Verformungsgeschwindigkeit von Kristallgröße und Wassergehalt abhängen. Auf ähnliche Weise wie Wasser erhöhen chemische Verunreinigungen im Eis dessen Verformbarkeit, indem sie zwischen den Korngrenzen salzreiche Lösungen mit niedrigerem Schmelzpunkt als reines Wasser bilden, die das Gleiten entlang der Korngrenzen erleichtern. Der Effekt von unlöslichen Verunreinigungen ist dagegen weniger klar, da kleine Partikel innerhalb der Kristallstruktur die Häufigkeit von Gitterfehlern erhöhen, was das Eis verformbarer macht, sie andererseits aber auch das Gleiten des Eises erschweren. Eine Messung der Verformbarkeit bei verschiedenem Sandgehalt ergab jedoch eine signifikante Erhöhung bei steigender Sandmenge. Insgesamt ist der Effekt von Verunreinigungen auf die Verformbarkeit von Gletschereis noch wenig erforscht und schwer einzuschätzen, da ein Zusammenhang zu anderen Größen wie Druck und Temperatur vermutet wird. Die Effekte von Verunreinigungen im Eis sollten aber vor allem am Felsbett eines Gletschers eine große Rolle spielen, da dort der Partikelgehalt am höchsten ist.
Normalerweise wirken die Scherkräfte in einem Gletscher in verschiedene Richtungen. Daher werden im allgemeinen Fall sowohl die Verformungsrate 
    {\displaystyle \sigma ={\begin{pmatrix}\sigma _{xx}&\sigma _{xy}&\sigma _{xz}\\\sigma _{yx}&\sigma _{yy}&\sigma _{yz}\\\sigma _{zx}&\sigma _{zy}&\sigma _{zz}\end{pmatrix}}}
  Da der Fluss des Gletschers unabhängig vom hydrostatischen Druck ist, wird nur der Spannungsdeviator 
    {\displaystyle \tau _{ii}=\sigma _{ii}-{\frac {1}{3}}(\sigma _{xx}+\sigma _{yy}+\sigma _{zz}).\qquad {\text{(i = x,y,z)}}}
    {\displaystyle {\dot {\epsilon }}_{xy}={\frac {1}{2}}\left({\frac {\partial v_{x}}{\partial y}}+{\frac {\partial v_{y}}{\partial y}}\right)}
   beschreiben eine Dehnung beziehungsweise Kompression entlang einer Achse. Die Nichtdiagonalelemente entsprechen Scherungen (das Element 
  )Ein allgemeines Fließgesetz soll Spannung und Verformungsrate mathematisch in Beziehung setzen. Eine Grundannahme ist hierbei, dass Verformungsrate und Spannungsdeviator proportional zueinander sind:
  Da das Fließgesetz unabhängig vom gewählten Koordinatensystem sein muss, sind die Invarianten der beiden Tensoren 
   von besonderem Interesse. Da der Spannungsdeviator spurfrei ist, folgt aus der angenommenen linearen Abhängigkeit (Gleichung (a)), dass 
Die zweite Invariante der Verformungsrate (beziehungsweise der deviatorischen Scherspannung) werden effektive Verformungsrate (effektive Scherspannung) genannt und sind definiert als
    {\displaystyle {\text{(b)}}\quad 2{\dot {\epsilon }}={\dot {\epsilon }}_{xx}^{2}+{\dot {\epsilon }}_{yy}^{2}+{\dot {\epsilon }}_{zz}^{2}+2({\dot {\epsilon }}_{xy}^{2}+{\dot {\epsilon }}_{xz}^{2}+{\dot {\epsilon }}_{yz}^{2})}
    {\displaystyle {\text{(c)}}\quad 2\tau =\tau _{xx}^{2}+\tau _{yy}^{2}+\tau _{zz}^{2}+2(\tau _{xy}^{2}+\tau _{xz}^{2}+\tau _{yz}^{2}).}
  Es wird für diese beiden Größen die den experimentellen Beobachtungen entsprechende Beziehung der Form
   des Spannungstensors ab, sondern auch von den in allen anderen Richtungen wirkenden Scherkräften, die in der effektiven Scherspannung 
   enthalten sind. Falls der Scherspannungstensor nur einen Eintrag hat, die Kraft also nur auf eine Fläche in eine Richtung wirkt, ist das verallgemeinerte Fließgesetz zum Glenschen Fließgesetz äquivalent.In der neueren Literatur werden auch komplexere Beziehungen zwischen Verformungsrate und Spannung angeführt. Ausgehend von der Beobachtung, dass je nach Ursache einer Verformung ein unterschiedliches Fließverhalten auftritt, entwarfen David L. Goldsby und David Kohlstedt (2001) ein Modell, in dem sich die Gesamtverformungsrate aus der Summe aller Beiträge der verschiedenen Deformationsmechanismen für polykristallines Eis zusammensetzt. Auch Beziehungen, die noch weitergehend von der Form des allgemeinen Fließgesetz abweichen, wurden diskutiert. Trotzdem wird das verallgemeinerte Fließgesetz in der oben angegebenen Form in den meisten gletscherdynamischen Modellen angewandt.
Gletscher können sich als Ganzes durch die Gravitation talwärts bewegen, was als basales Gleiten bezeichnet wird. Die Geschwindigkeit des basalen Gleitens hängt dabei weniger von der Größe der Gravitationskraft ab, sondern mehr von den Bedingungen am Felsbett, die von der Temperatur des Gletschers abhängen. Ist die Temperatur dort höher als der Druckschmelzpunkt, kann sich durch Schmelzen ein dünner Wasserfilm bilden, der ein Gleiten des Gletschers ermöglicht. Anderenfalls geschieht das Gleiten nur sehr langsam und ist daher für die meisten kalten Gletscher, das heißt Gletschern, deren Temperatur sich unter dem Druckschmelzpunkt befindet, irrelevant. Ein Maß für die Gleitfähigkeit eines Gletscherfelsbettes ist der drag factor 
  .Je höher der drag factor, desto schwerer fällt das Gleiten. Sein Zahlenwert variiert selbst für Gletscher mit Schmelze am Felsbett stark. Aus diesem Grund kann auch nicht allgemein gültig angegeben werden, wie relevant basales Gleiten für die Bewegung der Gletscher als Ganzes ist. Bei Gletschern mit Temperaturen über dem Druckschmelzpunkt ist es im Durchschnitt für etwa 50 % der Gesamtbewegung verantwortlich, teilweise aber auch für erheblich mehr.
Dass das Eis sich über Unebenheiten am Felsbett hinwegbewegen kann, ist hauptsächlich auf zwei Mechanismen zurückzuführen, welche schon von Deeley und Pfarr 1914 beschrieben und von Weertman 1957 in einer ersten Theorie des basalen Gleitens mathematisch behandelt wurden. Grundannahme seiner Theorie ist ein Eiskörper, der sich über einen dünnen Wasserfilm über ein nicht deformierbares Felsbett bewegt. Falls eine Unebenheit des Felsbettes dem Fluss entgegensteht, entsteht einerseits durch die Kraft des von oben auf das Hindernis drückenden Gletschers ein Druckgradient zwischen den beiden Seiten des Hindernisses. Der höhere Druck auf der dem Berg zugewandten Seite des Hindernisses sorgt dafür, dass der Druckschmelzpunkt auf dieser Seite erniedrigt wird. Da die Temperatur des Gletschers am Felsbett dem Druckschmelzpunkt entspricht, ist das Eis hier also kälter als auf der Talseite. Durch diesen Temperaturunterschied entsteht ein Wärmefluss, der das Eis auf der Bergseite zum Schmelzen bringt. Im flüssigen Zustand kann das Hindernis überwunden werden, und das Wasser gefriert anschließend wieder. Die Effizienz dieses Mechanismus ist vom Wärmefluss durch das Hindernis abhängig, der umso kleiner wird, je größer das Hindernis ist. Daher ist er für große Hindernisse vernachlässigbar.
Auf der anderen Seite wird durch dem Fluss entgegenstehende Hindernisse eine höhere Spannung verursacht, die eine höhere Fließgeschwindigkeit zur Folge hat. Je größer das Hindernis ist, desto größer ist dieser Effekt, sodass er nur für große Hindernisse relevant ist. Die Kombination beider Effekte schließlich ermöglicht eine Bewegung sowohl über große als auch über kleine Hindernisse hinweg.Nach der ersten Formulierung dieser Theorie durch Weertman wurden weitere Theorien zum basalen Gleiten ausgearbeitet, ohne dass sich grundlegende Änderungen ergaben. Die von Weertman anfangs nur postulierten Mechanismen sind inzwischen auch experimentell bestätigt. Eine Modifikation ergibt sich aber durch das Vorhandensein von größeren Wassereinschlüssen am Felsbett. Der Wasserdruck innerhalb dieser Einschlüsse kann so groß werden, dass sich das Eis nicht mehr nur über einen dünnen Wasserfilm über das Felsbett hinwegbewegt, sondern teilweise komplett angehoben wird, was die Kontaktfläche zwischen Gletscher und Felsbett verringert. Damit wird der Reibungswiderstand drastisch gesenkt und die Fließgeschwindigkeit erhöht. Eine vollständig korrekte Beschreibung dieses Falles ist bisher noch nicht entwickelt worden. Des Weiteren können im felsbettnahen Eis eingeschlossene Partikel den Reibungswiderstand erhöhen, was ebenfalls die Fließgeschwindigkeit merklich beeinflusst.
In den obigen Betrachtungen wurde davon ausgegangen, dass der Gletscher sich über ein vollkommen starres Felsbett hinwegbewegt. Das hohe Gewicht des Gletschereises kann jedoch dazu führen, dass sich das Felsbett selbst deformiert und das unterliegende Sediment mitbewegt. Eine experimentelle Bestätigung hierzu liefert der derzeitige Rückzug der Gletscher, der normalerweise kein starres Felsbett, sondern Gesteinsschutt zurücklässt, der durch die Deformation des Felses entstanden ist. Untersuchungen mittels Bohrlöchern sind zwar nur an wenigen Gletschern durchgeführt worden, bestätigten jedoch die wichtige Rolle der Felsbettdeformation, die in einigen Gletschern sogar die Hauptursache der basalen Gletscherbewegungen ist. Felsbettdeformationen zeigen starke räumliche und zeitliche Fluktuationen, bedingt durch räumliche Änderungen der Geometrie und des Materials des unterliegenden Felsbettes sowie Änderungen des Wassergehalts des Gletschers. Auch aus diesem Grund ist das genaue Fließverhalten eines Gletschers mit deformierbaren Bett sehr schwer zu beschreiben. Da die Deformation des Felsbettes jedoch signifikant das Fließverhalten beeinflusst, wird es inzwischen dennoch von allen modernen Gletschermodellen zu parametrieren versucht.
Die Fließgeschwindigkeit von Gletschern variiert zwischen wenigen Metern und einigen Kilometern pro Jahr. Grundsätzlich tragen zwei Komponenten zur Fließgeschwindigkeit bei: ein konstanter Anteil, der durch basales Gleiten und Felsbettdeformation verursacht wird, sowie einer, der auf interne Deformation zurückzuführen ist und abhängig von der Masse des aufliegenden Eises, mithin mit der Tiefe des Gletschers variiert:
    {\displaystyle u=u_{\mathrm {b} }+\int _{\mathrm {B} }^{z}{\frac {\partial u}{\partial z}}\mathrm {d} z.}
  Der Anteil des basalen Gleitens wird hierbei durch Wassergehalt und Beschaffenheit des Felsbettes bestimmt, die interne Deformation durch die angelegten Spannungen und Geometrie des Gletschers. Beide Prozesse sind auch abhängig von der Temperatur. Multipliziert man die Fließgeschwindigkeit mit der Querschnittsfläche des Gletschers, erhält man die Menge an Eis, die pro Zeiteinheit durch diese Fläche fließt, den Eisfluss (gemessen in 
Ausgehend von der spezifischen Massenbilanz, also der Massenbilanz an verschiedenen Punkten eines Gletschers, lassen sich generelle Voraussagen über gemittelte Gleichgewichtsgeschwindigkeiten und Richtung der Fließlinien treffen. Um die genauen Fließgeschwindigkeiten in Abhängigkeit von der Tiefe zu erhalten, müssen die Geschwindigkeiten mit dem allgemeinen Fließgesetz berechnet werden. Außer für idealisierte Fälle ist hierbei eine analytische Lösung nicht mehr möglich, sodass man auf numerische Modelle angewiesen ist.
Oberflächengeschwindigkeiten von Gletschern wurden früher durch Triangulation gemessen. Zu diesem Zweck wurde Stangen über den Gletscher verteilt und der Abstand zwischen ihnen bestimmt. Mittels Messungen zu verschiedenen Zeiten ergeben sich die Verschiebungen der Stangen und damit die Geschwindigkeit. Auch wenn technische Verbesserungen wie automatische Winkelmessung und Laserentfernungsmessung derartige Messungen vereinfachten, benötigen sie immer einen Referenzpunkt. Außerdem ist der Arbeitsaufwand der hierfür erforderlichen regelmäßigen Feldmessungen relativ hoch und ungünstige Wetterbedingungen können Messungen zu bestimmten Zeiten unmöglich machen. Eine grundlegende Verbesserung ergab sich mit Einführung des Global Positioning System (GPS) und dem Einsatz von GPS-Empfängern an den Vermessungsstangen. Wenn nahe dem Gletscher eine Referenzstation vorhanden ist, kann somit eine Messgenauigkeit von bis zu einem Zentimeter erreicht werden. Außerdem können kontinuierlich Daten gemessen werden, nicht nur während weniger Feldmessexkursionen. Messmethoden mit Stangen liefern jedoch immer nur Daten für eine räumlich sehr begrenzte Fläche. Inzwischen werden daher vor allem terrestrische Laserscanner zur Bestimmung der Oberflächengeschwindigkeit benutzt, die eine Reichweite von mehreren Kilometern haben. Die Gletschergeschwindigkeit wird mit ihnen durch Verschiebung charakteristischer Strukturen in aufeinanderfolgenden Scans bestimmt (feature tracking).Mittels Fernerkundung können auch Oberflächengeschwindigkeiten von größeren und bisher unzugänglichen Gebieten gemessen werden. Hierbei werden Daten oder Bilder von Flugzeug- oder Satellitenmessungen zur Geschwindigkeitsbestimmung genutzt. Die Geschwindigkeit kann auch hier mittels feature tracking anhand der Verschiebung markanter Fixpunkte bestimmt werden. Hierzu dienen zum Beispiel Gletscherspalten, die auf verschiedenen, zeitlich aufeinanderfolgenden Bildern des gleichen Gebietes aufgenommen wurden. Alternativ ist es möglich, die Geschwindigkeit mittels Mikrowellen-Interferometrie zu erhalten, indem die Phasenverschiebung eines vom Gletscher reflektierten Mikrowellensignals gemessen wird.Messungen von Geschwindigkeiten innerhalb des Gletschers sind schwieriger. Eine Möglichkeit hierzu ist die Beobachtung der Deformation von Eisbohrlöchern, die Informationen über die Deformationsrate und Fließgeschwindigkeit liefern kann.
Für jeden Punkt eines Gletschers kann unter Annahme einer ausgeglichenen Massenbilanz eine mittlere Geschwindigkeit, die sogenannte Gleichgewichtsgeschwindigkeit, berechnet werden. Hierbei wird ausgenutzt, dass auf Grund der Massenerhaltung Eis innerhalb des Gletschers weder „aus dem Nichts“ entstehen noch verloren gehen kann. Die Änderung der Gesamtmasse des Gletschers oberhalb eines Querschnitts des Gletschers 
   muss daher der Differenz von Akkumulation (beziehungsweise Ablation) oberhalb dieses Querschnitts und dem Eisfluss durch ihn entsprechen:
    {\displaystyle {\frac {\mathrm {d} M}{\mathrm {d} t}}=\rho \int _{A}{\dot {b}}\mathrm {d} A-\int _{Y}Q\mathrm {d} y}
  . Wenn die Änderung der Masse vernachlässigbar klein gegenüber den anderen Termen dieser Gleichung ist (also im Falle eines Gletschers mit annähernd ausgeglichener Massenbilanz und vernachlässigbarer kurzzeitiger Variabilität durch Schneefall oder -schmelze), gilt 
    {\displaystyle \rho \int _{A}{\dot {b}}\mathrm {d} A=\rho {\dot {b}}A={\overline {\rho }}HU_{\mathrm {g} }y}
  . In Worten besagt diese Gleichung, dass das gesamte Eis, das oberhalb des beobachteten Querschnitts akkumuliert wird, auch durch diesen hindurchfließen muss. Die Geschwindigkeit, mit der das geschieht, 
  , wird Gleichgewichtsgeschwindigkeit genannt. Da sie eine gemittelte Größe ist, gibt die Kenntnis der Gleichgewichtsgeschwindigkeit keine Information über die tatsächliche Geschwindigkeitsverteilung innerhalb des Gletschers, hierzu ist eine genauere Betrachtung der beschleunigenden und bremsenden Kräfte, die auf den Gletscher wirken, nötig. Falls die Massenbilanz eines Gletschers nicht ausgeglichen ist, weicht die tatsächliche Geschwindigkeit natürlich ebenfalls von der Gleichgewichtsgeschwindigkeit ab. Deutliche Differenzen zwischen gemessener und Gleichgewichtsgeschwindigkeit sind daher ein Zeichen dafür, dass die Massenbilanz des Gletschers nicht ausgeglichen ist.Bei Talgletschern ist die Gleichgewichtsgeschwindigkeit im Akkumulationsgebiet positiv, im Ablationsgebiet dagegen negativ, da der Eisfluss in zunehmender Tiefe immer geringer wird. Im Meer endende Gletscher arktischer Regionen sowie die großen Eisschilde können selbst am Rand einen großen Eisfluss aufweisen. Sie verlieren ihr Eis nicht durch Schmelzen, sondern durch das Kalben von Eisbergen. In diesen Fällen ist die Gleichgewichtsgeschwindigkeit auch an der Küste noch relativ hoch, im Falle der Antarktis liegen die Bereiche mit der höchsten Gleichgewichtsgeschwindigkeit sogar direkt an der Küste, da es aufgrund der extrem niedrigen Temperaturen kein Ablationsgebiet auf dem antarktischen Festland gibt.
Die Gleichgewichtsgeschwindigkeit beschreibt die mittlere horizontale Geschwindigkeit eines Gletschers. Wenn der Gletscher tatsächlich im Gleichgewicht ist oder nicht wesentlich davon abweicht, ist es zusätzlich möglich, Aussagen über die vertikale Geschwindigkeit des Eises zu treffen. Damit sich die Höhe des Gletschers an keiner Stelle ändert, muss die vertikale Geschwindigkeit genau der Nettoakkumulation – also der Differenz aus Akkumulation und Ablation – entsprechen:
   trägt der Tatsache Rechnung, dass der Gletscher auch eine horizontale Geschwindigkeit hat, sodass die Vertikalgeschwindigkeit nicht exakt der Nettoakkumulation entspricht, sobald der Gletscher um einen Winkel 
   daher negativ. Die Fließlinien zeigen in den Gletscher hinein und das Eis fließt tendenziell nach unten. Dieses verhalten wird Submergenz genannt. Im Ablationsgebiet ist die Situation genau umgekehrt, die Fließlinien zeigen nach oben und das Eis fließt wieder Richtung Gletscheroberfläche (Emergenz). Nur auf Höhe der Gleichgewichtslinie fließt das Eis parallel zur Gletscheroberfläche. Für Talgletscher zeigen deutliche Abweichungen von diesem Fließverhalten an, dass sich der Gletscher nicht im Gleichgewicht befindet. Dann entspricht die vertikale Geschwindigkeit nicht mehr der Nettoakkumulation und der Gletscher verliert oder gewinnt an Substanz. Für Eisschilde muss diese Gesetzmäßigkeit nicht unbedingt gelten, da sie Eis auch durch andere Mechanismen wie zum Beispiel das Kalben von Eisbergen verlieren.
Treibende Kraft der Gletscherbewegungen ist das Eigengewicht des Eises. Diese kann auf zwei verschiedene Arten dazu führen, dass es zu einer Bewegung der Gletscher kommt:
   befindet sich auf einem geneigten Felsbett. Die auf den Gletscher wirkende Kraft entspricht dann der Hangabtriebskraft 
Ein Gletscher befindet sich auf einem flachen Felsbett, besitzt aber eine variable Höhe, sodass Eis vom höheren zum flacheren Teil des Gletschers fließt. In diesem Fall gilt für die wirkende Kraft 
   die Neigung der Gletscheroberfläche symbolisiert.Auch eine Kombination beider hier beschriebener Situationen ist möglich. Da die Oberflächenneigung von Gletschern selten mehr als 20° beträgt, gilt die Kleinwinkelnäherung 
   und die wirkende Kraft kann in diesem Fall alleine durch die Oberflächenneigung bestimmt werden. Die Kraft ist immer auf eine Fläche des Gletschers bezogen, hat also die Einheit einer Spannung (Kraft pro Fläche). In der Glaziologie wird sie deshalb auch als Spannung bezeichnet, selbst wenn sie nicht nur auf eine Oberfläche, sondern auf den gesamten Gletscher wirkt.Der Gravitation entgegengesetzt sind die Reibung am Felsbett 
  ). Da Beschleunigungen innerhalb eines Gletschers minimal sind, kann im Normalfall ein Gleichgewicht angenommen werden, in dem die bremsenden Kräfte die Beschleunigung durch die Gravitation ausgleichen:
  .Die Reibung am Felsbett ist im Allgemeinen die wichtigste dieser drei Kräfte und macht bei Talgletschern 50 bis 90 % der gesamten bremsenden Kräfte aus. Im Falle von Eisschelfs und im Meer mündenden Eisströmen wird 
  , die Reibung am „Felsbett“, jedoch vernachlässigbar klein und ihr Beitrag zu den bremsenden Kräften geht gegen null. Dies ist der Grund, weshalb diese Gletscher eine höhere Dynamik aufweisen. Außerdem wirken sich in diesem Fall lokale Änderungen der Spannung in einem Teil des Gletschers auch stark auf andere Regionen aus, wohingegen sie bei Gletschern mit Felsbett aufgrund der Reibung lokal begrenzt bleiben können.
Um nicht nur die gemittelte Gleichgewichtsgeschwindigkeit, sondern auch einen Wert für die Geschwindigkeit an einem einzelnen Punkt des Gletschers berechnen zu können, muss man die wirkenden Spannungen mittels des verallgemeinerten Fließgesetzes in Geschwindigkeiten übersetzen. Im einfachsten Fall, einer einfachen Scherung (parallel flow), wirken nur Kräfte auf die z-Ebene des Gletschers (Reibung an den Seiten wird vernachlässigt) und der Gletscher fließt immer parallel zum Felsbett. Das bedeutet, dass 
   die die Bewegung am Felsbett durch basales Gleiten. Mittels dieses einfachen Modells erkennt man, dass die Geschwindigkeit deutlich mit der Höhe über dem Felsbett zunimmt und das Eis direkt über dem Felsbett sich folglich nur sehr langsam bewegt und darum ein sehr hohes Alter aufweisen kann. Dies ist ein Grund, weshalb Eisbohrkerne ein bis weit in die Vergangenheit reichendes Klimaarchiv darstellen können.
   den experimentell gefundenen Wert von ungefähr drei annimmt. Daher haben schon kleine Änderungen der Geometrie starke Auswirkungen auf die Geschwindigkeit. Auch der Parameter A ist stark abhängig von Faktoren wie Temperatur, Wassergehalt und anderen Einflüssen.Die Eigenschaften des Eises nahe dem Felsbett unterscheiden sich stark vom restlichen Eis, was zu Abweichungen von den erwarteten Fließgeschwindigkeiten in beide Richtungen führen kann. Ein hoher Anteil von löslichen Verunreinigungen führt zu einer erhöhten Fließgeschwindigkeit, wie es zum Beispiel an der Agassiz-Eiskappe beobachtet wird. An anderen Stellen, wie auf der Devon-Insel, beobachtet man wiederum eine außergewöhnlich geringe Geschwindigkeit am Felsbett. Bei Gletschern, die noch sehr altes Eis besitzen, kann sich innerhalb der Schichtfolge das Fließverhalten in tieferen Schichten deutlich steigern, sobald man auf eine Schicht mit Eis der letzten Eiszeit trifft. In grönländischen und kanadischen Gletschern wurden Geschwindigkeiten gemessen, die ungefähr um den Faktor drei vom erwarteten Wert abwichen. Wahrscheinlich liegt dies an einer durch Verunreinigungen entstandene kleineren durchschnittlichen Kristallgröße in diesen Gletschern, welche das Gleiten der einzelnen übereinander liegenden Eiskristallschichten vereinfacht.
Die im vorigen Abschnitt dargestellte einfache Beziehung gilt streng genommen nur für horizontal unendlich ausgedehnte Gletscher mit konstanter Oberflächenneigung und Höhe. Änderungen der Neigung oder in der Höhe über dem Felsbett führen zu Gradienten in 
  , die die Geschwindigkeit beeinflussen. Bei Talgletschern spielt die seitliche Begrenzung des Gletschers eine große Rolle, die zu zusätzlicher Reibung führt. Akkumulation und Ablation führen zu Submergenz und Emergenz, die ebenfalls nicht als einfache Scherung beschrieben werden können. In diesem Fall nimmt das Fließgesetz eine kompliziertere Form an, da nicht mehr alle Einträge des Spannungstensors außer 
Bei zunehmender Komplexität stoßen analytische Beschreibungen an ihre Grenzen, sodass numerische Simulationen zur Lösung benötigt werden. Bei gegebenen Randbedingungen wie Felsbett, Stärke des basalen Gleitens und Massenbilanz können so die Geschwindigkeiten an jedem Punkt des Gletschers bestimmt werden.
Sowohl Gebirgsgletscher als auch die polaren Eisschilde weisen im Zuge der globalen Erwärmung eine signifikant negative Massenbilanz auf, seit Mitte des 19. Jahrhunderts wird weltweit ein Rückzug der Gletscher beobachtet. Die veränderte Massenbilanz hat Auswirkungen auf das Fließverhalten des Gletschers. Umgekehrt können Änderungen des Fließverhaltens wiederum die Massenbilanz beeinflussen, indem sie durch höheren oder niedrigeren Eisfluss die Geschwindigkeit des Rückgangs bestimmen.
Bei Gebirgsgletschern führt eine höhere Temperatur zu einer Erhöhung der Gleichgewichtslinie und einer Verkleinerung des Akkumulationsgebiets, was auch Einfluss auf die Fließgeschwindigkeit hat. Eventuell kann sich die Gleichgewichtslinie so stark verschieben, dass sie über dem höchsten Punkt des Gletschers liegt und dieser im gesamten Bereich im Jahresmittel mehr Eis verliert als er durch Akkumulation gewinnt. Langfristig führt dies zum kompletten Abschmelzen des Gletschers. Falls Schmelz- oder Regenwasser von der Gletscheroberfläche bis zum Gletscherbett vordringt, kann der Eisfluss durch verstärktes basales Gleiten zusätzlich beschleunigt werden. Die Auswirkungen klimatischer Änderungen auf Gebirgsgletscher und deren Fließverhalten sind insgesamt stark von der jeweiligen Geometrie und lokalen Gegebenheiten abhängig, sie können gar keinen Einfluss haben oder bis zum vollständigen Verschwinden eines Gletschers führen.Das Grönländische Eisschild verlor zwischen 1992 und 2012 jährlich im Durchschnitt 300 Gigatonnen Eis. Hierbei hat nicht nur die Schmelze an der Gletscheroberfläche signifikant zugenommen, auch der Verlust von Eis durch Ausfluss in den Ozean, der in vergleichbarer Größenordnung zur Massenbilanz beiträgt, hat sich erhöht. Flugzeuggestützte Laser-Altimetermessungen zeigten, dass sich die grönländischen Gletscher in den 1990er-Jahren um bis zu zehn Meter pro Jahr verdünnten, wobei sich die Fließgeschwindigkeiten teilweise fast verdoppelten. Bei etwa der Hälfte der Gletscher kann dies nicht vollständig durch Schmelze an der Gletscheroberfläche erklärt werden. Stattdessen wird als Grund der sich erwärmende Ozean angenommen, der die Reibungskräfte über dem Felsbett der bis weit ins Meer hineinreichenden Gletscher verändert.Auch der antarktische Eisschild verliert an Masse, obwohl sich die Niederschläge nicht signifikant änderten und die Temperaturen ganzjährig weit unter dem Gefrierpunkt liegen, sodass Verluste an der Oberfläche vernachlässigbar sind. Die größten Eisverluste des Antarktischen Eisschildes werden auf der antarktischen Halbinsel beobachtet. Hier verursachte das Aufbrechen mehrerer Eisschelfe eine Erhöhung der Fließgeschwindigkeit, wodurch eine größere Eismenge als bisher in den Ozean entweicht. Auch die Verluste im Gebiet der Amundsensee sind mit einiger Sicherheit (medium confidence) auf das Ausdünnen der Eisschelfe zurückzuführen. Veränderungen sind aufgrund von Satellitenmessungen auch in der Ostantarktis zu beobachten, sie sind aber nur geringfügig und noch nicht vollständig verstanden.
Kurt M. Cuffey, William S. B. Paterson: The Physics of glaciers. 4th Edition. Butterword-Heinemann, Amsterdam u. a. 2010, ISBN 978-0-12-369461-4.
Roland A. Souchez, Reginald D. Lorrain: Ice composition and glacier dynamics (= Springer Series in Physical Environment. Band 8). Springer, Berlin u. a. 1991, ISBN 3-540-52521-1.
Cornelis J. van der Veen: Fundamentals of Glacier Dynamics. A. A. Balkema, Rotterdam u. a. 1999, ISBN 90-5410-471-6.
Andrew Fowler: Mathematical Geoscience (= Interdisciplinary Applied Mathematics. Band 36). Springer, London u. a. 2010, ISBN 978-0-85729-699-3, Kapitel 10 (Glaciers and Ice Sheets) und 11 (Jökulhlaups).
Hester Jiskoot: Dynamics of Glacier. In: Vijay P. Singh, Pratap Singh, Umesh K. Haritashya (Hrsg.): Encyclopedia of Snow, Ice and Glaciers. Springer, Dordrecht 2011, ISBN 978-90-481-2641-5, S. 415–428.
Christian Schoof, Ian Hewitt: Ice-Sheet Dynamics. In: Annual Review of Fluid Mechanics. Band 45, Nr. 1, 2013, S. 217–239, doi:10.1146/annurev-fluid-011212-140632. 

Der Glienicker Weg war ein ursprünglich von Coepenick bei Berlin zum  Vorwerk Glienicke durch die Köllnische Heide führender Weg. Seit Anfang des 18. Jahrhunderts dient er als direkte Verbindung dieser beiden späteren Berliner Ortsteile, aus dem infolge der Eingliederung einige Abschnitte unter neuen Namen ausgegliedert wurden. Heute trägt nur noch das Mittelstück dieses Weges die Bezeichnung „Glienicker Weg“.
Nach dem Ende des Dreißigjährigen Kriegs konnte sowohl die Bevölkerungsentwicklung als auch die wirtschaftliche Entwicklung innerhalb der vom Militär geschützten Grenzen der Mark Brandenburg vorangetrieben werden. Der Große Kurfürst Friedrich Wilhelm unterstellte dem Amt Cöpenick weitere Dörfer, darunter auch Glienicke.
Das durch Joachim II. im 16. Jahrhundert in Cöpenick errichtete Jagdschloss wurde im Jahr 1677 abgerissen. Es wurde durch das heute noch bestehende Barockschloss ersetzt.
Nach seiner Hochzeit mit Elisabeth Henriette von Hessen-Kassel im Jahr 1679 bezog der Sohn Friedrich Wilhelms, der Kurprinz Friedrich, das Schloss Cöpenick. Das Paar lebte dort bis zum Tod Henriettes im Jahr 1683. Danach lebte Kurprinz Friedrich mit seiner zweiten Frau Sophie Charlotte von Hannover bis 1687 im Schloss. Nachdem der Prinz 1688 Kurfürst Friedrich III. geworden war, hatte er für diesen Wohnsitz keine Verwendung mehr und residierte in Berlin.
Andere Wege waren bereits ab 1677 durch den Königlichen Forst geschlagen worden, so die Allee zu den Müggelbergen und zum Adlergestell.
Der nordwestliche Beginn des vormaligen Glienicker Wegs, von Köpenick bis zur Bahnbrücke des Berliner Außenrings, wurde vor 1920 in Glienicker Straße umbenannt, und das Ende, vom Adlergestell bis Alt-Glienicke, heißt seit vor 1890 Köpenicker Straße.
Nördlich des mittleren Teils des alten Glienicker Wegs entstand Mitte des 18. Jahrhunderts die Flur „Am Süßen Grund“ zwischen Rudower und Glienicker Weg. Hier befand sich eine Kolonie von acht Büdnern, die jeweils einen Morgen Gartenland und einen Morgen Wiese sowie das Recht zur Haltung einer Kuh hatten. Aus dieser Kolonie ging 1879 das an den Glienicker Weg angrenzende Adlershof hervor.
Der Glienicker Weg führte ursprünglich durch feuchtes Gebiet. Bei Glienicke im „Bruchland“ begann der Lauf des „Voll Kropp“, der sich zweimal den Glienicker Weg schneidend bis etwa zum heutigen Berliner Außenring hinzog, um dann weiter bis zur Dahme zu fließen. Dieses letzte Stück wird als „Vollkropfgraben“ bezeichnet, während der Beginn mittlerweile zugeschüttet ist. Entlang des Grabens befanden sich südlich des Glienicker Wegs ausgedehnte feuchte Wiesen, die nach ihrem ehemaligen Besitzer am Ende des 19. Jahrhunderts noch als „Kahlbaums Wiesen“ bezeichnet wurden. Am Graben stand zudem eine der acht noch heute in Berlin vorhandenen Windmühlen, die 1820 als Wuhlkropfmühle gegründet wurde und seit 1850 nach dem Graben Vollkropfs Mühle hieß. Die Mühle steht heute unter ihrem letzten Namen Bohnsdorfer Bockwindmühle im Deutschen Technikmuseum in Berlin-Kreuzberg.
Am östlichen Beginn des noch als Glienicker Weg bezeichneten Mittelstücks des ursprünglichen Königlichen Verbindungswegs von Köpenick nach Glienicke an der Brücke des Berliner Außenrings befindet sich auf der nördlichen Straßenseite zwischen dem Bahndamm des Außenrings und der Wohnanlage Zinsgutstraße die Kleingartenanlage „Lange Gurke“. Diese grüne Insel bildet den Puffer zur denkmalgeschützten Wohnanlage, die von 1929 bis 1931 nach Plänen des Architekten Julius Schüler errichtet wurde. Seitens des Glienicker Wegs betrifft dies die geraden Hausnummern 88–96.
Der Glienicker Weg beginnt nicht, wie sonst üblich, mit der Nummer 1. Die Nummerierung des Glienicker Wegs stammt noch aus der Zeit, als dieser Weg in Köpenick seinen Anfang nahm. Mit der Umbenennung des ersten Teilstücks in Glienicker Straße wurde die alte Nummerierung für den Glienicker Weg beibehalten.
Gegenüber, auf der südlichen Seite des Glienicker Wegs, steht bei Nummer 95 ein Blockheizkraftwerk (BHKW) des Energieversorgers Vattenfall.
Westlich der in den Glienicker Weg mündenden Zinsgutstraße befindet sich eine weitere, als Denkmal geschützte Wohnanlage, die nach Plänen der Architekten Max Abicht und Johannes Ruppert in den Jahren 1936/1937 errichtet wurde. Die Anlage nimmt seitens des Glienicker Wegs die geraden Hausnummern 100–110 in Anspruch und schließt westlich mit der Wassermannstraße ab. Diese Wohnanlage wurde als Gegenplan zu den Mietskasernen der Berliner Innenstadt angelegt. Sie bildet einen geschlossenen Wohnbereich mit großzügigem, begrüntem Innenhof, der von dreietagigen Häusern umbaut ist.
Weiter westlich der Wassermannstraße folgen weitere Wohnhäuser bis zur Nipkowstraße. Danach folgen auf der nördlichen Seite des Glienicker Wegs diverse Großmärkte auf dem bis zum Adlergestell reichenden Gewerbegelände.
Südlich des Glienicker Wegs wird praktisch die gesamte angrenzende Fläche als Industriegelände genutzt. Gegenüber den Wohnanlagen, direkt am Glienicker Weg, befinden sich noch einige Wohnhäuser in Klinkerbauweise, welche ursprünglich als Wohnungen für Werksangehörige dienten.
Der Haupteingang zum denkmalgeschützten Fabrikgelände am Glienicker Weg 125/127 führt zum Bürogebäude der zur Menarini Group gehörenden Berlin-Chemie. Das Gelände wurde nach Plänen des Architekten Max Jacob in den Jahren 1904 bis 1906 mit Fabrikbauten versehen. Zubringergleise verbanden dieses Gelände mit dem Berliner Außenring. Nach 1920 waren die Gebäude in das Eigentum der Reichsmonopolverwaltung für Branntwein übergegangen. Infolge des Zweiten Weltkriegs gab es diese Behörde nicht mehr, und der VEB Bärensiegel Berlin übernahm die Immobilie. Hier produzierten mehrere tausend Beschäftigte bis zur Wende Liköre und Weinbrände. Aktuell werden die denkmalgeschützten Klinkerverblendbauten zu einer Filiale einer Möbelhauskette umgebaut. Das Fabrikgelände zieht sich bis zur Ecke Glienicker Weg 181/Adlergestell 327 hin.
Auf dem westlichen Stück des historischen Glienicker Wegs fuhr ab 1909 die Straßenbahn Adlershof–Altglienicke der Teltower Kreisbahnen. Nach deren Übernahme durch die Berliner Straßenbahn wurde die Verbindung als Linie 84 bezeichnet. Vor dem Zweiten Weltkrieg bis 1962 fuhr sie östlich des Bahndamms der S-Bahn auf dem Adlergestell bis zur Kreuzung mit dem Glienicker Weg. Nachfolgend musste durch die ohnehin nicht breite Eisenbahnbrücke in Richtung Westen befahren werden, um über die Köpenicker Straße bis zur Endstation Am Falkenberg zu gelangen. Ab 1962 wurde die bisherige Trasse vom Adlergestell auf die westliche Seite der S-Bahn verlegt. Dadurch konnte die etwas breitere Brücke über die Rudower Chaussee für die Straßenbahn genutzt werden. Seit 1993 endet sie bereits am S-Bahnhof Adlershof.
Da einerseits die Bebauung nördlich des Glienicker Wegs im Rahmen des Wohnungsbauprogrammes der DDR verdichtet wurde und andererseits das Getränkekombinat Berlin südlich des Glienicker Wegs mit dem Produktionsstandort VEB Bärensiegel Berlin mehrere Tausend Arbeiter beschäftigte, wurde die Buslinie 23 von „Rudower Chaussee“ nach „Altglienicke“ über die Nipkowstraße zum Haupteingang des Werks geführt. Außerdem fuhr die Buslinie 89 vom Werkseingang Bärensiegel über Johannisthal bis zum Bahnhof Schöneweide.
Nach der Wende verödete das Firmengelände südlich des Glienicker Wegs. Nur die Berlin-Chemie AG hat hier als größeres Unternehmen seinen Standort wieder ausgebaut. Die Buslinie 164 verbindet den S-Bahnhof Kaulsdorf über Köpenick, den Glienicker Weg und die Nipkowstraße durch Adlershof hindurch mit dem U-Bahnhof Rudow.
Im Jahre 2006 begann die Erweiterung der Abwasserrohre auf einer Strecke von knapp einem Kilometer. Da zu diesem Zeitpunkt bereits der südliche Abschnitt der Tangential-Verbindung Ost (TVO), die Spindlersfelder Straße, im Bau war, deren Verlängerung und Anbindung in Richtung A 113 und A 117 der Glienicker Weg ist, wurden Teile der Straßendecke modernisiert und für einen vierstreifigen Betrieb vorgesehen.
Der Glienicker Weg stellte als zweispurige Straße einen Flaschenhals dar, da die zubringenden Straßen (Adlergestell, Köpenicker Straße, Spindlersfelder Straße) vier- bis sechsstreifig ausgebaut sind. Der Straßenausbau wurde nicht rechtzeitig geplant und verzögerte sich seit 2003 immer wieder. Insbesondere im Berufsverkehr und Sonntags-Rückreiseverkehr war der Glienicker Weg eine permanente Staufalle. Durch die Verzögerungen beim Ausbau der Straße verfielen Förderzusagen der Europäischen Union in Höhe von insgesamt ca. 5,2 Mio. Euro. Der Baubeginn erfolgte im Mai 2009.
Die Eisenbahnunterführung wurde ab Mai 2009 halbseitig und Ende 2009 mehrere Wochen vollständig gesperrt
Der erste Straßenabschnitt – auf dem westlichen Teilstück die nördliche Fahrbahn – wurde im November 2010 freigegeben. Die alte südliche Fahrbahn wurde anschließend abgebrochen.
Der zweite Straßenabschnitt – auf dem östlichen Teilstück die südliche Fahrbahn – wurde zum Mai 2011 freigegeben. Der Neubau der alten Fahrbahnen wurde am 30. November 2012 freigegeben, einschließlich der neuen Ampelkreuzung Nipkowstraße.
Rudi Hinte: Die Landschaft, in der Adlershof entstand und sich entwickelte – Die Cöllnische Heide. T. 1. in: Adlershofer Zeitung. Berlin 8.2001, S. 9

Das Glioblastom (auch Glioblastoma multiforme) ist der häufigste bösartige hirneigene Tumor bei Erwachsenen. Das Glioblastom weist feingewebliche Ähnlichkeiten mit den Gliazellen des Gehirns auf und wird aufgrund der sehr schlechten Prognose nach der WHO-Klassifikation der Tumoren des zentralen Nervensystems als Grad IV eingestuft. Die Behandlung besteht in operativer Reduktion der Tumormasse, Bestrahlung und Chemotherapie. Eine endgültige Heilung kann derzeit nicht erreicht werden. Die mittlere Überlebenszeit liegt bei wenigen Monaten ohne Behandlung und rund 15 Monaten bei aktuell gängigen Therapiemethoden. Manche Erkrankte überleben länger, nur wenige jedoch mehrere Jahre. In einigen seltenen Fällen haben Betroffene sogar über 20 Jahre überlebt.
Die Glioblastom-Zelllinie U87MG war die erste Krebszelllinie, deren Genom vollständig sequenziert wurde.
Der Begriff Glioblastoma multiforme wurde 1926 von Percival Bailey und Harvey Cushing geprägt. Die Begriffsbildung basierte auf der Vorstellung, dass sich der Tumor aus primitiven Vorstufen von Gliazellen (Glioblasten) entwickelt, sowie der Beobachtung, dass das Erscheinungsbild mit Nekrosen, Einblutungen und Zysten sehr variabel (multiform) sein kann. Der von dem Pathologen Frank Burr Mallory bereits 1914 verwendete Begriff Spongioblastoma multiforme konnte sich nicht durchsetzen.
Glioblastome sind bei Erwachsenen die häufigsten bösartigen hirneigenen Tumore. Unter den aus dem Hirngewebe entstehenden (neuroepithelialen) Tumoren machen sie etwa die Hälfte aller Fälle aus. Der Tumor tritt am häufigsten bei älteren Erwachsenen zwischen dem 60. und 70. Lebensjahr auf; das durchschnittliche Alter bei Diagnosestellung beträgt 64 Jahre. Männer sind deutlich öfter betroffen als Frauen (Verhältnis 1,7:1). Daten des amerikanischen Hirntumorregisters zeigen, dass Glioblastome bei Weißen mindestens doppelt so häufig sind wie in der schwarzen Bevölkerung. Im Vergleich zu Erwachsenen sind Glioblastome bei Kindern sehr selten. Die Inzidenz wurde in Europa und Nordamerika mit 2,9 bis 3,5 Neuerkrankungen pro Jahr auf 100.000 Einwohner ermittelt und ist in Entwicklungsländern geringer. Als einziger gesicherter ursächlicher (ätiologischer) Umweltfaktor gilt derzeit eine Exposition durch ionisierende Strahlung.
Bei der Mehrzahl der Glioblastome handelt es sich um sporadisch auftretende Fälle ohne Hinweis auf eine Erblichkeit.
Bei bestimmten seltenen erblichen Erkrankungen, unter anderem bei dem Li-Fraumeni-Syndrom oder dem Turcot-Syndrom, können Glioblastome jedoch in Familien gehäuft auftreten.
Glioblastome können völlig neu (de novo) oder durch fortschreitende Entdifferenzierung aus weniger bösartigen Astrozytomen entstehen. Daher kommt es nicht selten vor, dass therapierte Astrozytome sich im Rezidiv als Glioblastom manifestieren. Diese sogenannten sekundären Glioblastome treten eher bei jüngeren Patienten auf und haben ein anderes Spektrum genetischer Veränderungen als neuentstandene (siehe Molekularpathologie). In einer in der Schweiz durchgeführten epidemiologischen Studie waren primäre Glioblastome im Kanton Zürich etwa zwanzigmal häufiger als sekundäre.
Das Glioblastom geht von der weißen Substanz aus. Seine mit Abstand häufigste Lokalisation ist das Großhirn, wo es in allen Hirnlappen entstehen kann, aber den Frontal- und den Temporallappen bevorzugt. Im Bereich von Kleinhirn, Hirnstamm und Rückenmark sind Glioblastome selten. Oft wachsen hemisphärielle Glioblastome über den Balken auf die andere Seite hinüber. Solche Tumoren werden als sogenannte „Schmetterlingsgliome“ bezeichnet. Das Wachstum von Glioblastomen ist diffus infiltrierend.
Wegen des raschen Wachstums entwickeln sich die Beschwerden meistens rasch innerhalb weniger Wochen bis Monate. Erste Symptome können anhaltende und ungewohnte Kopfschmerzen, aber auch neu auftretende epileptische Anfälle sein. Fokale neurologische Ausfälle wie Lähmungen, Aphasien und Sehstörungen können lokalisationsabhängig hinzukommen. Schließlich sind es oft auffällige Persönlichkeitsveränderungen, Apathie oder psychomotorische Verlangsamung, die den Patienten zum Arzt führen. Hirndruckzeichen wie Stauungspapille, Erbrechen, Somnolenz und Koma treten spät auf und sind prognostisch ungünstig.
Die Diagnose wird zunächst durch bildgebende Verfahren wie Computertomographie (CT) oder Magnetresonanztomographie (MRT) gestützt. In der CT-Bildgebung mit Kontrastmittel erscheint das Glioblastom unregelmäßig geformt mit randständig starker Kontrastmittelaufnahme (ringförmiges Enhancement). Bei kleineren Tumoren ist dieses ringförmig konfiguriert, bei größeren bildet es eine girlandenartige Formation aus. In der Umgebung des Tumors bildet sich typischerweise ein erhebliches Ödem aus. Der MRT-Befund ist recht typisch: die soliden Anteile des Glioblastoms reichern Kontrastmittel stark an, dagegen heben sich die Aussparungen durch zystische Anteile und die Blutungen ab. Letztendlich wird die Diagnose am Tumorgewebe, das bei einer stereotaktischen Hirnbiopsie oder Tumorresektion gewonnen wurde, neuropathologisch bestätigt. Im Einzelfall werden Supplementäruntersuchungen wie Elektroenzephalografie und Lumbalpunktion durchgeführt, die der Einschätzung der Anfallsneigung bzw. der differentialdiagnostischen Abgrenzung gegen Hirnabszesse oder Lymphome dienen.
Das Glioblastom ist durch seine inhomogene und vielfältige (daher: multiforme) Erscheinung gekennzeichnet: die Tumorschnittfläche weist häufig rötliche Einblutungen und gelbliche Gewebsuntergänge (Nekrosen) auf.
Feingeweblich (histologisch) handelt es sich um zelldichte astrozytär differenzierte Tumoren, die diffus das umgebende reaktiv veränderte Hirngewebe infiltrieren. Die Tumorzellen sind mit multipolaren feinen Fortsätzen fibrillär-astrozytär differenziert oder weisen mit einem aufgeblähten Zytoplasma eine gemästet-zellige Differenzierung auf. Auch Riesenzellen mit bizarren Kernen oder kleinzellige Areale mit wenig ausgedehnten Zellkörpern kommen vor. Die Zellkerne sind meist chromatinreich und vielgestaltig (polymorph). Mitotische und proliferative Aktivität sind erhöht.
Entscheidend für die Diagnose des Glioblastoms (und die Abgrenzung gegenüber dem anaplastischen Astrozytom) ist nach der WHO-Klassifikation der Tumoren des zentralen Nervensystems jedoch der Nachweis von Tumornekrosen (flächenhaft oder typischerweise strichförmig mit perifokaler Zelldichtesteigerung) oder hochgradig pathologischer Blutgefäße.
Bei Gliosarkomen handelt es sich um Glioblastome, die neben den oben beschriebenen astrozytären Tumoranteilen auch bindegewebsreiche sarkomatöse Abschnitte mit spindelzelligen Tumorzellen aufweisen. Als Riesenzellglioblastome werden Glioblastome mit einer ausgeprägten riesenzelligen Komponente bezeichnet. Ebenfalls abzugrenzen sind Glioblastome mit oligodendroglialer Komponente, die möglicherweise eine etwas günstigere Prognose haben.
Immunhistochemisch ist in den Tumorzellen – wie in denen anderer glialen Hirntumoren – das saure Gliafaserprotein (glial fibrillary acidic protein, GFAP) nachweisbar, was in den meisten Fällen die Abgrenzung gegenüber Hirnmetastasen erlaubt.
Die Genverluste (Deletionen), die das Glioblastom ausmachen, betreffen in den meisten Fällen das Tumorsuppressor-Gen TP53 (Chromosom 17), das Retinoblastom-Suppressorgen RB-1 (Chromosom 13) und Deletionen des Chromosoms 22 sowie den Komplettverlust des langen Arms von Chromosom 10. Diese genetischen Schäden liegen häufig kombiniert vor. Bei neu entstandenen primären Glioblastomen, die überwiegend bei älteren Patienten auftreten, treten häufiger Verluste des PTEN-Gens oder eine Amplifikation des EGFR-Gens auf. Bei den überwiegend im mittleren Lebensalter auftretenden sekundären Glioblastomen, welche durch eine schrittweise Fortentwicklung (Progression) aus weniger bösartigen (weniger malignen) Astrozytomen entstandenen sind, liegen häufig Mutationen des TP53-Gens vor. Zudem sind Punktmutationen in für eine Isocitrat-Dehydrogenase codierenden IDH1- und IDH2-Genen in dieser Gruppe häufiger, insbesondere die R132H-Mutation im IDH1-Gen. Die häufigste Mutation (IDH-R132H) kann zuverlässig mittels Immunhistochemie unter Anwendung eines spezifischen Antikörpers nachgewiesen werden.Die seltenen kindlichen Glioblastome unterscheiden sich im Muster genetischer Veränderungen von den bei Erwachsenen auftretenden Tumoren: hier spielen vor allem Mutationen des H3F3A-Gens eine Rolle.
Anhand genetischer und epigenetischer Veränderungen wurde 2012 eine Klassifizierung der Glioblastome in sechs Untergruppen vorgeschlagen.
Eine kurzfristige klinische Besserung kann durch Behandlung des praktisch immer vorhandenen perifokalen Hirnödems mit Corticosteroiden erreicht werden. Die neurochirurgische Operation mit Verminderung der Hauptmasse des Tumors (Tumorreduktion) kann das Fortschreiten der Erkrankung verlangsamen, aber nicht dauerhaft verhindern, da praktisch immer einzelne Tumorzellen das gesunde Gehirngewebe schon infiltrativ durchwandert haben und deswegen eine vollständige Tumorentfernung nicht möglich ist.
Ein neues innovatives Verfahren zusätzlich zur neurochirurgischen Behandlung von bösartigen Hirntumoren (z. B. dem Glioblastom) ist die fluoreszenz-gestützte Chirurgie mit 5-Aminolävulinsäure (5-ALA). Dabei erhält der Patient etwa vier Stunden vor der Operation eine körpereigene Substanz (5-ALA) als Trinklösung, die sich im Hirntumor stark anreichert und dort in einen fluoreszierenden Farbstoff umgewandelt wird. Während der Operation kann dann dieser Farbstoff durch blau-violettes Licht (Wellenlänge 410 bis 440 nm) zum Leuchten (Fluoreszenz) angeregt werden, sodass sich der Tumor (dunkelblau) vom gesunden Hirngewebe (rosa) besonders deutlich abgrenzen lässt. Durch dieses Verfahren ist eine weitgehend komplette Entfernung der Tumoren viel sicherer und effektiver möglich. Das führt zu einer Verlängerung der Zeit bis zum Nachwachsen dieser Tumoren (rezidivfreies Intervall), wodurch die Prognose dieser Erkrankung deutlich verbessert wird. Das Verfahren wurde 2004 in Düsseldorf und München entwickelt und wird in vielen deutschen Kliniken angewandt.
Zur Verlängerung der rezidivfreien und absoluten Überlebenszeit schließt sich an die Operation praktisch immer eine Bestrahlung und häufig auch eine Chemotherapie an, wobei insbesondere Patienten mit Nachweis epigenetischer Veränderungen (Hypermethylierung) des Promotors des DNA-Reparaturenzyms O6-Methylguanin-DNA-Methyltransferase (MGMT) von einer Chemotherapie mit Zytostatikum profitieren.
Bei Patienten mit neu-diagnostiziertem Glioblastom und methyliertem MGMT-Promoter wird seit 2017 in der Erstlinientherapie eine Kombination aus CCNU und Temozolomid sowie Strahlentherapie eingesetzt. Weitere Chemotherapeutika, die unter anderem bei einem Rezidiv eingesetzt werden, sind Nitrosoharnstoffe, Vincaalkaloide, Fotemustin und Cytosinarabinosid, wobei verschiedene Behandlungsschemata in Gebrauch sind. Welche Patienten von einer lokalen Chemotherapie mit Implantation von an Polymere gebundenem Carmustin profitieren können, ist noch unklar.Ein weiteres Verfahren zur Glioblastom-Behandlung sind Tumortherapiefelder. Dabei werden schwache elektromagnetische Wechselfelder im Langwellenbereich über äußere Elektroden auf den erkranken Körperbereich gerichtet. So soll das Wachstum krebsartiger Tumore gehemmt werden. Sie sind eine Ergänzung zum Stupp-Protokoll bei der Erstdiagnose.
Die Wirkung von Methadon wurde inzwischen bei vielen fortgeschrittenen Krebsarten, wie Bauchspeicheldrüsenkrebs, Brust-, Eierstock-, Darm-, Magen-, Lungen- Blasen- und Prostatakrebs und bei Hirntumoren (Glioblastome), Leukämien und Melanomen aufgrund von Patientenberichten um Friesen, der Entdeckerin des Wirkungsmechanismus, beschrieben. Außerdem scheint es bei Tumorerkrankungen mit Aszites und Pleuraergüssen unter Methadongabe häufiger zu Rückbildungen dieser Wasseransammlungen zu kommen.
Obwohl es inzwischen viele positive Fallbeispiele gibt, ist es schwierig, die Beobachtungen auf andere Patienten zu übertragen. Es gibt noch keine aussagekräftigen Studien an Patienten, um Methadon als Wirkverstärker unterstützend zu Chemotherapien einzusetzen.
Die einzige klinische Studie zu Gliomen und D, L-Methadon konnte bei 27 Patienten zeigen, dass Methadon ohne erhebliche Nebenwirkungen zur Therapie bei Gliomen, einer bestimmten Art von Hirntumoren, eingesetzt werden kann. In dieser kleinen Patientengruppe war das rezidivfreie Überleben mit Methadon und Chemotherapie prozentual höher als im Vergleich zur historischen Kontrolle (Onken J / Anticancer Res 2017). Andere Studien mit Patienten gibt es bisher nicht.
Fazit: Die Kombination von Methadon und Chemotherapie kann den Behandlungserfolg verbessern helfen. Das betrifft vor allem Patienten mit Metastasen oder Rezidiven, die auf eine ausschließlich konventionelle Therapie nicht mehr oder nur unbefriedigend ansprechen.
Auch wenn ein besseres Tumoransprechen nicht bei jedem Patienten auftritt, kann zumindest bei den meisten Betroffenen die Lebensqualität verbessert werden. Denn Krebspatienten, die von anderen starken Opioiden auf Methadon umgestellt werden, haben danach einen geringeren Schmerzmittelbedarf und fühlen sich oft weniger müde (Sugiyama Y / Journal of Palliative Medicine 2016 und Courtemanche F / J Palliat Med 2016).
Die Entwicklung neuer Behandlungsformen bei Glioblastomen ist Gegenstand intensiver Forschung. Im Februar 2013 waren 257 klinische Studien bei Clinicaltrials.gov, einem Register der United States National Library of Medicine als aktiv oder in Vorbereitung registriert. Tyrosinkinaserezeptoren, wie die Rezeptoren für epidermalen Wachstumsfaktor (EGFR) und Platelet Derived Growth Factor (PDGF), stellen mögliche Zielmoleküle für neue therapeutische Ansätze dar.Eine Behandlung mit Bevacizumab, einem den Vascular endothelial growth factor (VEGF) neutralisierenden Antikörper, konnte in klinischen Studien in Kombination mit dem Topoisomerasehemmer Irinotecan das Gesamtüberleben nicht verbessern, wobei einzelne Patientengruppen möglicherweise günstig auf diese Behandlung ansprechen.Die Therapie im Rahmen einer klinischen Studie mit APG101, einem vollständig humanen CD95-Fc-Fusionsprotein, das die Bindung des CD95-Liganden an den CD95-Rezeptor verhindert, stellt einen neuartigen Behandlungansatz dar. Sie basiert auf Erkenntnissen aus dem Deutschen Krebsforschungszentrum und dem Universitätsklinikum Heidelberg, wonach in Glioblastom-Zellen die Bindung des CD95-Liganden an den CD95-Rezeptor das invasive Wachstum und die Migration der Tumorzellen stimuliert. Eine Blockade dieser Bindung durch APG101 soll daher zu einer Reduktion des invasiven Wachstums und der Migration dieser Zellen führen. Eine Phase I-Studie mit 34 gesunden Probanden zur Untersuchung der Sicherheit und Verträglichkeit von APG101 zeigte eine gute Verträglichkeit der Substanz. Die mögliche Wirksamkeit von APG101 wurde in einer randomisierten, kontrollierten klinischen Phase II-Studie mit Patienten untersucht, die an GBM erkrankt sind und bei denen ein Rückfall der Erkrankung aufgetreten ist. Die Patientenrekrutierung ist abgeschlossen. Insgesamt wurden 83 Patienten im Rahmen der klinischen Prüfung behandelt. Der primäre Endpunkt der Studie, eine Verdoppelung der Zahl von Patienten mit progressionsfreiem Überleben nach sechs Monaten, wurde übertroffen. Während der Behandlung mit APG101, die bis zu zwei Jahre andauerte, wurden keine schwerwiegenden Nebenwirkungen beobachtet, die im Zusammenhang mit dem Wirkstoff stehen.
Auch gentherapeutische Verfahren werden im Rahmen klinischer Studien erprobt.Ein anderer experimenteller Ansatz ist die Behandlung mit Nanoteilchen. Diese bestehen aus einem Eisenoxidkern sowie einer Hülle, die das Eindringen der Eisenoxidpartikel in die Krebszellen erleichtern soll. Die Partikel werden direkt in den Tumor injiziert. In mehreren Durchgängen wird der so mit den Eisenoxid-Teilchen, welche ein Ferrofluid bilden, angereicherte Tumor mit Magnetwechselfeldern auf über 46 °C erwärmt. Im Tiermodell ergaben sich deutlich verbesserte Überlebenszeiten. Studienergebnisse beim Menschen liegen seit September 2010 vor, seit Mitte 2011 ist die Therapie verfügbar.In einem anderen Forschungsansatz wurde wie bei anderen Krebserkrankungen mit Parvoviren gearbeitet. Bis auf eine Phase I/II Studie an 18 Patienten mit Glioblastomen aus dem Jahre 2012 wurden bislang keine weiteren Daten publiziert. Ein vergleichbarer Ansatz ist die Behandlung mit genetisch verändertem, attenuiertem Poliovirus (PVS-RIPO), die sich noch in einem frühen experimentellen Stadium befindet.
Das Glioblastom ist äußerst schwierig zu behandeln. Eine endgültige Heilung ist bislang in der Regel nicht möglich. Die Behandlung mit Operation, nachfolgender Bestrahlung und Chemotherapie kann nach aktueller Studienlage die mittlere Überlebenszeit um einige Monate verlängern und die Symptome lindern. Eine Studie aus dem Jahr 2003 unterteilt die Prognose mithilfe der Recursive Partitioning Analysis (RPA) in drei Gruppen in Abhängigkeit vom Alter des Patienten, von der Art der Behandlung und vom Karnofsky-Index (KPS).
Wegen der diffusen Infiltration des Hirngewebes durch Tumorzellen kommt es nach der Behandlung häufig innerhalb von Monaten zu einem Rezidiv. Einzelne Patienten können dessen ungeachtet mehrere Jahre bei relativ guter Gesundheit mit einem Glioblastom leben. Die Identifizierung klinischer und molekularer Faktoren, die charakteristisch für solche Langzeitüberlebenden sind, ist Gegenstand intensiver Forschung.
Wolfgang Wick, Jörg-Christian Tonn, Michael Weller: Primäre intrakranielle und spinale Tumoren. In: Thomas Brandt, Johannes Dichgans und Hans Christoph Diener (Hrsg.): Therapie und Verlauf neurologischer Erkrankungen. 5. Aufl., Kohlhammer, Stuttgart 2007, ISBN 978-3-17-019074-0.
Jörg-Christian Tonn, F. W. Kreth: Hirntumoren und spinale Tumoren. 4. Aufl., Zuckschwerdt Verlag, Germering 2016, ISBN 978-3-86371-199-3.
Arbeit und Struktur Tagebuch des Schriftstellers Wolfgang Herrndorf über sein Leben mit der Diagnose Glioblastom
Hoffnung trotz Glioblastom – ein Erfahrungsbericht (PDF; 369 kB) in Brainstorm 01/2006, herausgegeben von der Deutschen Hirntumorhilfe
