
Der Ford Ka ist ein Pkw-Modell des Automobilherstellers Ford. Der Wagen hatte seine Markteinführung am 11. September 1996 und war einer der ersten Vertreter der Kleinstwagen-Klasse. Der Produktionszeitraum der auffällig aussehenden ersten Modellgeneration war mit zwölf Jahren ungewöhnlich lang.
Auf der Mondial de l’Automobile in Paris stellte Ford Anfang Oktober 2008 die zweite Modellgeneration vor, die gemeinsam mit dem italienischen Automobilhersteller Fiat entwickelt wurde. Die Markteinführung in Deutschland war am 14. Februar 2009. Die Produktion lief nach mehr als sieben Jahren im April 2016 aus.Anfang Juni 2016 präsentierte Ford die Europaversion der dritten Generation, die seit Oktober 2016 unter dem Namen Ford Ka+ verkauft wird.
Als günstiger Zweitwagen für die Stadt gedacht, verteidigte der Ka die Marktanteile, die sich Ford in den 1980er-Jahren mit den Modellen Escort und Fiesta im europäischen Kleinwagensektor erarbeitet hatte. Obwohl er deutlich kleiner als ein Fiesta war, handelte es sich bei dem Dreitürer um einen vollwertigen Viersitzer. Außerhalb von Europa war die erste Modellgeneration von 1997 bis 2007 in Mittel- und Südamerika, der Türkei sowie von 1999 bis 2006 in Japan erhältlich. Von 1999 bis 2002 wurde er darüber hinaus in Australien angeboten.Die europäischen Modelle produzierte Ford in Almussafes bei Valencia (Spanien). 2002 lief dort das einmillionste Fahrzeug vom Band, insgesamt wurden rund 1,5 Millionen Exemplare verkauft. Die Fahrzeuge für den süd- und mittelamerikanischen Markt kamen aus dem Werk São Bernardo (Brasilien).
Der Ursprung des Namens liegt in einem Bestandteil der altägyptischen Seelenvorstellung, wo Ka der Teil einer Seele war, der die Lebenskraft spendet. Noch heute ist im Ägyptischen Ka als Bezeichnung für die menschliche Seele üblich. Dies nahm Ford bei der Markteinführung zum Anlass, mit dem Slogan „Auto mit Seele“ für den auffälligen Wagen zu werben.
Die weit verbreitete Annahme, der Name entspringe einer alternativen, nicht-rhotischen Schreibweise des englischen Wortes für Automobil, car, ist nicht korrekt.
Mit dem Ka führte der damalige Ford-Designchef Claude Lobo das anfänglich umstrittene New-Edge-Design ein, eine Kombination von gewölbten Flächen und kantigen Details. 1994 auf dem Genfer Auto-Salon vorgestellt, errang der Wagen weltweit zahlreiche Auszeichnungen. 1995 präsentierte Ford auf der North American International Auto Show in Detroit den Prototyp des vom New-Edge-Design geprägten Supersportwagen GT90. Nach der Markteinführung des Ka erschienen mit der neuen Generation des Mondeo, dem Puma, dem Cougar und dem Focus weitere Fahrzeuge dieser Designlinie.
Auf der Essen Motor Show 1998 präsentierte Ford ein von Luigi Colani aerodynamisch optimiertes Redesign des Ka, das auf große Nachfrage stieß und vom Kölner Ford-Haupthändler R&S Mobile in einer Kleinserie von 200 Exemplaren produziert wurde.
Der Ka baute technisch auf dem Fiesta ’96 auf und teilte sich mit diesem die Bodengruppe. Daher gab es viele Gemeinsamkeiten zwischen den Fahrzeugen, sie waren beide frontgetrieben, hatten die gleichen Motoren und die gleichen vorderen Einzelradaufhängungen mit MacPherson-Federbeinen und Verbundlenker-Hinterachsen. Ebenfalls gleich war die Bremsanlage, massive Scheibenbremsen vorne und Trommelbremsen hinten.
Motorisiert war der Ka zunächst ausschließlich mit einem 1,3-Liter-Endura-E-Motor, wahlweise mit 37 kW oder 44 kW. Dieser Vierzylinder-Motor mit seitlicher Nockenwelle und hängenden Ventilen, ebenfalls in Valencia produziert, basierte auf dem gleichen Motor, der schon den ersten Ford Fiesta antrieb. Da die untenliegende Nockenwelle durch eine Steuerkette angetrieben wurde, waren die Wartungskosten niedriger als bei einem Nockenwellenantrieb mit Zahnriemen, der in regelmäßigen Abständen ersetzt werden muss. Die beiden Motorvarianten waren zwar bis auf die Drosselklappe gleich, die 37-kW-Version war jedoch nicht mit der bei der 44-kW-Variante serienmäßigen Servolenkung ausgerüstet. Sie wurde im Oktober 1998 leicht auf 36 kW gedrosselt, und im August 1999 aus dem Programm genommen.
Ende 2002 wurde dieser Motor durch einen Vierzylinder-Duratec-Motor ersetzt, der im südafrikanischen Ford-Werk in Port Elizabeth produziert wurde. Wie beim Endura-E bestand der Zylinderblock aus Grauguss und die Nockenwelle wurde mit einer Steuerkette angetrieben, war aber obenliegend angeordnet. Darüber hinaus gab es beim Duratec einen modernen Aluminiumzylinderkopf und Rollenschlepphebel mit hydraulischem Ventilspielausgleich, wodurch das regelmäßige Einstellen des Ventilspiels entfiel.
Außerdem erreichte er die Abgasnorm Euro 4. Es gab diesen Motor ebenfalls mit 1,3 Litern Hubraum, wahlweise mit 44 kW und 51 kW sowie in einer 70 kW starken 1,6-Liter-Version, die jedoch dem SportKa vorbehalten war. Anders als beim Endura-E war für den Leistungsunterschied der beiden 1,3-Liter-Varianten ein anderes Motorsteuergerät verantwortlich.
Die Kraft wurde bei allen Motoren über eine hydraulisch betätigte Kupplung und ein Fünfgang-Schaltgetriebe übertragen. Fahrzeuge mit Klimaanlage waren kürzer übersetzt, um eine ähnlich schnelle Beschleunigung bei eingeschalteter Klimaanlage zu erreichen. Dies erhöhte den Kraftstoffverbrauch, er lag auf 100 Kilometern um bis zu 0,3 Liter höher als bei Fahrzeugen ohne Klimaanlage.
In Südamerika war der Ka schon bei seiner Markteinführung mit einem 1,0-Liter-Endura-E-Motor erhältlich, da hier Fahrzeuge mit maximal einem Liter Hubraum steuerbegünstigt sind. Dieser Motor lieferte 39 kW und wurde 2000 von einem 1,0-Liter-Duratec-Turbomotor mit 48 kW abgelöst. Ab 2001 war der Ka als sogenannter Ka XR, ähnlich wie der SportKa hier, mit dem 1,6-Liter-Motor erhältlich. Der Motor wurde in Südamerika jedoch nicht als Duratec, sondern unter der Bezeichnung Zetec-RoCam vertrieben.
Der Ka war eines der ersten Großserienfahrzeuge, das serienmäßig mit Klarglasscheinwerfern ausgestattet wurde.
Bedingt durch die lange Bauzeit wurden dem Ka mehrere kleinere Facelifts zuteil. Allerdings griffen diese nicht nennenswert in die äußere Erscheinung des Modells ein, sondern waren hauptsächlich auf technische Änderungen beschränkt.
November 1998: Der Ka wurde mit einer anderen Hinterachse mit geänderten Stoßdämpferaufnahmen (Auge statt Gabel) gebaut. Darüber hinaus änderte sich die Position der Spritzdüsen, sie wurden vom Windlauf auf die Motorhaube versetzt.
Februar 1999: Die Fahrzeuge bekamen serienmäßig eine dritte Bremsleuchte. Außerdem unterschied sich die Form von Antennenfuß und Heckklappengriff, dieser wurde kleiner. In den Ablagefächern der Türverkleidungen waren Becherhalter eingebaut, der Schalter für die Warnblinkanlage wurde verlegt, statt am Radio war er nun am Lenkrad zu finden. Auf Wunsch waren die Stoßfänger jetzt in Wagenfarbe lackiert erhältlich.
März 2000: Der Ka wurde mit bequemeren Sitzen angeboten, die in Deutschland serienmäßig mit Seitenairbags ausgestattet waren. Der Fahrersitz erhielt eine manuelle Höhenverstellung, auf der Rücksitzbank gab es jetzt Kopfstützen. Des Weiteren wurden die hinteren Fahrwerksfedern geändert, sie waren seitdem deutlich schmaler als zuvor. Der Ka verfügte nun über innenbelüftete Bremsscheiben an der Vorderachse, außerdem wurde die Anordnung der Flüssigkeitsbehälter im Motorraum verändert.
Oktober 2002: Die Motorisierung des Ka wurde auf Duratec-Motoren umgestellt, darüber hinaus gab es ein neues Kombiinstrument, das nicht mehr über eine Tachowelle funktionierte, sondern elektronisch. Teilweise gab es hier serienmäßig einen Drehzahlmesser. Den Beifahrerairbag gab es jetzt auf Wunsch deaktivierbar. Der Ka bekam andere Radzierblenden, außerdem wurden die vorderen Bremsleitungen verändert.
Januar 2005: Die auffälligste Überarbeitung war ein anderes Armaturenbrett mit „klassischem“ Handschuhfach. Es gab neue elektronische Funktionen, zum Beispiel ein frei programmierbares Scheibenwischerintervall. Schaltknauf und Schaltsack bestanden aus einer Einheit und waren gesteckt statt geschraubt.
Sonstige Aufwertungen: Im Gegensatz zu diesen äußerlich kaum erkennbaren Änderungen wurde der Ka in Südamerika 2003 deutlich überarbeitet, auffälligste Merkmale sind hier das an der Heckklappe und nicht am Stoßfänger angebrachte Kennzeichen, die Stoßfänger und die Heckleuchten.
Der Ka hatte seit seiner Markteinführung in Deutschland serienmäßig einen Fahrerairbag, bis Oktober 1997 war ein Beifahrerairbag aufpreispflichtig. Vierkanal-Antiblockiersystem war ab Januar 1997 ebenfalls gegen 560 DM Aufpreis verfügbar, darüber hinaus bei einigen Sondermodellen serienmäßig eingebaut. Seit 2000 gab es den Ka mit innenbelüfteten Bremsscheiben an der Vorderachse. Dazu gab es in Deutschland mehr Sicherheit durch serienmäßige Seitenairbags, die in Österreich und anderen europäischen Ländern nur gegen Aufpreis erhältlich waren. Seit 2005 war Antiblockiersystem europaweit serienmäßig. Elektronisches Stabilitätsprogramm war beim Ka jedoch nicht erhältlich.
Bei dem vom ADAC im Oktober 2000 durchgeführten Euro-NCAP-Crashtest erreichte der Ka lediglich drei von fünf Sternen. Allerdings wurde hier die Europa-Version ohne Beifahrer- und Seitenairbag getestet.
Es gab eine Reihe von Problemen, die beim Ka immer wieder auftraten. Hierzu gehörte vor allem Rost im Bereich des Tankdeckels, an den Einstiegsleisten (unter der Türdichtung), der Heckklappe, an den A-Säulen, den Schwellern und unter dem vorderen und hinteren Stoßfänger. Dies betraf auffällig oft Fahrzeuge der Baujahre ab 2000. Außerdem gab es, genau wie bei Fiesta und Escort, häufiger Probleme mit den Türschlössern. Kabelbrüche im Türschlauch sorgten des Öfteren für Defekte bei der Zentralverriegelung, den elektrischen Fensterhebern oder den Lautsprechern. Gelegentlich kam es zu schadhaften Gebläsewiderständen, in diesem Fall funktionierte das Gebläse nur noch auf höchster Stufe. Ähnlich wie beim Fiesta war beim Ka der Lenkstockschalter anfällig für Defekte. In den meisten Fällen war hier jedoch nur eine einzige Funktion des Schalters betroffen, beispielsweise die Warnblinkanlage oder der Heckscheibenwischer.
Bei Fahrzeugen mit Endura-E-Motor kam es oft zu Problemen mit dem Drosselklappen-Potentiometer und dem Leerlaufregelventil, was sich durch unkontrolliertes Hochdrehen oder Absterben des Motors beim Auskuppeln äußert. Des Weiteren kam es bei Fahrzeugen aus den Jahren 1996 und 1997 durch Materialfehler zu eingelaufenen Nockenwellen, spätere Baujahre waren nicht betroffen. Fahrzeuge mit Duratec-Motor hatten gelegentlich Störungen des Motormanagements, Hauptursache waren defekte Lambdasonden. Als Spätfolge resultierte hieraus in seltenen Fällen ein defekter Katalysator. Darüber hinaus kam es häufiger zu Defekten bei Zündspule und Lichtmaschine.Bei Fahrzeugen mit Klimaanlage kam es durch einen verstopften Abflussschlauch gelegentlich zur Ansammlung von Kondenswasser im Beifahrerfußraum.
Der Ka war neben der normalen Ausstattungsvariante in Deutschland in einer ganzen Reihe von Sondereditionen erhältlich.
In Zusammenarbeit mit dem Künstler HA Schult entstand in einer Kleinserie von 50 Stück der Kugel-Ka, ein schwarzer Ka, auf dessen Seiten mit Airbrush Schults Weltkugel verewigt wurde, ein Kunstobjekt in Form eines teililluminierten Globus, das zunächst auf einem Brückenpfeiler der Kölner Severinsbrücke stand und später auf dem Dach eines Versicherungsgebäudes am Kölner Rheinufer Platz fand. Der Schaltknauf des Kugel-Ka war eine in Kunstharz gegossene Miniatur der Schult’schen Weltkugel. Für das Airbrush wurde unter anderem Kölner Brückengrün verwendet.
Im Ausland gab es eine ganze Reihe anderer Sondermodelle. So wurden in den Niederlanden von 1997 bis 2008 unter der Bezeichnung Kunst met een grote Ka insgesamt zwölf Sondermodelle des Ka in einer Auflage von 200 bis 1000 Stück auf den Markt gebracht. Jedes dieser Modelle wurde gemeinsam mit Werken eines bekannten Künstlers ausgestellt und nach diesem benannt. Beim Kauf eines dieser Modelle erhielten die Käufer eine Reproduktion eines Kunstwerks des Künstlers, beispielsweise in Form einer Skulptur oder eines Kunstdrucks.
In Belgien gab es 1997 eine nur 24 Fahrzeuge umfassende Sonderedition namens Zebra, hier wurde ein weißer Ka im Zebra-Look mit schwarzen Streifen lackiert.
Weitere Zusatzausstattungen wie beispielsweise das Audiosystem 6000, Zentralverriegelung oder elektrische Fensterheber waren bei diesen Sondermodellen ebenfalls gegen Aufpreis lieferbar.
Saetta: Im Frühjahr 1995, noch vor der Markteinführung des Ford Ka, entwickelte Ghia diesen Prototyp, einen zweisitzigen Roadster, der sich design-technisch an den Ka anlehnt und später Grundlage für die Entwicklung des Ford Streetka werden sollte.Step 1: 1997 wurde auf der London Motor Show der Prototyp eines Dreiliterautos auf Basis des Ka vorgestellt, der von einem 1,3-Liter-Dieselmotor mit Direkteinspritzung angetrieben wurde und 3,1 Liter auf 100 Kilometer verbrauchte. Neben einer deutlich verbesserten Aerodynamik wurde dies durch rollwiderstandsarme Reifen und Gewichtsreduktion um 25 Prozent auf nunmehr 610 kg erreicht.Ford Ka Pickup: 1997 entstand im Prototypen-Rohbau des Kölner Ford-Werkes eine Pickup-Version des Ka ohne Straßenzulassung, die später, in Schwarz und Regenbogenfarben lackiert, mehrere Jahre beim Kölner Christopher Street Day, der Cologne Pride, von der Mitarbeiter-Gruppe Ford GLOBE (Gay, Lesbian Or Bi-Sexual Employees) für den Straßenumzug genutzt wurde.
Turing Ka: Auf dem Turiner Autosalon 1998 zeigte Ghia diese Studie, ein sportlicher, viertüriger Kleinstwagen auf Basis des Ka. In Zink-Gelb und Titan-Grau gehalten, war er fast 43 cm länger als der Ka. Er war mit 17-Zoll-Leichtmetallräder, Scheibenbremsen vorne und hinten, einem integrierten Dachgepäckträger sowie dem im Ford Puma verbauten 1,7-Liter-Motor mit 92 kW ausgestattet. Besonders edel der Innenraum: In Dunkelblau gehalten, mit braunem Pekari-Leder und vier Veloursschalensitzen. Mit einem maßgeschneiderten Kofferset war eine optimale Raumausnutzung des Kofferraums gewährleistet.
Streetka: 2000 als Studie vorgestellt, wurde dieses Cabrio auf der technischen Basis des Ka von 2003 bis 2005 produziert. Es handelte sich jedoch um ein komplett eigenständiges Modell, die Motorisierung war identisch mit der des Sportka. 
e-Ka: Im Jahr 2000 präsentierte Ford diesen 65 kW starken, elektro-betriebenen Prototypen, der gemeinsam mit dem französischen Batterie-Hersteller SAFT entwickelt wurde. Durch Verwendung von Aluminium bei Hinterachse und Bremsen, sowie des Verbundstoffes Hylite, aus dem Motorhaube und Dach bestehen, konnte das Gewicht des Wagens um 45 kg verringert werden. Mit Hilfe eines Lithium-Ionen-Akkumulators betrug die Reichweite maximal 150 km.CNG-Ka: Ford stellte Anfang 2001 zunächst ein erdgasbetriebenes Versuchsfahrzeug vor. Ab Juli 2001 wurde beim Kauf eines neuen Kas die Umrüstung auf bivalenten Erdgasbetrieb mit einem 75 Liter fassenden Gas-Tank im Kofferraum für 5.580 Mark (etwa 2.853 Euro, heute etwa 3.710 Euro) angeboten. Mit der Umstellung auf Duratec-Motoren wurde dieses Angebot eingestellt, da diese Motoren zur Umrüstung nicht geeignet waren.
Motion Ka: Ford hatte drei deutschen Hochschulen einen Ford Ka zur Verfügung gestellt, der nach einem eigenen Konzept umgestaltet werden sollte. Im September 2001 stellten sieben Studenten der Universität Essen den zu einem Freizeit- und Strandmobil umgebauten Wagen vor. Das Fahrzeug wurde höher gelegt, außerdem wurden Kotflügel, Heck- und Frontpartie verändert, um einen Offroad-Charakter und eine dynamische Seitenlinie zu vermitteln. Der Wagen konnte mit einem Wasserschlauch gereinigt werden, da im Innenraum sämtliche Bodenvertiefungen und Kanten entfernt wurden. Die Vordersitze konnten auch als Strandstuhl, die Rücksitzbank als Hängematte oder Gepäcknetz verwendet werden. In der Mittelkonsole war ein Multifunktionsdisplay integriert, das als Tachometer diente, herausnehmbar war und als Gameboy benutzt werden konnte.
Der Ka erhielt, hauptsächlich durch das auffällige Design, zahlreiche internationale Auszeichnungen.
Das japanische Car Styling Magazine verlieh dem Ka 1997 ihren höchsten Designpreis, den Golden Marker. Die Jury sieht im Ka „Die Verkörperung des Designs der Zukunft“.
Das britische Automagazin What Car? wählte den Ka von 1997 bis 2002 sechsmal in Folge zum Wirtschaftlichsten Fahrzeug in Großbritannien.
Das britische Automagazin What Car? wählte den Ka zum Besten Stadtauto 1999, „Praktisch, einfach, mit Spaß zu fahren und einfallsreichen Stauraumlösungen ausgestattet“.
Bei seiner Markteinführung in Deutschland lag der Grundpreis für das 44 kW-Modell bei 18.150 DM (rund 9.280 Euro, heute etwa 12.860 Euro), das bis 1998 erhältliche 37 kW-Modell kostete 16.750 DM (rund 8.564 Euro, heute etwa 11.870 Euro). Obwohl die Ausstattung seitdem deutlich umfangreicher wurde, blieben die Preise weitestgehend identisch, das Grundmodell kostete in Deutschland 2006 9.300 Euro (heute etwa 11.200 Euro) und ab 2007 wegen der Mehrwertsteuererhöhung 9.600 Euro (heute etwa 11.400 Euro).
Im Januar 2005 wurde mit dem Ka Student ein preisreduziertes Sondermodell für 7.990 Euro (heute etwa 9.780 Euro) eingeführt, das ab 2007 8.190 Euro (heute etwa 9.720 Euro) kostete.
Der Ka verfügte entgegen gängigen Standards aus Kostengründen nicht über eine vollverzinkte Karosserie, daher gab Ford lediglich sechs Jahre Garantie gegen Durchrosten von Karosserieteilen, bei den meisten anderen aktuellen Ford-Modellen waren dies zwölf Jahre.Seit September 2006 verfügte der Ka über das von der TÜV-Rheinland-Gruppe vergebene Label Allergie getesteter Innenraum.Ein Ford Ka der Baujahres 2001 konnte in den Rennspielen Gran Turismo 4 und 5 gefahren werden.
Bei der im Frühjahr 2008 diskutierten, deutschlandweit flächendeckenden Einführung von E10-Kraftstoff wurde der Ford Ka als größtes Opfer dieser Regelung bezeichnet. Er war das in Deutschland mit Abstand weitestverbreitete Fahrzeugmodell, für das zu diesem Zeitpunkt der Fahrzeughersteller E10-Kraftstoff nicht freigegeben hatte. Dies wurde von Ford später revidiert, dem Ka wurde die Freigabe für E10-Kraftstoff erteilt. Jedoch setzte das Bundesministerium für Umwelt, Naturschutz und Reaktorsicherheit die Verordnung zur Einführung von E10 kurz darauf aus.
Ab dem 14. Februar 2009 war in Deutschland die zweite Modellgeneration des Ford Ka erhältlich, die gemeinsam mit Fiat auf Basis des Panda entwickelt wurde.
Im September 2005 gaben Ford und Fiat bekannt, ein Memorandum of Understanding zur gemeinsamen Entwicklung eines Kleinstwagen unterzeichnet zu haben. Bereits seit Oktober 2007 bot Fiat in Deutschland den aus dieser Kooperation entstandenen Fiat 500 im Retro-Design an. Beide Fahrzeuge, sowohl der Fiat 500 als auch der Ford Ka, wurden im Fiat-Werk in Tychy (Polen) produziert. Jährlich wurden von beiden Modellen jeweils 120.000 Exemplare hergestellt, das Werk wurde hierzu eigens ausgebaut.
In Österreich fand die Markteinführung des Ka bereits am 19. Januar 2009 statt.Fotos von getarnten Erlkönigen wurden bereits Ende 2007 in der Fachpresse veröffentlicht. Offiziell vorgestellt wurde die Modellgeneration jedoch erst im Oktober 2008.
Der Ka hat trotz gleicher Plattform starke stilistische Unterschiede zum Fiat 500. Das Designer-Team um Martin Smith gestaltete die Außenhaut des Ka im Kinetic-Design und orientierte sich an aktuellen Modellen des Mondeo, Focus und Fiesta, ganz im Gegensatz zum Retro-Look des Fiat 500. Auch der Innenraum wurde modern gestaltet.
Der Ka besitzt eine andere Fahrwerksabstimmung als der Fiat 500, die Torsionssteifigkeit der Stabilisatoren wurde an der Vorderachse um 20 Prozent und an der Hinterachse um 70 Prozent erhöht. Daraus resultierten geringere Wankbewegungen des Fahrzeugs, was eine deutlich komfortablere Stoßdämpfereinstellung ermöglichte. Auch die elektrische Servolenkung (EPAS) wurde deutlich überarbeitet.
Die beim Ka verwendeten Motoren sind identisch mit denen des Fiats – ein Reihenvierzylinder-Benzinmotor (FIRE-Motor) mit 1,2 Liter Hubraum (51 kW) sowie ein 1,2-Liter-Dieselmotor mit Rußpartikelfilter (55 kW), der gemeinsam mit Fiat Powertrain Technologies entwickelt wurde und u. a. bereits den Opel Corsa C antrieb. Beide Motoren erreichen die Abgasnorm EURO 4. Die Ausführung mit Dieselmotor hat mit innenbelüfteten Bremsscheiben ein verbessertes Bremssystem an der Vorderachse. An der Hinterachse sind bei allen Modellen Trommelbremsen. Der Dieselmotor wurde ungeachtet seines Hubraums als 1.3 TDCI bezeichnet.
Seit November 2010 war der Ford Ka serienmäßig mit Start-Stopp-System und einer Schaltempfehlungsanzeige ausgerüstet, außerdem erreichten beide Motoren nun die Abgasnorm EURO 5. Ende 2013 wurde der Dieselmotor aus dem Programm genommen.
Erhältlich war der Ka bei seiner Markteinführung in Deutschland in zwei verschiedenen Ausstattungsvarianten. In der Basisversion Trend hatte das Fahrzeug unter anderem vier Airbags, Antiblockiersystem mit elektronischer Bremskraftverteilung (EBD, Electronic Brakeforce Distribution), elektrische Servolenkung, Bordcomputer, Drehzahlmesser und Reifen der Größe 175/65 R 14. Lenksäule und Fahrersitz waren höhenverstellbar; die Rücksitzlehne war 50/50 geteilt. Ab dem Modelljahr 2010 enthielt das Modell Trend zusätzlich elektrische Fensterheber, Zentralverriegelung mit Funkfernbedienung und elektrisch verstell- und beheizbare Außenspiegel.
In Österreich und der Schweiz hieß das Basismodell Ambiente, das in Österreich serienmäßig keine geteilte Rücksitzlehne und keine Seitenairbags hatte. Zusätzlich wurde in Österreich im Januar 2011 das Modell Trend eingeführt, die Ausstattung war identisch mit der des gleichnamigen Modells in Deutschland.
Die umfangreichere Titanium-Ausstattung bot neben äußerlichen Aufwertungen unter anderem mit Reifen der Größe 195/50 R 15 eine Zentralverriegelung mit Funkfernbedienung, elektrisch verstell- und beheizbare Außenspiegel, elektrische Fensterheber, Klimaanlage mit Pollen- und Staubfilter, eine Außentemperaturanzeige im Bordcomputer und Nebelscheinwerfer. Auch hier wurde die Serienausstattung zum Modelljahr 2010 erweitert, nun enthielt das Modell serienmäßig ein CD-Radio, eine verchromte Auspuffblende, ein Lederlenkrad und Türgriffe im Chrom-Dekor.
Bei beiden Ausstattungen waren beheizte Vordersitze und ein hinteres Park-Pilot-System erhältlich; beim Titanium-Modell eine Teillederpolsterung mit Sitzheizung sowie eine automatische Temperaturregelung der Klimaanlage.
Ab Juli 2009 war der Ka mit 1,3-Liter-Dieselmotor nur noch in der Ausstattungsvariante Titanium erhältlich, ab 2011 dann auch wieder in der Ausstattungsvariante Trend.
Im Januar 2010 führte Ford in Deutschland das Editionsmodell Concept ein, das mit einer geringeren Basisausstattung und einer stark gekürzten Aufpreisliste den Nachfolger des Ford Ka Student darstellte. Im Vergleich zum Trend hatte das Modell lediglich zwei Airbags serienmäßig, außerdem fehlten die beim Trend ab Modelljahr 2010 serienmäßigen Extras. Seit Januar 2011 hieß diese Ausstattungsvariante bei identischer Ausstattung Ambiente, wie das Basismodell in Österreich und der Schweiz.
Der Ka war bei seiner Markteinführung in Deutschland und der Schweiz serienmäßig mit Airbags für Fahrer und Beifahrer, Seitenairbags für die Frontpassagiere, Antiblockiersystem mit einer elektronischen Bremskraftverteilung und Notbremslicht ausgestattet. Kopfschulterairbags für Fahrer und Beifahrer und das elektronische Stabilitätsprogramm mit Berganfahrassistent und einer Antriebsschlupfregelung waren gegen Aufpreis erhältlich. Beim Ford Ka Concept, seit 2011 Ambiente genannt, fehlten die serienmäßigen Seitenairbags.
In Österreich waren beim Basismodell Ambiente lediglich Frontairbags für Fahrer und Beifahrer und Antiblockiersystem mit elektronischer Bremskraftverteilung serienmäßig. Seitenairbags, Kopfschulterairbags und die Kombination elektronisches Stabilitätsprogramm mit Berganfahrassistent und Antriebsschlupfregelung kosteten Aufpreis.
Bei dem vom ADAC im November 2008 durchgeführten Euro-NCAP-Crashtest erreichte der Ka vier von fünf Sternen. Auch hier wurde, wie bereits bei der ersten Modellgeneration im Jahr 2000, eine Europa-Version ohne die in Deutschland serienmäßigen Seitenairbags getestet. Das Erreichen des optimalen Testergebnisses von fünf Sternen, die unter anderem 2007 der technisch gleiche Fiat 500 bekam, war ohne diese Seitenairbags nicht möglich.
Im Gegensatz zu seinem Vorgänger hat die zweite Modellgeneration des Ka relativ wenige Schwachstellen, so dass nach der Markteinführung ein Trend zum Besseren in der ADAC Pannenstatistik zu erkennen war, wo bis 2010 beide Generationen zusammengefasst wurden. Störungen im Motormanagement treten häufiger auf, darüber hinaus ist die Wegfahrsperre oder der zugehörige Transponder überdurchschnittlich oft defekt. Auch ein loses Schaltgestänge sowie eine entladene Batterie wurden vom ADAC als Ursache für Pannen genannt.In der ADAC Pannenstatistik 2011 erreichte das Modell, das nun erstmals ohne den Vorgänger ausgewertet wurde, Platz eins von zwölf Fahrzeugen in der Klasse „Kleinstwagen“. Im Jahr 2012 konnte der Ford Ka Platz zwei von elf Fahrzeugen erreichen, danach verzichtete der ADAC auf ein Ranking innerhalb der Klassen.
Ford bot unter dem Namen Ford Individual verschiedene Optikpakete als Individualisierungsmöglichkeit für den Wagen an.
Der Ford Ka Grand Prix war in Sunrise-Rot, Disco-Schwarz, Cloud-Silber, Lago-Grau, Crystal-Weiß und Cashmere-Weiß erhältlich. Kühlergrill, Seitenspiegel und Zierstreifen waren in weiß, bei weißer Außenlackierung in rot gehalten. Optional waren beide Seitentüren mit wählbaren Startnummern beklebt (0 bis 9). Dazu waren im Innenraum Sitzbezüge, Türverkleidungen, Lenkrad und Kopfstützen farblich auf die Wagenfarbe abgestimmt, das Armaturenbrett war zudem mit Aluminium-Applikationen besetzt. In Deutschland wurde dieses Optikpaket im April 2011 aus dem Programm genommen.
Der Ford Ka Digital war bis 2011 in Schwarz erhältlich, die bereits beim Grand Prix farblich abgesetzten Teile und das Armaturenbrett waren in der Farbe Jump lackiert.
Der Ford Ka Tattoo, angeboten bis Januar 2011, besaß große Tribals auf den vorderen Kotflügeln, dem Dach, den Fußmatten und den Sitzen. Gegen Aufpreis waren Teilledersitze, ebenfalls mit Tribals, erhältlich.
Der Ford Ka Metall, in Österreich Ford Ka Metal, wurde 2011 eingeführt und besaß ein spezielles Lederlenkrad mit schwarzen Dekorelementen, eine Stoffpolsterung mit silberfarbenen Ringen im „Metall“-Design, sowie Mittelkonsole und Instrumententafel mit schwarzen und Chrom-Dekorelementen. Das Modell gab es nur in den Metallic-Lackierungen Midnight-Schwarz, Lago-Grau und Moonlight-Silber, der Kühlergrill war in schwarz lackiert, außerdem waren die B-Säulen schwarz verkleidet, mit einem Ka-Logo im Chrom-Dekor auf der Fahrerseite. Der Öffentlichkeit vorgestellt wurde das neue Stylingpaket erstmals auf dem Genfer Autosalon im März 2011.
Der Ford Ka Grand Prix II wurde ebenfalls 2011 in Deutschland eingeführt und war ausschließlich in Sunrise-Rot erhältlich. Kühlergrill, Seitenspiegel und Leichtmetallräder waren hochglänzend schwarz lackiert, die Zierstreifen in schwarzer Carbon-Optik gehalten. Dazu waren im Innenraum Sitzbezüge, Türverkleidungen, Lenkrad und Kopfstützen farblich auf das Äußere des Wagens abgestimmt.
Der Ford Ka Grand Prix III wurde 2013 in Deutschland eingeführt und war in Flame-Rot, Midnight-Schwarz, Moonlight-Silber und Cashmere-Weiß erhältlich. Kühlergrill und Seitenspiegel waren in Kontrastfarbe, die Leichtmetallräder hochglänzend schwarz lackiert. Dazu gab es im Innenraum exklusiv für dieses Modell Sitzbezüge in schwarz-anthrazit, Lenkrad, Schaltknauf und Handbremsgriff mit Dekorelementen sowie Velours-Teppichfußmatten mit farblich abgesetzten Nähten.Styling-Pakete für Exterieur und Interieur (nur für die Ausstattungsvariante Titanium) waren ursprünglich in separaten Paketen erhältlich, zu jedem Exterieur-Paket gehörte immer auch eine verchromte Auspuffblende. Seit Dezember 2009 waren diese Pakete nur noch komplett erhältlich.
Mehrere Audio-Systeme wurden für den Ford Ka angeboten, unter anderem mit USB-Anschluss und AUX-Eingang für MP3-Player, die über die Lenkradfernbedienung bedient werden konnten. Die CD-Radios sind MP3-fähig. Darüber hinaus war eine Bluetooth-Schnittstelle mit Sprachsteuerung für ein Mobiltelefon, eine Kommunikationskonsole für mobile Navigationssysteme und ein erweitertes Lautsprechersystem mit Verstärker und einem Subwoofer erhältlich.
Neben dem Unilack Jump gab es zwei weitere Uni- und neun Metalliclacke, die in Deutschland alle aufpreispflichtig waren. Bei den Farben Cashmere-Weiß und Funky Magenta handelte es sich um aufwändig zu lackierende Vier-Schicht-Lacke.
2010 ersetzte Ford zwei Farben, Cloud-Silber wurde durch Moonlight-Silber und Disco-Schwarz durch Midnight-Schwarz ersetzt. 2013 wurde Sunrise-Rot durch Flame-Rot ersetzt, außerdem kam Mustard Olive-Gelb in Deutschland als Farbe hinzu, wurde in Österreich jedoch nicht angeboten.
Bei seiner Markteinführung in Deutschland war der Ka als Trend mit 1,2-Liter-Benzinmotor ab 9.750 Euro und als Titanium für 10.750 Euro erhältlich, Fahrzeuge mit Dieselmotor kosteten jeweils 2.000 Euro mehr. Noch im Jahr der Markteinführung wurden diese Preise insgesamt dreimal und zum Jahr 2011 sowie zum Jahr 2012 je ein weiteres Mal erhöht, auf nun 10.900 Euro für den Trend und 12.700 Euro für den Titanium. Das 2010 eingeführte Editionsmodell Concept war ab 9.650 Euro erhältlich, in der umbenannten Version Ambiente ab Januar 2011 9.700 Euro und ab Januar 2012 9.800 Euro.
Ende 2013 wurden die Preise deutlich gesenkt, der Ambiente kostete nun 9.140 Euro, der Trend 9.640 Euro und der Titanium 11.140 Euro.
In Österreich war er als Ambiente bereits ab 9.600 Euro und als Titanium ab 11.600 Euro erhältlich, der Aufpreis für den Dieselmotor betrug 2000 Euro. Auch hier wurden im März 2009 und im Januar 2011 die Preise erhöht. Das Modell Ambiente kostet nun 10.040 Euro, der Titanium 12.290 Euro.
Auch in Österreich wurden die Preise Ende 2013 gesenkt, der Ambiente kostete nun 8.990 Euro, der Trend 9.490 Euro und der Titanium 10.990 Euro.
Im April 2009 gab Ford bekannt, im Zuge der Werbe-Aktion „Eigener Ford Ka – Eigene Cola“ in den ostdeutschen Bundesländern in Kooperation mit Vita Cola ein auf 300 Fahrzeuge limitiertes Sondermodell Ford Ka Vita auf den Markt zu bringen. Hierbei handelte es sich um ein Titanium-Modell mit weißlackierten Leichtmetallräder, getönten Scheiben, Dachspoiler und CD-Radio. Der Ford Ka Vita Original war in Sunrise-Rot lackiert, der Ford Ka Vita Schwarz in Disco-Schwarz Metallic und verfügte zusätzlich über das erweiterte Audiosystem mit Verstärker und Subwoofer.
In Österreich bot Ford ab März 2011 das Sondermodell Ford Ka iconic an. Hierbei handelte es sich um Fahrzeuge mit der Trend-Ausstattung, die zusätzlich über eine Klimaanlage und ein CD-Radio mit AUX-Eingang und MP3-Funktion verfügten. Zusätzlich erhielt der Käufer einen Code, mit dem er binnen eines Jahres maximal 1000 Songs von der Ford Music Load & Play Plattform, die über 10.000 Musiktitel aus verschiedenen Genres verfügt, herunterladen konnte.
Ford bot ab Januar 2012 das Sondermodell Ford Ka Champions Edition an. Hierbei handelte es sich um Fahrzeuge mit der Trend-Ausstattung, die über Velours-Teppichfußmatten und B-Säulenverkleidung mit dem Logo der UEFA Champions League verfügten.
Auf der internationalen Messe für Umwelttechnologie Clean Tech World stellte der Hamburger Energieversorger mk-group Holding GmbH den MK1 E-KAT vor, ein Elektroauto auf Basis des Ford Ka. Der Wagen wurde für Asch Motorsport entwickelt, erreichte bis zu 120 km/h und verfügte über einen 15 kWh-Lithium-Polymer-Akkumulator, mit dem das Fahrzeug eine Reichweite von 120 km hatte. 1500 Fahrzeuge liefen jährlich vom Band, offizieller Fahrzeughersteller ist mk-group.Eine Ka ST genannte, leistungsstärkere Variante als Nachfolger des SportKa war geplant und wurde auch in der Fachpresse vorgestellt. Die Motorisierung sollte identisch mit der des Fiat 500 Abarth sein, ein 99 kW starker, aufgeladener 1,4-Liter-Motor, der den Wagen auf eine Höchstgeschwindigkeit von 205 km/h beschleunigen sollte.Darüber hinaus war auch eine Cabrio-Version angekündigt. Diese sollte wie der Vorgänger Streetka ein reiner Zweisitzer sein, jedoch ein elektrisch öffnendes Stahlverdeck besitzen. Das als Ka CC bezeichnete Modell wurde nie gebaut.Auch eine Kombi-Variante und ein Hybridfahrzeug sowie ein Automatikgetriebe für den Ka wurden als mögliche Optionen angekündigt, aber nicht umgesetzt.
Die Leser der Zeitschrift JOY wählten den Ka zum Gewinner des JOY Trend Award 2009 in der Kategorie Lifestyle.
150 Fuhrparkmanager wählten den Ka bei den jährlichen Vergleichstests der Fachzeitschrift „Firmenauto“ von 2009 bis 2011 dreimal in Folge zum Firmenauto des Jahres in der Kategorie Minicars.
Der Ford Ka mit dem 1,2-Liter-Benzinmotor wurde Wertmeister 2011 in der Kategorie „Minis“. Bei der durch Auto Bild und EurotaxSchwacke vorgenommenen Berechnung der wertstabilsten Fahrzeuge erreichte er nicht nur den „geringsten Wertverlust in Euro“, sondern auch den „besten Werterhalt in Prozent“.
Im Mai 2012 gewann der Ford Ka den J. D. Power-Award im Segment „Kleinstwagen“. Bei der jährlich vom amerikanischen Marktforschungsunternehmens J.D. Power veröffentlichten Kundenzufriedenheitsstudie „VOSS“ (Vehicle Ownership Satisfaction Study) erreichte der Ka mit 78,9 Prozent die höchste Kundenzufriedenheitsquote in diesem Segment.
Rund ein Jahr vor der Markteinführung gab Ford bekannt, dass ein eigens angefertigter Prototyp des neuen Ford Ka-Modells im James Bond-Film Ein Quantum Trost zu sehen sein wird. Der Wagen wurde im Film vom Bondgirl „Camille“ bewegt, und von einer Brennstoffzelle angetrieben. Er besaß eine goldene Metalliclackierung mit grafischen Elementen und war mit einer besonderen Innenausstattung ausgerüstet.
Am 14. Dezember 2007 stellte Ford in Brasilien den neuen Ford Ka für den südamerikanischen Raum vor. Im Gegensatz zum europäischen Nachfolger war dieses ein eigenständiges Modell, das auf der ersten Baureihe des Ford Ka basierte und daher weder optisch noch technisch Gemeinsamkeiten mit der zweiten europäischen Modellgeneration hatte.
Erhältlich war das Fahrzeug dort ab Januar 2008. Im Gegensatz zu den beiden europäischen Modellgenerationen verfügte das Fahrzeug über fünf Sitzplätze. Die beiden verfügbaren Motorisierungen waren die gleichen, die in Südamerika auch in der ersten Modellgeneration erhältlich waren, ein 1,0-Liter-Turbomotor und ein 1,6-Liter-Motor. Durch kleinere Überarbeitungen wurde bei beiden Motoren jedoch ein kleiner Leistungszuwachs erzielt – sie leisteten daher 51 kW beziehungsweise 75 kW. Beide Motoren waren außerdem für die Verwendung des in Brasilien weit verbreiteten Ethanol-Kraftstoffes geeignet, bei dessen Verwendung die Motorleistung leicht anstieg.
Das Modell war nur in Südamerika erhältlich, ein Export nach Mittelamerika, wie dies bei der ersten Modellgeneration noch der Fall gewesen war, fand nicht statt.
2011 fand eine optische Überarbeitung statt, außerdem wurde der 1,6-Liter-Motor aus dem Programm genommen. 2013 wurde der Ford Ka dann komplett aus dem Programm genommen.
Im November 2013 gab Ford bekannt, im Werk Camaçari in Brasilien die dritte Modellgeneration des Ford Ka zu produzieren. Später war zudem eine Produktion in Indien, China und Thailand vorgesehen. Das neue Modell soll, ähnlich wie andere Ford-Modelle wie Fiesta und Focus, als Weltauto auf vielen Märkten in nahezu gleicher Version (teilweise als Ford Figo) angeboten werden.
Erstmals ist der Ka als vier- und fünftüriges Modell erhältlich. Auf dem südamerikanischen Markt wird er ausschließlich mit Dreizylinder-EcoBoost-Motoren verkauft. Auf vielen Märkten, allerdings nicht in Deutschland, wird zudem eine Stufenheckvariante angeboten, die zum Teil als Ka+ oder Figo Aspire bezeichnet wird.
Der Wagen basiert, genau wie die Modelle B-Max, EcoSport und Fiesta auf der sogenannten B-Plattform.
Anfang Juni 2016 wurde die europäische Version vorgestellt. Sie wird wie der EcoSport in Indien produziert, jedoch in einem eigens erbauten Werk in Sanand im Distrikt Ahmedabad. Nach Angaben von Ford erfolgte die europäische Markteinführung deutlich später, da zahlreiche Änderungen unter anderem an Fahrwerk, am Geräuschverhalten und der Verarbeitung vorgenommen wurden. In Deutschland wird ausschließlich der Fünftürer als Ka+ verkauft. Das Modell wird in Europa von einem neuen, 1,2-Liter-Duratec-Ottomotor angetrieben, der wahlweise 51 kW (70 PS) oder 63 kW (85 PS) leistet.
Serienmäßig sind alle Fahrzeuge mit elektrischen Fensterheber vorne, elektrisch verstellbaren Seitenspiegeln, Zentralverriegelung mit Funkfernbedienung, ESP und sechs Airbags ausgestattet. Der Einstiegspreis des Ka+ liegt bei 9.990 Euro, der stärkere und zusätzlich serienmäßig mit Klimaanlage und dem sprachgesteuerten Ford Sync ausgestattete Ka+ Cool&Sound ist ab 11.400 Euro erhältlich. Die ersten Fahrzeuge wurden in Europa Mitte Oktober 2016 ausgeliefert.Im Februar 2018 wurde für die europäische Variante eine Modellpflege angekündigt. Der Kühlergrill wird dem Aussehen der anderen aktuellen Ford-Modelle angepasst und deutlich höher, außerdem gibt es neue Scheinwerfer. Serienmäßig sind hier Tagfahrlicht und Nebelscheinwerfer vorhanden. Neu erhältlich ist ein 1,2-Liter-Dreizylinder-Ottomotor, der mit einem Start-Stopp-System ausgestattet ist und wahlweise 52 kW oder 63 kW leistet. Außerdem ist erstmals ein Dieselmotor für den Ford Ka verfügbar. Der 1,5-Liter-Vierzylinder-Turbodieselmotor leistet maximal 70 kW bei einem Drehmoment von 215 Nm und verfügt über ein Energierückgewinnungs-System der Lichtmaschine (SRC), das die Fahrzeugbatterie beim Bremsen stärker auflädt.Neu im Sortiment ist der Ka+ Active, eine Crossover-Variante. Dieser verfügt über eine um 23 mm erhöhte Bodenfreiheit, spezielle 15-Zoll-Leichtmetallräder, eine Dachreling und ein spezielles Bodykit. Der vordere und hintere Stoßfänger ist mit einem unteren Einsatz in Silber versehen, die Seitenschweller sind schwarz mit silberfarbenen Einsätzen, außerdem sind die Radhäuser ebenfalls in Schwarz gehalten. Mit Havanna Braun gibt es zudem eine neue Lackierung, die exklusiv beim Ka+ Active erhältlich ist. Auch im Innenraum gibt es Unterschiede: Die Sitze haben ein spezielles Design, der Dachhimmel ist Anthrazit und das Lederlenkrad hat Bedienelemente für die Geschwindigkeitsregelanlage.

Die Forrest-Sherman-Klasse war eine Klasse von Zerstörern der United States Navy. Die 18 Schiffe der Klasse waren die ersten neuen Zerstörer, die nach dem Zweiten Weltkrieg gebaut und ab 1955 in Dienst gestellt wurden. Mitte der 1960er Jahre wurden vier Schiffe umgerüstet und als Decatur-Klasse eingestuft. Diese verfügten über erweiterte Flugabwehrfähigkeiten. Acht weitere Schiffe wurden ebenfalls umgerüstet und erhielten als Barry-Klasse verbesserte U-Jagd-Kapazitäten.
Die Planungen für die neuen Zerstörer begannen Ende der 1940er Jahre, nachdem nach dem Zweiten Weltkrieg die ursprünglich als Zerstörer geplanten Schiffe der Mitscher-Klasse (DD-927 bis DD-930) als Zerstörerführer (DL), später als Fregatten klassifiziert, fertiggestellt wurden. Die Entwicklung der Klasse fällt in die Vor-Lenkwaffen-Zeit, sie sind die letzten „echten“ Artilleriezerstörer mit Schiffsgeschützen als Hauptbewaffnung. Bei der Planung der aus der Allen-M.-Sumner-Klasse entwickelten Schiffe wurde, im Gegensatz zu den Kriegszerstörern, mehr auf Komfort für die Mannschaft geachtet, so waren die Unterkünfte geräumiger und das gesamte Schiff war klimatisiert. Der Bau der Schiffe wurde 1953 vom Kongress genehmigt, nachdem die ersten Schiffe schon zwei Jahre zuvor bei den Werften bestellt worden waren. Die Finanzierung erstreckte sich über die nächsten drei Finanzjahre.
Der Bau der ersten Einheit, Forrest Sherman, begann am 27. Oktober 1953 bei Bath Iron Works, der Stapellauf erfolgte am 5. Februar 1955. Die Indienststellung bei der Navy erfolgte am 9. November 1955. Das 18. und letzte Schiff der Klasse, Turner Joy, wurde am 5. Mai 1958 bei Puget Sound Bridge and Dredging Company vom Stapel gelassen und am 3. August 1959 in Dienst gestellt. Weitere Schiffe wurden bei Bethlehem Steel in Quincy, Massachusetts und bei Ingalls Shipbuilding gebaut. Ursprünglich waren noch sieben weitere Schiffe geplant (DD-952 bis DD-959), diese wurden jedoch zugunsten der Lenkwaffenzerstörer der Charles-F.-Adams-Klasse gestrichen.
Zu Beginn der 1960er Jahre wurde deutlich, dass die Schiffe mit reiner Rohrbewaffnung und veralteten U-Boot-Abwehrwaffen gegen die sich schnell weiterentwickelnden Flugzeuge und modernen U-Boote keine Abwehr- und Überlebenschance mehr hatten. 1964 wurde daher beschlossen, vier Schiffe (DD-932, DD-936, DD-947 und DD-949) zu Lenkwaffenzerstörern umzurüsten. Nach zweijährigem Werftaufenthalt wurden die Schiffe als DDG (Lenkwaffenzerstörer) mit den Nummern 31 bis 34 reklassifiziert und als Einheiten der Decatur-Klasse wieder in Dienst gestellt. Ebenfalls im gleichen Jahr wurde der Umbau der Barry (DD-933) genehmigt, die erweiterte U-Jagd-Kapazitäten erhielt. Sieben weitere Schiffe wurden ab 1967/68 nach dem Muster der Barry umgerüstet. Die acht Schiffe wurden dann als Barry-Klasse geführt. Der Umbau der Schiffe kostete zwischen 14 und 17 Mio. US-Dollar. Die übrigen sechs Schiffe der Forrest-Sherman-Klasse wurden wegen Geldmangels nicht umgerüstet und blieben bis zu ihrer Ausmusterung nahezu im Ursprungszustand.
Alle Schiffe der Forrest-Sherman-Klasse und ihrer Unterklassen blieben bis zu Beginn der 1980er Jahre in Dienst. Durch die in den davor liegenden Jahren in Dienst gestellten Zerstörer der Spruance-Klasse waren die Shermans weitgehend überflüssig geworden, die Kosten für eine notwendige Modernisierung hätten sich nicht gerechnet. Bis zu Beginn der 1990er Jahre blieben die Schiffe aber der Reserveflotte zugeteilt. Zwei Schiffe (Edson, Turner Joy) sind bereits als Museumsschiffe erhalten, das Schicksal der Forrest Sherman ist ungewiss, eine Stiftung bemüht sich um die Rettung des derzeit in Philadelphia liegenden Schiffes. Alle übrigen Schiffe wurden entweder verschrottet oder als Zielschiffe versenkt.
Der Rumpf der Shermans war 127,5 Meter lang und 13,8 Meter breit, was etwa dem bei Zerstörern typischen schlanken Länge-Breite-Verhältnis von 10 zu 1 entsprach. Der Tiefgang betrug 6,7 Meter am Rumpfsonarwulst. Die Verdrängung betrug 4.050 Tonnen bei der Forrest-Sherman- und der Barry-Klasse; die Decatur-Klasse verdrängte aufgrund ihrer schweren Lenkwaffenanlage 4.150 Tonnen. Rumpf und Aufbauten waren aus Stahl gefertigt. Die niedrigen Decksaufbauten, deren Design sich stark an dem der Zerstörer des Zweiten Weltkriegs anlehnte, erstreckten sich über die mittlere Hälfte des Rumpfes. Hinter der Brücke befanden sich die beiden Schornsteine, vor denen jeweils ein Dreibeinmast aufgestellt war. Der Maschinenraum befand sich mittschiffs.
Die Zerstörer verfügten über einen Dampfantrieb, der dafür nötige Dampfdruck von etwa 80 bar wurde in vier Kesseln von Foster-Wheeler (Babcock-Wilcox-Kessel bei DD-937, DD-943, DD-944, DD-945, DD-946 und DD-948) erzeugt und dann zu zwei Getriebeturbinen geleitet, von denen jede eine Welle mit je einer Schraube antrieb. Die Leistung des Systems betrug 70.000 PS. Die Höchstgeschwindigkeit der Schiffe betrug 32,5 Knoten, bei einem Ölvorrat von 750 Tonnen hatten die Schiffe bei 20 Knoten eine Reichweite von 4.000 Seemeilen.
Die ursprüngliche Hauptbewaffnung aller Schiffe bestand aus drei Mark 42 Mehrzweckgeschützen, eins vor der Brücke sowie zwei achtern. Gegen U-Boote befanden sich zwei Hedgehog-Werfer backbord und steuerbord vor der Brücke. Diese wurden in den 1960er Jahren entfernt, ebenso die beiden 7,62-cm-L/50-Flugabwehrgeschütze auf Doppellafetten, die sich vor der Brücke und hinter dem zweiten Schornstein befanden. Die zwei Doppeltorpedorohre Mk. 25 für U-Jagdtorpedos hinter der Brücke wurden im Rahmen der Modernisierung durch Dreifachtorpedorohre ersetzt, die ebenfalls dort montierte Werfer für Leichtgewicht-U-Jagdtorpedos wurden entfernt. Zwischen 1975 und 1978 war die Hull Testplattform für das 8-Zoll-Leichtgewichtsgeschütz Mark 71, das Programm wurde aber eingestellt. Die Bigelow diente Ende der 1970er Jahre als Erprobungsplattform für das Mk. 15 Phalanx-Close-in-Weapon-System.
Die Schiffe der Barry-Klasse erhielten beim Umbau ab 1964 anstelle des zweiten Geschützes einen ASROC-Werfer mit acht Zellen sowie ein automatisches Nachlademagazin vor dem Werfer. Außerdem erhielten die Schiffe zusätzliche Mark 32-Dreifachtorpedorohre samt Magazin vor der Brücke auf dem Aufbau der ehemaligen Hedgehog-Werfer und des vorderen 76-mm-Geschützes.
Die Decaturs erhielten anstelle der achteren Geschütze ab 1964 ebenfalls einen ASROC-Werfer, jedoch ohne Magazin, sowie einen Mk. 13-Einarmstarter für Flugabwehrraketen vom Typ RIM-24 Tartar. Diese wurden später durch RIM-66 SM1 MR ersetzt. Der Werfer verfügte über ein Trommelmagazin für 40 Flugkörper. Zwischen dem ASROC-Starter und dem Lenkwaffenstarter wurde ein geräumiges Deckhaus aus Leichtmetall errichtet, auf dem sich der Zielbeleuchter SPG-51 für die Lenkwaffen befand.
Auf allen Versionen der Forrest-Sherman-Klasse wurde als Navigationsradar das SPS-10 von Raytheon eingesetzt. Dessen Antenne befand sich auf dem vorderen Mast und war die höchstgelegene Antenne an Bord. Die nichtmodifizierten Shermans und die Barrys besaßen auf dem vorderen Mast außerdem ein Luftüberwachungsradar, entweder ein SPS-37 von der Westinghouse Electric Corporation oder, auf DD-931, -944 und -945 sowie DD-928, -941, -943, -948 und -950 das modernere SPS-40 von Lockheed. Die Decaturs verwendeten für die Luftraumüberwachung das SPS-29E, ebenfalls von Westinghouse, DDG-34 bereits SPS-40. Außerdem war dort auf dem achterlichen Mast das 3D-Radar SPS-48 von ITT-Gilfillan installiert. Dafür wurde der hintere leichte Dreibeinmast durch einen schweren Vierbein-Gittermast ersetzt.
Die Feuerleitung der 127-mm-Geschütze erfolgte mittels eines Mark 56-Radars, die Zielzuweisung für die Flaks durch ein Mark 68, das mit dem Luftaufklärungsradar kombiniert war. Unter dem Kiel befand sich ein SQS-23-Sonar. Die Barry (DD-933) erhielt außerdem zu Testzwecken schon 1960 ein SQS-35 Independent Variable Depth Sonar, ein Hecksonar mit erweiterten und verbesserten Ortungs- und Verfolgungskapazitäten.
Für die Elektronische Kriegführung besaßen alle Einheiten das System WLR-2 oder -6, die Antennen waren auf den beiden Masten angebracht.
Die Zerstörer der Forrest-Sherman-Klasse waren hauptsächlich als Begleitschiffe für Flugzeugträger sowie als U-Bootjäger konzipiert, konnten aber im beschränkten Rahmen auch eigenständig operieren und unterstützten unter anderem amphibische Landungsoperationen mit ihrer Rohrbewaffnung. Die Schiffe der Barry-Klasse wurden zumeist als U-Jagdschiffe im Flugzeugträgerverband eingesetzt, die Decaturs konnten sehr gut eigenständig operieren, da sie auch über Luftabwehrfähigkeiten verfügten.
Erste Einsätze in Konflikten fuhren die Shermans im Rahmen der Quarantäne-Operationen während der Kuba-Krise. Die Turner Joy war am Tonkin-Zwischenfall beteiligt, im Anschluss kreuzten die Schiffe dann im Vietnamkrieg vor Südostasien.
Stefan Terzibaschitsch: Zerstörer der U.S. Navy. Koehlers Verlagsgesellschaft, Herford 1986, ISBN 3-7822-0395-X
Norman Friedman: U.S. Destroyers: An Illustrated Design History, Revised Edition. Revised edition. Naval Institute Press, Annapolis 2003, ISBN 1-55750-442-3

Die Forschungsgeschichte des Klimawandels beschreibt die Entdeckung und Untersuchung von Klimawandel-Ereignissen im Rahmen geologischer und historischer Zeiträume, einschließlich der gegenwärtig stattfindenden globalen Erwärmung. Die systematische Erforschung von natürlichen Klimawechseln begann in der ersten Hälfte des 19. Jahrhunderts mit der allmählichen Rekonstruktion der Eiszeit-Zyklen und anderen klimatisch bedingten Umweltveränderungen im Rahmen der Paläoklimatologie und der Quartärforschung. Bereits Ende des 19. Jahrhunderts wurden menschliche Einflüsse auf das Erdklimasystem über Treibhausgase vermutet, entsprechende Berechnungen wurden aber bis in die 1960er Jahre hinein stark angezweifelt. Detaillierte Darstellungen zur Forschungsgeschichte des Klimawandels, insbesondere zu der im Laufe des 20. Jahrhunderts feststellbaren anthropogenen Klimaveränderung, finden sich beispielsweise im 1. Kapitel des Vierten Sachstandsberichts des IPCC und ausführlicher bei dem US-amerikanischen Physiker und Wissenschaftshistoriker Spencer R. Weart. Eine auf der Arbeit von Spencer Weart basierende deutschsprachige Ausarbeitung findet sich auf der Homepage von Skeptical Science.Während der Treibhauseffekt bereits im Jahr 1824 entdeckt wurde, konnte die klimaerwärmende Wirkung der stetig ansteigenden Konzentration von Kohlenstoffdioxid in der Erdatmosphäre aufgrund verbesserter Messmethoden und einer breiteren Datenbasis erst gegen Ende der 1950er Jahre quantifiziert werden. Zwar stellten einige Wissenschaftler fest, dass die vom Menschen verursachte Luftverschmutzung das Klima auch abkühlen könne, es wurde von der Klimaforschung aber ab Mitte der 1970er Jahre zunehmend die Annahme einer Erwärmung favorisiert. In den 1990er Jahren bildete sich durch weiterentwickelte Computermodelle und ein tieferes Verständnis der Kaltzeiten folgender Konsens heraus: Treibhausgase spielen beim Klimawandel eine große Rolle, und durch den Menschen verursachte Emissionen sind für die laufende globale Erwärmung hauptverantwortlich.
Als einer der frühesten Pioniere des Vorzeitgedankens vermutete der englische Universalgelehrte Robert Hooke bereits gegen Ende des 17. Jahrhunderts anhand von Versteinerungen aus dem Jura (wie Ammoniten und Meeresschildkröten), dass das südenglische Klima in der erdgeschichtlichen Frühzeit erheblich wärmer gewesen sein musste. Darauf aufbauend schlug er vor, die Klimata urweltlicher Habitate mit Hilfe von Fossilien zu bestimmen. Gegen den damals verbreiteten Glauben an den biblischen Schöpfungsmythos konnte sich die Annahme einer urweltlichen Epoche, die erheblich größere Zeiträume als die historisch belegte Menschheitsgeschichte umfasste, jedoch erst ein Jahrhundert später durchsetzen. Im Zuge der Aufklärung und mit der Entwicklung der Geologie zur modernen Wissenschaft ab 1750 gewann der Vorzeitgedanke allmählich an Boden. Gleichwohl waren viele Wissenschaftler in ihrem Denken noch von der Religion beeinflusst wie der Basaltstreit zeigte. Einen ersten Beitrag zur Etablierung der Eiszeittheorie leistete im Jahr 1742 der Ingenieur und Geograph Pierre Martel. Nach seiner Ansicht waren die Gletscher von Chamonix einst viel ausgedehnter, was auf ein kälteres Klima in der Vergangenheit hindeutete. Eine ähnliche Meinung vertrat sein Schweizer Landsmann Gottlieb Sigmund Gruner, der 1778 in seinem Buch Reise durch die merkwürdigsten Gegenden Helvetiens das Geröllkonglomerat alter Endmoränen mit früheren Gletscherständen in Verbindung brachte.S. 69 Mit diesen Erkenntnissen waren Martel und Gruner ihrer Zeit jedoch um Jahrzehnte voraus. Die Vorstellung weiträumiger Vergletscherungen infolge eines eiszeitlich geprägten Klimas war Ende des 18. Jahrhunderts eine zu revolutionäre Idee, um von der Wissenschaft akzeptiert zu werden.
Zwischen 1780 und 1830 wurde eine zum Teil religiös motivierte Grundsatzdebatte zwischen Neptunisten und Plutonisten geführt (Basaltstreit). Ein zentrales Thema der Neptunisten war die Sintflut, die vielfach bis in die 1. Hälfte des 19. Jahrhunderts hinein als reales geologisches Ereignis oder als Synonym für mehrere globale Überschwemmungskatastrophen betrachtet wurde. Die Kontroverse zwischen Neptunisten und Plutonisten wurde auch beim Disput über Herkunft und „Wanderung“ der erratischen Blöcke (Findlinge) offenkundig, die von Eiszeitgletschern in der Alpenregion, in der norddeutschen Tiefebene und in Skandinavien abgelagert worden waren und die charakteristisch für glazial geformte Landschaften sind. Das Rätsel der weiträumig verstreuten Findlinge wurde ab 1760 zunehmend intensiver diskutiert, wobei neben der favorisierten Drifttheorie vor allem Wasser-, Schlamm- und Geröllfluten sowie vulkanische Eruptionen als Erklärungen für den Transport der erratischen Blöcke herangezogen wurden.S. 108 ff Von einem tieferen Verständnis der Gletscherdynamik und Glazialmorphologie war man zu dieser Zeit noch weit entfernt, und erst die Arbeiten und Untersuchungen von Louis Agassiz, Johann von Charpentier, Karl Friedrich Schimper und Ignaz Venetz zeichneten ab 1830 ein zunehmend differenziertes Bild des Eiszeitklimas und der damit verbundenen Prozesse.
Jedoch konnte man in dieser frühen Phase der Forschung die postulierten Klimaänderungen weder zeitlich eingrenzen noch ansatzweise datieren, und ebenso wenig herrschte Klarheit über die möglichen Ursachen.
Im Jahr 1801 entdeckte der Astronom Wilhelm Herschel, dass zwischen 1650 und 1800, einem Zeitraum, der später als Kleine Eiszeit bekannt wurde, eine geringe Zahl der Sonnenflecken mit schlechter Weizenernte und, so seine Folgerung, ungewöhnlich niedrigen Temperaturen einherzugehen schien. Der von ihm postulierte Zusammenhang zwischen den zyklischen Veränderungen der Sonnenaktivität und natürlichen Klimaschwankungen war jedoch damals bereits umstritten und wurde in der Folge bis zum Ende des 20. Jahrhunderts in der Wissenschaft immer wieder diskutiert.
Bis zur Mitte des 19. Jahrhunderts hatten die inzwischen zahlreicher gewordenen Befürworter der Eiszeittheorie so viele Belege und „Klimazeugen“ für die Existenz einer früheren Kaltzeit gesammelt, dass es allmählich schwieriger wurde, die vorgebrachten Argumente zu ignorieren. Im Zuge der geologischen Erkundung Nordamerikas wurde darüber hinaus deutlich, dass die in Europa festgestellte Kältephase kein regionales Phänomen war, sondern offenbar die gesamte nördliche Hemisphäre erfasst hatte. Eine weitere Bestätigung erhielt das Eiszeitmodell durch die Entdeckung sehr alter Vergletscherungsspuren in Afrika, Australien und Indien, die nach heutigem Kenntnisstand der Permokarbonen Vereisung vor etwa 300 Millionen Jahren zugeordnet werden.Als einer der unermüdlichsten Exponenten warb der Schweizer Naturforscher Louis Agassiz (1807–1873) für die wissenschaftliche Akzeptanz des Eiszeitgedankens. Auf zahlreichen Reisen, verbunden mit Vorträgen vor akademischem Publikum, sowie durch die Veröffentlichung mehrerer Bücher trug er entscheidend zur Popularisierung seiner Ideen bei. Dennoch war um 1850 ein wissenschaftlicher Konsens zu diesem Thema noch nicht in Sicht. Dieser verzögerte sich hauptsächlich aus folgenden Gründen:S. 532 ff
Ein „Weltwinter“, wie er von Forschern wie Karl Friedrich Schimper postuliert wurde, bedeutete für die Mehrzahl der zeitgenössischen Geowissenschaftler einen Rückfall in den von Georges de Cuvier begründeten Katastrophismus und der damit verknüpften Kataklysmentheorie. Diese Anschauung galt inzwischen als veraltet und widerlegt und war durch das „moderne“ aktualistische Konzept des englischen Geologen Charles Lyell ersetzt worden.
Gleichzeitig mit den geologischen Befunden einer Glazialperiode fanden sich in entsprechenden stratigraphischen Schichten auch deutliche Hinweise auf frühere Warmzeiten. Diese scheinbare Unvereinbarkeit wurde mit den Fortschritten von Chronostratigraphie und Geochronologie gegenstandslos, vor allem durch die Entdeckung, dass die Quartäre Eiszeit mehrmals von Interglazialen wie der Eem-Warmzeit unterbrochen worden war.
Die Vorstellungen über die mögliche Ausdehnung und das Fließverhalten von Gletschern orientierten sich über Jahrzehnte am Beispiel der Alpengletscher. Aus dieser lokalen Perspektive globale Schlüsse ziehend, lehnten die damaligen Geowissenschaftler ein Größenwachstum von Eisfeldern, die halbe Kontinente bedeckten, nahezu einhellig ab. Diese Lehrmeinung änderte sich gravierend mit der beginnenden Erforschung und Vermessung des Grönländischen Eisschilds in der 2. Hälfte des 19. Jahrhunderts.Abgesehen von wenigen Ausnahmen wurde die Eiszeittheorie spätestens um 1880 allgemein akzeptiert und entwickelte sich in Form der Quartärforschung zu einer wichtigen Stütze der Geowissenschaften. Allerdings fehlte lange Zeit ein fundiertes theoretisches Modell, das die Ursachen der verschiedenen Warm- und Kaltzeiten in der Erdgeschichte physikalisch korrekt beschreiben konnte. Dessen ungeachtet entstanden die Grundlagen der heutigen Klimatologie zum Teil parallel zur Eiszeittheorie und reichen in ihren Anfängen weit in das 19. Jahrhundert zurück.
Die verbreitete Verwendung von Thermometern, auch in Gewächshäusern, begann in der ersten Hälfte des 18. Jahrhunderts (Temperaturskalen nach Fahrenheit, Réaumur und Celsius 1724, 1730 bzw. 1742). Horace-Bénédict de Saussure maß 1767 die Intensität der Sonnenstrahlung in Tälern und in der Höhe als Temperatur in übereinander gestülpten Glaskästen. In einer verbesserten Version, einer ersten „Solarkochkiste“, erreichte er Temperaturen von über 100 °C.Die Beobachtung der Dynamik von Temperaturänderungen führte Joseph Black, den Entdecker des Kohlenstoffdioxids, in der zweiten Hälfte des 18. Jahrhunderts zur Unterscheidung der Wärmemenge von der Temperatur. Er begründete die Konzepte latente Wärme und Wärmekapazität, hatte aber vom Antrieb des Temperaturunterschiede ausgleichenden Wärmestromes eine falsche Vorstellung, siehe Kalorische Theorie. Im Jahr 1791 folgerte Pierre Prévost aus Experimenten von Saussure und Marc-Auguste Pictet, die heiße bzw. kalte Körper mit metallenen Hohlspiegeln auf Thermometer abgebildet hatten, dass sich allein durch Strahlung ein thermisches Gleichgewicht zwischen Körpern einstellen kann, siehe Prévostscher Satz.
Jean Baptiste Joseph Fourier (1768–1830) erklärte im Jahr 1824 den atmosphärischen Treibhauseffekt. Ihm fiel auf, dass die Erde viel wärmer war, als sie bei grober Abschätzung ohne Atmosphäre sein dürfte. Er stellte fest, dass die Atmosphäre sehr „gut transparent“ für sichtbares Licht ist, nicht jedoch für die vom erwärmten Boden emittierte Infrarotstrahlung. Wolken würden Nächte milder machen, indem sie diese Strahlung absorbieren. Er verglich den Effekt mit dem der Kochkiste von Saussure.Fourier erkannte korrekt, dass der größte Teil der resultierenden Erwärmung nicht dem Treibhauseffekt, sondern der unterbundenen Konvektion zuzuschreiben ist. Die Erwärmung der Box beruhte somit hauptsächlich darauf, dass die Sonneneinstrahlung als Wärmequelle fungierte und dass die Zirkulation zwischen Außen- und Innenluft unterbunden war. Der davon abgeleitete Begriff Treibhauseffekt (englisch greenhouse effect) ist in der Klimatologie bis heute gebräuchlich, obwohl der atmosphärische Treibhauseffekt vor allem auf der Klimawirkung verschiedener Treibhausgase basiert. Ebenso korrekt konstatierte Fourier, dass sowohl natürlich ablaufende Veränderungen wie auch Einflüsse der menschlichen Zivilisation auf das Klima einwirken können. Er erwartete derartige Veränderungen allerdings nur durch Veränderungen der Reflektivität, also der Albedo der Erde. Obwohl Fourier zweifellos zu den besten Mathematikern und Naturwissenschaftlern seiner Zeit zählte, vermochte er den wärmenden Effekt des Treibhauseffekts nicht mathematisch zu beschreiben.
„So wie ein Staudamm ein lokales Anschwellen eines Flusses bewirkt, so erzeugt unsere Atmosphäre, die als Barriere für die von der Erde kommende Strahlung wirkt, einen Anstieg der Temperaturen an der Erdoberfläche.“So beschrieb John Tyndall (1820–1893) im Jahr 1862 sehr treffend den natürlichen Treibhauseffekt. Im Rahmen umfangreicher und mit der damals möglichen Präzision durchgeführter Messungen identifizierte er die dafür verantwortlichen Gase. Er fand heraus, dass der Wasserdampf für den größten Teil des Treibhauseffekts verantwortlich ist. Ebenso korrekt bezeichnete er den Beitrag der übrigen Gase wie Kohlenstoffdioxid (CO2) oder Ozon (O3) als zwar deutlich schwächer, aber nicht zu vernachlässigen.
Schon 1856 hatte die US-Amerikanerin Eunice Newton Foote in Experimenten eine stärkere Erwärmung in luftgefüllten Glaszylindern durch Sonnenstrahlung beobachtet, wenn die enthaltene Luft feucht oder durch CO2 ersetzt war. Sie merkte an, dass höhere Konzentrationen von CO2 in der Atmosphäre mit einer höheren Temperatur auf der Erde verbunden sein würden. Allerdings dominierten in ihrem Versuchsaufbau andere Eigenschaften der Gase die gemessenen Temperaturdifferenzen.Tyndalls Messungen fußten unter anderem auf Vorarbeiten von Macedonio Melloni, der in Bezug auf die dafür nötige Messtechnik Pionierarbeit geleistet hatte. In Tyndalls Apparatur kam eine etwa ein Meter lange Röhre zum Einsatz, deren Enden er mit Fenstern aus Steinsalz abdeckte, da diese im Gegensatz zu Glasscheiben transparent für Infrarotstrahlung sind. An einem Ende platzierte er kochendes Wasser, dessen Temperatur sehr einfach stabil beim Siedepunkt zu halten ist, am anderen Ende ein Thermoelement, das an ein empfindliches Strommessgerät angeschlossen war. Der Ausschlag des Strommessgerätes war ein Maß für die Menge an Infrarotstrahlung, die durch das Rohr bis zum Thermoelement gelangen konnte. Untersuchungen des Absorptionsspektrums der Gase der Erdatmosphäre waren dabei nicht Gegenstand seiner Messungen; er fokussierte sich auf eine Quantifikation der Absorptionsfähigkeit für Infrarotstrahlung.Von der Richtigkeit der damals kontrovers diskutierten Eiszeittheorie überzeugt, reiste er ab der Mitte der 1850er Jahre mehrmals in die Schweiz (1856 zusammen mit dem Biologen Thomas Henry Huxley), wo er die Plastizität von Eis und das Fließverhalten von Gletschern vor Ort studierte. Daraus resultierte in den folgenden Jahren eine Vielzahl von Aufsätzen zu diesem Thema, die in englisch-, deutsch- und französischsprachigen Zeitschriften erschienen. Ausgehend von geologischen und geophysikalischen Fragestellungen widmete sich Tyndall in dieser Zeit auch verstärkt der Meteorologie sowie der Auswirkung von Treibhausgasen auf das Klima.S. 495 ff Er argumentierte, dass eine geringfügige Absenkung der Konzentration von Kohlenstoffdioxid in der Erdatmosphäre eine leichte globale Temperaturänderung bewirken würde. Dadurch werde jedoch die Konzentration des weitaus wirkungsvolleren Treibhausgases Wasserdampf beeinflusst, was letztlich eine starke Abkühlung zur Folge hätte.Um die Klimamechanismen von früheren Warm- und Kaltzeiten im Detail zu verstehen, bedurfte es jedoch weiterer physikalischer Erkenntnisse, die im Wesentlichen erst im Laufe des 20. Jahrhunderts gewonnen wurden. Wissenschaftler, die gegen Ende des 19. und Anfang des 20. Jahrhunderts darauf hinwiesen, dass der Mensch in der Lage sei, durch sein Wirken das Erdklima zu verändern, fanden lange Zeit kaum Beachtung. Weder war nach allgemeiner Einschätzung eine Erwärmung in den nächsten Jahrhunderten zu erwarten noch wäre ein anthropogen bedingter Einfluss auf das Erdklimasystem messtechnisch überprüfbar gewesen. Zudem gab es bis zur Mitte des 20. Jahrhunderts aufgrund des Fehlens systematischer Messungen keine signifikanten Belege für eine Änderung der Treibhausgas-Konzentrationen in der Erdatmosphäre.
Bereits im frühen 19. Jahrhundert wurde über verschiedene astronomische Ursachen der Eiszeiten spekuliert. So veröffentlichte der dänische Geologe Jens Esmark im Jahr 1824 die Hypothese, dass die Umlaufbahn der Erde um die Sonne in der Urzeit stark exzentrisch gewesen sei und der eines periodisch wiederkehrenden Kometen geähnelt habe. In den 1830er Jahren vermutete der französische Mathematiker Siméon Denis Poisson auf der Grundlage der damals vorherrschenden Äthertheorie eine Unterteilung des Weltalls in wärmere und kältere Regionen, durch die sich das Sonnensystem im Laufe längerer Zeitabschnitte bewegte.S. 475 ff Die erste fundierte und gut begründete Eiszeittheorie formulierte der schottische Naturforscher James Croll (1821–1890). Sich auf die Berechnungen des Mathematikers Joseph-Alphonse Adhémar und des Astronomen Urbain Le Verrier stützend, vertrat er 1864 in einer Aufsehen erregenden Arbeit im Philosophical Magazine den Gedanken, dass Veränderungen der Erdumlaufbahn in Verbindung mit der starken Eis-Albedo-Rückkopplung für das Entstehen der Eiszeiten verantwortlich sein könnten. Er war der erste, der auf die Mächtigkeit dieses Rückkopplungsglieds im globalen Klimasystem hinwies. Etwa ab 1870 wurde die Möglichkeit kosmischer beziehungsweise solarer Einflüsse auf das irdische Klima auf breiterer Basis wissenschaftlich diskutiert.Crolls Theorie wurde in den zwanziger und dreißiger Jahren des 20. Jahrhunderts von Milutin Milanković und Wladimir Köppen mit konkreten Berechnungen gestützt. Bis in die 1960er Jahre glaubten jedoch nur wenige Klimatologen, dass in den Milanković-Zyklen die Ursache für die Eiszeiten zu finden war: Die Veränderung der Intensität der Sonneneinstrahlung war sehr klein im Vergleich zu den beobachteten Temperaturschwankungen. Sie war auch dann zu klein, wenn man die Wasserdampf- und Eis-Albedo-Rückkopplung in die Betrachtungen mit einbezog. Außerdem fand man geologische Befunde über vergangene Eiszeiten, die scheinbar im Widerspruch zu der Theorie standen. In der ersten Hälfte des 20. Jahrhunderts waren die Klimadaten über vergangene Eiszeiten und deren zyklische Abläufe außerdem zu ungenau, um damit die Thesen von Croll und Milanković zu belegen oder zu widerlegen.
Neben Tyndalls Arbeiten bildeten das von Gustav Robert Kirchhoff im Jahre 1859 formulierte Kirchhoffsche Strahlungsgesetz und das im Jahr 1879 von Josef Stefan und Ludwig Boltzmann entwickelte Stefan-Boltzmann-Gesetz wesentliche Grundlagen. Letzteres ermöglichte, die Leistung zu errechnen, die von einem Strahler einer bestimmten Temperatur emittiert wird. Wilhelm Wien ergänzte das Stefan-Boltzmann-Gesetz im Jahr 1893. Mit Hilfe seines Wienschen Verschiebungsgesetzes konnte man nun auch die Wellenlänge der höchsten Photonenflussrate errechnen, die ein Strahler einer bestimmten Temperatur emittiert. Max Planck vereinigte diese Gesetze im Jahr 1900 schließlich im Planckschen Strahlungsgesetz, das bis heute die wichtigste physikalische Grundlage zum Verständnis des Strahlungshaushaltes der Erde darstellt.
Der schwedische Physiker und Chemiker Svante Arrhenius (1859–1927) war von Tyndalls Idee fasziniert, dass wechselnde Konzentrationen von Kohlenstoffdioxid ein wesentlicher Faktor für die Erklärung der großen Temperatursprünge zwischen Warm- und Eiszeiten sein könne. Aufbauend auf Vorarbeiten von Samuel Pierpont Langley stellte er als erster umfangreiche Berechnungen an. Er berechnete letztlich ein stark vereinfachtes Klimamodell, das er während mehrerer Monate ohne maschinelle Hilfe durchrechnete. Im Jahr 1896 veröffentlichte er seine Ergebnisse zusammen mit der Hypothese, dass eine Halbierung der Kohlenstoffdioxid-Konzentration ausreiche, eine Eiszeit einzuleiten. Herausragend war, dass er die von James Croll beschriebene Eis-Albedo-Rückkopplung in seinen Berechnungen berücksichtigte.
Prominente Unterstützung für seine Theorie erhielt er unter anderem von Nils Ekholm und Thomas Chrowder Chamberlin. Cyrus F. Tolman schätzte in einer 1899 erschienenen Publikation, dass sich in den Ozeanen dieser Welt in Form von Kohlensäure etwa 18-mal mehr Kohlenstoffdioxid als in der Atmosphäre befindet; die Löslichkeit von Kohlenstoffdioxid ist jedoch temperaturabhängig. Von daher sei es durchaus möglich, dass dies die Reservoirs sind, in denen das atmosphärische CO2 während der Eiszeiten gelöst ist. Es könnte bei zunehmender Erderwärmung freigesetzt werden und damit auf den jeweiligen Trend der globalen Durchschnittstemperaturen verstärkend wirken.Dass eine anthropogene CO2-Anreicherung in der Atmosphäre auch die aktuelle Erdtemperatur weiter erhöhen könne, erwähnte Arrhenius zunächst nur als Nebenaspekt. Erst in einer 1908 erschienenen Publikation diskutierte er dies detailliert. Für die Klimasensitivität ermittelte er 5 bis 6 °C. Den für solch eine Temperaturerhöhung nötigen, doppelt so hohen atmosphärischen Kohlenstoffdioxidgehalt erwartete er auf Basis der weltweiten Emissionsraten des Jahres 1896 in ca. 3000 Jahren, und erst in einigen Jahrhunderten erwartete er, dass eine Temperaturerhöhung überhaupt messbar sei. Er hoffte dabei auf „gleichmäßigere und bessere klimatische Verhältnisse“ sowie „um das Vielfache erhöhte Ernten“. Er verstand aber auch, dass eine dauerhafte Nutzung fossiler Brennstoffe aufgrund der damit verbundenen globalen Erwärmung langfristig zu Problemen führen würde.Arrhenius’ Zeitgenosse Walther Nernst griff Arrhenius’ Gedanken auf und schlug vor, zusätzliches Kohlenstoffdioxid für die Erwärmung der Erdatmosphäre zu produzieren. Er wollte dafür Kohle verbrennen, die nicht wirtschaftlich zu fördern war.
In der ersten Hälfte des 20. Jahrhunderts stand man der Theorie von Arrhenius zunächst überwiegend ablehnend gegenüber. Seine Annahmen basierten auf zu vielen unbestätigten und vereinfachenden Annahmen, sodass die Skepsis berechtigt war. Arrhenius hatte in seinen Berechnungen die Eis-Albedo-Rückkopplung und die Wasserdampf-Rückkopplung in Ermangelung konkreter Daten nur durch Schätzwerte berücksichtigt. Einen Wärmetransport durch Konvektion und Meeresströmungen betrachtete er gar nicht, und zur Untermauerung seiner Eiszeittheorie fehlte ihm die Kenntnis der eiszeitlichen atmosphärischen Treibhausgaskonzentrationen. Auch bezog er in seine Betrachtungen mögliche, durch eine Erwärmung ausgelöste Veränderungen der Wolkenbildung nicht in seine Berechnungen mit ein. Wolken können die Strahlungsbilanz der Erde jedoch signifikant verändern, und einige Wissenschaftler seiner Zeit gingen davon aus, dass eine Erwärmung über eine verstärkte Wolkenbildung selbige vollständig ausgleichen würde.
Im Jahr 1900 erschien eine Publikation des namhaften Physikers Knut Ångström. In dieser legte er dar, dass eine Halbierung des atmosphärischen Kohlenstoffdioxid-Gehaltes die Infrarotabsorption nur um 0,4 % verändern würde, was auf das Klima keinen signifikanten Einfluss haben könne. Wie sich später herausstellte, führte Ångströms Laborassistent die Messung jedoch fehlerhaft durch, die damals verfügbaren Spektrometer waren für die Aufgabe zu ungenau, und überdies interpretierte er die Messergebnisse falsch. Ångström ging fälschlicherweise davon aus, dass sich die Absorptionsspektren von Wasserdampf und Kohlenstoffdioxid weitestgehend überdecken und die Absorptionswirkung des Spurengases daher vernachlässigbar sei. Dies war jedoch das Ergebnis der zu dieser Zeit für diese Messung unzureichenden Messgeräte. Bei korrekter Messung hätte Ångströms Assistent eine, aus der Halbierung der Kohlenstoffdioxidkonzentration resultierende Absorptionsänderung von 1 % gefunden.
Ein weiterer Fehler erwuchs daraus, dass Ångströms Assistent seine Messungen auf Höhe des Meeresspiegels durchführte. Selbst wenn es dort keinen messbaren Absorptionsunterschied gäbe, würde es an der Wirkung einer Konzentrationsänderung des Treibhausgases Kohlenstoffdioxid nichts ändern: Für die Stärke des Gesamttreibhauseffekts ist die Treibhauswirkung in höheren Atmosphärenschichten entscheidend, wo die Luft aufgrund der herrschenden Kälte sehr trocken ist. Daher wirkt sich die Überschneidung der Absorptionsbande des Kohlenstoffdioxids mit denen von Wasserdampf insgesamt kaum aus. Da die Luft in großen Höhen nicht nur sehr trocken, sondern auch erheblich weniger dicht ist als am Boden, bewirkt eine Erhöhung der Kohlenstoffdioxidkonzentration dort durchaus in Form einer verstärkten Absorption eine Verstärkung des Treibhauseffekts.
Die Erde strahlt im Mittel in einer Höhe von 5500 m ihre Wärme ins All ab. Eine Erhöhung der mittleren globalen Treibhausgaskonzentrationen führt dazu, dass der Bereich, in dem die Erde abstrahlt, in größere Höhen verschoben wird. Da es dort aber kälter ist, wird dort weniger effektiv Wärme abgestrahlt; der zusätzliche Wärmestau lässt alle darunter liegenden Atmosphärenschichten wärmer werden, bis die abstrahlende Schicht wieder so viel Energie in Richtung All verliert wie von der Sonne eingestrahlt wird. Arrhenius erkannte die Fehler in Ångströms Argumentation und widersprach aufs Heftigste.
In den 1930er Jahren bemerkte man in den USA, dass sich die Temperaturen in ihrer Region in den vorangehenden Jahrzehnten erhöht hatten; Wissenschaftler gingen mehrheitlich von einem natürlichen Klimazyklus aus, und ein verstärkter Treibhauseffekt war nur eine von vielen möglichen Ursachen.Nach Auswertung der Temperaturdaten der letzten 50 Jahre von 200 meteorologischen Stationen ermittelte Guy Stewart Callendar (1898–1964) eine statistisch signifikante globale jährliche Erwärmungsrate von 0,005 °C. Er war der Ansicht, dass diese Erwärmung zu ausgeprägt und zu umfassend war, um auf einer natürlichen Klimafluktuation zu basieren. In einer im Jahr 1938 publizierten Arbeit schätzte er die über die vergangenen 50 Jahre emittierte Menge Kohlenstoffdioxid auf 150.000 Mio. Tonnen. Er nahm an, dass sich davon noch etwa drei Viertel in der Atmosphäre befänden. Die von dieser Treibhausgasmenge resultierende Erwärmung schätzte er auf 0,003 °C/Jahr (Stand 2011: 0,02 °C/Jahr) und ging von einer CO2-Konzentration von 274 ppm im Jahr 1900 aus; durch Fortschreiben der damaligen, geschätzten jährlichen Emissionsrate von 4.500 Mio. Tonnen Kohlenstoffdioxid erwartete er für das Jahr 2100 eine atmosphärische Kohlenstoffdioxidkonzentration von 396 ppm (dieser Wert wurde im Jahr 2013 erreicht). Die aus der Verbrennung fossiler Brennstoffe resultierende Erwärmung schätzte er für das 20. Jahrhundert auf 0,16 °C, für das 21. Jahrhundert auf 0,39 °C und für das 22. Jahrhundert auf 0,57 °C. Auch Callendar betrachtete die globale Erwärmung als etwas Positives, da durch die anthropogene Erwärmung das Risiko einer bald wiederkehrenden Eiszeit auf absehbare Zeit gebannt schien.
Hermann Flohn war der erste deutsche Klimaforscher, der die globale Klimawirkung von anthropogen erhöhten CO2-Konzentrationen bzw. den anthropogenen Klimawandel seit seiner Antrittsvorlesung 1941 an der Universität Würzburg vertrat und hierzu zahlreiche Publikationen bis zu seinem Tod 1997 veröffentlichte. Flohn gilt international als einer der Wegbereiter der internationalen und nationalen Klimaforschung und hat auf die CO2-Problematik seit der Nachkriegszeit immer wieder hingewiesen. Wenngleich diese Position unter Klimatologen damals nicht unumstritten war, so erhielt er von Experten Unterstützung, unter anderem von dem führenden Klimatologen Michail Iwanowitsch Budyko.
Auch wenn einzelne Wissenschaftler schon zu einem sehr frühen Zeitpunkt die Klimarelevanz zunehmender Kohlenstoffdioxid-Konzentrationen betonten, wurden Callendars Arbeiten überwiegend kritisiert. Zur damaligen Zeit gab es keinen stichhaltigen Beleg dafür, dass der atmosphärische CO2-Gehalt tatsächlich ansteigt. Die verfügbaren Daten zum atmosphärischen CO2-Gehalt waren sehr ungenau. Messungen ergaben Werte, die abhängig von Messort und Messzeit so stark voneinander abwichen, dass weder eine Durchschnittskonzentration bekannt noch ein eventueller Anstieg nachweisbar war. In den Ozeanen der Welt ist in Form von Kohlensäure 50-mal mehr Kohlenstoffdioxid gelöst als die gesamte Atmosphäre enthält. Da sich Kohlenstoffdioxid gut in Wasser löst, ging die überwiegende Zahl der Wissenschaftler davon aus, dass alle vom Menschen zusätzlich eingebrachten Mengen des Spurengases Kohlenstoffdioxid im Meer verschwinden. Zumal man wusste, dass die durch Verbrennung fossiler Brennstoffe emittierte CO2-Menge nur ein winziger Bruchteil jener Menge ist, welche im Rahmen von Stoffwechselvorgängen wie Photosynthese und Atmung umgesetzt wird.
Die Arbeiten von Tyndall, Arrhenius und Callendar wurden kaum mehr diskutiert. Sie enthielten auch zu viele auf absehbare Zeit unüberprüfbare Thesen. Der unleugbare Befund der Eiszeiten wartete zwar noch auf eine Lösung, doch erklärte man sich die Eiszeiten durch geologische Ursachen, die über veränderte Wind- und Meeresströmungen das Klima lokal beeinflusst hatten. Globale Klimaveränderungen hielt zu dieser Zeit kaum jemand für möglich.1951 schrieb die American Meteorological Society im Compendium of Meteorology: „Die Idee, dass eine Erhöhung des Kohlenstoffdioxid-Gehaltes der Atmosphäre das Klima verändern könne, war nie weit verbreitet und wurde schließlich verworfen, als man herausfand, dass alle von Kohlenstoffdioxid absorbierte Infrarotstrahlung bereits von Wasserdampf absorbiert wird.“ Dass dies falsch ist und Arrhenius mit seinem Einwand recht hatte, war allerdings bereits fast 20 Jahre zuvor publiziert worden – unter anderem von E.O. Hulburt und Guy Callendar.
Die 1950er Jahre brachten einen enormen Wissenszuwachs in allen Bereichen der Wissenschaft. Bedingt durch den kalten Krieg erhöhte die amerikanische Regierung die Forschungsausgaben in vielen Bereichen von Naturwissenschaft und Technik, so auch in Geologie, Ozeanographie und Meteorologie. Die Militärs interessierten sich dafür, wie die Strahlung der Atombomben absorbiert wird, und wie sich der Fallout in Atmosphäre und den Weltmeeren verteilt. Auch wollte man wissen, wann irgendjemand irgendwo einen überirdischen Atombombentest durchführt. Es gab kaum einen Bereich, der für das Militär unwichtig hätte sein können.Eine der wichtigsten Errungenschaften der Paläoklimatologie war die Kombination der radiometrischen Datierung mit der Chromatographie und der Massenspektrometrie. Damit wurde es möglich, das absolute Alter und damit die Entstehungszeit vieler Fossilien zu ermitteln.
Willard Libby hatte bereits in den 1930er Jahren Zählverfahren für sehr schwache Radioaktivität entwickelt. Darauf aufbauend stellte er im Jahr 1949 die Radiokohlenstoffdatierung vor. Mit diesem revolutionären Verfahren konnte man das Alter von kohlenstoffhaltigen Fossilien, die nicht älter als 50.000 Jahre waren, mit bislang unbekannter Genauigkeit bestimmen. Oberirdische Tests von Kernwaffen führten zu einem starken Konzentrationsanstieg von radioaktivem 14C, dem Kernwaffen-Effekt. Mit Hilfe der neuen Technik von Libby war es nun möglich, auch das durch Atombombentests erzeugte 14C nachzuweisen.
Der Direktor der US-amerikanischen Scripps Institution of Oceanography, Roger Revelle, hatte sich am Anfang seiner Laufbahn in den 1930er Jahren intensiv mit der Chemie der Ozeane beschäftigt. Er galt als Experte in diesem Feld und hatte enormes Wissen über den Gasaustausch zwischen Atmosphäre und Ozeanen gesammelt. Er verfügte jedoch nicht über geeignete Methoden, um den Gasaustausch von Kohlenstoffdioxid untersuchen zu können, sodass er sich anderen Dingen zuwandte.Um zusätzlich durch die Verbrennung fossiler Brennstoffe erzeugtes Kohlenstoffdioxid aufnehmen zu können, mussten sich die Ozeane durchmischen. Zufällig fand Revelle im Rahmen eines Forschungsprojektes heraus, dass sich radioaktiver Kohlenstoff, der bei einem Unterwasser-Bombentest entstanden war, in einer Schicht bewegte, die zwar nur einen Meter dick war, sich aber über hunderte von Quadratkilometer erstreckte. Dies belegte zu seinem großen Erstaunen, dass es kaum zu einer vertikalen Durchmischung der Wasserschicht gekommen war. Wenn dies für 14C aus Atomtests galt, so musste es auch für jede andere Substanz gelten, die in die Meere eingebracht wurde – auch für Kohlenstoffdioxid.Eines Tages wurde Revelle auf die Arbeiten von Hans E. Suess aufmerksam, der sich mit Optimierungsverfahren der neuen Radiokohlenstoffdatierung beschäftigte. Dies passte gut zu seinen Forschungsprojekten über die Durchmischung und den Gasaustausch der Ozeane; glücklicherweise hatte er Budgets, um Suess anzuwerben, mit ihm zusammen die ungelösten Fragen zum Kohlenstoffdioxidaustausch der Ozeane anzugehen.
Nach Auswertung der 14C-Studien publizierten Revelle und Suess im Jahr 1957, dass die durchschnittliche Verweilzeit von Kohlenstoffdioxid in der Atmosphäre in der Größenordnung von ca. 10 Jahren liege. Dies war in guter Übereinstimmung mit den Ergebnissen der Arbeitsgruppe um James R. Arnold, der zuvor bei Willard Frank Libby gearbeitet hatte und gerade an der Princeton University tätig war. Im Jahr 1958 wechselte Arnold zu Revelle an den damals neu gegründeten Campus der University of California in San Diego.Die Dauer einer kompletten Umwälzung der Ozeane schätzten die Forscher auf ein paar hundert Jahre. Die Ergebnisse ließen den Schluss zu, dass sich durch die Verbrennung fossiler Brennstoffe entstandenes Kohlenstoffdioxid sehr schnell in den Ozeanen löse, weshalb es unwahrscheinlich schien, dass es sich in der Atmosphäre anreichert. Dies aber würde Spekulationen über eine mögliche, menschengemachte globale Erwärmung, die durch einen Konzentrationsanstieg des Spurengases hervorgerufen wurde, substanzlos machen.Diese Ergebnisse standen jedoch im Widerspruch zu Guy Callendars Analysen. Dieser wurde nicht müde darauf hinzuweisen, dass die ihm vorliegenden (recht ungenauen) Messreihen des Spurengases Kohlenstoffdioxid klar darauf hindeuteten, dass dieses sich in der Atmosphäre anreichere. Es gab aber noch einen viel gewichtigeren Hinweis: Suess hatte bei seinen Studien der Radiokohlenstoffdatierung entdeckt, dass jüngere Holzproben ein verschobenes 12C/14C-Verhältnis aufwiesen: Je jünger sie waren, desto weniger 14C enthielten sie. Und zwar weniger als durch radioaktiven Zerfall begründet werden konnte. Erklärbar war dieser Effekt, wenn das Kohlenstoffdioxid der Atmosphäre mit Kohlenstoffdioxid vermischt worden war, das aus der Verbrennung fossiler Brennstoffe stammte, in dem 14C aufgrund ihres hohen Alters praktisch vollständig zerfallen war. Dieser Effekt wurde später als Suess-Effekt bekannt. Die Argumente waren nicht von der Hand zu weisen. Revelle und Suess suchten nach Fehlern in ihrer Arbeit. Zunächst versuchten sie den Widerspruch dadurch zu erklären, dass sie die Aufnahme von Kohlenstoffdioxid durch Pflanzen nicht in ihre Überlegungen aufgenommen hatten. Schließlich fanden Bert Bolin und Erik Eriksson jedoch das Hauptproblem: Die Forscher hatten den Stoffaustausch bei Vorliegen einer Gleichgewichtskonzentration zwischen Atmosphäre und Ozeanen untersucht. Die Verbrennung fossiler Brennstoffe führt jedoch zu einem steten CO2-Zustrom, ein Gleichgewicht liegt nicht vor. Wird daneben auch die sehr langsame Umwälzrate der Ozeane berücksichtigt, führt dies zu einem gänzlich anderen Ergebnis: Demnach würde atmosphärisches Kohlenstoffdioxid zwar rasch gelöst, aber eben auch ebenso rasch wieder in die Atmosphäre emittiert werden, sodass nur etwa 25 % von den Ozeanen aufgenommen würde. Das Verhältnis zwischen atmosphärischer Emission und Aufnahme durch die Ozeane wurde nach Roger Revelle benannt und Revelle-Faktor genannt.
Alles deutete nun darauf hin, dass Callendar recht hatte, dass sich Kohlenstoffdioxid in der Tat in der Atmosphäre anreicherte.
Für die Klärung der Frage, ob der von Revelle und Suess vorausgesagte Konzentrationsanstieg des Treibhausgases Kohlenstoffdioxid in der Atmosphäre auch tatsächlich messtechnisch nachweisbar sei, bewarb sich das Scripps-Institut mit dem Projekt einer atmosphärischen Kohlenstoffdioxid-Messung für das Internationale Geophysikalische Jahr 1957/58. Der junge Chemiker Charles David Keeling wurde mit dem Projekt betraut; er konnte schon ein Jahr später mit der nach ihm benannten „Keeling-Kurve“ aufwarten, die der erste zweifelsfreie Beleg war, dass die Konzentration dieses Treibhausgases ansteigt. Im Gegensatz zu seinen Vorgängern, die an dieser Aufgabe gescheitert waren, führte Keeling seine Messungen fernab von Quellen und Senken des Spurengases durch und benutzte erstmals einen nichtdispersiven Infrarotsensor mit einem Messaufbau, der Ergebnisse höchster Präzision lieferte. Zusätzliche Genauigkeit erhielten seine Messwerte dadurch, dass er diese nicht punktuell, sondern an mehreren, weit voneinander entfernten Stationen kontinuierlich erfasste.
Gilbert Plass nutzte 1956 erstmals Computer zur Berechnung der zu erwartenden Erwärmung, wobei in diese Modellprojektionen erstmals genaue Absorptionsspektren des CO2 einflossen. Entsprechende Messungen hatten Physiker der Johns Hopkins University durchgeführt, und Plass konnte im Rahmen einer Kooperation auf diese Daten zurückzugreifen. Ihm gelang als erster der Nachweis, dass sich die Absorptionsbande von Wasserdampf und Kohlenstoffdioxid nicht überlagern. Zudem stellte er fest, dass eine durch einen Konzentrationsanstieg von Kohlenstoffdioxid verursachte globale Erwärmung auch dann nicht verhindert würde, wenn sich die Absorptionsbande vollständig überlagerten. Er errechnete eine globale Erwärmung um 3,6 °C für eine angenommene Verdoppelung der atmosphärischen Kohlenstoffdioxidkonzentration.
Für das Jahr 2000 nahm er einen um 30 % höheren Kohlenstoffdioxidgehalt der Atmosphäre an und erwartete eine daraus resultierende globale Erwärmung von etwa einem Grad.Nach dieser Datenlage war eine messbare anthropogene globale Erwärmung nicht mehr in Jahrhunderten, sondern bereits in Jahrzehnten zu erwarten. Der Treibhauseffekt war durch die Berechnungen von Plass genauer quantifiziert, und auch der Konzentrationsanstieg des Treibhausgases Kohlenstoffdioxid war nunmehr eindeutig belegt. Roger Revelle kommentierte dies mit den häufig zitierten Worten: „Die Menschheit hat ein großangelegtes geophysikalisches Experiment begonnen, das es in dieser Form weder in der Vergangenheit gab, noch in der Zukunft ein zweites Mal geben wird.“
Seit den 1940er Jahren und im gesamten Verlauf der 1960er Jahre nahmen die Durchschnittstemperaturen global ab. Zweifler an der Theorie einer menschengemachten Erwärmung fanden sich bestätigt, denn die Kohlenstoffdioxidkonzentrationen stiegen ja während dieser Zeit an. Von einer globalen Abkühlung durch Aerosole war die Rede. Für die Abkühlung wurde von einigen Forschern eine zunehmende Umweltverschmutzung der Luft verantwortlich gemacht. Zur Zeit der kriegsbedingten Rüstungsindustrialisierung bis 1945 häuften sich in Europa die Winter mit extremer Kälte. In der Zeit bis 1960 wurden auch Wohnhäuser durchweg mit Kohle beheizt, Heizöl stand noch nicht ausreichend zur Verfügung. Diese Smog-Problematik durch Kohleabbrand sollte sich 50 Jahre später bei der rasanten Industrialisierung Chinas wiederholen.
Die Verfügbarkeit von ersten Computern hatte in den 1950er Jahren zur ersten numerischen Wettervorhersage geführt, und man wollte Computer natürlich auch zur Berechnung klimatologischer Prozesse einsetzen. Dies brachte jedoch zunächst eher Verwirrung als Klärung und schürte den Zweifel an der Richtigkeit der These der globalen Erwärmung.Mit Hilfe der genauen Absorptionsdaten von Wasserdampf und Kohlenstoffdioxid, die Gilbert Plass wenige Jahre zuvor publiziert hatte, rechnete Fritz Möller ein eindimensionales Klimamodell durch, bei dem er nicht nur den durch zusätzliche Erwärmung freigesetzten Wasserdampf, die so genannte Wasserdampf-Rückkopplung mit einbezog, sondern auch den Wärmeaustausch zwischen Boden und Atmosphäre. Zu seinem Erstaunen ergaben seine Berechnungen massive Erwärmungen und unter bestimmten Bedingungen sogar eine nicht endende Erwärmung, die sich immer weiter verstärkte, bis alle Ozeane verdampfen würden. Aber unter der Annahme, dass aus der Erwärmung eine Erhöhung der Wolkenbedeckung um ein Prozent resultierte, hätte dies den wärmenden Effekt einer Erhöhung der Kohlenstoffdioxid-Konzentration selbst um 10 % vollständig ausgeglichen. Und niemand kannte die Reaktion der Wolkenbildung auf eine Temperaturänderung. Die korrekte Beschreibung des Einflusses der Wolken war ein großes Problem und sollte es auch in den folgenden Jahrzehnten bleiben.Der Grund für die starke Erwärmung, die Möller fand, war schnell gefunden: Er hatte in seinem eindimensionalen Klimamodell zwar den Wärmetransport zwischen Boden und Luft berücksichtigt, nicht jedoch den Wärmetransport durch Konvektion. Dies realisierte Syukuro Manabe schon Mitte der 1960er Jahre und entwickelte es zusammen mit Richard Wetherald weiter. Das 1967 erstellte “Manabe-Wetherald one-dimensional radiative-convective model” gilt als das erste einigermaßen realistische Atmosphärenmodell, das sowohl den Strahlungshaushalt der Erde wie auch die stattfindende Konvektion berücksichtigte. Dieses Modell ergab eine Erwärmung um 2,3 °C, die aus einer Verdoppelung der Kohlenstoffdioxidkonzentration der Atmosphäre resultieren würde.
Mitte der 1960er Jahre wurde eine weitere Schlüsseltechnologie für die Klimatologie nutzbar: Erdbeobachtungssatelliten. Schon die zweite Generation der TIROS-Satelliten wurde ab 1966 operativ für die Klimaforschung eingesetzt und verfügte über Radiometer und Spektrometer. Von nun an konnte man den Wärmehaushalt der Erde, deren Eisbedeckung oder Spektrum und Intensität der Sonneneinstrahlung vom All aus vermessen. Sonnenbezogene Messungen waren erstmals vollkommen frei von verfälschenden atmosphärischen Einflüssen und führten zu einer genauen Definition der Solarkonstante, die vorher nur annähernd bestimmt werden konnte.
Mit Hilfe des Nimbus-III-Satelliten konnte Manabe im Jahre 1969 sein Klimamodell mit Messdaten aus dem All verifizieren. Es zeigte sich eine gute Übereinstimmung.Die Zahl und Qualität der verbauten Satelliten-Instrumente sollte in den kommenden Jahrzehnten stark ansteigen, wobei auch bei der Miniaturisierung erhebliche Fortschritte zu verzeichnen waren.
Weitere Pionierarbeit leistete Michail Iwanowitsch Budyko. Er berechnete die Strahlungsbilanzen für die ein- und ausgehende Strahlung in arktischen Regionen und lieferte quantitative Angaben für die bislang nur qualitativ beschriebene Eis-Albedo-Rückkopplung. Nachdrücklich warnte er vor daraus resultierenden Klimaveränderungen, die allerdings erst im nächsten Jahrhundert zu erwarten seien. Ein Beratungsgremium der US-Regierung äußerte im Jahr 1965 ebenfalls die Befürchtung, dass die globale Erwärmung eine ernsthafte Bedrohung darstelle („… a matter of ‘real concern’“). Von den Experten wurde deshalb empfohlen, die Chancen und Risiken von Geoengineering zu prüfen. Damit sollte die Albedo der Erdoberfläche erhöht werden, um den wärmenden Effekt der steigenden Kohlenstoffdioxid-Konzentration in der Atmosphäre zu kompensieren.Um 1965 lautete eine Einschätzung:
Der Widerspruch steigender Kohlenstoffdioxidkonzentration trotz weltweit zurückgehender Temperaturen veranlasste John D. Hamaker zur Entwicklung einer Theorie, nach der ein verstärkter Treibhauseffekt über veränderte Wolkenbildung, veränderte Niederschlagsmuster und Prozessen in der Biosphäre zwar zunächst zu einer Erwärmung, aber in der Folge zu einer zunehmenden Vereisung an den Polen führe und über die Eis-Albedo-Rückkopplung damit den Beginn einer Eiszeit auslösen würde. Anhand von Forschungsergebnissen späterer Jahre – insbesondere durch die Daten des Wostok-Eisbohrkernes – wurde seine Theorie jedoch widerlegt.
Bis Mitte der 1970er Jahre sanken die globalen Durchschnittstemperaturen weiter, was in der Klimatologie zu heftigen Kontroversen führte. Schon damals wurde vermutet, dass die massiven Einträge von Aerosolen in der Atmosphäre die Ursache für die beobachtete Abkühlung sein könnten. So wurde der US-Präsident unter anderem von George Kukla und Reid Bryson vor einer dadurch ausgelösten Eiszeit gewarnt. In einer unter anderem von Stephen Schneider publizierten Arbeit wurde über die Möglichkeit spekuliert, dass die kühlende Wirkung der Aerosole den wärmenden Effekt der Treibhausgase überdecken könne. Das Problem war, dass zur damaligen Zeit Kenntnisse über das genaue Ausmaß kühlender oder wärmender Effekte fehlte und man daher nicht wusste, welcher Effekt stärker war.
Auf der anderen Seite wurde jedoch von einer deutlich größeren Gruppe von Forschern vor einer kommenden, signifikanten globalen Erwärmung gewarnt. Bei den aktuellen Kohlenstoffdioxid-Emissionen könnte die Erwärmung gegebenenfalls schon im Jahr 2050 zu einem eisfreien Polarmeer führen. Im Jahr 1975 schrieb Wallace Broecker im Abstract einer seiner Publikationen:
Broecker sollte mit seiner Prognose recht behalten – sogar ohne die Verstärkung durch einen etwa 80-jährigen natürlichen Zyklus, von dem er damals irrtümlich noch zusätzlich ausging, bewirkten vor allem steigende CO2-Konzentrationen einen derartigen Temperaturanstieg.
Nicht nur wurde seine Arbeit häufig zitiert, auch der darin verwendete Begriff eines Global Warming wurde aufgegriffen. Global Warming bzw. dessen Übersetzung globale Erwärmung wurde fortan zum Synonym für den menschengemachten Klimawandel.
Das Bild vergangener Eiszeiten konnte immer klarer nachgezeichnet werden und zeigte, dass Klimaveränderungen sehr schnell ablaufen können. Ganz im Gegensatz zu der jahrzehntelang verbreiteten Annahme eines unveränderlichen und stabilen Klimas deutete nun alles darauf hin, dass selbst kleine Parameter-Änderungen sprunghafte Klimawandel zur Folge haben konnten. Vorarbeiten aus dem Jahr 1966 hatten bereits Belege geliefert, dass es zum Ende der letzten Eiszeit zu schnellen und heftigen Klimaveränderungen gekommen war. Die Befunde, die in den 1960er Jahren ausschließlich von Sedimentbohrkernen vom Meeresboden um Grönland stammten, konnten nun auch an anderen Orten der Erde und mit anderen Nachweismethoden wie z. B. Eisbohrkernen in Einklang gebracht werden. Sie zeigten darüber hinaus übereinstimmend, dass eine Warmzeit wie die des Holozäns in der Klimageschichte des Quartärs nicht die Regel, sondern eine Ausnahme war. Kurze warme Perioden wechselten sich mit langen kalten Perioden ab. Auch in den 1970er Jahren konnte kein messtechnischer Beleg für die seit langem vorausgesagte, aber nie bestätigte globale Erwärmung geliefert werden. Überdies dauerte die aktuelle Warmzeit, das Holozän, bereits 11.700 Jahre an, während die letzte Warmzeit, die Eem-Warmzeit, nach einer Dauer 11.000 Jahren endete. Eine baldige Eiszeit schien für einige daher wahrscheinlicher als eine Erwärmung.
Bohrkernuntersuchungen aus Grönland zeigten, dass zusammen mit dem Klima auch der Salzgehalt des Meerwassers in der Vergangenheit geschwankt hatte. Der Nordatlantikstrom hatte sich offenbar mehrfach verändert. Dies stützte Vermutungen, dass Meeresströmungen wegen der sehr großen Menge der durch sie transportierbaren Energie eine wichtige Rolle im Klimageschehen zukommt. Syukuro Manabe hatte die große Bedeutung der Meere zum Verständnis des Klimageschehens erkannt und 1969 ein erstes Klimamodell entworfen, mit dem er das Verhalten der Meere modellierte. Leider waren aber auch in den 1970er Jahren Computer bei weitem nicht leistungsfähig genug, ein solch komplexes Klimamodell über längere Zeiträume durchzurechnen. Die 14C-Untersuchungen von Revelle und Suess hatten gezeigt, dass die Ozeane für eine vollständige Umwälzung knapp 1000 Jahre brauchten. Dies war in geologischen Zeitskalen kurz, als Berechnungszeitraum für ein komplexes Klimamodell war aber die Dauer einer einzigen Ozean-Umwälzung deutlich zu viel. Klimamodelle, die neben der Strahlungsbilanz und der Konvektion auch das Verhalten der Meere berücksichtigten, mussten daher sehr stark simplifiziert werden, um berechenbar zu bleiben.Zusammen mit dem Ozeanographen Kirk Bryan schaffte es Manabe, ein vereinfachtes Klimamodell zu entwerfen, in dem neben der Strahlungsbilanz und der Konvektion auch Jahreszeiten und das Verhalten der Ozeane enthalten waren. Im Jahr 1979 konnte ihr Modell über einen Zeitraum von 1000 Jahren durchgerechnet werden. Wenngleich es viele Unzulänglichkeiten aufwies, hatte es doch einige Merkmale unseres Erdklimas; so bildeten sich beispielsweise das Wüstengebiet der Sahara sowie die starken Niederschläge der Pazifikregion aus, ohne dass die Forscher das Modell speziell darauf ausgelegt hätten, diese Phänomene zu zeigen.
Forscher versuchten nun mit Hilfe von Klimamodellen, das Klima während der Eiszeiten wie auch das der Neuzeit korrekt nachzubilden. Wenn dies gelänge, wüsste man, welche Rückkopplungen im Klimasystem wie stark wirken und könnte diese Parameter verwenden, das Ausmaß einer kommenden Erwärmung abzuschätzen. Voraussetzung dafür war jedoch, das Klima vergangener Eiszeiten zu kennen. In den 1970er Jahren wurde mit dem CLIMAP-Projekt genau dies versucht, denn Fortschritte im Bereich der Isotopenuntersuchung und Massenspektrometrie erlaubten, die Klimavergangenheit immer besser zu rekonstruieren.
Im Jahr 1953 hatte Willi Dansgaard gezeigt, dass die Zusammensetzung von 18O (Sauerstoff-18) und 2H (Wasserstoff) in Regenwasser in Abhängigkeit von der herrschenden Temperatur schwankt. Dieses Prinzip der so genannten Sauerstoff-Isotopenstufe wurde in den 1970ern von Cesare Emiliani, John Imbrie und Nicholas Shackleton dafür verwendet, die Temperaturen des Känozoikums zu analysieren. Die einen Zeitraum von einer Million Jahre abdeckende Analyse offenbarte auf hervorragende Weise, dass Schwankungen der Sonnenintensität durch Veränderungen der Erdumlaufbahn für die starken Klimaschwankungen während dieser Zeit verantwortlich waren. Die Arbeit von Imbrie und Shackleton beseitigte die letzten Zweifel an der Korrektheit der Theorie von Croll und Milankovic; diese erlangte unter dem Begriff Milanković-Zyklen allgemeine Bekanntheit. Leicht modifiziert ist die Theorie seit den 1980er Jahren fester Bestandteil von Paläoklimatologie und Quartärforschung und gilt als unverzichtbares Instrument sowohl bei der Rekonstruktion der Eiszeiten als auch bei der Erforschung verschiedener Klimawandel-Ereignisse im Laufe des Phanerozoikums.
Darüber hinaus können mithilfe der Milanković-Zyklen auch künftige klimatische Entwicklungen prognostiziert werden. So erwartete Shackleton aufgrund seiner Analysen eine neue Eiszeit innerhalb der kommenden 20.000 Jahre.
Anfang der 1970er Jahre hatten theoretische Überlegungen über den Aufbau von Eisschilden ergeben, dass diese inhärent instabil sind und unter bestimmten Bedingungen zum Kollaps neigen. Der Glaziologe John Mercer erkannte dann im Jahr 1978, dass der westantarktische Eisschild eine besondere Topologie besitzt, die zu einem derartigen Kollaps führen kann. So ruht der Eisschild der Westantarktis auf Gesteinsflächen, die sich unterhalb des Meeresspiegels befinden; der Meeresboden steigt dort an, je weiter man sich vom Kontinentalsockel in Richtung Meer entfernt, um dann wieder abzufallen. Die Aufsetzlinie (engl. grounding line) ist die Stelle, an der der Eisschild den Kontakt zum festen Boden verliert und zu schwimmen beginnt. Ab diesem Punkt spricht man nicht mehr von einem Eisschild, sondern von einem Eisschelf. Wenn die Aufsetzlinie des Eisschildes, bedingt durch eine Schmelze, den höchsten Punkt dieses Profils überwinden würde, würde eine unaufhaltbare Dynamik einsetzen, die einen beschleunigten und unaufhaltsamen Zerfall des Gletschers zur Folge hätte. Mercer betonte, dass ein derartiger Kollaps zu den ersten desaströsen Folgen eines menschengemachten Klimawandels gehören würde. Er erwähnte in derselben Publikation, dass sich ein derartiges Ereignis durch Aufbrechen mehrerer großer antarktischer Eisschilde ankündigen würde.
In den Folgejahren fanden die Forscher eine Reihe weiterer Faktoren, die ebenfalls zur Erderwärmung beitragen.
Die Atmosphärenchemie machte große Fortschritte. Der geplante Bau einer Flotte hochfliegender Überschallflieger sowie eine große Zahl erwarteter Weltraumflüge lenkte die Aufmerksamkeit der Forscher auf die Auswirkungen der damit verbundenen Emissionen in der Stratosphäre. Untersuchungen ergaben, dass die Ozonschicht durch Stickoxide und FCKWs geschädigt würde, die neben einer sehr hohen Lebensdauer in der Atmosphäre auch ein enormes Potential als Treibhausgas besaßen. Erstmals wurde dabei auch auf die Wirkung bislang wenig beachteter Treibhausgase wie Methan und Lachgas hingewiesen. Jedoch fanden diese Stimmen wenig Beachtung, es waren ja doch nur Bestandteile der Luft, deren Konzentration selbst im Vergleich zum Spurengas Kohlenstoffdioxid sehr gering war. Man spekulierte lieber über das Ausmaß, mit dem Schwefelsäure über eine veränderte Wolkenbildung das Rückstrahlvermögen (also die Albedo) der Erde verändern und hierdurch einen abkühlenden Effekt haben könnte.
In den ersten beiden Berichten an den Club of Rome von 1972 und 1974 wurden als Ursachen für eine globale Erwärmung neben dem anthropogenen Treibhauseffekt auch erstmals die „thermische Umweltverschmutzung“ durch Abwärme diskutiert. Mit deren hypothetischer Fortsetzung bei maximaler Nutzung photovoltaische Energie würde die von einer gerade noch tolerierbaren Erwärmung bestimmte globale Wachstumsgrenze in den kommenden Jahrhunderten erreicht. Bei ausschließlicher Verwendung nicht erneuerbarer Energien mit 2 % jährlicher Zunahme wurde ein anthropogener Abwärme-Beitrag zur globalen Erwärmung von mindestens 3 Grad im Jahr 2300 berechnet, was angesichts der Einfachheit des verwendeten Modells erstaunlich gut mit neueren, aufwändigeren Simulationen übereinstimmt.
Zentraler Meilenstein für die Anerkennung des Klimawandels als „ernstes Problem“ und Durchbruch für die internationale Klimaforschung war die 1. Weltklimakonferenz 1979, die unter anderem auf Initiative von Hermann Flohn (Mitglied der Expertengruppe der WMO) durchgeführt wurde. Ergebnis der Weltklimakonferenz war eine fundamentale Erklärung sowie die Initiierung des Weltklimaforschungs-Programms und des IPCC.
Ereignisse wie die Dürrekatastrophe in der Sahel-Zone steigerten den politischen Druck auf die Entscheidungsträger, die jedoch unschlüssig waren, wie konkret welche Bedrohung denn eigentlich war, denn die Klimaforscher waren ja selbst uneins. So beschloss der Wissenschaftsberater der US-Regierung (ein Geophysiker!), ein Expertengremium zu berufen, das in der Diskussion unvorbelastet war. Unter der Leitung von Jule Gregory Charney wurden Experten befragt, die in die laufende Debatte noch nicht involviert waren. Charneys Gruppe verglich zwei Klimamodelle, das eine vom japanischen Klima- und Meteorologen Syukuro Manabe, das andere vom US-Klimaforscher und späterem zeitweisem NASA-Direktor James E. Hansen. Beide Modelle unterschieden sich in Details, nicht jedoch in der Kernaussage, dass ein Konzentrationsanstieg des Spurengases Kohlenstoffdioxid zweifelsfrei zu einer deutlichen Temperatursteigerung führen würde. Die Experten prüften u. a. anhand einfacher, eindimensionaler Atmosphärenmodelle, ob die bisherigen Modelle einen wesentlichen Effekt vernachlässigt haben konnten – sie fanden aber nichts. Für die bei einer Verdoppelung des Kohlenstoffdioxidgehaltes der Atmosphäre zu erwartende Erwärmung hatte Manabes Modell 2 Grad ergeben, Hansens Modell zeigte eine Erwärmung um 4 Grad. Man einigte sich schließlich als wahrscheinlichsten Wert auf 3 Grad, wohl wissend, dass dies letztlich nur eine Schätzung war.
Im 1979 erschienenen „Report of an Ad hoc Study Group on Carbondioxide and Climate“ mit dem Titel „Carbon Dioxide and Climate, A Scientific Assessment“ des National Research Council war daneben zu lesen, dass eine signifikante Erwärmung aufgrund der thermischen Trägheit der Ozeane erst in einigen Jahrzehnten zu erwarten sei. Der Bericht wurde später kurz Charney-Report genannt und stimmte inhaltlich gut mit einem Gutachten der JASON-Expertengruppe überein, das im selben Jahr erschien.
Schon in den 1960er Jahren hatten Wissenschaftler erkannt, dass die Vorgänge im Klimasystem das Ergebnis einer großen Zahl miteinander in Wechselwirkung stehender Prozesse ist. Es kann nur verstanden werden, wenn auch die gegenseitige Beeinflussung aller beteiligten Komponenten und Prozesse verstanden und in geeigneter Form abgebildet werden kann. Im Jahr 1972 wurde zur Förderung diesbezüglicher Forschung in Wien das Internationale Institut für angewandte Systemanalyse gegründet. In der Folge wurde von der US-amerikanischen NASA im Jahr 1983 das Earth System Sciences Committee gegründet und in Deutschland im Jahr 1992 das Potsdam-Institut für Klimafolgenforschung. Mit der in diesen Instituten betriebenen Erdsystemwissenschaft werden seither die Entwicklung und die Auswirkungen globaler Umweltveränderungen erforscht.
Die Zahl wissenschaftlicher Publikationen zum Klimawandel in den 1980er Jahren war ca. doppelt so hoch wie in den 1970ern. Viele Details zur Klimageschichte kamen zum Vorschein: Beispielsweise wurden die bereits in den 1970er Jahren entdeckten sprunghaften Klimawandel als Heinrich-Ereignisse und Dansgaard-Oeschger-Ereignisse genauer beschrieben. Die Kenntnisse über die Stärke des Treibhauseffekts und der Veränderung der Treibhausgaskonzentration waren nun so gut, dass T. Wigley und Philip D. Jones in einem im Jahr 1981 in der Zeitschrift Nature erschienenen Artikel schrieben: „Wenngleich die Ansicht, dass ein Anstieg der Kohlenstoffdioxid-Konzentration zu einer Erwärmung des Klimas führt, weit verbreitet ist, so ist diese Erwärmung aufgrund des Rauschens im Klimasystem noch nicht feststellbar.“ In ihrer Arbeit führen sie aus, dass diese Erwärmung erst gegen Ende des Jahrhunderts ausgeprägt genug sein wird, dass sie sich klar vom Hintergrundrauschen abhebt.In Deutschland kam es wegen Smog zu ernsten Verkehrseinschränkungen, das Problem einer zunehmenden Luftverschmutzung wurde erkannt. Das im Jahr 1987 in Kraft getretene Helsinki-Protokoll ließ die Schadstoffbelastungen weltweit zurückgehen, wodurch sich der seit den 1940er Jahren beobachtete Abkühlungstrend umkehrte. Aber nicht nur wurde es seit 1974 wieder wärmer, das Jahr 1988 ging sogar als bislang wärmstes Jahr seit Beginn systematischer Wetteraufzeichnungen in die Geschichte ein.
Zu Beginn der 1980er Jahre fanden Forscher heraus, dass der Kohlenstoffzyklus über eine Art Regelmechanismus verfügt, der die Erde die meiste Zeit über in einem Temperaturbereich hielt, der günstig für die Entwicklung des Lebens war. Man wusste schon seit langem, dass das Treibhausgas Kohlenstoffdioxid über vulkanische Aktivität in die Atmosphäre gelangt und über die Verwitterung von Gestein wieder entzogen wird. Der Verwitterungsprozess von Gestein hat zwei wesentliche Eigenschaften. Zum einen ist seine Intensität von der Durchschnittstemperatur der Erde abhängig, und bei höheren Temperaturen wird mehr Kohlenstoff gebunden, und zum zweiten verläuft er sehr langsam. Es war also anzunehmen, dass es Phasen in der Erdgeschichte gegeben haben muss, in denen dieser Prozess versagt hatte, weil er zu träge reagiert. Geologen fanden schließlich in Namibia dazu passende Gesteinsschichten: Die älteren Schichten belegten eine lang andauernde Vereisung, die, wie man wusste, weite Teile des Globus betraf. Da Eisflächen ankommendes Sonnenlicht sehr effizient reflektieren, war eine sehr hohe Treibhausgaskonzentration nötig, um die Erde von dieser Vereisung wieder zu befreien. Und in der Tat zeugten darüber liegende, jüngere Schichten von einer anschließend sehr hohen Verwitterungsrate, wie sie in einem Warmklima zu erwarten war. Während des langen Eiszeitalters war die Verwitterung sehr gering, während Vulkane die Kohlenstoffdioxidkonzentration der Atmosphäre kontinuierlich immer höher schraubten, bis die Eismassen zu schmelzen begannen und damit der kühlende „Spiegel“ der weißen Eisflächen zunehmend kleiner wurde, was über die Eis-Albedo-Rückkopplung den Erwärmungsprozess beschleunigte, bis alles Eis geschmolzen war und die Erde sich in einem Heißklima befand. Dieses extreme Warmklima dauerte für einige zehntausend Jahre an, genau so, wie man es anhand der Dynamik der Gesteinsverwitterung erwartet hatte. Der Grund war, dass die Eismassen erheblich schneller schmolzen als das Kohlenstoffdioxid aus der Atmosphäre verschwinden konnte. Mit der Entdeckung des Regelmechanismus war ein wesentliches Element zur Klärung des sogenannten Paradoxon der schwachen jungen Sonne gefunden. Wissenschaftler hatten lange nach einer Erklärung für die Tatsache gesucht, dass während der gesamten Erdgeschichte flüssiges Wasser existierte, obwohl die Strahlungsleistung der Sonne im Archaikum um etwa 25 % schwächer war als heute und sich erst im Verlauf einiger Milliarden Jahre stetig erhöht hatte.
Forscher nahmen nun auch Fluktuationen der Sonnenaktivität in ihre Berechnungen mit auf und begannen, Details der Landmassen zu berücksichtigen. Beispielsweise parametrierten sie die Geschwindigkeit, mit der Regen auf unterschiedlichen Böden abfließt sowie das geringere Reflexionsvermögen (Albedo) von Wäldern im Vergleich zu Wüsten. Trotz großer Anstrengungen waren Klimamodelle aber auch in den 1980er Jahren in vielerlei Hinsicht mangelhaft. Mehrjährige Simulationsläufe endeten meist in unrealistischen Zuständen; Modellierer wählten mangels Alternativen oftmals Parameter ohne empirische Grundlage aus, nur um solch unmögliche Zustände auszuschließen.
Eines der in den 1980er Jahren ungelösten Probleme war der in den Klimamodellen nicht wiederfindbare geringe Temperaturgegensatz zwischen polaren und äquatornahen Gegenden, der während der Eiszeiten offenbar bestand. Die CLIMAP-Daten passten nicht zu den Modellen, egal wie die Forscher sie zu parametrieren suchten. Ein Vergleich von 14 Klimamodellen zeigte überdies, dass Wolken in den Modellen in keiner Weise adäquat abgebildet waren. Verfügbare Messdaten von Satelliten waren leider auch nicht genau genug, um diesen Mangel anhand von Beobachtungen zu korrigieren.
Als in den 1980er Jahren der kalte Krieg zu eskalieren schien, begannen auch Atmosphärenphysiker, die möglichen Folgen eines global geführten Atomkriegs zu untersuchen. In mehreren unabhängigen Studien wurde sobald eindringlich auf die Gefahr eines nuklearen Winters hingewiesen, der vom massiven Aerosoleintrag herrühren würde, den die Explosion einer größeren Zahl von Atombomben verursachen würde. Da durch solch ein Ereignis der Fortbestand der Menschheit bedroht würde, erlangte die Diskussion darum öffentliche Aufmerksamkeit. Das Thema wurde auch in den Medien verarbeitet und war unter anderem Gegenstand des erfolgreichen Fernsehfilms The Day After – Der Tag danach, der anschließend weltweit in vielen Kinos lief.
Schon Alfred Wegener entnahm dem grönländischen Eis in den 1930er Jahren Eisbohrkerne, um daraus wertvolle Informationen über die Klimavergangenheit zu gewinnen. Fortschritte in der physikalischen und chemischen Analytik ermöglichten es den Forschern in den Folgejahren, den Proben mehr und mehr Informationen zu entlocken. Nach jahrelangen vergeblichen Bemühungen war man Anfang 1980 endlich so weit, aus winzigen, im Eis gespeicherten Luftbläschen auch die Kohlenstoffdioxidkonzentration vergangener Tage zuverlässig zu rekonstruieren. Was man fand, war eine Sensation: Zum Höhepunkt der letzten Eiszeit vor 20.000 Jahren war die Kohlenstoffdioxidkonzentration nur halb so groß gewesen wie in der Warmzeit des 20. Jahrhunderts. Damit war erstmals belegt, was John Tyndall, Svante Arrhenius und Thomas Chamberlin 80 Jahre zuvor vermutet hatten, aber zu Lebzeiten nicht beweisen konnten: Ein drastisches Absinken der atmosphärischen Kohlenstoffdioxidkonzentration war wesentlich für das Entstehen der Eiszeiten. Weitere Gewissheit brachte eine Bohrung in der Antarktis, bei der ein Bohrkern die Rekonstruktion der letzten 150.000 Jahre erlaubte. Er zeigte den Verlauf der Kohlenstoffdioxidkonzentration im Verlauf eines gesamten Eiszeitzyklus: warm – kalt – warm. Die Kohlenstoffdioxidkonzentration der Atmosphäre verlief verblüffend synchron zum Temperaturverlauf, sie war niedrig während der Eiszeit, hoch während der Warmphase.
Eisbohrkerne zeigten nicht nur ein Auf und Ab der CO2-Konzentration, sondern fast genau parallel dazu auch ein Auf und Ab der Methankonzentration. Sie war hoch, wenn es warm war und niedrig, wenn es kalt war. Isotopenuntersuchungen zeigten, dass Lebewesen die Quellen dieses Methans waren. Auf der Suche nach möglichen Kandidaten fand man viele in Frage kommende Quellen: Reisfelder, Bakterien in den Mägen von Wiederkäuern, im Boden von Mooren und Sümpfen. Lebewesen hatten offensichtlich einen signifikanten Einfluss auf die Entwicklung des globalen Klimas.
Die Konzentration dieses Treibhausgases war zwar deutlich geringer als die des CO2 und hatte auch nur eine mittlere Verweildauer von 12 Jahren in der Atmosphäre, jedoch ist die Wirkung von Methan als Treibhausgas über einen Zeitraum von 20 Jahren gesehen 72-mal so groß wie die von CO2. Die atmosphärische Methankonzentration stieg in den 1980er Jahren um 1 % pro Jahr. Und sie war bereits seit dem späten 16. Jahrhundert am Steigen.
Der Ozeanograph Veerabhadran Ramanathan gehörte zu der Gruppe derer, die Mitte der 1970er Jahre vor wenig beachteten Treibhausgasen sehr geringer Konzentration gewarnt hatte. Im Jahr 1981 schrieb Ramanathan, dass allein die sehr starke Treibhauswirkung der FCKW die Erde bis zum Jahr 2000 um ein ganzes Grad erwärmen könne, wenn die Emissionen dieses Gases so weiter liefen wie bisher; im Jahr 1985 publizierte er in einer aufsehenerregenden Arbeit, dass nicht weniger als 30 Spurengase als Treibhausgase wirken und der Mensch die Konzentration einer Reihe dieser Gase bereits deutlich erhöht habe und weiter erhöhe. Zusammengenommen hätten die Gase nahezu dasselbe Treibhauspotential wie Kohlenstoffdioxid, das bislang alleine im Fokus der Betrachtungen stand.Es traf sich, dass im Jahr seiner Veröffentlichung das Ozonloch über der Antarktis entdeckt wurde. Atmosphärenchemiker hatten mit ihren Warnungen zur Bedrohung der Ozonschicht also recht gehabt. Und auch für fachfremde Politiker war nun ersichtlich, wie groß der Einfluss von Spurengasen geringster Konzentration auf die Atmosphäre sein kann. War die globale Erwärmung durch Kohlenstoffdioxid allein schon eine Bedrohung, war nun klar, dass das Problem im Kern deutlich größer war. Internationales Handeln war gefragt. Zwei Jahre darauf, im Jahr 1987, wurde im Rahmen des Montreal-Protokolls beschlossen, die Herstellung von FCKWs zu verbieten, und im darauffolgenden Jahr 1988 erfolgte die Gründung des Intergovernmental Panel on Climate Change, abgekürzt IPCC.
Nachdem sich in den 1980er Jahren die seit vielen Jahren erwartete Erwärmung in den Aufzeichnungen der globalen Temperaturdaten abzuzeichnen begann, stellten sich die Wissenschaftler die Frage, welche Auswirkungen des menschengemachten Klimawandels noch akzeptabel seien und wo die Grenze zu einem gefährlichen Klimawandel zu sehen sei. Dabei plädierte die Deutsche Physikalische Gesellschaft im Dezember 1985 und gemeinsam mit der Deutschen Meteorologischen Gesellschaft im Jahr 1987 für die Einhaltung eines Ein-Grad-Ziels. Bei Überschreitung einer globalen Erwärmung um ein Grad im Vergleich zum Durchschnittswert, wie er vor dem menschlichen Eingriff in das Weltklima bestand hatte, sei mit schwerwiegenden negativen Konsequenzen zu rechnen.
Im November des Jahres 1988 wurde vom Umweltprogramm der Vereinten Nationen (UNEP) und der Weltorganisation für Meteorologie (WMO) der Zwischenstaatliche Ausschuss über den Klimawandel (Intergovernmental Panel on Climate Change, IPCC) eingerichtet. Das IPCC wurde unter der Führung der konservativen Reagan-Administration mit der Aufgabe gegründet, Berichte und Empfehlungen aller weltweit im Bereich Klimatologie führenden Wissenschaftler zusammenzufassen, wobei für jeden Bericht der Konsens der beteiligten Regierungen zwingend erforderlich war.
Die Zahl wissenschaftlicher Publikationen zum Klimawandel verdoppelte sich erneut in den 1990er Jahren. Gab es 1990 nur 40 Konferenzen, bei denen Papers zur globalen Erwärmung präsentiert wurden, so waren es im Jahr 1997 bereits über 100. Der Wissenszuwachs war entsprechend groß.Im Jahr 1992 wurde das World Radiation Monitoring Center an der Eidgenössischen Technischen Hochschule Zürich (ETH Zürich) gegründet und in der Folge weiter ausgebaut. Daraus entstand ein weltweites Netzwerk von mehr als 50 Bodenstationen, deren Messergebnisse nahezu in Echtzeit abrufbar sind und die die Auswertung aller relevanten Strahlungskomponenten ermöglichen, darunter Global-, Reflex- und Direktstrahlung, darüber hinaus auch terrestrische Komponenten wie die atmosphärische Gegenstrahlung. Dadurch wurde es möglich, Veränderungen des Treibhauseffekts beziehungsweise des Strahlungsantriebs (Radiative Forcing) im Rahmen des Global Climate Observing Systems (GCOS) messtechnisch präzise zu untersuchen, zu belegen und zu archivieren.
Einem russisch-französischen Forscherteam der ostantarktischen Wostok-Station gelang Ende der 1990er Jahre die Bergung eines Eisbohrkerns in der neuen Rekordlänge von über 3000 Metern. Dieser zeigte im Klimaverlauf der letzten 420.000 Jahre vier vollständige Eiszeitzyklen mit einer Dauer von jeweils 100.000 Jahren. Mittels verbesserter Analysemethoden konnte die schon in Grönland überraschend gute Übereinstimmung mit den Milanković-Zyklen und der parallele Anstieg und Abfall der Kohlenstoffdioxid- und Methan-Konzentrationen nachvollzogen werden. Bei genauer Analyse bestätigte sich eine schon Jahre zuvor geäußerte Annahme: Die Zunahme der Kohlenstoffdioxid-Konzentration fand immer nach dem Temperaturanstieg statt. Während frühere Ergebnisse einen Zeitversatz von 600 bis 800 Jahren nahelegten, deuten neuere Arbeiten darauf hin, dass zwischen der Erwärmung und dem Anstieg der CO2-Konzentration kein oder nur ein geringer Zeitversatz von wenigen Jahren oder Jahrzehnten auftrat.Die Erwärmung verlief dabei nicht synchron; es gab zwischen der nördlichen und südlichen Hemisphäre einen signifikanten zeitlichen Unterschied, wobei die Erwärmung der Südhalbkugel vor der Erwärmung der Nordhalbkugel einsetzte.Die Auswertung des Eisbohrkerns belegte erneut den Stellenwert der Treibhausgase wie auch der übrigen Rückkopplungsmechanismen: Die durch die Milanković-Zyklen ausgelöste, geringfügige Veränderung der Strahlungsbilanz der Erde wurde durch eine Konzentrationsveränderung der atmosphärischen Treibhausgas-Konzentration verstärkt. Zusammen mit der Eis-Albedo-Rückkopplung, der Wasserdampf-Rückkopplung und anderen, schwächeren Rückkopplungsgliedern ergab sich ein derart großer Effekt, dass dies zum Kommen und Gehen von Eiszeiten geführt hatte. Zwar blieb unklar, ob das freigesetzte Kohlenstoffdioxid aus den Weltmeeren, aus dem Permafrost, aus Methanhydraten oder aus anderen Quellen stammte. Sicher war nur, dass der Konzentrationsanstieg dieser Gase eine Folge dieser geringen Erwärmung war und sie weiter verstärkt hatte.
War im Verlauf der Quartären Eiszeit die Treibhausgas-Konzentration oftmals als Reaktion auf die Erwärmungstendenz der Milanković-Zyklen angestiegen, sind gegenwärtig menschliche (anthropogene) Emissionen dafür verantwortlich, dass die Treibhausgas-Konzentration der eigentlichen Temperaturzunahme vorausläuft. Der Effekt ist freilich in beiden Fällen derselbe: eine zunehmende Erwärmung, gekoppelt mit einer weiteren Freisetzung von Treibhausgasen, wie dies zum Beispiel in auftauenden Permafrostgebieten der Fall sein könnte. Ähnliches gilt für das in vielen ozeanischen Bereichen vorhandene Methanhydrat, das in fester Form auf Schelfsockeln und in der Tiefsee lagert und umfangreiche Mengen Methan in der Größenordnung von etwa 10 Billionen Tonnen bindet.
So könnte in den kommenden Jahrzehnten eine vermehrte Freisetzung von Methan aus Methanhydraten oder dem Permafrostboden ein deutliches Warnsignal für eine sich selbst verstärkende Erwärmungsspirale sein.
Im ersten Sachstandsbericht des IPCC, der 1990 veröffentlicht wurde, war zu lesen, dass man sich sicher ist, dass es einen natürlichen Treibhauseffekt gibt und dass der Mensch die Konzentration einiger Treibhausgase erhöhe, was zu einer globalen Temperaturerhöhung führen wird. Bislang gäbe es jedoch nur wenige empirische Belege für einen vom Menschen verursachten Klimawandel (“little observational evidence”).
Im sechs Jahre später veröffentlichten zweiten Sachstandsbericht wurde unter Vorsitz von Benjamin D. Santer erstmals festgestellt: Die Abwägung der Datenlage deutet darauf hin, dass der Mensch einen merklichen Einfluss auf das globale Klima des 20. Jahrhunderts hat (“‘the balance of evidence’ suggested there had been a ‘discernible’ human influence on the climate of the 20th century”).
Schon seit den 1950er Jahren wurde die Wirkungen von Aerosolen auf das Klima diskutiert: der Treibhauseffekt aufgrund der Wechselwirkung mit Infrarotstrahlung und die Streuung und Absorption des Sonnenlichts als direkte Wirkungen sowie die indirekte Wirkung als Kondensationskeime für Wasserdampf, wodurch auch eigentlich dunkle Aerosole kühlend wirken könnten – das Vorzeichen des Gesamteffekts war auch nach den 1990er Jahren noch unsicher.
Anders sah die Situation bei den hellen Sulfataerosolen aus. James E. Hansen hatte Daten von Vulkanausbrüchen des Mount Agung im Jahre 1963 und des El Chichón im Jahre 1982 benutzt, um die kühlende Wirkung von Vulkanausbrüchen zu quantifizieren. Schon seit den 1960ern war daher klar, dass Sulfataerosole eine kühlende Wirkung auf das Klima haben, was anhand von Eisbohrkernen auch gut für weit zurückliegende Ausbrüche nachvollzogen werden konnte.
Der Ausbruch des Pinatubo im Jahr 1991 sollte sich für Klimatologen als Glücksfall erweisen. Nun konnten sie nachprüfen, ob ihre Annahmen zur Wirkung von Sulfaten stimmten, denn der Vulkan stieß knapp 20 Millionen Tonnen Schwefeldioxid aus, eine Sulfatwolke, so groß wie der US-amerikanische Bundesstaat Iowa. Hansens Gruppe sagte eine Abkühlung um ein halbes Grad voraus, das sich in erster Linie über höheren nördlichen Breiten darstellen und ein paar Jahre andauern sollte. Genau dies wurde beobachtet.
Klimamodelle wurden in den 1990er Jahren mit dem Wissen über die kühlenden Sulfatemissionen von Vulkanausbrüchen parametriert. Damit konnte ein Widerspruch gelöst werden: Sollte die Klimasensitivität, d. h. die erwartete Erwärmung bei Verdoppelung der Konzentration von Kohlenstoffdioxid, tatsächlich im Bereich von drei Grad liegen, hätte sich dies in den 1960er und 1970er Jahren im Verlauf der globalen Durchschnittstemperatur zeigen müssen, was aber nicht beobachtet werden konnte. Nachdem der kühlende Effekt von Schwefeldioxid Bestandteil der Modelle geworden war, war auch der Temperaturverlauf des 20. Jahrhunderts gut darstellbar.Auch das Problem des zu großen Temperaturgegensatzes zwischen polaren und äquatorialen Breiten konnte in diesem Jahrzehnt gelöst werden: Untersuchungen an Eisbohrkernen ergaben, dass Rekonstruktionen der CLIMAP-Studie, die auf nahezu unveränderte Temperaturen in äquatornahen Breiten während der Eiszeiten hingedeutet hatten, wahrscheinlich nicht korrekt waren.
Während in den Jahrzehnten zuvor einige Parameter in Klimamodellen ohne physikalische Grundlage gewählt werden mussten, um zu verhindern, dass das Modell unrealistische Zustände einnahm, hatten Klimamodelle in den 1990er Jahren offenbar eine Qualität erreicht, die so gut war, dass sie nicht mehr dazu gebracht werden konnten, fehlerhafte Messdaten durch Wahl geeigneter Parameter nachzubilden.
Als international geltende Grenze für einen gerade noch akzeptablen Klimawandel wurde in den späten 1990er Jahren dann das Zwei-Grad-Ziel formuliert. Es wurde möglicherweise vom Wissenschaftlichen Beirat der Bundesregierung Globale Umweltveränderungen (WGBU) erstmals vorgeschlagen. Der WBGU befürwortete die Grenze 1995 in einem Gutachten. Das Zwei-Grad-Ziel wurde von der Politik übernommen und in den Fokus der europäischen Klimaschutzpolitik gestellt. Es basiert auf der Annahme, dass bei Überschreiten einer globalen Erwärmung um zwei Grad Kipppunkte (tipping points) erreicht würden, die unumkehrbare und in ihren Konsequenzen kaum einschätzbare negative Folgen nach sich zögen.
Das erste Jahrzehnt des 21. Jahrhunderts war das wärmste seit Beginn systematischer Temperaturaufzeichnungen. Betrachtet man die einzelnen Jahre, so waren 2005 und 2010 die wärmsten Jahre seit Beginn der Messungen.In der Klimatologie war seit längerer Zeit bekannt, dass über 90 % der vom Menschen über den Anstieg der Treibhausgaskonzentration in das Klimasystem eingebrachten Wärme nicht in der Atmosphäre, sondern in den Weltmeeren landet. Leider waren ozeanische Temperaturdaten nur sehr lückenhaft vorhanden, und insbesondere von der Tiefsee gab es nur wenige Daten, so dass über eine möglicherweise stattfindende Erwärmung der Ozeane keine verlässliche Aussage möglich war.
Mit dem im Jahr 2000 begonnenen Argo-Projekt wurde genau dieses Problem adressiert. Mit Hilfe einer Flotte von automatisierten Tauch- beziehungsweise Treibbojen war man nun in der Lage, die Temperaturentwicklung und den Wärmeinhalt der Ozeane in bisher nicht möglicher Qualität und Quantität zu erfassen. Im November 2002 konnte das einmillionste Datenprofil im Rahmen des Argo-Projekts übertragen werden. Damit wurde die Anzahl der im gesamten 20. Jahrhundert von Forschungsschiffen geleisteten ozeanographischen Messungen bereits zu einem frühen Projekt-Zeitpunkt um das Doppelte übertroffen. Gegenwärtig sind mit steigender Tendenz etwa 3.950 Argo-Bojen in allen Weltmeeren im Einsatz (Stand: September 2018).
Im 2001 erschienenen dritten Sachstandsbericht des IPCC konnte der Einfluss des Menschen auf das Klima nicht nur mit größerer Sicherheit nachgewiesen werden, man war darüber hinaus aufgrund der verbesserten Datenlage nun schon in der Lage, das Ausmaß des menschlichen Einflusses auf den Klimawandel zu quantifizieren. Während im zweiten Sachstandsbericht von einem merklichen Einfluss des Menschen auf das Klima berichtet wurde, schrieb man nun von deutlichen Hinweisen (engl. strong evidence) dass der Mensch das Klima der Erde verändert. Zur Visualisierung, wie groß der Einfluss des Menschen ist, enthielt der Bericht eine Temperaturrekonstruktion von Michael E. Mann, die unter dem Namen Hockeyschläger-Diagramm große Bekanntheit erlangte.
Bereits seit den 1970er Jahren war bekannt, dass das Klima der Erde oftmals chaotisch reagiert: Kleine Veränderungen können große Effekte nach sich ziehen; dies war bei abrupten Klimawechseln in der Vergangenheit häufig geschehen. In den 2000er Jahren wies Hans Joachim Schellnhuber darauf hin, dass es überdies im Klimasystem und in Ökosystemen eine Reihe von Elementen gibt, die zu schwer bis gar nicht umkehrbaren Veränderungen neigen; das heißt, dass sie selbst dann in ihrem neuen Zustand verharren, wenn der die Veränderung auslösende Effekt wieder verschwunden ist. Dieses Verhalten ist in der Systemtheorie als Hysterese bekannt. Seit ihrer erstmaligen Erwähnung in der wissenschaftlichen Literatur wurde eine ganze Reihe von Kippelementen im Erdsystem gefunden, darunter der Eisschild Grönlands oder der Brasilianische Regenwald.
Durch Vergleich von Satellitendaten, die im Jahr 1970 aufgezeichnet wurden, mit Messungen aus dem Jahr 1997 konnte in einer 2001 erschienenen Publikation erstmals messtechnisch belegt werden, dass sich das Emissionsspektrum der Erde verändert hatte. In den Spektren war der verstärkte Treibhauseffekt durch die seit 1970 deutlich erhöhte Treibhausgaskonzentration klar erkennbar. Daneben konnte im Rahmen einer anderen Studie auch gezeigt werden, dass sich der durch die Erhöhung der Konzentration der atmosphärischen Konzentration des Treibhausgases Kohlenstoffdioxid der Strahlungsantrieb so stark erhöht hatte, dass man dies im Rahmen einer 8-jährigen Datenreihe auch messtechnisch nachweisen konnte.
Das Ergebnis dieser Studie wurde im Rahmen einer anderen Arbeit, bei der Daten über einen Zeitraum von 10 Jahren ausgewertet wurden, im Jahr 2015 bestätigt.Im Jahr 2003 trat eine weitere Vorhersage ein: Der britische Meteorologe Ernest Gold hatte im Jahr 1908 publiziert, dass zu erwarten sei, dass die Tropopause mit wachsender CO2-Konzentration durch den dadurch verstärkten Treibhauseffekt höher steigt. Auch dies konnte nun gemessen werden.Bereits seit Jahrzehnten hatten Klimaforscher angenommen, dass eine wärmere Welt zu einer Freisetzung von Kohlenstoffdioxid und Methan aus Permafrost führen würde. Wie man in den 1990er Jahren durch Analyse von Bohrkerndaten herausfand, war dies in der Erdgeschichte sogar regelmäßig geschehen. Schon in den 2000er Jahren trat die Befürchtung ein: In den Sommermonaten konnte in den großen Permafrostgebieten von Sibirien und Alaska ein großer Konzentrationsanstieg dieser Gase beobachtet werden. Zu den Emissionen von Treibhausgasen, die vom Menschen stammen, gesellten sich also nun auch Emissionen aus fossilen Kohlenstoffquellen, die durch die menschengemachte Erwärmung aus der Erde ausgasen.
Im Jahr 2002 erregte der Kollaps des antarktischen Eisschildes Larsen B internationale Aufmerksamkeit; im Jahr 2008 brach das Wilkins-Eisschild auf; es waren dies die Indikatoren, die John Mercer im Jahr 1978 als Zeichen für einen sich anbahnenden Kollaps des westantarktischen Eisschildes gesehen hatte.
Im Jahr 2008 befand die stratigraphische Kommission der Geological Society of London, dass es mittlerweile genügend Argumente dafür gibt, dass der Mensch einen neuen stratigraphischen Abschnitt eingeleitet habe. Artensterben, Überfischung, Versauerung der Meere, globale Erwärmung und andere vom Menschen ausgelöste Prozesse haben die Erde bereits so stark beeinflusst, dass damit ein klares und nachhaltiges biostratigraphisches Signal erzeugt wurde und wird. Der Begriff Anthropozän (aus altgriechisch ἄνθρωπος ánthrōpos ‚Mensch‘) wurde gewählt, da der Mensch der die Erde vorrangig prägende Faktor geworden ist. Die Entscheidung über die Implementierung des Anthropozäns in das stratigraphische System liegt bei der International Commission on Stratigraphy (ICS), in deren Working Group on the ’Anthropocene’ die verschiedenen Aspekte des Vorschlags derzeit eingehend diskutiert werden.
Im vierten Sachstandsbericht des IPCC von 2007 wird als hauptsächliche Ursache der Erderwärmung mit einer angegebenen Wahrscheinlichkeit von über 90 % „sehr wahrscheinlich“ die vom Menschen verursachten Emissionen von Treibhausgasen angegeben. Ebenfalls 2007 erhält das IPCC zusammen mit dem ehemaligen US-Vizepräsidenten Al Gore den Friedensnobelpreis. Im Jahr 2009 erschien mit der „Copenhagen Diagnosis“ ein Update nach Erscheinen des AR4 von 2007. Die Autoren schrieben, dass einige im letzten IPCC-Report angegebene Entwicklungen in ihrem Ausmaß unterschätzt wurden. So lag die arktische Meereisbedeckung im Erscheinungsjahr des AR4 (2007) um 40 % niedriger als die Computermodelle vorausgesagt hatten. Der Meeresspiegelanstieg der letzten 15 Jahre lag um 80 % über den Vorhersagen des IPCC. Dementsprechend wurden die Vorhersagen bezüglich des zukünftigen Meeresspiegelanstiegs bis zum Jahr 2100 nach oben korrigiert: inzwischen wurde ein doppelt so hoher Anstieg erwartet.Ebenfalls im Jahr 2009 wurde mit der Fachzeitschrift Nature Climate Change eine neue Plattform ins Leben gerufen, auf der Wissenschaftler ihre Erkenntnisse zu den Prozessen und Folgen des Klimawandels publizieren können.
Das Jahr 2011 war nicht nur das Jahr mit dem höchsten je gemessenen Kohlenstoffdioxidgehalt der Atmosphäre, es war auch das Jahr der weltweit größten je gemessenen Kohlenstoffdioxid-Emissionen, wobei im Vergleich zum Vorjahr ein Anstieg um 3 % zu beobachten war. Aufgrund der stattgefundenen Investitionen in Kohlenstoffdioxid emittierende Energieträger schien ein 80%iger Anstieg der Emissionsrate von 2010 bis zum Jahr 2020 nahezu sicher.Die raschen Fortschritte bei der radiometrischen Datierung und der Entwicklung biogeochemischer Nachweisverfahren im Rahmen der Paläoklimatologie führten zu einer erheblichen Zunahme der Messgenauigkeit und damit zu einer teilweisen Neubewertung geologischer, geophysikalischer und biologischer Ereignisse. Mithilfe moderner Datierungsmethoden wurde es möglich, die Klimaschwankungen der erdgeschichtlichen Vergangenheit zeitlich genauer einzugrenzen, zunehmend detaillierter zu rekonstruieren und ihren Verlauf beziehungsweise Umfang mit der gegenwärtigen Erwärmung zu vergleichen. Diese und ähnliche Untersuchungen trugen in entscheidendem Maße dazu bei, dass sich das in der wissenschaftlichen Literatur dokumentierte Grundlagenwissen ständig erweitert. Laut einer Übersichtsstudie von 2016 wurden im Zeitraum von 1980 bis 2014 über 220.000 peer-reviewte Arbeiten zum Themenbereich Klimatologie veröffentlicht.Durch Analyse von Satellitenmessungen, die mittlerweile einen Zeitraum von mehr als 30 Jahren abdecken, war es im Jahr 2013 möglich, den menschlichen Einfluss auf den fortschreitenden Klimawandel auf eine weitere Weise klar zu belegen. So zeigten die Messdaten eine Abkühlung der Stratosphäre bei gleichzeitiger Erwärmung der Troposphäre. Dieser Effekt tritt nur dann auf, wenn die Erwärmung durch einen Anstieg der Treibhausgaskonzentrationen verursacht wird, da eine erhöhte Sonnenaktivität auch die Stratosphäre aufgeheizt hätte.
Im fünften Sachstandsbericht des IPCC (September 2013) wurden die Aussagen der vorherigen Klimaberichte bestätigt und Unsicherheiten in Bezug auf den Einfluss des Menschen auf das Klima verringert. So schreiben die Experten nun, dass es extrem wahrscheinlich sei, dass der Mensch der Hauptgrund für die beobachtete globale Erwärmung seit 1950 ist.
Im Jahr 2014 wurde in mehreren unabhängigen Publikationen festgestellt, dass das Abschmelzen des westantarktischen Eisschilds höchstwahrscheinlich bereits seinen Kipppunkt überschritten hat, d. h. der Eisschild inzwischen so instabil ist, dass das weitere Abschmelzen nicht mehr aufzuhalten ist (vgl. Kippelemente im Erdsystem). Eine Eisfläche von der Größe Frankreichs wird sehr wahrscheinlich in den kommenden 100 bis 300 Jahren zerfallen und in der Folge den Meeresspiegel im globalen Mittel um einen Meter steigen lassen. Diese Befunde bestätigten die Vorhersagen von John Mercer aus dem Jahr 1978.
Im Gebiet der Antarktischen Halbinsel brach am 12. Juli 2017 vom Larsen C-Schelfeis ein Stück von etwa 5.800 km² ab und verkleinerte so dessen Fläche um etwa 12 %. Die Masse des Eisbergs beträgt rund eine Billion Tonnen; er zählt zu den größten jemals beobachteten Eisbergen. Mit dem Abbruch droht die Destabilisierung und Auflösung von Larsen C.Hingegen zeichnet sich für die Ostantarktis keine klare Tendenz ab. Ein international besetztes Expertenteam, bestehend aus etwa 80 Erdsystem- und Geowissenschaftlern, publizierte im Juni 2018 die bisher umfangreichste Untersuchung zu diesem Thema mit dem Ergebnis, dass der Ostantarktische Eisschild in seinen Kernbereichen derzeit stabil ist und im Unterschied zu anderen antarktischen Regionen keinen signifikanten Masseverlust aufweist.
Die Jahre 2014, 2015 und 2016 waren die global wärmsten Jahre seit Beginn regelmäßiger Klimaaufzeichnungen. Es war das erste Mal, dass in drei aufeinanderfolgenden Jahren globale Temperaturrekorde zu verzeichnen waren.
Das von der Deutschen Physikalischen Gesellschaft und der Deutschen Meteorologischen Gesellschaft Ende der 1980er Jahre als Grenze für einen gefährlichen Klimawandel angesetzte 1-Grad-Ziel wurde damit erreicht. Gleichzeitig überschritt auch die atmosphärische Konzentration des Treibhausgases Kohlenstoffdioxid die Marke von 400 ppm. Durch diese Erwärmung waren extreme Wärmeanomalien, die im Zeitraum 1950–1981 nur mit einer Wahrscheinlichkeit von 0,13 % auftraten – sogenannte 3 Sigma-Ereignisse – nun pro Sommer statt vormals auf weit weniger als einem Prozent der Erdoberfläche auf 10 % der Fläche anzutreffen. Im Jahr 2017 wurde in einer Studie festgestellt, dass die den IPCC-Berichten zugrundeliegenden Klimamodelle die bis zum Ende des Jahrhunderts zu erwartende Erwärmung sehr wahrscheinlich unterschätzen. Bei ungebremsten Emissionen liegt die zu erwartende Erwärmung demnach um etwa 0,5 Kelvin höher als bislang angenommen.
In der Wissenschaft besteht weitgehend Einigkeit darüber, dass der gegenwärtig zu beobachtende Klimawandel im vorhergesagten weiteren Verlauf rascher vonstatten gehen wird als alle bekannten Erwärmungsphasen der letzten 50 Millionen Jahre. Selbst während des Paläozän/Eozän-Temperaturmaximums – ein extrem ausgeprägtes Warmklima innerhalb eines geologisch sehr kurzen Zeitraums – hatte der atmosphärische Kohlenstoffeintrag und die damit gekoppelte Temperaturzunahme im jährlichen Durchschnitt erheblich geringere Steigerungsraten als gegenwärtig. Im Unterschied zu früheren Annahmen wird sich der zusätzliche CO2-Eintrag auch bei einem weitgehenden Emissionsstopp nur allmählich verringern und in signifikantem Umfang noch in mehreren tausend Jahren nachweisbar sein. Darauf aufbauend postulieren einige Studien unter Einbeziehung der Erdsystem-Klimasensitivität eine längere Warmzeit im Bereich von 50.000 bis 100.000 Jahren. Als zusätzliche Gefährdungspotenziale wurden verschiedene Kippelemente im Erdsystem identifiziert, die bei weiterer Erwärmungszunahme kurzfristige und irreversible Prozesse einleiten würden. Derartige Entwicklungen würden das Bild der Erde gravierend verändern, vor allem durch die damit gekoppelte Verlagerung der Klima- und Vegetationszonen und das weitgehende Abschmelzen der antarktischen und grönländischen Eisschilde mit entsprechendem Anstieg des Meeresspiegels um mehrere Dutzend Meter.Dagegen sind die natürlichen Regelmechanismen der Kohlenstoffdioxid-Bindung wie Sedimentation oder geochemische Verwitterungsprozesse (CaCO3-Verwitterung) zu langsam, um innerhalb überschaubarer Zeitabschnitte eine nachhaltige CO2-Reduzierung zu bewirken. So benötigt ein kompletter Austausch des atmosphärischen Kohlenstoffdioxids auf der Basis des Carbonat-Silicat-Zyklus ungefähr 500.000 Jahre. Zwar sind die Ozeane als effektive Kohlenstoffsenke bekannt, doch wird mittelfristig nur ein relativ geringer Teil des CO2 in Tiefsee-Sedimenten gespeichert. Zudem könnten erhebliche Mengen an CO2 (zusammen mit Methan) bei Zunahme der Meerwassertemperaturen wieder ausgegast werden. Die relativ träge Reaktion des anorganischen Kohlenstoffzyklus auf eine rasche Zunahme von Treibhausgasen war bereits Svante Arrhenius bekannt. Obwohl gegen Ende des 19. Jahrhunderts die damaligen Emissionsraten noch eine vergleichsweise geringe Relevanz besaßen, erwähnte Arrhenius in seinem Werk Über den Einfluss von Kohlensäure in der Luft auf die Bodentemperatur (1896) explizit die lange Verweildauer von Kohlenstoff in der Atmosphäre und in den Ozeanen. Die Rolle von Verwitterungsprozessen als wichtiger Einflussfaktor im Klimasystem bildete in der Fachliteratur lange Zeit ein Nischenthema und wurde erst ab den 1980er Jahren auf breiterer Basis behandelt.Ein wesentlicher Aspekt der gegenwärtigen globalen Erwärmung ist ihre Auswirkung auf das nächste Glazial-Ereignis innerhalb des Känozoischen Eiszeitalters. Der seit dem Klimaoptimum des Holozäns herrschende Abkühlungstrend von ≈0,12 °C pro Jahrtausend gilt als Vorbote und erstes Anzeichen eines nahenden Eiszeitklimas. Die in jüngster Zeit veröffentlichten Studien, die auf einer genauen Analyse vergangener Eiszeitphasen unter Einbeziehung der Milanković-Zyklen beruhen, kommen zu dem Resultat, dass eine Kälteperiode bereits durch geringfügige Schwankungen im Erdklimasystem und hier vor allem durch die allmählichen Veränderungen der Erdbahnparameter verursacht wird. Demnach würde die nächste Eiszeit unter normalen Rahmenbedingungen (also unter Ausschluss der anthropogenen Emissionen) erst in einigen zehntausend Jahren beginnen. Dieser für ein Interglazial wie das Holozän ungewöhnlich lange Zeitraum wird sich mit hoher Wahrscheinlichkeit bei einem atmosphärischen CO2-Ausgangswert von über 500 ppm auf insgesamt 100.000 Jahre ausdehnen und damit nahezu verdoppeln. Das bedeutet den Ausfall eines kompletten Eiszeitzyklus aufgrund menschlicher Eingriffe in das Klimasystem.
Nachdem erkannt worden war, dass das Klimaproblem in Zusammenhang mit menschlichen Verhaltensweisen und Entscheidungen innerhalb sozialer Systeme stand, wurden seit den 1970er Jahren auch sozialwissenschaftliche Aspekte des Klimawandels untersucht. Zu den ersten Klimatologen, die sich für interdisziplinäre Forschung aussprachen (also die Einbeziehung der Sozialwissenschaften bei der Erforschung des Klimawandels) und Workshops zu dem Thema organisierten, gehörte Stephen H. Schneider. Er wies 1983 darauf hin, dass die Basis des CO2-Problems (der steigenden Emissionen) ein sozialwissenschaftliches Thema sei. So hänge das Ausmaß zukünftiger CO2-Emissionen maßgeblich vom menschlichen Verhalten ab, und zwar u. a. in Bezug auf Bevölkerungszahl (Fortpflanzungsverhalten), Pro-Kopf-Konsum fossiler Energien oder Entwaldung und Wiederaufforstung. Neben den sozialwissenschaftlichen Analysen der Ursachen der globalen Erwärmung wurden bereits früh die gesellschaftlichen Reaktionen auf den anthropogenen Klimawandel diskutiert, wie zum Beispiel die Risikowahrnehmung, die Entscheidungsfindung oder auch die Adaptation an klimatische Veränderungen. Die wirtschaftlichen Aspekte des Klimawandels fasste neben anderen Nicholas Stern 2006 zusammen (Stern-Report). Zudem wurde angesichts der mangelnden Klimaschutzaktivitäten trotz zunehmender Sicherheit der wissenschaftlichen Erkenntnisse zum Klimawandel verstärkt nach den Ursachen für das Phänomen der „Inaktivität“ geforscht. Neben individuellen Faktoren wurde – im Kontext der Kontroverse um die globale Erwärmung – u. a. von Naomi Oreskes näher untersucht, wie vor allem durch wirtschaftlich motivierte Interessengruppen gezielt Zweifel an den wissenschaftlichen Erkenntnissen verbreitet wird (vgl. Leugnung der menschengemachten globalen Erwärmung) und wie sich dies wiederum auf politische Entscheidungen auswirkt.
Tobias Krüger: Die Entdeckung der Eiszeiten – Internationale Rezeption und Konsequenzen für das Verständnis der Klimageschichte. Schwabe-Verlag, Basel, 2008, ISBN 978-3-7965-2439-4.
Mark David Handel, James S. Risbey: Reflections on more than a century of climate change research. In: Climatic Change. Band 21, Nr. 2, 1. Juni 1992, ISSN 1573-1480, S. 91–96, doi:10.1007/BF00140913 (springer.com). 
Mark D. Handel, James S. Risbey: An annotated bibliography on the greenhouse effect and climate change. In: Climatic Change 21 (2), 1992, S. 97–255, doi:10.1007/BF00140914.
James R. Fleming: Historical Perspectives on Climate Change. Oxford, New York, Oxford University Press, 1998, ISBN 0-19-518973-6.
G. Stanhill: The growth of climate change science: A scientometric study. In: Climatic Change 48 (2–3), 2001, S. 515–524, doi:10.1023/A:1010721600896.
Spencer R. Weart: The Discovery of Global Warming. Harvard University Press, 2003, 2008, ISBN 978-0-674-03189-0.
H. Le Treut, R. Somerville, U. Cubasch, Y. Ding, C. Mauritzen, A. Mokssit, T. Peterson, M. Prather: Historical Overview of Climate Change Science. In: Climate Change 2007: The Physical Science Basis. Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change [Solomon, S., D. Qin, M. Manning, Z. Chen, M. Marquis, K.B. Averyt, M. Tignor and H.L. Miller (eds.)]. Cambridge University Press, Cambridge, Vereinigtes Königreich und New York, NY, USA, 2007.
Bert Bolin: A History of the Science and Politics of Climate Change. Cambridge University Press, 2007, ISBN 978-0-521-88082-4.
Naomi Oreskes, Erik M. Conway: Merchants of Doubt: How a Handful of Scientists Obscured the Truth on Issues from Tobacco Smoke to Global Warming. Bloomsbury Press, 2010, ISBN 978-1-59691-610-4.
Die Machiavellis der Wissenschaft. Das Netzwerk des Leugnens. Wiley-VCH, Weinheim 2014, ISBN 978-3-527-41211-2.
Mike Hulme: The Discovery of Climate Change. In: M. Hulme: Why We Disagree About Climate Change. Cambridge University Press, 2009, ISBN 978-0-521-89869-0.
J. Donald Hughes: Climate Change: A History of Environmental Knowledge. In: Capitalism Nature Socialism 21 (3), 2010, S. 75–80, doi:10.1080/10455752.2010.508619.
Matthias Heymann: Klimakonstruktionen: Von der klassischen Klimatologie zur Klimaforschung. In: NTM Zeitschrift für Geschichte der Wissenschaften, Technik und Medizin 17 (2), 2009, S. 171–197, doi:10.1007/s00048-009-0336-3
Matthias Heymann: The evolution of climate ideas and knowledge. In: Wiley Interdisciplinary Reviews: Climate Change 1 (4), 2010, S. 581–597, doi:10.1002/wcc.61.
Jinfeng Lia, Ming-Huang Wanga, Yuh-Shan Hob: Trends in research on global climate change: A Science Citation Index Expanded-based analysis. In: Global and Planetary Change 77 (1–2), 2011, S. 13–20, doi:10.1016/j.gloplacha.2011.02.005.
Mark Carey: Climate and history: a critical review of historical climatology and climate change historiography. In: Wiley Interdisciplinary Reviews: Climate Change 3 (3), 2012, doi:10.1002/wcc.171.
Spencer R. Weart: Rise of interdisciplinary research on climate. In: PNAS 110, Suppl. 1, 2013, S. 3657–3664, doi:10.1073/pnas.1107482109.
G. Thomas Farmer, John Cook: Climate Change Science: A Modern Synthesis. Volume 1 – The Physical Climate. Springer Science+Business Media, 2013, ISBN 978-94-007-5756-1, doi:10.1007/978-94-007-5757-8.
American Institute of Physics: The Discovery of Global Warming. Webseite zum gleichnamigen Buch von Spencer Weart (englisch)
Tyndall Lecture: GC43I. Successful Predictions, Vortrag von Ray Pierrehumbert im Rahmen des AGU Fall Meetings 2012 der American Geophysical Union
Video zur Erklärung des Zeitversatzes zwischen dem Anstieg der Temperaturen und dem Anstieg der Treibhausgaskonzentrationen am Ende der Eiszeiten: Climate Change: The “800 year time lag” unravelled (englisch)

Fort Sumter ist ein Fort auf einer künstlichen Insel an der Einfahrt vom Atlantischen Ozean in die Bucht von Charleston in South Carolina. Seine historische Bedeutung erlangte das Fort als Schauplatz der ersten militärischen Auseinandersetzung des Amerikanischen Bürgerkrieges, die am 12. April 1861 um 4:30 Uhr begann.
Das Bauwerk wurde 1829 begonnen und war 1861 fast fertiggestellt. Es ist nach Thomas Sumter (1734–1832), einem General des Amerikanischen Unabhängigkeitskrieges, benannt. Seit dem Ende der militärischen Nutzung 1948 ist das Fort eine Gedenkstätte vom Typ eines National Monuments und wird vom National Park Service verwaltet.
Nach dem Ende des Britisch-Amerikanischen Krieges von 1812 bis 1814 wurde an der amerikanischen Atlantikküste und einigen anderen Standorten unter dem Namen Third System eine Kette von Küstenbefestigungen errichtet.
Charleston war die bedeutendste Stadt South Carolinas und nach Savannah der zweitwichtigste Hafen an der südlichen Atlantikküste. Die Stadt liegt auf einer Halbinsel zwischen den Mündungen der drei Flüsse Ashley River, Cooper River und Wundo River in der nach der Stadt benannten Bucht. Diese ist durch einen Kanal zwischen zwei breiten, flachen Inseln Sullivan’s Island im Nordosten und Morris Island im Südwesten mit dem Ozean verbunden. Der Kanal war zu breit, um mit der damaligen Artillerie von den Inseln aus die Einfahrt in den Hafen bestreichen und damit sperren zu können. Deshalb wurde eine Untiefe im Kanal künstlich erhöht und als Fundament für eine der Befestigungen genutzt. Dort entstand Fort Sumter. Ihr gegenüber, auf Sullivan's Island, wurde Fort Moultrie angelegt. Die Bauarbeiten für beide Forts wurden 1829 begonnen. Zum Zeitpunkt der Zerstörung im Jahr 1861 waren beide fast vollendet, Fort Moultrie war bereits von Truppen unter Major Robert Anderson bezogen.
Zur Errichtung der künstlichen Insel wurden über 70.000 Tonnen Granit aus Neuengland importiert. Die äußere Form des Forts beschreibt ein unregelmäßiges Fünfeck mit Seitenlängen zwischen 51 und 58 Metern. Die Backsteinmauern waren 16 m hoch, die ursprüngliche Mauerstärke wird mit 5 Fuß (1,6 Meter) angegeben. Die Festung war darauf ausgelegt, bis zu 650 Menschen und 135 Kanonen auf drei Stockwerken zu beherbergen. Alle fünf Seiten waren mit zwei Stockwerken Kasematten ausgestattet, die mit Kanonen bestückt wurden. Auf den drei dem Land zugewandten Seiten waren die Unterkünfte, Lager, Werkstätten und sonstigen Räume in dreistöckigen Ziegelbauten untergebracht, die konstruktiv von den Kasematten getrennt waren. Im Zentrum lag der offene Paradeplatz.Während der Bauphase wurde das Fort im September 1858 kurzzeitig genutzt, um 300 schwarze Sklaven unterzubringen. Sie waren auf dem Schiff Echo aufgegriffen worden, das die in Cabinda an der Mündung des Kongo gekauften Sklaven in die Vereinigten Staaten bringen sollte. Der Handel mit afrikanischen Sklaven war in den USA seit 1808 illegal, nur noch in den Vereinigten Staaten lebende und geborene Schwarze durften als Sklaven gehalten werden. Der illegale Handel wurde als Piraterie verfolgt, Kapitän Townsend von der Echo wurde der Prozess gemacht, die Sklaven wurden in die damalige US-Kolonie Liberia deportiert und dort freigelassen.
Nach der Wahl von Abraham Lincoln zum US-Präsidenten, im November 1860, verließen viele sklavenhaltende Staaten des Südens die USA (die „Union“) und gründeten einen eigenen Gesamtstaat, die Konföderierten Staaten von Amerika. Allerdings hatten die Nordstaaten (die Union) immer noch Besitz im Süden, darunter Forts der U.S. Army. Eine wichtige Position war für den Norden der Tiefwasserhafen von Charleston in South Carolina, der sehr bedeutend für den Außenhandel war.
South Carolina verließ die Union am 20. Dezember 1860. Sechs Tage später zog Unionsmajor Robert Anderson auf eigene Initiative die Unionstruppen im Bereich von Charleston zusammen, und weil Fort Moultrie nicht gegen einen Angriff von der Landseite zu verteidigen war, verlegte er sie heimlich in das noch nicht fertiggestellte Fort Sumter.Monatelang forderte die Konföderation von den Unionstruppen im Fort die Kapitulation und versuchte es auszuhungern. Gleichzeitig baute die Confederate States Army eine Artilleriestellung auf der südwestlichen Insel auf, um im Kriegsfall Fort Sumter von dort beschießen zu können. Im Januar 1861 wurde ein Versorgungsschiff der Union für Fort Sumter von der neuen Artilleriestellung an der Küste beschossen und musste abdrehen. Am 1. Februar konnten alle Frauen und Kinder das Fort verlassen und in die Nordstaaten ausreisen. Es blieben 86 Mann: 10 Offiziere und 76 Unteroffiziere und Mannschaften. Die Lebensmittelvorräte der belagerten Unionstruppen reichten bis zum 15. April, Brennstoffe gingen schon vorher aus. Am 4. März trat Lincoln sein Amt als Präsident an. Er versuchte zunächst zu verhandeln und die Konföderation nicht durch militärische Unterstützungsaktionen zugunsten des belagerten Forts zu provozieren. Als bekannt wurde, dass die europäischen Staaten Großbritannien, Frankreich, Spanien und Russland erwogen, die Konföderation anzuerkennen, schlug die Stimmung um, und der Konflikt wurde unausweichlich.
Im März organisierte die Union den Entsatz, also die Befreiung ihrer Truppen in Fort Sumter und im ebenfalls belagerten Fort Pickens in Florida. Anfang April sollte die Aktion stattfinden. Wegen widersprüchlicher Befehle fuhr das für Fort Sumter vorgesehene Kriegsschiff nach Florida, die beiden Schlepper kamen ebenfalls nie vor Charleston an, so dass drei unbewaffnete Frachtschiffe in der Nacht des 11. auf den 12. April alleine eintrafen und ohne Deckung nicht zum Fort vorstoßen konnten.Das Eintreffen dieser Versorgungsschiffe war der Anlass für die konföderierte Truppen unter General Pierre Gustave Toutant Beauregard, am 12. April 1861 um 4:30 Uhr von der passiven Belagerung zu Kampfhandlungen überzugehen. Sie eröffneten das Feuer auf Fort Sumter. Beim Bombardement aus den umliegenden Hafenbefestigungen (Fort Moultrie, dem alten Fort Johnson und der neu errichteten Stellung am Cummings Point) kamen Artilleriegeschütze und Mörser zum Einsatz. Die Kampfhandlungen dauerten 34 Stunden. Am 13. April 1861 um 14:00 Uhr ergab sich die Besatzung des Forts unter Major Anderson, nachdem ein Brand im Offiziersquartier außer Kontrolle geraten war und Gefahr bestand, dass in der Folge das Pulvermagazin explodierte. Während der gesamten Auseinandersetzung wurden auf keiner Seite Menschen getötet.Während eines 100-Schuss-Saluts für die Unionsfahne, eine der Kapitulationsbedingungen von Major Anderson, wurde der Unionssoldat Private Daniel Hough getötet und einige weitere Unionssoldaten teils schwer verletzt, als eine der Kanonen während des Nachladens frühzeitig feuerte. Private Edward Galloway, einer der Verletzten, starb wenig später in einem Hospital in Charleston. Diese beiden Männer gelten als erste Todesopfer des Sezessionskrieges. Nach der Kapitulation wurde das Fort von konföderierten Truppen besetzt und notdürftig instand gesetzt.
Im April 1863 versuchten die Unionstruppen einen Angriff auf Charleston und beschossen Fort Sumter schwer. Sie verhängten eine Seeblockade gegen die Stadt und besetzten Folly Island westlich von Morris Island. Im Juli griffen sie zweimal das improvisierte konföderierte Fort Wagner auf Morris Island an, konnten es aber nicht einnehmen. Mitte August 1863 griffen sie Fort Wagner und Fort Sumter erneut an und zerstörten beide systematisch durch Artilleriebeschuss. Am 7. September gaben die Konföderierten Fort Wagner auf. Eine Landungsoperation der Unionstruppen gegen Fort Sumter am 9. September wurde von General Beauregard abgewehrt. Erst gegen Ende des Krieges evakuierte die Armee der Südstaaten Charleston und Fort Sumter. Am 17. Februar 1865 wurde die Insel von Unionstruppen unter General William Tecumseh Sherman formell in Besitz genommen. Man schätzt, dass während des Krieges Geschosse mit einem Gewicht von insgesamt sieben Millionen Pound (etwa 3175 Tonnen) auf Fort Sumter abgefeuert wurden.
Nach dem Krieg war Fort Sumter eine Ruine. Anfängliche Bestrebungen einer Wiederherstellung kamen bald zum Erliegen, und das Fort wurde nur teilweise instand gesetzt. Die Außenmauern wurden lediglich bis zum ersten Stockwerk aufgebaut, die Schießscharten der Kasematten nicht wieder geöffnet. Stattdessen wurde auf der Oberfläche Raum für Geschütze geschaffen, diese wurden aber nicht mehr bestückt. Als einzige Nutzung der Insel blieb der seit 1855 bestehende Leuchtturm.Erst unter dem Eindruck des Spanisch-amerikanischen Krieges vom 25. April bis 12. August 1898 wurde eine neuerliche militärische Nutzung beschlossen. Im selben Jahr wurde mit dem Bau einer massiven Betonkonstruktion namens Battery Huger als Fundament für schwere Geschütze in den historischen Ruinen des Forts begonnen, die zwar in beiden Weltkriegen stets mit Truppen und Artillerie bemannt waren, jedoch nie Schauplatz einer militärischen Auseinandersetzung wurden.
Seit 1948 ist Fort Sumter eine Gedenkstätte vom Typ eines National Monuments unter der Verwaltung des amerikanischen National Park Service. Die Betonkonstruktion Battery Huger steht noch heute inmitten des Areals und beherbergt ein Besucherzentrum mit Museum. Während des Beschusses von 1861 wurde der Fahnenmast des Forts getroffen. Soldaten richteten ihn und die Flagge der Vereinigten Staaten mit den damals 33 Sternen unter schwerem Beschuss wieder auf. Die Flagge ist erhalten und wird im Museum des Forts ausgestellt.
Fort Sumter kann ausschließlich per Boot von Charleston aus besichtigt werden. Zum National Monument gehören auch ein Besucherzentrum in Charleston auf dem Festland, sowie auf Sullivans Island das Fort Moultrie und seit 2008 das Sullivan's Island Lighthouse, der dienstjüngste Leuchtturm der Vereinigten Staaten.
Stephen R. Wise: Gate of hell: campaign for Charleston Harbor 1863. University of South Carolina Press, 1994, ISBN 0-87249-985-5
James M. Ferguson: An Overview of the Events at Fort Sumter 1829–1991. National Park Service, 1991 (auch im Volltext online; PDF; 5,6 MB)
Tulane-Universität: Crisis at Fort Sumter – Darstellung der Abläufe um Fort Sumter im Sezessionskrieg

Fort Union National Monument ist eine Gedenkstätte vom Typ eines National Monuments im Nordosten des US-Bundesstaats New Mexico. Sie bewahrt einen historischen Militärstützpunkt, der 1851 angelegt wurde, um das im Mexikanisch-Amerikanischen Krieg 1846/48 eroberte Territorium New Mexico zu kontrollieren und die Handelszüge auf dem Santa Fe Trail vor Indianer-Überfällen zu schützen.
Unter schwierigen Bedingungen mit einfachsten Mitteln errichtet, wurde das Fort mehrfach ausgebaut, bevor im Amerikanischen Bürgerkrieg eine neue befestigte Stellung rund eineinhalb Kilometer entfernt bezogen wurde. Diese wurde nach dem Bürgerkrieg wieder aus- und umgebaut und spielte eine wichtige Rolle in den Indianerkriegen der 1870er Jahre. Das Fort wurde 1891 aufgegeben, als die Eisenbahn den Standort überflüssig machte und die Bedrohung durch die Indianer Vergangenheit war.
Der Santa Fe Trail war der wichtigste Handelsweg von den besiedelten Regionen der Vereinigten Staaten am Missouri River durch die Steppen und Wüsten des späteren Kansas und Colorado nach Santa Fe, der Hauptstadt der mexikanischen Provinz Nuevo Mexico. Der Handel hatte erst 1822 nach der Unabhängigkeit Mexikos von Spanien begonnen und erfuhr bis in die 1840er Jahre einen bedeutenden Aufschwung. Über Konflikte zwischen der seit 1836 unabhängigen Republik Texas, Mexiko und den USA begann 1846 der Mexikanisch-Amerikanische Krieg, den Mexiko 1848 verlor. Mexiko musste im folgenden Vertrag von Guadalupe Hidalgo die Territorien der heutigen US-Bundesstaaten Kalifornien, Arizona, Nevada, Utah, Teile von Colorado und Wyoming und auch New Mexico an die Vereinigten Staaten abtreten. 
New Mexico gehörte zum Ninth Military Department (ab 1853 dann New Mexico Military Department), dem seit dem Krieg 10 % der gesamten Mannstärke der US-Armee zugeordnet waren. Nach dem Krieg hatte die US-Armee zunächst elf kleine Stützpunkte durch den ganzen Südwesten verteilt, die sich 1850/51 als unpraktisch herausstellten; einzeln waren sie zu schwach gegen die Apachen und Komantschen, für koordinierte Aktionen lagen sie zu weit auseinander. In einer abgelegenen Region mit extremem Klima, galten die Posten nicht als attraktiv. Das hatte negative Folgen für die Disziplin und Schlagkraft der Truppenteile.
Daraufhin errichtete die Armee unter der Koordination von Lieutenant Colonel Edwin Vose Sumner zwei neue Forts an den Knotenpunkten des Santa Fe Trails: 1850 Fort Atkinson (später Fort Dodge) am nordöstlichen Verzweigungspunkt und 1851 Fort Union, wo sich die beiden alternativen Wege wieder vereinten. 
Als Vorposten an der Grenze der Zivilisation waren das Military Department und sein wichtigstes Fort weitgehend selbständig. Da Befehle aus Washington monatelang unterwegs gewesen wären, mussten die Kommandeure Entscheidungen selbst treffen. Fort Union war ein frühes Kommando in der Karriere einiger Offiziere, die später hohe Positionen erreichten. Darunter waren James Henry Carleton, Brevet-Brigadegeneral und Autor von militärischen Fachbüchern, in Fort Union 1852, William T. H. Brooks, Generalmajor, in Fort Union 1852, George Bibb Crittenden, Generalmajor, in Fort Union 1860–61, John R. Brooke, Generalmajor, in Fort Union 1867–68, John Irvin Gregg, Brevet-Brigadegeneral, in Fort Union 1870–73.
Fort Union liegt auf etwa 2000 m über dem Meer an den Hängen der bis zu 3000 m hohen Berge der Sangre-de-Cristo-Kette, rund 150 km südlich des Raton Passes und etwa 175 km nordöstlich von Santa Fe. Den Standort hatte Sumner persönlich ausgesucht, der die Gegend aus dem Mexikanisch-Amerikanischen Krieg kannte.
In Fort Union waren von Anfang an Regimenter der Infanterie und der Dragoner stationiert, ab 1852 kam eine Batterie leichter Artillerie hinzu, ab 1856 auch mounted rifleman (berittene Infanterie). Erster Kommandeur war Captain Edmond B. Alexander vom 3. Infanterie-Regiment.
Das erste Fort von 1851 war eine offene Konstruktion aus einzeln stehenden Holzhütten, es wurde mehrfach erweitert und war 1861 mit maximal 1669 Soldaten der größte Stützpunkt westlich des Mississippi Rivers und wegen der Gehälter der Soldaten und lukrativen Versorgungsverträgen ein wichtiger Wirtschaftsfaktor für die Region. Anfangs war das Fort nur als temporäre Einrichtung geplant und wurde durch die Soldaten selbst mit einfachsten Mitteln aus Blockhäusern erbaut. Die Bauten waren bald in schlechtem Zustand, ein von Sumner befohlener Versuch der Selbstversorgung durch eine von den Soldaten betriebene Farm scheiterte unter den klimatischen Bedingungen New Mexicos.
Trotzdem erfüllte Fort Union seine Aufgabe. Die Truppen organisierten Patrouillen auf dem Santa Fe Trail und eskortierten einzelne Planwagenzüge. Händler und Autor William Davis gab im Jahr 1857 seinen erfreuten Eindruck des Forts wieder, als er nach den menschenleeren Steppen den Stützpunkt erreichte, der für die Sicherheit seines Geschäftes verantwortlich war.
Fort Union, 175 Kilometer vor Santa Fe, liegt im lieblichen Tal des Moro Bachs. Es ist ein offener Posten, ohne jede Palisaden oder Brustwehr und würde man nicht die Offiziere und Soldaten sehen, würde es eher wie ein ruhiges Grenzdorf erscheinen, denn wie eine Militärstation. Es ist mit breiten, geraden Straßen angelegt, die sich im rechten Winkel kreuzen. Die Hütten sind aus Kiefernholz gebaut, das in den benachbarten Bergen geschlagen wurde und die Unterkünfte sowohl der Offiziere, wie der Mannschaften machen einen gepflegten und angenehmen Eindruck.
Nach Beginn des Amerikanischen Bürgerkriegs 1861 wurde der alte Standort in ein Depot umgewandelt und eineinhalb Kilometer entfernt eine gegen leichten Artilleriebeschuss gesicherte Festung mit großen sternförmigen Erdwällen nach dem Tenaillensystem errichtet. Dabei wurde an Fort Union das zentrale Depot an militärischen Versorgungsgütern (Waffen, Munition, Verpflegung) für den Südwesten angelegt. Bis zum Ende des Krieges waren zusätzlich Freiwilligen-Einheiten und Milizen in Fort Union stationiert. Sie kamen überwiegend aus New Mexico und dem benachbarten Colorado, gegen Ende des Krieges sogar aus Kalifornien. 
1862, nach der Niederlage der Unionsarmee in der Schlacht von Valverde, war das Fort das einzige Hindernis zwischen den konföderierten Truppen und den kriegswichtigen Goldfeldern in Colorado. Ironischerweise war Henry Hopkins Sibley, der Befehlshaber der konföderierten Invasionsarmee, bevor er sich der Konföderation anschloss, 1861 selbst zeitweilig Befehlshaber des Forts gewesen. Die Garnison des Forts wurde durch Milizen aus Colorado unter Oberst John Potts Slough verstärkt. Slough marschierte mit dem Gros seiner Truppen auf die Konföderierten zu und schlug deren Vorhut rund 100 km südlich des Forts in der Schlacht am Glorieta Pass. Die Konföderierten zogen sich daraufhin zurück, der New-Mexico-Feldzug war gescheitert. 
Der Unterhalt der Erdwälle wurde im Frieden bald als zu aufwändig angesehen. Das dritte Fort wurde gleich neben den Wällen mit einfachen, landestypischen Wänden aus Adobe-Ziegeln erbaut. Es wurde mehrfach erweitert, ab 1867 etwa um ein Militärkrankenhaus. Baukosten und die Stationierung der Truppen brachten Aufträge für die lokale Bevölkerung und trugen wesentlich zum wirtschaftlichen Aufschwung der Region bei.
Gab es schon vor dem Bürgerkrieg einzelne Kampagnen gegen die Indianervölker in unmittelbarer Nähe des Forts, gingen von Fort Union in der Spätphase des Sezessionskriegs und danach mehrere Feldzüge im Rahmen der Indianerkriege aus. Dazu wurde Kit Carson im Jahr 1866 kurzzeitig als Brevet-Brigadegeneral in Fort Union verpflichtet, um seine Kenntnisse vom Land und dessen Bewohnern der Armee zur Verfügung zu stellen. Carson war einer der berühmtesten Trapper und Scouts (Kundschafter), ausgezeichnet für seine Rolle im Mexikanisch-Amerikanischen Krieg und im Bürgerkrieg und er hatte Erfahrung in militärischen Kampagnen gegen Indianer, seit er 1864 den Feldzug gegen die Navaho geführt hatte, der zu einer als „Langer Marsch“ bekannt gewordenen Zwangsumsiedlung führte. In Fort Union führte er eine Kampagne gegen die Mescalero-Apachen. 
Die Cheyenne, Arapaho, Kiowa und Komantschen wurden 1867 zwangsweise aus ihren traditionellen Jagdgebieten in Indianerreservaten im Indianer-Territorium, dem heutigen Oklahoma umgesiedelt. Anlass waren einige Überfälle auf weiße Siedler, Farmen, Händler und nicht zuletzt auf die Eisenbahn, die immer weiter in die Prärien von Kansas vorrückte. Die Völker hielten sich nur teilweise an die ihnen aufgezwungenen Verträge, sie konnten oder wollten ihre Jagdgebiete und die einzige Lebensweise, die sie kannten, nicht aufgeben.
Die Armee reagierte mit kleineren Kampagnen im ganzen Südwesten und gegen fast alle Völker der Region. Ab 1871 wurden die Soldaten von Fort Union auch eingesetzt, um illegalen Handel zwischen Bewohnern New-Mexikos, überwiegend indianischer Abstammung aus den Pueblo-Völkern, und den Indianern der Prärie, vor allem den Komantschen, zu verhindern. Dabei sollten einerseits die Prärie-Indianer vor dem für sie verbotenen Alkohol geschützt werden, andererseits lag es im Interesse der US-Regierung, die Völker in Abhängigkeit vom Bureau of Indian Affairs und seinen Agenturen zu halten.Die Konflikte eskalierten im Sommer 1874: Nach einigen Überfällen der Kiowa und widersprüchlichen Berichten über Angriffe einer Gruppe, die für Südliche Cheyenne gehalten wurde, auf Siedler in New Mexico und Texas, griff die Armee ein. Auch Truppen aus Fort Union beteiligten sich an einem Feldzug gegen die Komantschen, Arapaho, Kiowa und Südlichen Cheyenne am Red River zwischen Texas und dem Indianer-Territorium im heutigen Oklahoma, der als Red-River-Krieg eine der größten Militäraktionen gegen die Amerikanischen Ureinwohner war. In den folgenden Jahren gab es immer wieder Berichte über Indianer auf dem Weg zu Überfällen, aber die Armee konnte keine Spuren finden. Es gibt Hinweise, dass die Gefahr von Indianerüberfällen durch die Siedler systematisch übertrieben wurde, um Truppen in die Region zu holen, mit deren Versorgung die Siedler gute Geschäfte machen konnten.
Auch gegen Gesetzlose unter der weißen Bevölkerung wurde die Armee gerufen. Nach kleineren Goldfunden kam es 1869 in Cimarron, etwa 60 km nördlich des Forts, zu Konflikten zwischen den Ute, Siedlern, die sich das Land bisher friedlich mit den Indianern teilten, und einem Unternehmen namens Maxwell Land Grant and Railway Company, das mit Geldern britischer und niederländischer Kapitalgeber und beachtlicher politischer Rückendeckung den kompletten Boden für sich reklamierte. Ein Pfarrer, der die Siedler unterstützte, wurde von Revolverhelden ermordet, ein Constable, der im Verdacht stand, in den ersten Mord verwickelt zu sein, gefoltert und ermordet. Die Justiz war machtlos, da alle Seiten Rückendeckung bekamen. Die Indianer waren völlig schutzlos. Offiziere aus Fort Union versuchten zu ermitteln, erreichten aber nichts, als ihnen der einzige Verdächtige nach einem Teilgeständnis auf dem Weg zwischen Gericht und Gefängnis von einem vermeintlich spontanen Mob entrissen und gelyncht wurde.Die Bahnlinie der Atchison, Topeka and Santa Fe Railway erreichte 1879 Fort Union, in der Folge wurde das Depot überflüssig und geschlossen. Zu diesem Zeitpunkt waren Fort Union und die anderen Forts, die gegen die Indianer angelegt   worden waren, bereits umstritten. Es dauerte jedoch noch bis zum 21. Februar 1891, bis der Standort völlig aufgelassen wurde. Zufällig am selben Tag wurde in New York City General William T. Sherman begraben, Held des Bürgerkriegs und später als Oberbefehlshaber der US-Armee für die Indianerkriege verantwortlich. Die Zeit des „Wilden Westens“ war vorüber: Die Besiedelung der amerikanischen Prärien soweit fortgeschritten, dass die frontier, die Grenze der Zivilisation, nicht mehr existierte.
Nach dem Zweiten Weltkrieg nahm das Interesse an der Siedlungsgeschichte im Westen der USA zu und 1955 schenkte die Union Land and Grazing Company den Boden des Forts der Bundesregierung, zur Errichtung eines National Monuments, das im folgenden Jahr eröffnet wurde. Es gehört heute nach Fläche und Besucherzahl zu den kleinen Schutzgebieten des National Park Service und besteht aus einer Besucherinformation und einem Rundweg durch die Ruinen des dritten Forts mit einigen historischen Exponaten, wie Planwagen und Kanonen. Im Gebiet sind auch noch nach über 150 Jahren an mehreren Stellen Ruts genannte Wegspuren der großen Planwagen des Santa Fe Trails zu erkennen.
Die Ruinen stehen in einer Kurzgras-Prärie mit einem auffallend kleinräumigen Mosaik aus verschiedenen Pflanzengesellschaften. Die Vielfalt verdankt das flächenmäßig kleine Schutzgebiet der Abwesenheit von Weidenutzung über mittlerweile fünf Jahrzehnte. Benachbarte Flächen, die von einer Rinderfarm beweidet werden, sind wesentlich ärmer an Arten und Pflanzengesellschaften. Insgesamt wurden 142 Pflanzenarten und sechzehn Pflanzengesellschaften nachgewiesen, außerdem 33 Arten von Reptilien und Amphibien sowie 16 Säugetierarten (Spitzmäuse und Fledermäuse konnten mit den verwendeten Methoden nicht erfasst werden, sind aber im Gebiet vorhanden). Typisch für die Strukturen sind Baumwollschwanzkaninchen, Silberdachs, Kojote und Gabelbock.
David Dary: The Santa Fe Trail – Its History, Legends, and Lore. Alfred A. Knopf, New York 2001, ISBN 0-375-40361-2.
Leo E. Oliva: Fort Union and the Frontier Army in the Southwest – A Historic Resource Study. Division of History – National Park Service, Santa Fe, New Mexico 1993 (auch online: Fort Union and the Frontier Army)

Das Forum Romanum in Rom ist das älteste römische Forum und war Mittelpunkt des politischen, wirtschaftlichen, kulturellen und religiösen Lebens. Es liegt in einer Senke zwischen den drei Stadthügeln Kapitol, Palatin und Esquilin und war der Ort vieler öffentlicher Gebäude und Denkmäler.
Ursprünglich ein von einem Bach durchzogenes, sumpfiges Tal, wurde es erst unter dem legendären etruskischen König Lucius Tarquinius Priscus zu Beginn des 7. Jahrhunderts v. Chr. in die Stadt einbezogen. Den Höhepunkt seines prachtvollen Ausbaus erlebte es in der Römischen Kaiserzeit. Es ist heute eine der wichtigsten Ausgrabungsstätten des antiken Roms.
An der Stelle des späteren Forum Romanum befand sich bis zum 7. Jahrhundert v. Chr. eine sumpfige Ebene, die sich zwischen Palatin und Kapitol erstreckte und von den frühen latinischen Siedlern als Begräbnisstätte verwendet wurde. Erst durch Anlage der Cloaca Maxima konnten dieser Sumpf und seine Verlängerung Richtung Tiber, das Velabrum, ausgetrocknet werden.
Die römische Geschichtsschreibung bringt die Initiative zum Bau der Cloaca Maxima mit Lucius Tarquinius Priscus um 600 v. Chr., die Vollendung mit Lucius Tarquinius Superbus, dem letzten römischen König, in Verbindung. Archäologisch nachweisbar ist das Ende der Bestattungen im Forumstal um 600 v. Chr. Zugleich entstand eine erste Pflasterung, die von der Nutzung des neuen Platzes zeugt. Um 600 v. Chr. entstand auch das Gebäude der Regia, obwohl traditionell als Haus oder Regierungssitz des zweiten römischen Königs Numa Pompilius bezeichnet und in der schriftlichen Überlieferung als eines der ältesten Gebäude Roms verankert. Mit Numa ist auch der Bau des Vestatempels verbunden. Archäologisch nachweisbar wurde die Regia nach einem Brand am Ende des 6. Jahrhunderts v. Chr. mit neuem Grundriss wiederhergestellt. Zusammen mit dem Tempel der Vesta bezeichnete sie ab dem 6. Jahrhundert v. Chr. die östliche Grenze des neuen, Forum genannten Platzes.
Nördlich dieses Platzes wurde bald ein zweiter angelegt, das durch die Rostra vom Forum abgegrenzte Comitium. Hier wurde der größte Teil der römischen Politik gemacht, da der Senatssitz, die Curia, und die Rostra, die öffentliche Rednertribüne, direkt daneben lagen. Um 490 v. Chr. wurden zwei Tempel im Tal erbaut, die den Göttern Saturn und Castor gewidmet waren. Sie bildeten zusammen mit einer Ladenzeile zwischen den Tempeln, den tabernae veteres, die südliche Begrenzung des als Marktplatz dienenden Forums. Um die Mitte des 5. Jahrhunderts v. Chr. zeugt die Anbringung des Zwölftafelgesetzes an den Rostra davon, dass sich das Forum schnell zum Zentrum der jungen Stadt entwickelte. Doch lässt sich diese Bedeutung archäologisch nicht fassen.
An der westlichen Schmalseite des Forums, am Fuße des Kapitols, wurde 367 v. Chr. ein erster der Concordia, der Eintracht, geweihter Tempel errichtet. Die Gründung soll auf Marcus Furius Camillus zurückgehen und das Ende der Ständekämpfe zwischen den Patriziern und den Plebejern symbolisieren. Hierfür das Forum als Standort gewählt zu haben, belegt dessen zunehmende Bedeutung in politischer Hinsicht. Es beginnt die Politisierung des Forums. Im Jahr 338 v. Chr. ließ Gaius Maenius die Schiffsschnäbel aus der Schlacht von Antium an der Rednertribüne anbringen. Zuvor wurde derlei Siegesbeute in die Tempel und Heiligtümer geweiht. Gaius Maenius wurde im selben Jahr mit einer Ehrensäule und der Aufstellung eines Reiterstandbildes geehrt. Auch Lucius Furius Camillus zu Ehren, Amtskollege des Maenius im Konsulat des Jahres, wurde ein Reiterstandbild auf dem Forum aufgestellt, mit dem Leistung und Sieg im Latinerkrieg gewürdigt wurden. Eine weitere Reiterstatue folgte im Jahr 306 v. Chr. für Quintus Marcius Tremulus vor dem Dioskurenempel. Gaius Duilius wurde 260 v. Chr. mit einer columna rostrata geehrt. An ihr wurden die von ihm erbeuteten Schiffsschnäbel (rostra) angebracht. Schließlich wurden auch die Gesandten Roms, die während ihrer Gesandtschaft ums Leben gekommen waren, mit Statuen auf oder bei der Rednertribüne geehrt. 
Neben diesen Ehrungen für Zeitgenossen traten solche für Personen der Vergangenheit, deren Vorbildhaftigkeit für die Zeit ihrer Aufstellung jeweils herausgekehrt wurde: für den Augur Attus Navius, der auf wundersame Weise die Ficus Ruminalis auf das Comitium versetzt hatte; für Horatius Cocles, für Hermodoros von Ephesos, der bei Erstellung des Zwölftafelgesetzes entscheidende Hilfe leistete; für Pythagoras und Alkibiades. Die zunehmende Konkurrenz innerhalb der römischen Nobilität führte schließlich zu einer derartigen Häufung von Ehrenstatuen auf dem Forum, dass im Jahr 158 v. Chr. alle Statuen, die nicht auf Volks- oder Senatsbeschluss aufgestellt worden waren, abgeräumt wurden. Dennoch wurden insbesondere im 1. Jahrhundert v. Chr. vermehrt Statuen, oft exzeptionell in der Ausführung, doch meist aus nichtigem Anlass gestiftet: für Sulla, Pompeius und Caesar, für Octavian, Lucius Antonius, Marcus Aemilius Lepidus und Marcus Antonius. Im politischen Kampf der ausgehenden Republik wurden solche Ehrenstatuen nun jedoch beständig umgestürzt oder zerstört, wiederaufgerichtet, eingeschmolzen oder in Sicherheit gebracht.
All dies wirkte sich auch auf die bauliche Gestaltung des Forums aus. Mit dem Ende der Punischen Kriege, mit der Ausweitung des faktischen Herrschaftsbereiches auf das östliche Mittelmeer und den aus den Kriegsbeuten gewonnenen finanziellen Mitteln setzte im 2. Jahrhundert v. Chr. nicht nur in Rom allgemein, sondern speziell auf dem Forum eine rege Bautätigkeit ein. Tempel wie die der Concordia und der Dioskuren wurden von Grund auf erneuert, vier Basiliken entstanden, darunter die 179 v. Chr. gebaute Basilica Aemilia und die 169 v. Chr. errichtete Basilica Sempronia. Beide gaben dem Forumsplatz einen festen Rahmen an Nord- und Südseite und ersetzten die tabernae veteres des alten Marktplatzes. Zugleich unterstrich der Bau des Macellum die Verlagerung der ursprünglich wirtschaftlichen Nutzung des Forums und seine politische Aufwertung. Hierfür wurde die Funktion des Comitium auf das Forum verlagert, die Nutzung der Rostra gedreht, so dass Redner nun zum Forum sprachen. Als neue Kulisse, vor der sich das Treiben auf dem Forum abspielte, wurde unter Sulla 83–80 v. Chr. das Tabularium erbaut. Noch hinter allen westlichen Forumsbauten gelegen, bildete es den optischen Rahmen und die Abgrenzung zum Kapitolshügel, an dessen Ostflanke es sich erhob.
Gaius Iulius Caesar ließ das Forum neu pflastern. In diesem Zusammenhang wurde unter der freien Platzanlage ein unterirdisches Gangsystem eingerichtet, das durch senkrechte Schächte und zugehörige Aussparungen im Pflaster zu erreichen war. Dieses Gangsystem diente wahrscheinlich einem weiteren Aspekt des Forum Romanum: Es war Veranstaltungsort für Gladiatorenkämpfe, bis mit dem Amphitheater des Statilius Taurus im Jahr 29 v. Chr. ein erstes steinernes und daher dauerhaftes Gebäude für diese Art von Veranstaltungen errichtet wurde.
Unter Augustus wurde das Forum stark umgestaltet. Durch die umfangreiche Verwendung von Marmor und die neue Pflasterung in weißem Travertin entstand so ein äußerst prunkvoller Platz, vergleichbar vielleicht mit der Akropolis in Athen. Durch geschickte Assoziationen verband Augustus die unter seiner Herrschaft neu errichteten Bauwerke mit der Familie der Julier, um seinen Machtanspruch direkt von den Göttern abzuleiten. Besonders deutlich wurde dies bei der bewussten Parallelisierung von Pollux und Castor, den Dioskuren, mit den Söhnen des Augustus, und der geänderten Orientierung des Platzes, der jetzt auf die verlagerte Rednertribüne und den Tempel des Divus Iulius ausgerichtet war. Die neue, auf Augustus und die Julier ausgerichtete Gestaltung fand ihren Niederschlag auch in dem Umstand, dass das nach einem Brand im Jahr 12 v. Chr. neu verlegte Pflaster die caesarischen Gangsysteme verschloss: Spiele fanden auf dem Forum nun nicht mehr statt. Um die erstrebte Würde des Platzes zu erzwingen, wies Augustus sogar die Ädilen an, auf dem Forum und in seiner Umgebung niemanden mehr zu dulden, der nicht mit der Toga bekleidet war. Der repräsentative Anspruch der neuen Platzgestaltung sollte durch Alltagskleidung nicht herabgewürdigt werden.Der bereits 29 v. Chr. errichtete Tempel für seinen vergöttlichten Adoptivvater bildete nun die neue östliche Begrenzung. Flankiert wurde er möglicherweise von Actiumbogen und Partherbogen, zwei Triumphbögen, die die militärischen Siege des Augustus feierten. Somit stand die Ostseite des Forums ganz im Dienste der augusteischen Präsentation und Selbstdarstellung, die dahinter befindliche Regia verschwand aus dem Blickfeld des Forumbesuchers. Tiberius, Adoptivsohn und Nachfolger des Augustus, errichtete auf der Westseite, zwischen der Nordwestecke der Basilika Julia und neben dem Saturntempel einen eigenen Bogen. Tiberius hatte bereits zuvor den Concordiatempel auf dieser Seite des Forums aufwendig erneuern lassen, so dass nun die Bauten des Augustus und des Tiberius die Platzanlage auf den Schmalseiten klammerten.
Unter den folgenden Kaisern verlagerte sich der Schwerpunkt der Platzfunktion, das Forum diente nun als Kulisse für prächtige religiöse Zeremonien. Da das Forum Romanum in der späten Republik zu klein geworden war, errichteten ab Caesar einige Herrscher die sogenannten Kaiserforen (Caesarforum, Augustusforum, Friedensforum der Flavier, Nerva-Forum, Trajansforum), die zwar einige Aufgaben des Forum Romanum übernahmen, es aber nie vollständig ersetzen konnten. Erst jetzt bekam das Forum zur Abgrenzung den selten verwendeten Zusatz Romanum. Strabon nennt es ἡ ἀρχαῖα ἀγορά, „das alte Forum“.Die Eingriffe der Nachfolger des Augustus waren zunächst bedächtig, neue Bauten ergänzten die Platzanlage, ohne sie neu zu definieren. So entstand unter Domitian an der Westseite der Tempel des Vespasian und des Titus, an der Nordseite, aber außerhalb des eigentlichen Forumbereiches errichtete Antoninus Pius im Jahr 141 den Tempel der Faustina, seiner vergöttlichten Ehefrau.
Einzig Domitian wagte es, grundlegend in die Forumsanlage einzugreifen, allerdings nicht durch Bauten, sondern durch die Aufstellung eines kolossalen Reiterstandbildes. Es nahm die Mitte des Platzes ein und degradierte die umgebenden Bauten zur reinen Kulisse seiner Selbstdarstellung. Unmittelbar nach seinem Tod im Jahr 96 wurde sie entfernt. Es dauerte bis zur Zehnjahresfeier der römischen Tetrarchie im Jahr 305, bis das Forum wieder vergleichbar instrumentalisiert wurde. Das Gepräge des Forums änderte sich noch einmal durch den Bau des Septimius-Severus-Bogen im Jahr 203, der einen neuen Akzent auf der Westseite setzte, indem er einen dreibogigen Zugang nördlich der Rostra bildete.
Um diese Zeit erscheint auch erstmals die Bezeichnung Forum magnum, „das große Forum“, obwohl Cassius Dio überliefert, die Bezeichnung – er nennt es auf Griechisch μεγάλη, „groß“ – wäre bereits nach dem Bau des Caesarforums aufgekommen. Noch in Notitia und Curiosum des Regionenkatalogs der Stadt Rom wird es als Forum Romanum vel magnum geführt und bildete den Kern der Regio VIII.
Zur Zeit Diocletians, der auch die Curia Iulia erneuern ließ und ihr die Grundzüge ihres heutigen Aussehens gab, erfuhr die Ostseite eine letzte entscheidende Veränderung, als die quasi die gesamte Breite einnehmenden neuen Rostra, die rostra Diocletiani, errichtet wurden. Sie ersetzten auf dieser Seite die alten Rostra des Caesartempels und korrespondierten nun den ebenfalls in dieser Zeit veränderten Rostra der Westseite. Die westlichen Rostra wurden um ein Fünfsäulenmonument ergänzt, das sich auf oder hinter der Tribüne erhob. Eine der zugehörigen Säulenbasen, die sogenannte Decennalienbasis, ist erhalten. Weitere Basen waren in der Renaissance entdeckt und dokumentiert worden, sind heute aber verloren. Ein ähnliches Monument wird für die durch Ziegelstempel datierten Rostra Diocletians angenommen. Anlässlich der Decennalien, der Zehnjahresfeier der Tetrarchie im Jahr 303, kündeten die Säulenmonumente von dem Ereignis und feierten die neue Herrschaftsform, durch die Rom zu einer neben mehreren Residenzstädten der römischen Kaiser reduziert worden war. Ebenfalls in tetrarchischer Zeit entstand an der Südseite des Forums, unmittelbar vor der Fassade der Basilika Julia, ein sieben Säulen umfassendes Denkmal, dessen Sockel noch erhalten sind.
Nachdem Konstantin der Große aus den tetrarchischen Machtkämpfen im Jahr 324 als Sieger hervorgegangen war, okkupierte er das Forum und ließ vor dem Septimius-Severus-Bogens sein kolossales Reiterstandbild aufstellen, dem wenige Jahre später eine weitere Reiterstatue für seinen Sohn Constantius II. vor dem nördlichen Durchgang des Bogens folgte.
Als letztes antikes Bauwerk wurde auf dem Forum Romanum im Jahr 608 zur Zeit von Papst Bonifatius IV. die Phokas-Säule zu Ehren des oströmischen Kaisers Phokas errichtet, dem damaligen Oberherrn der Stadt.
Im 8. Jahrhundert waren zahlreiche seiner Gebäude nicht mehr intakt, jedoch bildete das Forum Romanum weiterhin das Zentrum der Ewigen Stadt und für das Jahr 768 ist die letzte Volksversammlung auf seinem Gebiet überliefert. Einige antike Gebäude wurden zu Kirchen umfunktioniert. Der Tempel des Antoninus Pius und der Faustina wurde in die Kirche San Lorenzo in Miranda umgewandelt, die Curia Iulia zur Kirche Sant'Adriano al Foro Romano. Erst um die Wende vom 8. zum 9. Jahrhundert verlor das Forum langsam seine zentrale Funktion. Gebäude wurden teils bewusst zerstört, um an Baumaterialien und an Bauflächen zu gelangen. Der Normannenüberfall des Jahres 1084 unter Robert Guiskard wird auch zu weiteren Zerstörungen der Forumsbauten beigetragen haben.
Mitte des 12. Jahrhunderts war das Forumsareal nicht mehr passierbar, die Trümmer antiker Bauten einerseits und neu errichtete Festungsanlagen z. B. der Frangipani andererseits nötigten zu großen Umwegen, die das Areal weiträumig über die Kaiserforen umgingen. Auf den Trümmern wurden Wohnhäuser errichtet, einfache Ziegelbauten mit Holzdächern, umgeben von Gemüse- und Weingärten. Das Bodenniveau des Forums hatte sich mittlerweile signifikant erhöht. Das Innenraumniveau der Kirche Sant’Adriano al Foro Romano musste zu Beginn des 13. Jahrhunderts um vier Meter angehoben werden, um dem Gehhorizont außerhalb der Kirche zu entsprechen. Die antiken Strukturen waren größtenteils unter dem nun höheren Bodenniveau verschüttet, auf einem Teil der entstandenen Freifläche weideten Kühe. Dieser östliche Teil des alten Forum Romanum erhielt daher den bis ins 18. Jahrhundert gebräuchlichen Namen Campo Vaccino („Kuhweide“), der ursprünglich die umgangssprachliche Bezeichnung für das Forum Boarium war. Südlich des Campo vaccino begann mit dem Palatin und seinen Farnesinischen Gärten der disabitato genannte Bereich Roms, der bis ins 19. Jahrhundert weitgehend entvölkerte Teil der Stadt innerhalb der Aurelianischen Mauer.
Der Humanist Poggio Bracciolini berichtete in seinem 1448 erschienenen Dialog De varietate fortunae („Über die Vergänglichkeit des Glückes“), dass ein großer Teil der Cella des Saturntempels, der bei seiner Ankunft in Rom im Jahr 1402 noch aufrecht stand, inzwischen den Kalkbrennern zum Opfer gefallen war.
Eine der Ursachen für diesen zerstörerischen Umgang mit den Ruinen des Forums war das Ende des Papsttums in Avignon und die Rückkehr des Papstes nach Rom 1367. Urban VI., in seinem Bestreben, die Stadt wieder aufzubauen, ließ umfangreich antike Monumente plündern, um Baumaterial für seine Bauvorhaben zu bekommen. Vom Forum ließ er Material der Basilika Aemilia und des Tempels für Antoninus Pius und Faustina fortschaffen, um den alten Lateranpalast wiederherstellen zu können, der unter Sixtus V. gegen Ende des 16. Jahrhunderts dennoch abgerissen wurde. Zu Beginn desselben Jahrhunderts wurde das Areal des Forum Romanum systematisch auf der Suche nach Steinmaterial für den Bau des Petersdomes durchwühlt. Um den Interessen des humanistischen Forscherdrangs entgegenzukommen, wurden jedoch Skulpturen und Inschriften, die bei den Arbeiten im Steinbruch Forum Romanum zutage kamen, geschont. Wichtige Zeugnisse wie die Fasti Capitolini entgingen so den Kalköfen. 
Unter Paul III. wurde ein letztes Mal gestalterisch in das Forumsareal eingegriffen. Anlässlich des Besuchs Karls V. nach dem Tunisfeldzug wurde zwischen den Bögen des Septimius Severus und des Titus eine Triumphstraße angelegt. Hierfür wurden zahlreiche Wohnbauten und die im Weg befindlichen Festungstürme auf dem Campo Vaccino abgerissen. Die Fläche wurde eingeebnet und mit zusätzlich herbeigeschafftem Bauschutt künstlich erhöht. Ab dem 17. Jahrhundert markierte eine große Allee aus Ulmen den Verlauf dieser Straße.
So verschwand der größte Teil der baulichen Hinterlassenschaften auf dem Forum unwiederbringlich zwischen dem 13. und dem 16. Jahrhundert. Das nun als Steinbruch unbrauchbare, nur von einzelnen Häusern und Werkstätten besetzte, ansonsten landwirtschaftlich genutzte Areal trat zunehmend in das Blickfeld antiquarischer Interessen. Doch war seine einstige Funktion so weit in Vergessenheit geraten, dass beispielsweise Johann Wolfgang Goethe in seiner Italienischen Reise mit keinem Wort das Forum Romanum erwähnte, obwohl er Via sacra und die angrenzenden Ruinen durchaus kannte. Grundlage für diese Nichtbeachtung war ein heftiger Gelehrtenstreit des 16. Jahrhunderts, in dem sich Pirro Ligorio, der das Forum zwischen Kapitolshügel und Palatin lokalisierte, gegen Bartolomeo Marliani durchsetzte. Nachfolgende Gelehrte festigten diese Fehlinterpretation. Erst nach Beginn der systematischen Ausgrabungen zu Beginn des 19. Jahrhunderts wurde diese Ansicht revidiert.
Im Jahr 1788 führte der Schwede Carl Fredrik von Fredenheim (1748–1803) die erste Ausgrabung durch und legte einen Teil der Basilika Julia frei. Mit Carlo Fea und Antonio Nibby (1782–1839) setzte zu Beginn des 19. Jahrhunderts die systematische Freilegung des Forumareals ein. Zu diesem Zweck wurde zunächst die Wohnbebauung, für die man sich die Ruinen als bequeme Basis zunutze gemacht hatte, abgerissen. Was an Ruinen oberirdisch erkennbar war, wurde durch Tiefengrabungen freigelegt. Dies betraf etwa den Tempel der Faustina und den Bogen des Septimius Severus. Die Reste des Saturntempels und des Vespasianstempels sowie des Tabulariums wurden freigeräumt. Nach dem Ausscheiden Feas setzte Nibby die Tiefengrabungen von 1829 bis 1834 fort. Die Ausgrabung des gesamten Forums bis auf das antike Niveau blieb ihnen verwehrt. Bei der Auswertung der Grabungsergebnisse ergab sich als erste wichtige Erkenntnis, dass das Forum weiter nördlich als bislang angenommen lag. Beteiligt an dieser ersten wissenschaftlichen Beschäftigung mit der Topographie des antiken Forum Romanum waren unter anderem Christian von Bunsen, der 1829 zu den Mitbegründern des Instituto di corrispondenza archeologica gehörte, sowie der Architekt und Archäologe Luigi Canina (1795–1856). Da die Ausgrabungen jedoch nur sehr punktuell angelegt wurden, blieb dieser Generation ein zusammenhängendes Verständnis des antiken Befundes verwehrt.
Nachdem Rom 1871 Teil des italienischen Staates und dessen Hauptstadt geworden war, wurde zwischen 1871 und 1905 fast das gesamte, heute unter dem Namen Forum Romanum öffentlich zugängliche Areal archäologisch erschlossen. Der Zeit und dem Königreich Italien verpflichtet, war man in erster Linie bestrebt, den kaiserzeitlichen Zustand wiederzugewinnen. Ohne sich mit der Dokumentation weiter aufzuhalten, wurde alles, was jünger war oder als jünger beurteilt wurde, abgetragen. So wurde die vor dem Tempel des Divus Iulius gelegenen, sogenannten rostra Diocletiani aus dem 3. oder 4. Jahrhundert beseitigt und nur noch wenige Reste zeugen von dieser letzten Rednerplattform auf dem Forum.
Doch gelang es Pietro Rosa, Giuseppe Fiorelli und Rodolfo Lanciani unter dieser Prämisse, zwischen 1871 und 1885 den Forumsplatz als solchen, den Tempel des Divus Iulius und die Via Sacra zwischen Tempel der Faustina und der Maxentiusbasilika auszugraben. Giacomo Boni, von 1898 bis 1922 verantwortlich für die Ausgrabungen, setzte das großflächigen Vorgehen fort und legte die Basilica Aemilia, den Vestatempel samt dem zugehörigen Haus der Vestalinnen, die Regia, die Juturna-Quelle, den Lapis Niger und die Bestattungen archaischer Zeit, das sogenannte Sepulcretum, östlich des Faustinatempels frei.
Seit dem 20. Jahrhundert konzentrieren sich die Forschungen, neben der anhaltenden Dokumentation zu den Altgrabungen, auf spezialisierte Fragestellungen, denen mittels kleiner, gezielter Ausgrabungen nachgegangen wird. So wurden 1979 die Rostra Diocletiani erneut archäologisch untersucht, was wichtige Ergebnisse für das Monument, seine Datierung und Rekonstruktion erbrachte. Im 21. Jahrhundert wurden Ausgrabungen östlich des Vestalinnenhauses am Anstieg zum Palatin durchgeführt, die Reste wahrscheinlich der Roma quadrata aus der Königszeit freilegten. Darüber hinaus wurden und werden Bauaufnahmen durch- und Restaurierungsarbeiten ausgeführt.
Die im Folgenden beschriebenen Monumente umfassen Bauten und sakrale Orte des als Forum Romanum ausgewiesenen archäologische Areals, das eine deutlich größere Ausdehnung als das antike Forum besitzt. Es reicht von den Bauten unterhalb des Kapitols im Westen bis zur Velia genannten Höhe mit dem Titusbogen und dem Tempel der Venus und der Roma im Osten, ist also etwa doppelt so groß wie der eigentliche, antike „Marktplatz“. Die Monumente lassen sich grob in drei unterschiedlichen Zwecken dienende Gebäudetypen einteilen: Tempel und andere religiöse Bauten, politisch genutzte Bauten sowie wirtschaftlich und administrativ bedeutsame Gebäude. Eine klare Trennung der Nutzung war allerdings durchaus nicht immer gegeben – so wurde der Saturn-Tempel auch zur Verwahrung des römischen Staatsschatzes und für öffentliche Bekanntmachungen genutzt.
Einige der ältesten und wichtigsten Heiligtümer vor allem aus republikanischer Zeit finden sich auf dem Forum Romanum. Der im Westen des Forums unterhalb des kapitolinischen Hügels gelegene Tempel des Saturn hat eine lange Vorgeschichte. Zuerst gab es wohl einen Altar, bereits dem Saturnus (Gott des Ackerbaus) geweiht. 498 v. Chr. wurde dann der Tempel eingeweiht. Nach einem Brand wurde er 42 v. Chr. wieder aufgebaut, die heute sichtbaren Reste stammen von einer Renovierung des Jahres 283. Die öffentlichen Bekanntmachungen (Acta diurna) wurden an diesem Tempel angeschlagen und das Aerarium, der römische Staatsschatz, wurde in ihm aufbewahrt. Das laut Plinius auf die Gründungszeit Roms zurückgehende Heiligtum der Venus Cloacina diente staatlichen Reinigungszeremonien und soll diese Funktion schon nach Beendigung des Streits um den Raub der Sabinerinnen gehabt haben. Nach einem erhaltenen Münzbild zu schließen, war das südlich der Basilica Aemilia gelegene Heiligtum nicht überdacht und bestand nur aus der Umfassungsmauer und zwei Kultbildern. Lediglich das Fundament ist erhalten.Der ebenfalls aus der Gründungszeit der Stadt stammende Janustempel war dem zweigesichtigen Gott Janus gewidmet. Eigentlich war es ein Doppelbogen über dem Argiletum, der Straße zwischen Basilica Aemilia und Curia. Die Türen des Janustempels wurden geöffnet, wenn sich Rom im Krieg befand, und geschlossen, wenn in keinem Teil des Reiches ein Krieg stattfand. Heute ist nichts mehr erhalten. Lapis Niger (lat. Schwarzer Stein) bezeichnet eine quadratische Fläche aus schwarzem Marmor, an der nach der Überlieferung Romulus wegen Machtmissbrauch von den Senatoren ermordet wurde. Bei Ausgrabungen unterhalb des zwischen Curia Iulia und dem Bogen des Septimius Severus gelegenen Lapis Niger wurde ein frührömisches Heiligtum des Vulcanus entdeckt.
Das südlich des Bogens für Septimius Severus befindliche Volcanal, der Altar des Feuergottes Vulcanus, ist eines der ältesten Heiligtümer des Forums, das der Sabinerkönig Titus Tatius gegründet haben soll. Der in unmittelbarer Nähe gelegene Umbilicus urbis galt als Nabel des Imperium Romanum und somit der Welt. Weiter galt er als Stelle, an der sich Ober- und Unterwelt berührten, so dass hier Opfer dargebracht wurden. Hier findet man also auch die durchaus gewollte Doppelnutzung als Heiligtum und deren politische Bedeutung. Die Porticus der Dei Consentes, oberhalb des Saturn-Tempels gelegen und wahrscheinlich 367 wiederaufgebaut, ist ein Gebäude mit sechs Räumen, in dem anscheinend die zwölf vergoldeten Statuen der Dei Consentes (je sechs Götter und sechs Göttinnen) nach griechischem Vorbild aufgestellt waren.
Der Concordia-Tempel liegt am westlichen Rand des Forums und war der römischen Göttin der Eintracht, Concordia, gewidmet. Seine Errichtung soll bereits zum Ende der Standeskämpfe 367 v. Chr. gelobt worden sein, ausgeführt wurde dieser frühe Bau aber wohl nie. Umgestaltet wurde der Concordia-Tempel um die Zeitenwende durch Tiberius. Heute sieht man nur noch das Podium, unter der Treppe zum Kapitol gelegen. Zeitweise wurde der Tempel für Senatssitzungen genutzt. Die noch heute sich in Resten auf hohem Podium am südlichen Forumsrand erhebende Aedes Castoris, auch Dioskurentempel oder Tempel des Castor und Pollux (den mächtigen Söhnen des Zeus und der Leda) genannt, war einer der ältesten Tempel des Forum Romanum. Heute sieht man die neu errichtete und umgestaltete Form aus der Zeit des Augustus. 
Östlich des Dioskurentempels entsprang am Fuße des Palatin die Juturna-Quelle. Wie alle Quellen wurde auch diese als Gottheit verehrt (in diesem Falle als Quellnymphe Iuturna), ihr Wasser wurde als heilbringend angesehen, der Ort selbst als einer heiligsten des Forums. Nördlich der Quelle erheben sich die Reste des Tempels der Vesta als das zentrale Heiligtum des Forum Romanum. Dort wurde Vesta, die keusche Hüterin des ewigen Herdfeuers, verehrt. Der heilige Raum des Tempels, geschmückt mit einer Statue der Pallas Athene, durfte nur von den Vestalinnen und dem Pontifex Maximus betreten werden. Männer durften den Tempel nachts gar nicht betreten. Die hoch geehrten und jungfräulichen Vestalinnen wohnten im direkt daneben liegenden Haus der Vestalinnen (Atrium Vestae), einer prunkvoll ausgestatteten, zweigeschossigen Villa. Weiter nördlich lag die Regia, Sitz des Rex Sacrorum und des Pontifex Maximus. Dort fanden die Versammlungen der Pontifices statt und wurden auch die Annalen der Stadt verwahrt.
Der Tempel des Antoninus Pius und der Faustina an der Nordseite des Forums ist der besterhaltene Tempel des antiken Rom. Erbaut von Kaiser Antoninus Pius zu Ehren seiner Frau und nach seinem Tod auch ihm gewidmet, verdankt er seine Erhaltung der Umwidmung in eine Kirche im 7. oder 8. Jahrhundert. Der in Umgebung des Tempels befindliche archaische Friedhof zeigt den später nicht überbauten Rest der ehemals über das ganze Forumsgebiet verteilten Gräber. Etwa 40 bis ins 9. Jahrhundert v. Chr. zurückgehende Gräber sind erhalten.Der sogenannte Tempel des Romulus (angeblich zu Ehren des Sohns des Maxentius errichtet) befindet sich zwischen dem Tempel des Antoninus Pius und der Faustina und der weiter östlich gelegenen Maxentiusbasilika. Auch er verdankt seine gute Erhaltung der Umwidmung – bereits in der ausgehenden Spätantike (im 6. Jahrhundert) – in eine Kirche. Wahrscheinlich war der Tempel Iuppiter Stator gewidmet, daneben wurden auch die Penaten hier verehrt. Der Doppel-Tempel der Venus und der Roma liegt auf dem Abhang zum Kolosseum hin. Die dem Forum zugewandte Cella war der Roma, der Stadtgöttin, geweiht, die zum Kolosseum hin ausgerichtete Cella beherbergte den Kult der Venus. Heute ist in der westlichen Cella das Antiquarium am Forum Romanum eingerichtet.
Das Comitium war der Ort der römischen gesetzgebenden Volksversammlung (Comitia). Ursprünglich war es wohl nur ein offener Platz und erhielt später seine runde Form. Nach vielfältigen Umbauten und der Verlagerung der Versammlungen auf das Forum deutet heute nichts mehr auf die frühere Wichtigkeit hin. Die Rostra waren die Rednertribüne am Comitium. Die im Kampf gegen die Volsker erbeuteten Schiffsschnäbel (lateinisch rostra) wurden hier angebracht und gaben der Plattform ihren Namen. Der Goldene Meilenstein neben den Rostra, das Miliarium Aureum, war eine unter Augustus errichtete Bronze-Säule mit den Namen und Entfernungen aller Provinzhauptstädte.Die Curia Iulia war, als Nachfolgebau der Curia Hostilia, der Versammlungsort des Senats. Die Curia Hostilia bildete zusammen mit dem Comitium und den Rostra den politischen Schwerpunkt des Forums. Umgestaltet wurde das Gebiet vor allem durch Gaius Iulius Caesar und Augustus, die die neue Curia erbauten. Der Ziegelbau mit Zugang zum ebenfalls neu erbauten Forum Iulium verdankt die Grundzüge seines heutigen Aussehens Diokletian, der ihn nach einem Brand neu errichtete. Die Nutzung als Kirche ab dem 7. Jahrhundert erklärt den hervorragenden Erhaltungszustand, das heutige Erscheinungsbild ist das Resultat einer unter Benito Mussolini durchgeführten Restaurierung. Die bronzenen Eisentüren sind Nachbildungen, die Originale findet man im Hauptportal der Lateranbasilika. Der Carcer Tullianus war das Staatsgefängnis Roms, berühmte Gefangene waren unter anderem Jugurtha und Vercingetorix. Die christliche Legende nennt ihn auch als Ort, wo die Apostel Petrus und Paulus gefangen gehalten wurden.
Besonders prachtvoll gestaltet wurden „Propagandabauten“, die – auch hier wieder in gewollter Doppelfunktion – oft als Tempel errichtet wurden. Der Septimius-Severus-Bogen wurde 203 n. Chr. zur Erinnerung an die Erfolge von Septimius Severus, Caracalla und Geta gegen die Parther im Osten errichtet. Der mehr als 20 Meter hohe und mit Pentelischem Marmor verkleidete Bogen weist eine Inschrift auf, aus der Geta nach seiner Ermordung nachträglich getilgt und überschrieben wurde. Der Bogen lieferte die Vorlage für das repräsentative Westportal des Berliner Schlosses von Johann Friedrich Eosander von Göthe.
Der Tempel des Vespasian und des Titus war den Flaviern Vespasian und Titus nach ihrer Apotheose gewidmet und wurde unter Domitian fertiggestellt. Drei korinthische Säulen sind erhalten. Der Domitianische Gebäudekomplex war das Verbindungsglied zwischen dem Forum und dem Palatin, wohl ein großartiger Eingangstrakt zu den Kaiserpalästen. Die Prätorianerwache muss wohl ebenfalls hier untergebracht worden sein.
Der Tempel des Divus Iulius oder Caesar-Tempel wurde von Augustus zu Ehren seines unter die Götter erhobenen Adoptivvaters Gaius Iulius Caesar im Jahre 29 v. Chr. an der Stelle errichtet, an der der Leichnam Caesars verbrannt wurde. Südlich des Caesar-Tempels liegen noch Reste eines als Augustusbogen angesprochenen Monumentes, deren genaue Identifizierung jedoch nicht ganz sicher ist.
Der Titusbogen wurde aus Marmor Ende des 1. Jahrhunderts n. Chr. zu Ehren des vergöttlichten Kaisers Titus für dessen Sieg über die Aufständischen in Judäa und die Eroberung Jerusalems errichtet. Der Bogen steht am höchsten Punkt der Straße vom Forum zum Kolosseum. Im Mittelalter wurde er als Eingang zur Festung der Familie Frangipani genutzt. Der Titusbogen diente als Vorbild für den Arc de Triomphe in Paris. Die Phokas-Säule gilt als das letzte antike auf dem Forum errichtete öffentliche Bauwerk. Die Säule wurde 608 zu Ehren des oströmischen Kaisers Phokas durch dessen Exarchen Smaragdus erbaut und trug wohl seinerzeit eine vergoldete Statue des Kaisers.
Die Basilica Aemilia ist eine der ursprünglich vier Basiliken aus der römischen Republik. Sie trug nach ihren beiden Erbauern erst den Namen Basilica Aemilia et Fulvia, da sich aber nur die Aemilier an Restaurierungen in den folgenden Jahrhunderten beteiligten, wurde sie kurz Basilica Aemilia genannt. Nach mehreren Restaurierungen wurde sie nach der Zerstörung 410 durch Alarich ein letztes Mal neu aufgebaut.
Die Basilica Iulia wurde nach der Gens ihres Erbauers Gaius Iulius Caesar benannt. Sie ersetzte den kleineren Vorgängerbau der Basilica Sempronia, diente als Versammlungsgebäude des Senats und war die bis dahin größte errichtete Basilica, die auch die alte Ladenzeile der tabernae veteres in ihrer Fläche einbezog.
Die Maxentius- (oder Konstantins)-Basilika ist die größte und als letzte errichtete römische Basilika. Sie sollte Maxentius als Empfangshalle dienen, wurde aber nach seiner Niederlage an der Milvischen Brücke und seinem Tod von seinem Widersacher Konstantin vollendet. Der architektonische Höhepunkt war die Kreuzgewölbe-Decke, die bei einem Erdbeben 1349 einstürzte.
Kai Brodersen: Galenos: Die verbrannte Bibliothek. Marix, Wiesbaden 2015, ISBN 978-3-7374-0962-9 (über einen Großbrand auf dem Forum im Jahr 192 n. Chr.).
Filippo Coarelli: Rom. Ein archäologischer Führer. Verlag von Zabern, Mainz 2000, ISBN 3-8053-2685-8, S. 55–109.
Theodor Kissel: Das Forum Romanum. Leben im Herzen Roms. Artemis, Düsseldorf 2004, ISBN 3-7608-2307-6.
Joachim Losehand: Häuser für die Herrscher Roms und Athens? Überlegungen zu Funktion und Bedeutung von Gebäude F auf der Athener Agora und der Regia auf dem Forum Romanum. Kovac, Hamburg 2007, ISBN 3-8300-3397-4.
Leonard von Matt, Franco Barelli: Rom. Kunst und Kultur der Ewigen Stadt. DuMont, Köln 1975, ISBN 3-7701-0707-1, S. 32–39.
Klaus S. Freyberger: Das Forum Romanum. Wissenschaftliche Buchgesellschaft, Darmstadt 2012.Bernhard Steinmann, Robert Nawracala, Martin Boss (Hrsg.): Im Zentrum der Macht. Das Forum Romanum im Modell. Ausstellungskatalog Erlangen, Erlangen 2011.
Digital Roman Forum – virtuelle Rekonstruktion des Forum Romanum im Jahr 400 n. Chr. (University of California in Los Angeles)
,digitales forum romanum‘ – Forschungs- & Lehrprojekt des Winckelmann-Instituts der Humboldt-Universität zu Berlin
