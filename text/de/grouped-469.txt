
Das Space Shuttle (auch der Shuttle) war eine  Raumfähre. Das System wurde seit den 1970er-Jahren von der US-Raumfahrtbehörde NASA entwickelt, der erste Flug fand am 12. April 1981 statt. Durch die Wiederverwendung der Teile des Systems sollten die Flüge in den Weltraum deutlich billiger als mit Raketen werden. Die Erwartungen erfüllten sich nicht und hohe Instandsetzungskosten standen dem entgegen. Im Jahr 2011 kam es zum letzten Flug.
Die Komponenten gliederten sich neben dem Orbiter in den externen Treibstofftank und zwei Feststoffraketen. Dieses System wurde Space Transportation System (kurz STS) genannt. Fachleute nannten stets nur den Orbiter das Space Shuttle. Insgesamt gab es fünf Orbiter, von denen zwei durch Unglücke zerstört worden sind. Dabei kamen alle jeweils sieben Besatzungsmitglieder ums Leben.
Eine Raumfähre konnte bis zu acht Astronauten und gleichzeitig 24,5 Tonnen Nutzlast in eine niedrige Erdumlaufbahn (zwischen etwa 200 und 650 Kilometern Bahnhöhe) bringen. Zudem koppelte das Shuttle mit Hilfe von Andockadaptern mehrfach an Raumstationen (anfangs die russische Mir, später die ISS) an. Durch die Fähigkeit zum gleichzeitigen Transport von Mannschaft und Fracht war das Shuttle sehr vielseitig verwendbar. Es konnten Satelliten repariert oder zur Erde zurückgebracht werden, aber auch der Aufbau und die Versorgung der Raumstationen Mir und ISS waren zentraler Bestandteil der Shuttle-Missionen.
Nach dem letzten Apollo-Flug 1975 war das Shuttle ab 1981 das Arbeitspferd der NASA. Es wurden insgesamt 135 Flüge durchgeführt. Der letzte Shuttle-Flug startete am 8. Juli 2011. Mit der Landung der Atlantis am 21. Juli 2011 ging die Ära der Space Shuttles zu Ende.
Zu den wichtigsten Erfolgen gehören die Aussetzung diverser Raumsonden sowie des Hubble-Weltraumteleskops, diverse Flüge mit eingebauten Laboratorien sowie Flüge zur Mir und zur ISS. Insgesamt wurden fünf raumflugfähige Orbiter gebaut, sowie ein weiterer nicht weltraumtauglicher für atmosphärische Flugtests.
Als Nachfolger ist das Orion-Raumschiff in Entwicklung, das nach zwei unbemannten Testflügen 2014 und 2018 frühestens 2023Vorlage:Zukunft/In 4 Jahren zum ersten Mal bemannt gestartet werden soll. Außerdem befinden sich mit Dragon, dem CST-100 Starliner und Dream Chaser mehrere private US-amerikanische Raumschiffe in Entwicklung.
Nachdem die USA den Wettlauf zum Mond gewonnen hatten und das Feld der Raumstationen der Sowjetunion überlassen mussten, wandte man sich bei der NASA ab Mitte der 1960er Jahre vermehrt der Idee zu, eine wiederverwendbare Raumfähre zu entwickeln. Dahinter steckte vor allem der Gedanke, die Kosten für den Raumtransport drastisch zu senken und so eine Kommerzialisierung der Raumfahrt einzuleiten.
So wurde 1969, also im Jahr der ersten Mondlandung, von der NASA eine Studie in Auftrag gegeben, worauf die vier großen Raumfahrtunternehmen der USA (Lockheed, Grumman, McDonnell Douglas und North American Rockwell) je ein Konzept einreichten.
Das Programm befand sich einige Jahre in der Konzeptphase. Das Fortschreiten wurde jedoch durch eine ungünstige politische Stimmung im Weißen Haus und das enge Budget der NASA gegen Ende des Apollo-Programms behindert. Präsident Richard Nixon, „kein großer Freund der Raumfahrt […], dachte an seine Wiederwahl, für die er Arbeitsplätze in den bevölkerungsreichen Staaten Texas und Kalifornien schaffen musste – traditionell wichtige Zentren der Raumfahrt. Nixon entschied sich daher für das Naheliegende: Das Space Shuttle sollte gebaut werden. Und nur das Space Shuttle.“Frischen Wind bekam das Projekt, als im Jahr 1971 die US-Luftwaffe ebenfalls Interesse an einem wiederverwendbaren Raumfahrzeug bekundete. In der Folge versuchte man bei der NASA, die zusätzlichen Anforderungen der Luftwaffe in den Entwurf zu integrieren. Dabei ging es vor allem um eine vergrößerte Nutzlastbucht, um große Spionagesatelliten transportieren zu können, und um die Fähigkeit des Shuttle, nach einem einzigen Orbit auf einer polaren Umlaufbahn wieder den Startplatz erreichen zu können. Das erforderte eine sogenannte Cross-Range (Abweichung von der Umlaufbahn zum Landeplatz) von fast 1800 Kilometern, was nur mit größeren Deltaflügeln und einem verbesserten Hitzeschild zu erreichen war.Wernher von Braun demonstrierte die Idee eines wiederverwendbaren Schiffes mit Hilfe eines Weberschiffchens, das im Englischen als „Shuttle“ bezeichnet wird.Auch die Entwürfe der Industrie änderten sich. Einige sahen bemannte Unterstufen vor oder Außentanks mit Flügeln. Die meisten Konzepte scheiterten an Gewichtsproblemen. Schließlich schien sich das Problem zu lösen, indem man einen im Vergleich mit anderen Studien, die von einem riesigen Raumfahrzeug mit Platz für bis zu 20 Personen ausgingen, kleinen Orbiter auf einen großen Tank setzte und diesen zusätzlich mit Feststoffraketen ausstattete. Damit wurde zwar keine hundertprozentige Wiederverwendbarkeit erreicht, dafür konnten andere wichtige Vorgaben erfüllt werden.
Das dreiteilige Konzept des Shuttle mit der Aufteilung in Orbiter, Außentank und Booster wurde von der NASA offiziell am 15. März 1972 festgelegt. Am 9. August desselben Jahres erhielt North American Rockwell (heute Boeing) den Auftrag, den Orbiter zu bauen. Der Vertrag hatte einen Umfang von 2,6 Milliarden US-Dollar. Der Vertrag über den Bau der Feststoffbooster ging an Morton Thiokol (heute Alliant Techsystems), der Außentank sollte von Martin Marietta (heute Lockheed Martin) hergestellt werden.
Ein Jahr später waren erste detailliertere Planungen verfügbar. Diese enthielten aus heutiger Sicht völlig utopische Zahlen. Man ging von einem Erstflug im Jahr 1978 aus, und der Markt für wissenschaftliche, kommerzielle und militärische Missionen wurde auf 50 Flüge pro Jahr geschätzt. Dabei sollten so viele kommerzielle Nutzlasten in eine Umlaufbahn gebracht werden, dass sich das Shuttle-Programm von selbst finanzieren sollte.
Damals ging man von 10,5 Millionen US-Dollar pro Start aus. Im Laufe der Entwicklung stiegen diese Kosten jedoch beträchtlich – 1977 ging man schon von etwa 24 Millionen Dollar aus. In der Folge musste auch die Anzahl geplanter Flüge drastisch reduziert werden. Die Entwicklungskosten stiegen laufend an und erreichten bald über 12 Milliarden Dollar.
1978, in dem Jahr, in dem eigentlich der Erstflug des Shuttle hätte stattfinden sollen, stand das Programm kurz vor dem Aus. Wieder war es die US-Luftwaffe, die Druck auf den Kongress ausübte, um mehr Gelder für das Shuttle-Programm zu bewilligen. Man hatte mit dem Shuttle gerechnet und mehrere schwere Spionagesatelliten entwickelt, die nur mit der Raumfähre in den Orbit gebracht werden konnten. Diese Intervention verhinderte ein vorzeitiges Ende des Space-Shuttle-Programms.
Die erste flugfähige Raumfähre, die Enterprise, wurde im September 1976 fertiggestellt. Dieser Orbiter war aber nicht raumflugfähig und wurde nur für atmosphärische Flugtests verwendet.
Der erste Freiflug fand am 12. August 1977 statt. Dabei wurde die Enterprise mit einer modifizierten Boeing 747 – dem Shuttle Carrier Aircraft – in die Luft gebracht und dort ausgeklinkt. Anschließend glitt die Raumfähre, genau wie nach einem Raumflug, antriebslos zur Landebahn. Insgesamt wurden fünf solcher Freiflugtests durchgeführt.
Wie sich herausstellte, waren die Haupttriebwerke die schwierigsten Komponenten des Shuttle. Der erste Testlauf fand am 17. Oktober 1975 statt. Während der Tests kam es immer wieder zu Rückschlägen. Eine besonders heftige Explosion zerstörte sogar einen ganzen Teststand. Die Probleme konnten erst im Jahr 1979 nach über 700 Testläufen vollständig gelöst werden. Ihren abschließenden Test vor dem Erstflug absolvierten die Haupttriebwerke wenige Wochen vor dem Start, als mit der bereits auf der Startrampe stehenden Columbia der FRF-Test (Flight Readiness Firing) durchgeführt wurde, bei dem alle drei Triebwerke für 20 Sekunden auf volle Leistung hochgefahren wurden, ohne dass die Raumfähre abhob.
Die Columbia, der erste raumflugfähige Orbiter, wurde im März 1979 an die NASA ausgeliefert. Anschließend wurde die Raumfähre ins Kennedy Space Center (KSC) überführt, um dort auf ihre erste Mission vorbereitet zu werden. Im November 1980 wurde die Columbia mit dem Außentank verbunden und einen Monat später zur Startrampe gefahren. Nach mehreren Startverschiebungen fand am 12. April 1981 der Start des ersten wiederverwendbaren Raumfahrzeuges der Welt statt.
Ziel des ersten Fluges war es lediglich, die Columbia sicher in die Umlaufbahn und wieder zurück zu bringen. Der Flug dauerte insgesamt etwas über zwei Tage und endete mit einer Landung auf der Edwards Air Force Base in Kalifornien. Der Erstflug gilt bis heute als technische Meisterleistung, denn es war das erste Mal in der Geschichte der Raumfahrt, dass ein Trägersystem bei seinem Jungfernflug bemannt war.
Die folgenden drei Flüge (STS-2 bis STS-4), die alle mit der Raumfähre Columbia durchgeführt wurden, dienten der Erprobung aller Systeme des Shuttle. Danach wurde das System als einsatzfähig erklärt.
In den darauf folgenden 21 Missionen, die bis Januar 1986 durchgeführt wurden, stand der Satellitentransport im Vordergrund. Außerdem fanden einige rein wissenschaftliche Flüge statt, bevor es zum Challenger-Unglück kam.
Am 28. Januar 1986 hob die Raumfähre Challenger bei einer ungewöhnlich niedrigen Außentemperatur von 2 °C zur Mission STS-51-L ab. Die NASA hatte sich für den Start entschieden, obwohl Ingenieure des Booster-Herstellers Morton Thiokol, vor allem Roger Boisjoly, vor einem Start bei Temperaturen unter 12 °C eindringlich gewarnt hatten. Das Management von Thiokol überstimmte jedoch schließlich seine Ingenieure und gab seinem wichtigsten Kunden NASA offiziell die Startfreigabe.Wenige Sekunden nach dem Start versagte – wie von den Thiokol-Ingenieuren befürchtet – ein Dichtungs-O-Ring der rechten Feststoffrakete, und durch das entstandene Leck trat heißes Verbrennungsgas an einer Seite des Boosters aus. Die Flamme traf auf den Außentank und die Befestigung der Feststoffrakete, wodurch die Tankhülle zerstört wurde. Der Tank explodierte 73 Sekunden nach dem Start in 15 Kilometern Höhe, worauf das Shuttle durch die enormen aerodynamischen Kräfte zerstört wurde. Die sieben Astronauten überlebten das wahrscheinlich, starben aber spätestens beim Aufschlagen der Cockpitsektion auf die Wasseroberfläche des Atlantiks.
Nach dem Challenger-Unglück wurden einerseits die Feststoffbooster sowie die Flugabbruchmöglichkeiten überarbeitet, andererseits auch das Management neu strukturiert. Viele Entscheidungswege wurden geändert, die Ingenieure bekamen, um der Sicherheit willen, mehr Entscheidungskompetenzen.
Zwei Jahre nach dem Challenger-Unglück nahm die Shuttleflotte wieder ihren Dienst auf, womit die zweite Phase ihrer Nutzung begann. Das Shuttle wurde aus dem kommerziellen Satellitengeschäft zurückgezogen, man konzentrierte sich nun auf wissenschaftliche Aufgaben, staatliche Satellitenstarts sowie die Wartung von Satelliten. Das blieb auch das Aufgabengebiet des Shuttle, bis es 1995 erstmals an der Raumstation Mir andockte, was einer dritten Nutzungsphase gleichkam. Das Aufgabenfeld der Satellitenstarts und Wartungsmissionen wurde zugunsten der Versorgung von Raumstationen nach und nach eingeschränkt. Mit Baubeginn der Internationalen Raumstation wurden dann auch die rein wissenschaftlichen Missionen weniger zahlreich. Stattdessen nutzte man die Shuttles für den Transport der Module zur Station und für deren Montage.
Beim Start von STS-107 im Januar 2003 brachen einige Schaumstoffteile vom Außentank ab, möglicherweise auch Eisstücke. Diese trafen die linke Flügelvorderkante und schlugen ein großes Loch in die Hitzeschutzverkleidung. Zwar bemerkten die Techniker im Kontrollzentrum das Ereignis, waren sich des entstandenen Schadens jedoch nicht bewusst. Bei der Rückkehr des Fluges (1. Februar 2003) trat dann jedoch heißes Plasma, das beim Wiedereintritt entsteht, durch das Loch in die Flügelstruktur ein. Zusammen mit der dadurch bedingten Veränderung der Aerodynamik um den Flügel führte das zum Versagen der Struktur. In der Folge brach die Raumfähre auseinander. Alle 7 Astronauten starben. Sie waren zum Zeitpunkt des Unglücks in einer Höhe von 70 km und bewegten sich mit 23-facher Schallgeschwindigkeit (Mach 23).
Als Reaktion auf das Unglück wurden die Vorsichtsmaßnahmen für den Hitzeschild enorm verstärkt. Der Außentank wurde überarbeitet, um das Abplatzen von Schaumstoff zu minimieren, und der Hitzeschild wurde seit dem Unglück auf jedem Flug mit einer speziellen Erweiterung des Roboterarms (OBSS) auf Schäden überprüft. Zudem wurde ein Konzept zur Rettung eines Shuttles mit beschädigtem Hitzeschild ausgearbeitet. Schließlich kündigte die US-Regierung an, die Shuttle-Flotte zum September 2010 ausmustern zu wollen.
Mit der Wiederaufnahme des regulären Flugbetriebs 2006 blieb – abgesehen von STS-125, dem letzten Wartungsflug zum Hubble-Weltraumteleskop – nur der Aufbau der Internationalen Raumstation als Aufgabengebiet übrig. Es wurden weiterhin kleinere Satelliten in der Nutzlastbucht mitgeführt und nebenbei ausgesetzt.
Seit dem Ende des Space-Shuttle-Programms und Außerdienststellung werden die Orbiter und andere Teile des Programms in US-amerikanischen Einrichtungen ausgestellt:
Discovery: Steven F. Udvar-Hazy Center (Smithsonian Institution) in Washington, D.C. (ersetzt die bis dahin ausgestellte Enterprise)
Enterprise: Intrepid Sea, Air & Space Museum in New York CityAuch Trainings- und Ausrüstungsobjekte wurden der Öffentlichkeit präsentiert, beispielsweise ein Simulator im Adler Planetarium in Chicago, Astronautensitze im Johnson Space Center der NASA in Houston im US-Bundesstaat Texas und Steuertriebwerke der Raumfähre in Museen in Huntsville im US-Bundesstaat Alabama und in Washington, D.C.
Die Vorbereitung für eine Shuttle-Mission im engeren Sinn begann mit dem Zusammenbau der einzelnen Elemente des Shuttle-Systems. Zunächst wurden die Segmente der beiden Feststoffbooster zusammengesetzt. Das geschah im Vehicle Assembly Building (VAB) auf der mobilen Startplattform, mit der das Shuttle später zur Startrampe gefahren wurde. Danach wurde der Außentank, der mit einer Spezialfähre auf dem Wasserweg zum Kennedy Space Center gebracht wurde, mit den beiden Boostern verbunden. Zuletzt wurde der Orbiter ins VAB gebracht und an den Außentank montiert. Kurz darauf wurde das ganze System zu einer der beiden Startrampen, LC-39A oder LC-39B, gefahren.
Auf der Startrampe wurden die letzten Vorbereitungen durchgeführt. Meist wurde die Hauptnutzlast erst hier in den Frachtraum des Orbiters eingebaut.
Etwa 70 Stunden vor dem geplanten Startzeitpunkt begann der Countdown bei der T-43-Stunden-Marke. Planmäßig wurde der Countdown mehrere Male unterbrochen – das erklärte die Differenz von rund 27 Stunden. Damit wurde eine gewisse Standardisierung der Countdown-Prozedur erreicht: die gleichen Arbeiten wurden immer zur gleichen Countdown-Zeit ausgeführt.
Während der gesamten Zeit auf der Rampe, die meist mehrere Wochen betrug, war das Shuttle durch die schwenkbaren RSS-Arbeitsbühne (Rotating Service Structure) gegen Witterungseinflüsse geschützt. In der RSS befindet sich zudem der Payload Changeout Room, ein Reinraum, in dem die Nutzlast zwischengelagert wurde, bevor sie in die Ladebucht der Raumfähre eingebaut wurde. Diese Struktur wurde erst am Vortag des Starts weggeschwenkt.
Rund zehn Stunden vor dem Start wurde mit dem Befüllen des Außentanks mit flüssigem Wasserstoff (−252 °C) und flüssigem Sauerstoff (−183 °C) begonnen. Diese Prozedur dauerte drei Stunden. Danach, etwa vier Stunden vor dem Start, begab sich die Mannschaft in den Orbiter.
Ab neun Minuten vor dem Start wurden alle Vorgänge von den Computern des Startkontrollzentrums, dem Ground Launch Sequencer, überwacht. Ein manuelles Eingreifen in den Countdown war noch bis 31 Sekunden vor dem Abheben möglich. Danach konnte der Start nur noch vom Bordcomputer des Space Shuttle abgebrochen werden.
Das Sound Suppression Water System wurde 16 Sekunden vor dem Abheben aktiviert. Diese Vorrichtung goss innerhalb von 20 Sekunden 1135 Kubikmeter Wasser auf den Bereich unter den Haupttriebwerken und Boostern, um Shuttle und Nutzlast vor Schäden durch die auftretenden enormen Schallwellen zu bewahren. Um zu verhindern, dass austretender Wasserstoff Knallgasexplosionen erzeugt und die empfindliche Computersteuerung der Triebwerke beeinträchtigt, wurde 10 Sekunden vor dem Abheben das elektrische Funkensprühsystem (main engine hydrogen burnoff system) aktiviert. Außerdem wurden die Brennkammern der Triebwerke durch die Turbopumpen gefüllt und unter Druck gesetzt.Die eigentliche Startsequenz wurde dann mit den jeweils um 140 Millisekunden versetzten Zündungen der drei Haupttriebwerke 6,6 Sekunden vor dem Abheben eingeleitet. Die Triebwerke wurden während des Betriebs mit flüssigem Wasserstoff gekühlt.Nachdem die Haupttriebwerke gezündet waren, schwankte das gesamte Shuttle (mit Tank und Boostern) an der Spitze rund drei Meter nach vorn, weil die Triebwerke des Orbiters sich leicht hinter dem Schwerpunkt des gesamten Shuttle befanden. Danach schwang es wieder zurück. Während dieser Zeit wurde das korrekte Hochfahren der Haupttriebwerke überprüft, denn noch konnten sie wieder abgeschaltet werden. Wenn das Shuttle wieder genau senkrecht stand, zündeten die zwei Feststoff-Zusatzraketen (SRB, Solid Rocket Booster). Bis zu diesem Zeitpunkt wurden die Booster durch Bolzen an der Startrampe festgehalten. Diese wurden wenige Sekundenbruchteile nach Zündung der SRBs teilweise gesprengt, wodurch sie aus der Halterung rutschten und das ganze Shuttle zum Start freigaben. Anschließend hob das Space Shuttle ab.
Die beiden SRBs hatten eine Brennzeit von etwa zwei Minuten und produzieren rund 80 Prozent des Gesamtschubs. Sie verbrannten rund 4 Tonnen festen Brennstoff pro Sekunde. Insgesamt trieben 10–12 Tonnen Treibstoff und Sauerstoff pro Sekunde das Shuttle nach oben. Der Tankinhalt einer Boeing 737 wäre dabei in 2 Sekunden aufgebraucht. Nachdem sie ausgebrannt waren, wurden sie in einer Höhe von rund 50 km abgetrennt, stiegen jedoch durch ihre hohe Geschwindigkeit noch auf 70 km Höhe. Dann erst fielen sie zurück und erreichen eine Sinkgeschwindigkeit von 370 km/h. Bevor die SRBs auf die Meeresoberfläche auftrafen, wurden in knapp zwei Kilometern Höhe jeweils drei Fallschirme in den Nasen aktiviert. Mit etwa 80 km/h fielen die Booster schließlich in den Atlantischen Ozean. Zwei Bergungsschiffe der NASA (die Liberty Star und die Freedom Star) nahmen die leeren Hüllen auf und schleppten sie zum Kennedy Space Center zurück, wo sie für die Wiederverwendung vorbereitet wurden.
Nach der Abtrennung der Booster flog das Space Shuttle nur mit Hilfe seiner Haupttriebwerke weiter. Nach ungefähr achteinhalb Minuten Brenndauer wurde kurz vor Erreichen der Orbitalgeschwindigkeit (mit circa 7700 m/s) der Außentank in rund 110 km Höhe abgeworfen. Er verglühte größtenteils in der Atmosphäre, nachdem er eine halbe Erdumrundung absolviert hatte. Die übrigen Teile des Tanks fielen in den Pazifik.
Anschließend wurde die Raumfähre von den beiden Triebwerken des OMS (Orbital Maneuvering System) in eine elliptische Umlaufbahn mit einem tiefsten Punkt („Perigäum“) von etwa 110 km und einem höchsten Punkt („Apogäum“) von 185 km über der Erdoberfläche beschleunigt. Nach einem halben Erdumlauf zündeten die Manövriertriebwerke des Orbiters am bahnhöchsten Punkt, um die Umlaufbahn in eine Ellipse mit einem Perigäum von 185 km und einem Apogäum auf Höhe des Zielorbits zu verwandeln (zum Beispiel etwa 380 km für einen Flug zur ISS). Wenn der Orbiter wieder den bahnhöchsten Punkt erreichte, zündet er die Manövriertriebwerke ein weiteres Mal, um in dieser Höhe in eine Kreisbahn einzutreten. Damit erreichte der Orbiter seinen Zielorbit. Bei komplexen Missionen, die einen speziellen Orbit erfordern oder ein bestimmtes Ziel anfliegen müssen, wurde die Umlaufbahn im Verlauf der ersten Flugtage noch mehrfach angepasst. Das war zum Beispiel zum Erreichen der ISS oder des Hubble-Weltraumteleskops nötig.
Die Arbeiten im Orbit, die sogenannten On-Orbit-Operations begannen mit dem Öffnen der Ladebuchttore. Das war zwingend nötig, da auf den Innenseiten dieser Tore Radiatoren angebracht waren, die für die Kühlung des Orbiters sorgten. Konnten die Tore nicht geöffnet werden, musste die Mission sofort abgebrochen werden.
Das Space Shuttle konnte sehr vielfältig eingesetzt werden. Typische Aufgaben für eine Mission bestanden im Aussetzen bzw. Einfangen von Satelliten, dem Durchführen von wissenschaftlichen Experimenten oder dem Ausführen von Aufbauarbeiten an einer Raumstation, wie der ISS oder früher der Mir. Für wissenschaftliche Arbeiten konnte ein Labor wie Spacelab oder Spacehab mitgeführt werden. Diese Labors boten je nach Konfiguration Möglichkeiten für Experimente im freien Weltall oder in einem bemannbaren Modul.
Zudem war die Crew oft mit körperlichem Training beschäftigt, um der Muskelrückbildung in der Schwerelosigkeit Rechnung zu tragen. Ein beachtlicher Teil der Arbeitszeit der Astronauten wurde auch für die Betreuung und Bedienung der vielen Systeme des Space Shuttle eingesetzt.
Zum Verlassen der Umlaufbahn wurde die Raumfähre entgegen der Umlaufrichtung gedreht. Die OMS-Triebwerke wurden für ungefähr drei Minuten gezündet (sog. deorbit-burn), wodurch das Space Shuttle um etwa 300 km/h verlangsamt wurde. Danach wurde die Raumfähre mit ihrer Nase wieder in Flugrichtung gedreht. Durch das Bremsmanöver verließ der Orbiter die bisherige Umlaufbahn und wechselte aus seiner Kreisbahn in eine ellipsenförmige Bahn mit einem Perigäum von 80 km. Nach knapp einem weiteren halben Erdumlauf trat es in die äußeren Schichten der Atmosphäre ein und wurde dort aerodynamisch weiter abgebremst. Die Lageregelungstriebwerke (RCS) wurden auf einer Flughöhe von etwa 15.000 Metern deaktiviert; Anflug und Landung erfolgten antriebslos, es gab also nur einen einzigen Versuch.
Beim Wiedereintritt in die Erdatmosphäre wurde die Raumfähre durch spezielle Hitzeschutzkacheln an der Front- und Unterseite vor der extremen Hitze der Druckfront von bis zu 1650 °C geschützt. Bereits kurz nach dem Wiedereintritt, noch mehrere hundert Kilometer entfernt, erhielt sie von der vorgesehenen Landebahn Leitsignale. In einer Höhe von rund 13 km begann die aerodynamische Phase der Landung, in der der Orbiter in antriebslosem Flug (Gleitflug mit einem Gleitverhältnis von 4,5) die verbliebene Restenergie sukzessiv abbaute. Der Gleitweg wurde nötigenfalls korrigiert, indem Schlangenlinien geflogen wurden.
Abflachung des Gleitwinkels mit LandungAm Ende der ersten Phase waren Fluglage, Richtung, Höhe und Geschwindigkeit für die Landung optimiert. Bis zur Phase drei betrug der Gleitwinkel etwa 17 bis 18° (gegenüber 2 bis 3° bei Verkehrsflugzeugen) bei einer Geschwindigkeit von etwa 500 km/h. In der dritten Phase wurde der Gleitwinkel durch Änderung des Anstellwinkels auf 1,5° verringert, so dass das Shuttle mit einer Geschwindigkeit von rund 340 km/h, etwa dem Anderthalbfachen eines Verkehrsflugzeug („preflare“ Phase), mit seinem 30 Sekunden vorher ausgefahrenen Fahrwerk auf der Landebahn aufsetzte. Zur Verkürzung des Bremswegs wurde ein Bremsschirm verwendet. Erst bei Erreichen einer niedrigeren Geschwindigkeit kamen dann die Bremsen des Fahrwerks zum Einsatz. Der Pilot durfte das Shuttle kurzzeitig selbst fliegen, musste dann jedoch an den Kommandanten übergeben, der die Landung durchführte. Jedoch war der Pilot für das Ausfahren des Fahrwerks und das Auslösen des Bremsschirms verantwortlich.
Schlechte Wetterbedingungen am Hauptlandeplatz machten es mitunter erforderlich, auf günstigere Orte auszuweichen. Seit 1991 war grundsätzlich das Kennedy Space Center in Florida das primäre Landeziel. Dort befindet sich die sogenannte Shuttle Landing Facility, eine 4,5 km lange und 90 m breite Landebahn, die eigens für die Rückkehr der Orbiter aus dem Weltraum gebaut worden war. Wenn das Wetter eine Landung in Florida unmöglich machte, standen der NASA zwei Alternativen zur Verfügung. Erster Ausweichflughafen war die Luftwaffenbasis Edwards (Kalifornien), wo auch die Erprobung der damals neuentwickelten Raumfähre durchgeführt wurde, zweiter Ausweichstandpunkt war White Sands (New Mexico) (nur eine Landung, STS-3 1982).
Daneben gab es rund um die Welt weitere Notlandeplätze für die Startphase und den weiteren Missionsverlauf. Es wurde unter anderem unterschieden in East Coast Abort Landing Sites (ECAL) in den USA und Kanada und Transoceanic Abort Landing Sites (TAL). Letztere waren unter anderem die Istres Air Base in Frankreich sowie Zaragoza Air Base und Moron Air Base in Spanien. Weitere Flughäfen, die für eine Landung des Space Shuttle zertifiziert waren, war u. a. der deutsche Flughafen Köln/Bonn.
War es erforderlich, dass das Shuttle an einem anderen Ort landete als in Florida, wurde es huckepack auf einer modifizierten Boeing 747 (dem sogenannten Shuttle Carrier Aircraft, SCA) dorthin zurücktransportiert. Um die Aerodynamik bei diesem Überflug zu verbessern, wurde am Heck des zu transportierenden Shuttle eine nach hinten spitz zulaufende Abdeckung angebracht, die die Triebwerke des Shuttle verdeckte.
Eine chronologische Liste sowie eine tabellarische Übersicht aller geflogenen Missionen ist unter Liste der Space-Shuttle-Missionen zu finden.
Durch seine Bauart als Raumfähre bedingt war das Space Shuttle extrem flexibel einsetzbar. Es war das einzige Trägersystem, das in der Lage war, mehrere Tonnen Nutzlast vom Weltraum zur Erde zu bringen. Zudem konnten einige Komponenten der Raumstation ISS aufgrund ihrer Abmessungen nur mit dem Shuttle ins All gebracht werden. Dieser Umstand sowie die sich daraus ergebenden Verträge mit den Partnerländern waren auch einer der Hauptgründe, warum das Space-Shuttle-Programm trotz massiven Kostenüberschreitungen unterhalten wurde. Im Verlauf des Shuttleprogramms haben sich die Aufgaben des Systems recht stark gewandelt. Im Folgenden wird eine Übersicht über die wichtigsten Aufgaben des Shuttle gegeben.
Zu Beginn des Shuttle-Programms lag die Hauptaufgabe des Orbiters darin, Satelliten ins All zu bringen. Durch die Wiederverwendbarkeit hatte man sich enorme Einsparungen erhofft. So waren auch die ersten operationellen Flüge des Space Shuttle dieser Aufgabe gewidmet. Während der Mission STS-5 wurden etwa die beiden Nachrichtensatelliten Anik C-3 und SBS-C ins All gebracht. Auch die drei nachfolgenden Missionen wurden für den Satellitentransport eingesetzt.
Daneben hatte das Shuttle die einzigartige Fähigkeit, auch Satelliten vom All zur Erde zurückbringen zu können. Das geschah erstmals auf der Mission STS-51-A, als zwei Satelliten, die zuvor auf zu niedriger Umlaufbahn ausgesetzt worden waren, wieder eingefangen wurden. Zudem konnte man mit dem Shuttle auch Satelliten einfangen, um sie durch Astronauten reparieren zu lassen. Das wurde zum Beispiel während der Mission STS-49 durchgeführt, als die Oberstufe des Intelsat-IV-Satelliten ausgetauscht wurde.
Ein anderes Beispiel war das Hubble-Weltraumteleskop, das fünfmal von einem Space Shuttle zwecks Reparatur angeflogen wurde. Den letzten Besuch hat das Teleskop im Jahr 2009 von der Mission STS-125 erhalten.
Seit dem Challenger-Unglück im Jahre 1986 wurde das Shuttle aus dem kommerziellen Satellitengeschäft zurückgezogen. Seither wurden damit nur noch militärische, wissenschaftliche oder staatliche Nutzlasten in den Orbit gebracht. Die letzte Shuttle-Mission, die in erster Linie dem Transport eines Satelliten gewidmet war, war STS-93 im Sommer 1999. Während dieser Mission wurde das Röntgen-Teleskop Chandra ins All gebracht.
Ein weiteres wichtiges Einsatzgebiet des Shuttle war die Wissenschaft in der Schwerelosigkeit. Die Raumfähre bot eine sehr flexible Plattform für Experimente aller Art. Zunächst ist das Spacelab zu nennen, ein Labor, das in der Nutzlastbucht mitgeführt werden konnte. Der erste Spacelab-Flug war STS-9 im November 1983. Bis zum letzten Flug im Jahr 1998 an Bord des Fluges STS-90, wurden 22 Spacelabflüge durchgeführt.
Nachfolger des Spacelab war das Spacehab. Dieses konnte vielseitiger eingesetzt werden als das Spacelab – so konnte man damit beispielsweise auch Fracht zur ISS bringen, wie es etwa auf dem Flug STS-105 der Fall war. Die letzte reine Forschungsmission des Shuttleprogramms war STS-107 der Columbia, die dann beim Wiedereintritt in die Atmosphäre auseinanderbrach und teilweise verglühte, wobei die sieben Astronauten an Bord ums Leben kamen. Der letzte Flug eines Spacehab-Logistikmoduls war die Mission STS-118.
Auf anderen Missionen, zum Beispiel während STS-7, wurden Forschungsplattformen in der Nutzlastbucht mitgetragen, die dann während der Mission für mehrere Stunden in den Weltraum entlassen wurden, um danach mit dem Roboterarm wieder eingefangen zu werden. Wieder andere solcher Plattformen blieben gleich für mehrere Monate oder Jahre im All und wurden von einer späteren Shuttle-Mission wieder eingeholt.
Grundsätzlich hatten die meisten Shuttle-Missionen zu einem Teil wissenschaftliche Missionsziele. Oft wurden in der Nutzlastbucht sogenannte Get-Away-Behälter mit automatisch ablaufenden Experimenten mitgeführt, oder man hatte sogenannte Middeck Payloads, also Mitteldeck-Nutzlast dabei, die von der Shuttle-Crew nebenbei betreut wurde. Das war auch bei ISS-Flügen teilweise noch der Fall.
Aufgrund seiner unvergleichlichen Flexibilität war das Shuttle ein ideales Arbeitspferd für den Aufbau und die Wartung einer großen Raumstation. Viele Module der ISS waren so groß, dass sie nicht mit anderen Trägern ins All gebracht werden konnten. Zudem bot das Shuttle mit seinem Roboterarm die Möglichkeit, die Module direkt an die Station zu montieren. Das war unumgänglich, da die meisten ISS-Module keine eigenen Antriebs- und Lageregelungssysteme haben und so ein autonomes Andocken nicht möglich war. Auch der Crew-Transport wurde mit dem Shuttle vereinfacht; theoretisch konnten bis zu 5 Besatzungsmitglieder pro Flug ausgetauscht werden.
Wegen dieser kritischen Rolle des Shuttle wurde das ISS-Programm dann auch um mehrere Jahre zurückgeworfen, als die Shuttle-Flotte nach der Columbiakatastrophe im Februar 2003 mit einem Flugverbot belegt wurde. Einige Experimente mussten deshalb sogar gestrichen werden.
Vor der Zeit der ISS wurde das Shuttle auch auf mehreren Flügen zur russischen Raumstation Mir eingesetzt. Zwischen 1995 und 1998 dockte insgesamt neunmal eine Raumfähre an der Station an. Dabei ging es auch um ein politisches Zeichen – es war die erste nennenswerte gemeinsame Operation der beiden Supermächte im Weltraum seit dem Apollo-Sojus-Testprojekt im Jahre 1975. Der erste derartige Flug war STS-71 im Sommer 1995.
Über drei Viertel des zum Start eines Shuttle benötigten Schubes wurden von den beiden Feststoffboostern zur Verfügung gestellt. Die zwei weißen, 45 Meter langen Raketen waren die stärksten Antriebe ihrer Art, die je gebaut wurden. Jeder dieser Booster enthielt über 500 Tonnen APCP, einen Feststoff-Treibstoff auf Basis von Ammoniumperchlorat und Aluminium. Dieses Gemisch verlieh den Boostern eine Brenndauer von gut zwei Minuten und einen spezifischen Impuls (ISP) auf Meereshöhe von 242 s (auf die Masse des Treibstoffs bezogen). Die Booster waren mit schwenkbaren Düsen zur Lageregelung ausgestattet. Zudem waren im oberen Teil mehrere Kameras untergebracht, die während des Aufstieges eine Vielzahl von Bildern lieferten.
Auf einer Höhe von etwa 45 km über Grund wurden die nahezu ausgebrannten Booster abgetrennt und durch kleine Raketentriebwerke vom Außentank weggedrückt. So wurde eine Kollision zwischen den abfallenden Boostern und dem Tank verhindert. Die Booster stiegen dann, entlang einer ballistischen Bahn, weiter bis auf etwa 65 km, um dann den Abstieg einzuleiten. Zuerst wurden kleinere Stabilisierungsschirme ausgestoßen, die die Booster bereits etwas abbremsten. Schließlich wurden die Hauptfallschirme entfaltet, die Booster glitten zur Erde zurück und fielen etwa 230 km vom KSC entfernt mit einer Geschwindigkeit von 80 km/h ins Meer. Bereits wenige Stunden nach dem Start wurden sie von zwei Schiffen geborgen und nach Florida zurückgeschleppt. Dort wurden sie gereinigt, geprüft und für einen weiteren Flug aufbereitet und wiederbefüllt.
Die größte Komponente des Shuttle-Systems war der Außentank (englisch External Tank, ET). Genau genommen beinhaltete der orangefarbene Zylinder zwei Tanks, einen größeren Wasserstofftank im unteren Teil sowie einen kleineren Sauerstofftank im oberen Teil des Tanks. Dazwischen lag die sogenannte Intertank-Section; diese stand nicht unter Druck und enthielt einen großen Teil der Elektronik des Außentanks. Da die beiden Gase Wasserstoff und Sauerstoff in flüssigem Zustand vorlagen und deshalb sehr kalt waren (unter −200 °C), war der Tank mit einem speziellen Schaumstoff isoliert. Dieser verlieh ihm seine charakteristische orange Farbe. Lediglich bei den ersten zwei Flügen war der Tank mit einer weißen Farbschicht überzogen, diese wurde aber aus Gewichtsgründen ab der darauffolgenden Mission nicht mehr verwendet.
Das Shuttle war vorn an einem und hinten an zwei Punkten am externen Tank befestigt. Zudem verlaufen auf der Außenseite des Tanks mehrere Leitungen, die u. a. den Wasserstoff und den Sauerstoff in den Orbiter leiten, wo die Flüssigkeiten dann in den Haupttriebwerken verbrannt wurden. Der Tank war die einzige Komponente des Shuttle, die nicht wiederverwendbar war. Nach dem Brennschluss der Haupttriebwerke (engl. Main Engine Cutoff – MECO) wurde der Tank abgeworfen und trat in die Atmosphäre ein, wo er verglühte.
Seit dem Columbia-Unglück im Jahr 2003 war die Isolierung des Tanks vermehrt ins Gespräch gekommen. Ein Stück abgeplatzten Schaumstoffs hatte damals zu einer Beschädigung des Shuttle geführt, durch die während der Wiedereintrittsphase extrem heiße Gase in den Orbiter gelangten und ihn zerstörten. Seither war der Tank stellenweise stark überarbeitet worden. Auch im Verlauf des Shuttle-Programms wurde der Tank mehrfach überarbeitet. So hatten die ersten Tanks, welche einen weißen Anstrich besaßen, der das typische Orange des Isolationsschaums verdeckt, ein Leergewicht von etwa 35 Tonnen. In der letzten Version waren es weniger als 30 Tonnen.
Die Hauptkomponente des Shuttle-Systems stellte der Orbiter dar. In ihm befanden sich die Mannschaftsräume und das Cockpit (Flightdeck) sowie die Nutzlast der jeweiligen Mission. Seine äußere Formgebung war durch seine aerodynamischen Bauteile Deltaflügel und Seitenleitwerk geprägt, die ihm zum Abschluss einer Mission eine klassische Landung im Gleitflug ermöglichten. Insgesamt wurden fünf raumflugfähige Orbiter gebaut, davon wurden zwei (Challenger und Columbia) durch Unfälle zerstört. Der Orbiter war eines der komplexesten technischen Geräte, die je von Menschen gebaut wurden.
In der Startphase befand er sich in senkrechter Position auf dem Außentank montiert, um in die Umlaufbahn transportiert zu werden. Nachdem er zum Abschluss einer Mission den Orbit verlassen hatte, verlief der Beginn der Landung zuerst rein ballistisch, bevor sie mit einer aerodynamischen Phase abgeschlossen wurde.
Der Orbiter verfügte über drei große Haupttriebwerke, die Space Shuttle Main Engines, abgekürzt SSMEs. Die Haupttriebwerke wurden während des achtminütigen Aufstiegs ins All eingesetzt und dabei mit flüssigem Wasserstoff und Sauerstoff aus dem Außentank versorgt. Nach dem Abschalten und Abtrennen des Tanks konnten die Triebwerke daher während der Mission nicht erneut gezündet werden.
Sie waren kardanisch aufgehängt und hydraulisch um 10,5° schwenkbar. So konnte das Drehmoment ausgeglichen werden, das durch die Änderung von Schwerpunktlage und Schubvektor nach Ausbrennen und Abwurf der Booster auftrat.
Nach der Landung auf der Erde wurden die Triebwerke ausgebaut, geprüft und für ihren nächsten Einsatz vorbereitet. Sie sollten bis zu 55-mal bei einem Maximalschub von 109 % wiederverwendet werden können. Diese Anzahl wurde allerdings nie erreicht. Die Wiederverwendbarkeit machte sie zu technisch hochkomplexen Systemen; ein einziges Triebwerk kostete mit 51 Millionen US-Dollar ungefähr so viel wie eine komplette Delta-II-Rakete.
Getestet wurden die Haupttriebwerke für das Space-Shuttle-Programm mit dem Main Propulsion Test Article (MPTA-098).
Neben den Haupttriebwerken verfügte der Orbiter über 46 mittlere und kleinere Triebwerke, die während des Aufenthalts im Orbit und während der ersten Phase des Wiedereintritts eingesetzt wurden. Die zwei größten davon gehörten zum Orbital Maneuvering System (OMS). Sie lieferten einen Schub von je 27 kN und waren wie die SSMEs im Heck des Shuttle untergebracht. Mit ihnen wurden Bahnänderungen wie etwa das Einschießen in den definitiven Orbit oder die Bremszündung für den Wiedereintritt durchgeführt. Betrieben wurden sie mit hypergolen Treibstoffen, also mit zwei Komponenten, die bei Berührung zünden.
Die 44 kleineren Triebwerke gehörten zum sogenannten Reaction Control System (RCS). Mit ihrer Hilfe wurde die Lage des Shuttles im Raum gesteuert. Das war vor allem beim Andocken an eine Raumstation oder beim Einfangen eines Satellitens wichtig. Die RCS-Triebwerke wurden auch benötigt, um das Shuttle vor der Bremszündung mit dem Heck in Flugrichtung zu drehen. Die Düsen waren dabei an der Nase sowie am Heck angebracht und jeweils redundant ausgelegt. So konnte die Manövrierfähigkeit des Shuttles weitgehend sichergestellt werden. Wie die OMS-Triebwerke wurden die RCS-Düsen mit hypergolem Treibstoff betrieben.
Die Mannschaftsräume des Space Shuttles bestanden aus dem Flugdeck (engl. flight deck), dem Mitteldeck (engl. middeck) und der Luftschleuse (engl. airlock), die jedoch manchmal zum Mitteldeck gezählt wurde. Die gesamten Mannschaftsräume boten einen Rauminhalt von 65,8 m3. Das Flugdeck stellte das eigentliche Cockpit dar, während des Starts befanden sich dort die Sitze von Pilot und Kommandant. Wenn das Shuttle einen Orbit erreichte hatte, wurden sämtliche Sitze verstaut, um so Platz zu sparen. Das Mitteldeck war der Wohn- und Arbeitsbereich der Raumfähre. Hier befanden sich eine Toilette, Schlafabteile und die nötigen Gerätschaften für die Zubereitung der Mahlzeiten. Zudem bot das Mitteldeck Platz für Experimente sowie etwa 140 Liter Stauraum für Nutzlast. Ebenfalls im Mitteldeck befand sich ein Ergometer, ein Trainingsgerät, mit dem die Astronauten der Verringerung der Muskelmasse durch die Schwerelosigkeit entgegenwirkten.
Um das Leben der Astronauten an Bord zu ermöglichen, musste in der Kabine ständig ein lebensfreundliches Klima erhalten werden. Das wurde durch verschiedene Lebenserhaltungssysteme (engl. Environmental Control and Life Support System (ECLSS)) erreicht. So mussten etwa Temperatur und Druck in einem bestimmten Bereich bleiben. Die größte Herausforderung dabei war, eine Überhitzung des Orbiters zu verhindern. Dazu dienten zwei große Radiatoren im Innern der Ladebuchttüren. Diese strahlten während des ganzen Weltraumaufenthaltes Wärme in den Weltraum ab. Der Druck in der Kabine wurde von mehreren Tanks mit Stickstoff und Sauerstoff erhalten. So konnte im Shuttle eine Atmosphäre erzeugt werden, die der irdischen sehr ähnlich war.
Ebenfalls zu den Lebenserhaltungssystemen gehörte das Wassersystem. Im Shuttle waren vier Wassertanks installiert, die je etwa 75 Liter Wasser fassten. Weitere 10 Liter Wasser pro Stunde entstanden als Nebenprodukt bei der Stromerzeugung durch Brennstoffzellen. Abfallwasser wurde in einem entsprechenden Tank gesammelt und in regelmäßigen Abständen in den Weltraum abgegeben.
Die Nutzlastbucht (engl. payload bay) befand sich im mittleren Teil des Shuttle. Nach oben konnten zwei große Tore aufgeschwenkt werden, um die Nutzlastbucht dem freien Weltall auszusetzen. Dieser Vorgang wurde auf jeder Mission durchgeführt, da sich die Radiatoren, welche die Kühlung des Orbiters sicherstellten, auf der Innenseite der Nutzlastbuchttore befanden. Die Nutzlastbucht war 18,38 m lang und hatte einen Durchmesser von 4,57 m. Dieser zylindrische Bereich konnte voll für Nutzlast ausgenutzt werden.
Zudem konnte in der Nutzlastbucht ein Roboterarm, das Remote Manipulator System (RMS), installiert werden. Da das System in Kanada hergestellt wurde, wurde es manchmal auch Canadarm genannt. Der Arm verfügte über sechs Freiheitsgrade und hatte einen Greifmechanismus an seinem Ende, mit dem er Nutzlasten oder Astronauten bewegen sowie Satelliten einfangen konnte. Er war 15 m lang und wog 410 kg, konnte jedoch Massen bis zu 29 Tonnen verschieben. Die Steuerung geschah durch einen Astronauten, der sich auf dem Flugdeck des Shuttle befand. Neben den beiden rückwärtigen Fenstern des Flugdecks wurden mehrere Kameras auf dem Arm und in der Nutzlastbucht für die präzise Steuerung des Arms eingesetzt.
Bei 12 Flügen kam der Integrated Cargo Carrier zum Transport von nicht unter Druck stehenden Außenlasten in der Nutzlastbucht des Shuttle zum Einsatz. Hierbei konnten etwa 3 Tonnen Nutzlast auf einer Transportpalette mitgeführt werden.
Der Strom für den Betrieb der elektrischen Systeme wurde von Brennstoffzellen erzeugt. Diese wurden mit Wasserstoff und Sauerstoff betrieben. Im Orbiter waren drei Brennstoffzellen installiert, die je 7 kW leisten konnten, kurzzeitig waren sogar bis zu 12 kW möglich. Zudem waren die Orbiter Discovery und Endeavour mit dem Station-to-Shuttle Power Transfer System ausgerüstet. Dieses ermöglichte ihnen, Strom von der ISS zu beziehen, um eine längere Aufenthaltsdauer zu ermöglichen.
Weitere Systeme zur Energieerzeugung waren die Hilfskraftanlagen (engl. Auxiliary Power Units (APUs)). Diese drei mit Hydrazin betriebenen Turbinen erzeugten mechanische Leistung zum Betrieb von Hydraulikpumpen. Das Hydrauliksystem wurde benötigt für die Ventil- und Schubvektorsteuerung der drei Haupttriebwerke, die Bewegungen der aerodynamischen Steuerflächen, das Schließen der Treibstofftüren an der Unterseite des Orbiters und an verschiedenen Stellen innerhalb des Fahrwerks.
Verschiedene Bereiche der Außenhaut des Shuttle waren mit speziellen Hitzeschutz-Verkleidungen ausgestattet. Das war für den Wiedereintritt in die Atmosphäre unerlässlich, da wegen der sich vor dem Flugkörper aufbauenden Schockfront enorme Temperaturen auftraten. Ohne den Hitzeschutzschild wäre das Shuttle verglüht. Auch die früheren Raumschiffe der Apollo-, Gemini- und Mercury-Programme waren mit einem Hitzeschild ausgerüstet gewesen, wie auch die russischen Sojus-Kapseln. Einzigartig am Hitzeschutzschild des Shuttle war jedoch seine Wiederverwendbarkeit.
Den größten Teil des Hitzeschutzschildes stellten die über 20.000 Kacheln auf der Unterseite des Rumpfes des Orbiters dar. Die sogenannte High-temperature reusable surface insulation (HRSI) konnten bis zu 1260 °C aushalten. Die Kacheln waren maximal 12 cm dick und bestanden zum größten Teil aus Hohlraum (90 %) und Siliziumdioxid (10 %). Die Dichte betrug 0,14 bzw. 0,35 g/cm3 (Siliciumdioxid um 2,2 g/cm3).
Die hocherhitzten Bereiche am Shuttle wie die Nase und die Flügelvorderkanten waren mit einem speziellen Werkstoff, sogenanntem kohlenstofffaserverstärkten Kohlenstoff (CFC), im Englischen war der Begriff Carbon Fiber Reinforced Carbon (CFRC) oder Carbon-Carbon (C/C) gebräuchlich, verkleidet, der gegen Temperaturen über 1300 °C und mechanische Beeinträchtigungen wie Risse weitgehend resistent war. Ein vollständiger Schutz vor Beschädigung war nicht möglich. Die Columbia-Katastrophe im Jahr 2003 war auf ein großes Loch in einem CFC-Panel an der Flügelvorderkante zurückzuführen.
Weitere Bereiche des Shuttle waren mit der sogenannten Advanced flexible reusable surface insulation (AFRSI) ausgerüstet; das waren Kacheln, die etwa 650 °C aushalten können. Dazu gehörten das Cockpit, der vordere Rumpfteil sowie das Seitenleitwerk bzw. Ruder. Der Rest des Shuttle (hinterer Rumpfteil und Oberseite) hatte keinen speziellen Hitzeschutz. Die normale Außenhaut der Raumfähre konnte jedoch bis zu 370 °C aushalten.
Das Shuttle verfügte für die Datenübertragung (Kommunikation, Video, Telemetrie, Experimentdaten) u. a. über Mikrowellensysteme im S-Band und Ku-Band. Über die Tracking and Data Relay Satelliten (TDRS) stand während des gesamten Umlaufs eine (fast) ununterbrochene Datenstrecke zum Boden zur Verfügung. Die Ku-Band-Antenne befand sich in der Ladebucht, so dass dieses leistungsfähigste der Systeme nur in Flugphasen mit geöffneter Ladebucht genutzt werden konnte.
Wie bei jedem bemannten Raketensystem stand beim Space Shuttle die Sicherheit der Crew an erster Stelle. Durch das völlig neuartige Konzept des Raumgleiters mussten auch völlig neue Sicherheitskonzepte entwickelt werden. Ein Rettungsturm wie zu Apollo-Zeiten kam für den Orbiter nicht in Frage. Vor dem Columbia-Unglück wurden Wiedereintritt und Landung als die weniger kritische Phase des Fluges angesehen, später hat sich dieses Denken etwas gewandelt.
Im Fall eines Startabbruchs vor Abheben des Shuttle konnte auf ein Seilbahnsystem zurückgegriffen werden, das schon im Apollo-Programm bestand. Dieses konnte die Astronauten im Gefahrenfall sicher von der Startanlage wegtransportieren. Es wurde leicht modifiziert, so dass nun sieben Seilbahnkörbe bis zu 21 Personen von der Startanlage befördern können; das für den Fall, dass sich neben den Astronauten auch noch Techniker in der Nähe des vollgetankten Space Shuttle aufhielten. Es wurde bei regelmäßigen Übungen sowie den Terminal Countdown Demonstration Tests aktiviert, musste jedoch noch nie im Ernstfall verwendet werden.
Ein Abbruch ganz kurz vor dem Start konnte nur durch den Redundant Set Launch Sequencer (RSLS) durchgeführt werden. Dieses System prüfte nach dem Starten der Haupttriebwerke (6,6 Sekunden vor dem Abheben) deren Funktion und konnte den bevorstehenden Start noch abbrechen. Diese Art RSLS-Abort wurde insgesamt fünfmal durchgeführt, zuletzt während des Countdowns zum Start von STS-68 im August 1994. Dabei wurden die Triebwerke 1,9 Sekunden vor dem Start wieder abgeschaltet und die Zündung der Feststoffbooster verhindert.
Nach dem Abheben des Shuttle gab es abhängig vom Zeitpunkt und der Schwere eines auftretenden Fehlers zwischen dem Abtrennen der Booster und dem Abschalten der Haupttriebwerke mehrere Möglichkeiten, den Flug zu einem sicheren Ende zu führen. Von diesen vier „Intakten Abbrucharten“ wurde lediglich der Abort to Orbit (ATO) tatsächlich durchgeführt. Während STS-51-F fiel nach etwa sechs Minuten ein Triebwerk aus. Der Abwurf von nicht benötigtem Treibstoff erlaubte es der Challenger, einen zwar niedrigeren als geplanten, aber stabilen Orbit zu erreichen. Da das nur ein kleines Problem darstellte, konnte die Mission wie geplant durchgeführt werden.
Bei schwerwiegenderen Problemen, wie beispielsweise einem Leck in der Crewkabine, war es jedoch nötig, die Mission zu einem raschen Ende zu bringen. Dafür standen während der Startphase drei Optionen offen. Zum einen bestand die Möglichkeit, das Shuttle in einen instabilen Orbit zu bringen und nach weniger als einer Erdumrundung wieder landen zu lassen. Dieser Abort once Around (AOA) konnte nur während eines sehr kleinen Zeitfensters eingeleitet werden und wurde nie durchgeführt. Eine weitere Option, die Transatlantic Abort Landing (TAL), wäre eine Landung auf einem europäischen oder afrikanischen Flughafen gewesen. Für dieses Szenario würde das Shuttle genug Geschwindigkeit aufnehmen, um den anvisierten Landeplatz zu erreichen, um dann die Triebwerke auszuschalten und den Tank abzuwerfen. Wenig später würde das Shuttle dann auf der Zielpiste normal landen. Für einen Shuttlestart musste daher mindestens einer der vorbestimmten Landeplätze gutes Wetter vorweisen können. Auch diese Möglichkeit wurde nie angewandt.
Die letzte und gleichzeitig gefährlichste Abbruchart war Return to Launch Site (RTLS), die Rückkehr zum Startplatz. Sie wäre nur dann angewandt worden, wenn alle anderen Abbruchmodi als Optionen ausgeschlossen gewesen wären, z. B. weil die Raumfähre noch nicht genug Geschwindigkeit und Höhe erreichte hätte. Das Szenario sah vor, dass das Shuttle mit seinen Triebwerken in Flugrichtung gedreht wird und diese solange weiterlaufen, bis sie die aufgebaute Geschwindigkeit abgebaut haben. Anschließend verläuft der Flug wie ein TAL-Abbruch mit dem Ziel, am Startplatz niederzugehen. Diese Option wurde ebenfalls nie angewandt.
Falls während der ersten Minuten der Startphase mehr als ein Triebwerk ausgefallen wäre, so wäre als einzige Option eine Wasserung im Atlantik geblieben. Dazu sollte der Orbiter auf eine Höhe gebracht werden, aus der die Astronauten abspringen hätten können, da sie eine Wasserung wahrscheinlich nicht überlebt hätten. Der Orbiter wäre dann ferngesteuert auf der Meeresoberfläche aufgesetzt. Ein solches Szenario wäre vor dem Challengerunglück für die Besatzung in jedem Fall tödlich gewesen, da sie, abgesehen von den ersten Testflügen, keine Fallschirme dabei hatten. Eine Wasserung wurde nie durchgeführt.
Während des Fluges bestand weiterhin die Möglichkeit, das Shuttle kurzfristig auf einem Notlandeplatz niedergehen zu lassen. Das wäre beispielsweise angewendet worden, wenn sich die Laderaumtüren mit den Kühlungsradiatoren nicht hätten öffnen lassen und so eine Überhitzung des Shuttle gedroht hätte. Für Flüge zu Raumstationen bestand außerdem die Möglichkeit, dass die Besatzung auf der Station verweilte, um sich später von einem anderen Shuttle abholen zu lassen. Diese Möglichkeit entstand als Reaktion auf das Columbia-Unglück im Jahr 2003 unter dem Namen CSCS (Contingency Shuttle Crew Support). Deshalb musste bei jedem Shuttle-Start immer eine sofort einsatzbereite zweite Raumfähre verfügbar sein. Für den letzten Flug einer Raumfähre wurde auf diese Option verzichtet, aber die Besatzung auf nur vier Personen reduziert, damit diese mit dann von Russland zu startenden Sojus-Raumschiffen zur Erde gebracht hätten werden können.
War der Wiedereintritt einmal eingeleitet, konnte er nicht wieder abgebrochen werden. Deshalb wurde seit STS-114
auf jedem Shuttle-Flug der Hitzeschild mittels verschiedener Methoden (siehe Rendezvous Pitch Maneuver, OBSS) überprüft und ggf. per Außeneinsatz repariert, bevor die Bodenkontrolle die Erlaubnis zur Rückkehr gab. So sollten Unfälle wie jener der Columbia (STS-107) in Zukunft verhindert werden.
Für den Fall eines Schadens am Shuttle in der Umlaufbahn um die Erde, bspw. durch eine Kollision mit Weltraummüll, standen den Astronauten drei vollständige MMU-Raumanzüge zur Verfügung. Diese wurden regulär für Weltraumspaziergänge und Außeneinsätze der Astronauten benutzt. Da aber in der Regel mehr Besatzungsmitglieder an Bord eines Space Shuttles waren, wären die restlichen in Rettungskapseln (Personal Rescue Enclosure) gerettet worden. Diese waren ballonförmig, geschlossen und aus dem Material der Raumanzüge gefertigt. Die Astronauten hätten so außerhalb des Shuttles auf die Rettung durch ein Ersatzshuttle gewartet.
Aus sicherheits- und flugtechnischen Gründen wurden alle Orbiter mehrmals für umfangreiche Verbesserungen monatelang außer Dienst gestellt. Während dieser sogenannten Orbiter Maintenance Down Period (OMDP), die nach etwa 13 Flügen anstanden, wurden umfangreiche Tests und Wartungsarbeiten an der Raumfähre durchgeführt. Zusätzlich wurden jeweils größere Verbesserungen vorgenommen. Während der letzten derartigen Revision wurden die Orbiter mit einem sogenannten Glascockpit auf LCD-Basis ausgerüstet, das die alten Röhrenbildschirme und analogen Instrumente ersetzte. Weitere Verbesserungen waren unter anderem ein Bremsschirm, der bei der Landung zum Einsatz kam, und das Station-to-Shuttle-Power-Transfer-System, das es dem Shuttle erlaubte, bei einem Aufenthalt an der ISS Strom von der Station zu beziehen. Solche Modifikationen fanden zunächst im Herstellerwerk im kalifornischen Pasadena statt, wurden aber Ende der 1990er Jahre in die Orbiter Processing Facility (OPF) verlegt, in der auch die Wartung und Vorbereitung der Raumfähren durchgeführt wurde.
Auch nach dem Challenger-Unglück wurden diverse Verbesserungen vorgenommen, bei denen in erster Linie die Boosterverbindungen zum Außentank verstärkt wurden. Die Änderungen nach der Columbia-Katastrophe betrafen hauptsächlich die Schaumstoffisolierung des externen Tanks. Diese sollte dadurch nicht mehr so leicht abplatzen und den Hitzeschutzschild des Shuttle beschädigen können. Darüber hinaus wurden Sicherheitsbedingungen und Startkriterien verschärft.
Seit dem Beginn der Shuttle-Flüge im Jahr 1981 waren insgesamt fünf verschiedene Space Shuttles ins All geflogen. Davon waren bis zur Einstellung des Programms im Jahre 2011 noch drei (Discovery, Atlantis und Endeavour) im Einsatz. Zwei Space Shuttles (Challenger und Columbia) wurden bei Unglücken in den Jahren 1986 und 2003 zerstört.
Die Inspiration ist ein aus Holz und Kunststoff gefertigtes Modell, mit dem sich North American Rockwell für den Auftrag zur Fertigung der Orbiter des Space-Shuttle-Programms bei der US-Regierung beworben hat.
OV-098 Pathfinder war ein Handling-Modell aus Stahl, das nicht flugfähig war. Es wurde zum Erproben und Einüben der Abläufe am Boden eingesetzt. Pathfinder trug keine offizielle Nummer, wurde manchmal aber als OV-098 aufgeführt. Da für die jetzige Konfiguration der Pathfinder auch der Main Propulsion Test Article (MPTA-098) verwendet wurde. Der Pathfinder ist derzeit im U.S. Space & Rocket Center in Huntsville ausgestellt.
OV-101 Enterprise war ein flug-, jedoch nicht raumflugtauglicher Prototyp, der für Gleitversuche und für Flugversuche auf dem Rücken des Shuttle Carrier Aircrafts eingesetzt wurde. Die Enterprise kann seit August 2012 im Intrepid Sea, Air & Space Museum besichtigt werden. Es war geplant, die Enterprise später zu einem raumflugtauglichen Orbiter umzubauen, jedoch erwies es sich als kostengünstiger, die statische Versuchszelle STA-099 zur Raumfähre Challenger (OV-099) auszubauen.
OV-100 Independence, ehemals Explorer, ist ein originalgetreuer Nachbau der Raumgleiter. Er steht im Johnson Space Center.
Bis 2009 befand sich ein America genannter Nachbau im Six-Flags-Vergnügungspark in Gurnee, Illinois.
Ambassador: Ursprünglich für eine von Pepsi-Cola gesponserte Weltraummesse gebaut, lässt sich dieses Modell eines Space-Shuttle-Orbiters in Segmente zerlegen, die einen einfachen Transport ermöglichen. Es wurde im Kennedy Space Center, in Korea und Peru ausgestellt.
Durch die technische Entwicklung im Laufe des Space-Shuttle-Programms bedingt, waren die fünf raumflugfähigen Orbiter nicht exakt baugleich. Einige Merkmale wurden bei allen Orbitern nachgerüstet, so zum Beispiel das Glascockpit. Zuletzt flogen alle Orbiter mit LC-Displays und modernen Computern.
Andere Unterscheidungsmerkmale blieben aber bis zuletzt bestehen; so war die Columbia über drei Tonnen schwerer als ihre später gebauten Schwesterschiffe. Zudem wurde bei Challenger und Discovery eine Modifikation in der Nutzlastbucht eingebaut, die das Mitführen einer bereits betankten Centaur-Oberstufe erlauben würde. Das wurde aber nie umgesetzt.
Die NASA benannte die Shuttles, mit Ausnahme der Enterprise, nach berühmten Entdeckerschiffen der vergangenen Jahrhunderte.
Das Space Shuttle war aufgrund seines Aufbaus mehr Risiken ausgesetzt als eine Raumkapsel, wie sie beispielsweise im Apollo-Programm verwendet wurde. Bekanntestes Problem dabei war spätestens seit dem Columbia-Unglück der Hitzeschild. Dieser lag – anders als der Hitzeschild einer Raumkapsel – während der ganzen Mission offen und war dadurch anfällig für Beschädigungen durch Weltraummüll, Mikrometeoriten oder beim Start vom externen Tank abfallende Eis- oder Schaumstoffteile. Zwar entstanden bei jedem Start kleinere Beschädigungen an den Hitzeschutzkacheln des Shuttles, die keine weiteren Folgen hatten; jedoch konnte ein größeres Loch an den vorderen Flügelkanten oder der Nase des Orbiters eine ernsthafte Gefahr darstellen. Durch ein solches Loch drangen beim Wiedereintritt der Columbia am Ende der Mission STS-107 heiße Gase ein und führten zu strukturellem Versagen am linken Flügel und schließlich zur Zerstörung der ganzen Raumfähre.
Auch die Startphase barg mehr Risiken als ein Kapselsystem. Obwohl eine Rettung der Mannschaft durch die oben genannten Methoden möglich war, konnte der Abbruch nur sicher durchgeführt werden, falls kein zeitkritisches Problem vorlag. So ließ sich ein Abbruch mit Rückflug zum Startplatz (Return to Launch Site, RTLS) oder einem transatlantischen Landeplatz erst nach dem Abwurf der Feststoffraketen einleiten. Ein zeitkritisches Problem vor dem Abwerfen der Booster führte mit hoher Wahrscheinlichkeit zum Verlust von Besatzung und Shuttle (Loss of Crew and Vehicle, LOCV). Auch ein Abspringen der Crew an Fallschirmen kam erst in Frage, wenn ein RTLS-Abbruch erfolgreich durchgeführt wurde, aber kein geeigneter Landeplatz erreicht werden konnte. Ein pyrotechnisches Rettungssystem, bei dem die Crewkabine vom restlichen Shuttle abgetrennt wurde und dann an Fallschirmen niedergeht, wurde zwar in Betracht gezogen, dann aber ebenso wie die bei den ersten Testflügen verwendeten Schleudersitze aus Gewichts- und Kostengründen verworfen.
John Logsdon, einer der profiliertesten Kenner und Kritiker des amerikanischen Raumfahrtprogramms, sagte 2011: „… der Shuttle erwies sich als zu komplex, zu teuer und vor allem zu riskant: Bereits in den ersten Jahren des Programms erkannten die Verantwortlichen, dass sie sich sicherheitstechnisch auf sehr dünnem Eis bewegen. Sie verschlossen aber die Augen. Und schon 1985 gab es Ideen für eine zweite, zuverlässigere Shuttle-Generation. Doch nichts war passiert.“ … „Die USA wollten es sich aber nicht leisten, in Zeiten des Kalten Krieges viele Jahre keinen eigenen Zugang zum All zu haben. Zudem hätte ein sofortiges Ende des Shuttle-Programms auch das Aus für das Weltraumteleskop ‚Hubble‘ und die Jupitersonde ‚Galileo‘ bedeutet, deren Entwicklung weit fortgeschritten war, die aber nur mit einem Shuttle gestartet werden konnten.“
Die Untersuchung des Columbia-Unglücks zeigte innerhalb der NASA neben den technischen auch organisatorische Mängel auf, ähnlich wie früher bei der Challenger-Katastrophe. Um Kosten zu sparen, waren viele Tätigkeiten, die für die bemannte Raumfahrt bei der NASA Standard waren, eingestellt worden. So wurden zum Beispiel die Zeichnungen des Shuttle nicht nachgeführt, obwohl bedeutende Änderungen vorgenommen worden waren, so dass keine Basis für die notwendigen Verifikations-Modifikationen vorhanden war. Allgemein war das gesamte Space-Shuttle-Programm durch den niederschmetternden Untersuchungsbericht in der Öffentlichkeit als veraltet und anfällig, weil zu kompliziert, in Misskredit geraten. Darüber hinaus zeigte der Bericht, dass unüberlegte Kostenreduktionen, die vom NASA-Administrator Daniel Goldin („faster, better, cheaper“) gefordert wurden, ernste Folgen haben könnten.
Ein weiteres Problem des Shuttle-Programms war, dass die Wartungsarbeiten und die Herstellung von Ersatzteilen für den Orbiter fast völlig von der Firma Boeing bzw. deren Tochterfirmen übernommen wurden. Dasselbe galt für den Außentank (Lockheed Martin) und die Feststoffbooster (ATKs Launch Systems). Da deshalb Zehntausende von Menschen vom Space-Shuttle-Programm abhingen, so die Kritiker, erschien es in politischer Hinsicht lange Zeit als nicht opportun, das Programm zugunsten einer besseren Technik ganz einzustellen. Allerdings galt das auch für Vorläuferprogramme (beispielsweise Apollo-Programm) oder zukünftige Programme mit dem Ziel eines bemannten Marsfluges. Sie benötigen enorme finanzielle Ressourcen, die zum größten Teil direkt oder indirekt an Luft- und Raumfahrtkonzerne fließen und dort Abhängigkeiten erzeugen.
Darüber hinaus konnte das Space Shuttle teilweise als Fehlplanung erachtet werden: Der Kongress beschloss, sowohl für die US Air Force als auch für die NASA ein gemeinsames Trägersystem zu entwickeln, das alle bisherigen Trägerraketen ersetzen sollte. Weil das Space Shuttle beiden Partnern genügen sollte, stelle die Raumfähre für den zuletzt einzigen Betreiber, die NASA, ein suboptimales Produkt dar, das einige Air-Force-Anforderungen erfülle, die nicht nötig seien.
Ein weiterer Kritikpunkt war, dass die erhofften Transportpreise für „Weltraumgüter“ nie die angestrebten 200 US-Dollar pro Kilogramm erreicht haben – der Preis lag zuletzt bei rund 16.000 US-Dollar, was  nicht nur an der Inflation lag. Es gab mehrere technische Gründe für die Fehleinschätzung.
Nach dem Verlust der Challenger 1986 musste ein neues Shuttle, die Endeavour, in Auftrag gegeben werden. Dieser ursprünglich nicht geplante Shuttle-Neubau hat das Programm über zwei Milliarden US-Dollar gekostet, obwohl die Endeavour teilweise aus Ersatzteilen der anderen Shuttles zusammengebaut wurde. Der Verlust der Challenger und später der Columbia kosteten das Programm nicht nur Geld, sondern auch Zeit, da mehrjährige Startverbote für die verbliebenen Shuttles erteilt wurden. In dieser Zeit konnten sie keine kommerziellen Projekte durchführen. Auch die gesonderte Überprüfung war teuer. Gleichzeitig fehlte ein Shuttle, das seine Aufgaben erledigen konnte, da man mit einer Flotte von vier Shuttles kalkuliert hatte.
Auch der Wettbewerb im kommerziellen Raumtransportgeschäft nahm stetig zu. Als das Shuttle entwickelt wurde, war seine einzige Konkurrenz die Ariane-Rakete der ESA, die damals noch in den Kinderschuhen steckte, sodass kommerzielle Satelliten-Starts in der westlichen Welt nur durch die NASA durchgeführt werden konnten. Mittlerweile gab es aber zahlreiche weitere Konkurrenten:
Roskosmos, entstanden nach dem Zerfall der Sowjetunion, war ein Konkurrent für kommerzielle Projekte.
China hatte eigene bemannte und unbemannte Raketenprojekte erfolgreich gestartet und trieb deren Entwicklung weiter voran (siehe chinesische Raumfahrt, Langer Marsch (Rakete)).Die rasante Hardware- und Software-Entwicklung der letzten 30 Jahre führte dazu, dass die NASA die Space Shuttles mehrfach nachrüstete. Außerdem mussten strukturelle Probleme, die in der ursprünglichen Planung übersehen oder ignoriert worden waren, kostenintensiv behoben werden. Zudem war es notwendig, für das Shuttle-Mir-Programm spezielle Umbauten an den Raumfähren vorzunehmen, weshalb dauerhaft nur eine geringere Nutzlast in den Weltraum befördert werden konnte. Eine NASA-Raumstation war zwar im Planungsstadium, aber weit entfernt von der Realisierung. Die Einsparungen der weiteren Entwicklung einer Raumstation gingen zu Lasten der Transportpreise der Shuttles, die dadurch weniger kommerziell eingesetzt werden konnten.
Beim Bau der ISS war man gezwungen, auf die Shuttle-Flotte zurückzugreifen, um die größten und schwersten Lasten in den Weltraum zu befördern. Bei diesen Flügen konnten keine oder nur kleine kommerzielle Nutzlasten transportiert werden, da die Tragkapazität der Shuttles weitgehend ausgeschöpft war.
Vor allem aufgrund der immensen Kostenüberschreitungen während der Entwicklung und dem Betrieb des Shuttle wurden bereits einige Male Weiterentwicklungen und Nachfolgeprogramme angekündigt. Einige erreichten lediglich die Konzept-, andere wiederum die Prototypenphase. So basierte das nicht über die Entwicklungsphase hinausgekommene Constellation-Programm mit den Ares-Raketen auf dem Space Shuttle. Der Ares-I-Träger sollte um das Jahr 2015 das Shuttle als bemanntes Raumschiff ersetzen.
Zwischen 1984 und 1995 wurde eine Vielzahl von Konzepten für eine unbemannte Lastenversion des Space Shuttle entwickelt. Diese Studien fanden unter dem Namen Shuttle-C (C steht für Cargo) statt. Durch die fortschreitende Automatisierungstechnik sollte es möglich werden, den Shuttle-C auch ohne Mannschaft und die dadurch bedingten Mannschaftsräume und Lebenserhaltungssysteme zu starten. Zudem waren lediglich die Feststoffbooster und nicht wie beim Shuttle die gesamte Raumfähre wiederverwendbar ausgelegt. Man erhoffte sich dadurch nennenswerte Einsparungen bei den Flugkosten, vor allem für Satellitenstarts. Auch die Nutzlast sollte durch die Gewichtseinsparungen zunehmen, man ging von 50 bis 75 Tonnen aus. Zudem wollte man durch die bereits bestehende Hardware Entwicklungskosten für einen neuen Schwerlastträger sparen. In den frühen 1990er Jahren wurden auch einige Konzepte für bemannte Marsflüge auf Basis des Shuttle-C entwickelt. Keiner der Shuttle-C-Entwürfe kam je über die Konzeptphase hinaus.
Der VentureStar war ein geplanter Nachfolger für das Space Shuttle. Er sollte einige richtungsweisende Neuerungen beinhalten, etwa einen ganz neuen Hitzeschild und einen neuartigen Antrieb. 1996 wurde der Auftrag zum Bau eines Prototyps im Maßstab 1:3 an Lockheed Martin vergeben. Wegen technischer Probleme und Budgetüberschreitungen wurde dieser Prototyp, die X-33, jedoch nie fertiggestellt. Im Frühjahr 2001 wurde das Projekt aufgegeben, obwohl die X-33 bereits zu 85 Prozent fertig war und über eine Milliarde US-Dollar in das Projekt investiert worden waren.
Nach dem Verlust der Columbia legte der damalige US-Präsident George W. Bush am 14. Januar 2004 mit der Vision for Space Exploration ein neues, langfristiges Weltraumprogramm auf, das die Ausmusterung des Space Shuttle zum 30. September 2010 vorsah. Zudem beinhaltete das Programm bemannte Mondflüge ab 2018 und ab Mitte des Jahrhunderts sogar bemannte Marsflüge. Daher wurde für das Constellation-Programm wieder auf herkömmliche Raketen und Raumkapseln zurückgegriffen, die jedoch bewährte Technik des Space Shuttle weiterverwenden sollen. So wurde die Entwicklung der Ares-Raketenfamilie gestartet, die aus zwei Modellen bestand. Die Ares I sollte ab 2014 das Orion-Raumschiff in einen niedrigen Erdorbit befördern. Für Mondmissionen hätte die Ares V ab 2018 das Altair-Landemodul und die Earth Departure Stage in einen niedrigen Erdorbit gebracht, wo sie die Ankunft der Crewkapsel erwartet hätte.
Die Ares I baut auf dem Feststoffbooster des Space Shuttle auf. Eine gestreckte Version des Boosters wurde dabei als erste Stufe verwendet. Beim Schwerlastträger Ares V kam ein vergrößerter Außentank mit zwei gestreckten Boostern zum Einsatz. Durch das Zurückgreifen auf Shuttle-Hardware konnte einerseits Entwicklungsarbeit gespart werden und andererseits der übermäßige Verlust von Arbeitsplätzen durch das Ende des Space-Shuttle-Programms verhindert werden.
Das Constellation-Programm (Ares I, Ares V, Orion) wurde im Februar 2010 eingestellt. Laut US-Präsident Obama sei es weder zeitlich noch finanziell tragbar. Im Mai 2011 wurde von US-Präsident Obama jedoch die Weiterführung der Entwicklung des Orion-Raumschiffs verkündet.
Nach dem Ende des Constellation-Programms beauftragten der US-Senat und der US-Kongress die NASA zur Entwicklung einer neuen Schwerlastrakete, die sowohl bemannte als auch unbemannte Starts durchführen kann.
Das Dragon-Raumschiff hatte 2010 seinen Erstflug und dient seit 2012 als Versorgungsraumschiff für die ISS. Es ist ein privates Raumschiff der US-Firma SpaceX von Elon Musk. Die bemannte Version Dragon V2 hat bereits ihren ersten Testflug erfolgreich abgeschlossen. Im Juni 2019 soll es erstmals bemannt fliegen. An Bord sollen bis zu sieben Personen Platz haben. Als Trägerrakete dient die Falcon 9.
Auch die Raumfähre Dream Chaser ist ein privates Raumschiff. Entworfen von der US-Firma SpaceDev wird sie von der Sierra Nevada Corporation gebaut. Am 12. November 2017 fand ein Abwurftest mit erfolgreicher Landung statt. Wie das Dragon-Raumschiff soll auch die Dream-Chaser-Fähre Astronauten zur ISS bringen. Die Raumfähre kann sowohl suborbital als auch orbital fliegen. Als Trägerrakete soll die Atlas V verwendet werden.
Der CST-100 Starliner ist ebenfalls ein von der Privatindustrie entwickeltes Raumschiff der US-Firma Boeing, das Astronauten zur ISS bringen soll. Als Trägerrakete soll die Atlas V verwendet werden. Der erste unbemannte Flug soll im März 2019 stattfinden, der erste bemannte im August 2019.
Die NASA gab am 16. September 2014 bekannt, dass sie weitere Aufträge bzgl. der Entwicklung einer neuen Raumfähre an Boeing (CST-100, Auftragswert 4,2 Mrd. US-Dollar) und SpaceX (Dragon, Auftragswert 2,6 Mrd. US-Dollar) vergeben hat. Sierra Nevada war noch in der engeren Wahl, ging aber leer aus. Seit 2010 hat die NASA für Entwicklungen in diesem Bereich schon mehr als 1,4 Mrd. US-Dollar ausgegeben.
Das Space Shuttle war das einzige wiederverwendbare Raumfahrzeug, das je im regelmäßigen Einsatz stand. Jedoch gab es eine Reihe von ähnlichen Programmen, die von verschiedenen Raumfahrtbehörden betrieben werden. Einige davon dauern derzeit noch an.
Das russische Pendant zum Space Shuttle, die Raumfähre Buran, war neben dem Shuttle als einziges Raumgleiter-Projekt über die Entwurfsphase hinausgekommen und mit einem unbemannten Testflug erprobt worden. Das Programm wurde nach der Auflösung der Sowjetunion Anfang der 1990er Jahre gestoppt und die verbleibenden Fähren für Ausstellungen genutzt. Siehe dazu auch Vergleich von Buran und Space Shuttle.
War ein Projekt unter der Leitung von Wladimir Nikolajewitsch Tschelomei als eine kleinere und günstigere Antwort der Sowjetunion auf das Space Shuttle.
Der deutsche Ingenieur Eugen Sänger entwickelte ab 1961 bei Junkers Konzepte für einen wiederverwendbaren Raumgleiter, an dem bis 1974 gearbeitet wurde, der jedoch nie über die Konzeptphase hinauskam.
Die ESA begann 1987 mit der Entwicklung einer Raumfähre, die an der Spitze einer Ariane-Rakete ins All befördert werden sollte. Das Programm wurde 1993 gestoppt.
Die Kliper war ein teilweise wiederverwendbares Raumschiff, das als Ersatz für die Sojus entworfen wurde. Die Entwicklung begann im Jahr 2000 und wurde im Jahr 2007 endgültig eingestellt.
Über das Space-Shuttle-Programm und die damit verbundenen Missionen wurden zahlreiche Dokumentationen für das Fernsehen und Kinos (insbesondere IMAX-Filme) gedreht, beispielsweise über die erste Shuttle-Mission, die Hubble-Teleskop-Reparatur, Missionen zur MIR und zur ISS. Darunter waren auch Filme im 3D-Format.
The Dream Is Alive (1985) über den Alltag auf einem Space Shuttle.In Spielfilmen (und TV-Serien) spielten Space Shuttles ebenfalls größere und kleinere Rollen:
In 2001: Odyssee im Weltraum aus dem Jahre 1968 wird ein kommerzielles Space Shuttle mit Namen Orion zum Personentransport zwischen der Erde und einer rotierenden Weltraumstation verwendet.
Star Trek: Der Film aus dem Jahr 1979 zeigt in mehreren Szenen das erste Space Shuttle Enterprise in einer Bildergalerie an Bord der fiktionalen USS Enterprise NCC-1701; die fünf Galeriebilder zeigen von links nach rechts: Marineschiff USS Enterprise von 1775, Flugzeugträger USS Enterprise (CVN-65), Raumfähre Enterprise und zwei fiktive Vorgänger der USS Enterprise NCC-1701.
Im James-Bond-Film Moonraker – Streng geheim aus dem Jahre 1979 spielt ein britisches Space Shuttle mit Namen Moonraker die Titelrolle. Im Film tauchen auch zahlreiche weitere Space Shuttles auf.
Im Pilotfilm zur TV-Serie Buck Rogers aus dem Jahr 1979 fliegt der Titelheld in einer Ein-Personen-Raumfähre mit Namen Ranger 3 und wird in dieser in das 25. Jahrhundert katapultiert.
Im Film Geheimsache Hangar 18 begegnen die Astronauten eines Space Shuttles beim Aussetzen eines Satelliten einem außerirdischen Raumschiff.
Im Film Die unglaubliche Reise in einem verrückten Raumschiff aus dem Jahre 1982 werden Passagiere mit einem Space Shuttle mit dem Namen Mayflower, welches von einer einfachen Abschussrampe vom Flughafen startet, zum Mond geflogen. Auf Grund technischer Probleme geht die Reise gründlich schief und die Raumfähre fliegt erstmal Richtung Sonne. Kurz davor kann sie wenden und fliegt dann zum Mond zurück, auf dem sie unter der Leitung von William Shatner eine Bruchlandung hinlegt.
In Starflight One – Irrflug ins Weltall aus dem Jahre 1983 gerät ein modernes Überschallflugzeug bei seinem Jungfernflug aus der Erdatmosphäre ins Weltall. Das Space Shuttle Columbia wird mehrmals innerhalb von Stunden ins All geschickt, um Passagiere zu retten, was in der Realität technisch und zeitlich jedoch unmöglich war.
In Das Arche Noah Prinzip aus dem Jahr 1984 (Regie: Roland Emmerich) wird ein nicht genanntes Shuttle zur Abholung eines Astronauten aus der fiktiven Raumstation Florida Arklab verwendet.
Im Spielfilm Moontrap aus dem Jahre 1989 begegnet ein Space Shuttle mit Namen Camelot in der Erdumlaufbahn einem außerirdischen Raumschiff.
In Armageddon – Das jüngste Gericht aus dem Jahr 1998 wird zu Beginn des Filmes die Raumfähre Atlantis durch einen Meteoritenschauer zerstört; im weiteren Verlauf spielen zwei experimentelle Shuttles mit Namen Freedom und Independence, die von der NASA zusammen mit dem US-Militär entwickelt worden sein sollen, mit.
Im Film Deep Impact ebenfalls aus 1998 wird das Vorderteil eines Shuttles neben den Booster-Raketen zu einem neuen Raumschiff mit Namen Messiah zusammengebaut; angedockt an eine Raumstation ist das Shuttle Atlantis und zuvor dessen Start im Film sichtbar.
Im US-Fernsehfilm Max Q aus dem Jahr 1998 wird die Notlandung des Shuttles Endeavour nach einer Explosion an Bord gezeigt. Es landet auf einer Landstraße.
Im Spielfilm Space Cowboys aus dem Jahre 2000 wird ein Space Shuttle mit Namen Daedalus auf einer Mission mit der Nr. STS-200 verwendet (die echten Missionen endeten mit Nr. STS-135).
Im Spielfilm Mission to Mars ebenfalls aus dem Jahr 2000 ist in Rückblenden mehrfach ein gelandetes Shuttle im Hintergrund des Protagonisten zu sehen. Offenbar soll dieser früher ein Shuttle-Pilot gewesen sein.
Im US-Film Space Oddity aus dem Jahr 2001 wird die Notlandung eines Shuttles auf einem Boulevard in Kapstadt gezeigt.
In der TV-Serie Star Trek: Enterprise aus den Jahren 2001–2005 wird im Vorspann das Space Shuttle Enterprise gezeigt, das einen Vorläufer des namensgebenden Raumschiffes darstellt.
In der Neuverfilmung des Romans Die Zeitmaschine, dem Spielfilm The Time Machine aus dem Jahr 2002, wird ein Space Shuttle im Anflug auf eine Mondbasis gezeigt.
Im Spielfilm The Core – Der innere Kern aus dem Jahre 2003 kommt das Shuttle Endeavour infolge von Veränderungen des Magnetkerns der Erde beim Landeanflug vom Kurs ab und muss im Kanalbett des Los Angeles River notlanden.
Im Film Superman Returns aus dem Jahr 2006 spielt ein neu entwickeltes Shuttle namens Genesis mit, das vom Rücken eines Verkehrsflugzeuges starten kann.
Im Spielfilm Invasion aus dem Jahre 2007 wird zu Beginn der Absturz eines Shuttles mit Namen Patriot gezeigt, der dem Columbia-Absturz ähnelt.
Im Film Gravity aus dem Jahre 2013 wird ein Shuttle mit Namen Explorer in der Erdumlaufbahn während des Versuchs, das Hubble-Teleskop zu reparieren, durch Satellitentrümmer zerstört.
Der Spielfilm The Challenger von 2013 befasst sich mit den Schwierigkeiten der Untersuchung der Challenger-Katastrophe des Jahres 1986.
Dennis R. Jenkins: Space Shuttle: The History of the National Space Transportation System. Midland Publishing, 2006, ISBN 978-1-85780-116-3
Pat Duggins: Final Countdown: NASA and the End of the Space Shuttle Program University Press of Florida, 2009, ISBN 978-0-8130-3384-6
Space Shuttle Geo 2/1978, Seite 104–120 Verlag Gruner + Jahr, Hamburg, Bericht von Michael Collins, der als Steuermann des Apollo-11-Unternehmens, am 21. Juli 1969 erstmals Menschen auf den Mond brachte.
Space Shuttle. In: Bernd Leitenberger: US-Trägerraketen, Edition Raumfahrt, 2. Auflage von 2016, ISBN 978-3-73923-547-9, S. 629–691
NASA: Space Shuttle operational flight rules Einsatzregeln für Space Shuttle Flüge (englisch, pdf 5,3 MB)
Das Buch: o.A.Space Shuttle, Prepared by LYNDON B. JOHNSON SPACE CENTER, Scientific and Technical Information Office National Aeronautics and Space Administration, Washington, D.C. 1976, NASA-SP-407, bei NASA History Online (englisch).
Das Buch: Howard Allaway Space Shuttle At Work, Scientific and Technical Information Office National Aeronautics and Space Administration, Washington, D.C. 1979, NASA-SP-432/NASA-EP-156, bei NASA History Online (englisch).
Korb, Morant, Calland und Thatcher: Das Hitzeschutzschild der Weltraumfähren. Physik in unserer Zeit, 16. Jahrgang 1985, Seite 78–85

Als Spam [spæm] oder Junk (englisch für ,Müll') werden unerwünschte, in der Regel auf elektronischem Weg übertragene Nachrichten (Informationen) bezeichnet, die dem Empfänger unverlangt zugestellt werden und häufig werbenden Inhalt enthalten. Dieser Vorgang wird Spamming oder Spammen genannt, der Verursacher Spammer.
SPAM war ursprünglich ein Markenname für Dosenfleisch, der bereits 1936 entstanden ist aus SPiced hAM, fälschlicherweise auch Spiced Pork And Meat/hAM oder Specially Prepared Assorted Meat genannt. Während der Rationierung im Krieg war Spam eines der wenigen Nahrungsmittel, die in Großbritannien praktisch überall und unbeschränkt erhältlich waren. Die Omnipräsenz dieses Fleisches, ähnlich wie später die unerwünschten Botschaften (zum Beispiel als E-Mails), förderte die Entwicklung des Begriffs. Als Synonym für eine unnötig häufige Verwendung und Wiederholung wurde der Begriff durch den Spam-Sketch der englischen Comedyserie Monty Python’s Flying Circus geprägt: In einem Café besteht die Speisekarte fast ausschließlich aus Gerichten mit Spam. Jede Erwähnung des Wortes führt erschwerend dazu, dass eine Gruppe Wikinger lauthals ein Lied anstimmt, dessen Text auch fast nur aus dem Wort Spam besteht und so jede normale Kommunikation unmöglich macht. Selbst im Abspann der Episode wimmelt es von "spam".
Die Nutzung des Begriffs Spam im Zusammenhang mit Kommunikation hat ihren Ursprung wahrscheinlich in den Multi User Dungeons, also textbasierten Computer-Rollenspielen für mehrere Mitspieler, in den 1980er Jahren. Dort bezeichnete Spam zunächst nicht Werbung, sondern das von manchen Nutzern praktizierte massenhafte Überschwemmen des Text-Interfaces mit eigenen Botschaften.
In den Zusammenhang mit Werbung wurde das Phänomen Spam zum ersten Mal im Usenet gebracht. Dort bezeichnet man damit mehrfach wiederholte Artikel in den Newsgroups, die substanziell gleich sind oder für dieselbe Dienstleistung werben.
Die erste Spam-E-Mail wurde wohl am 3. Mai 1978 mit einem Werbeinhalt der Firma DEC versendet, allerdings erst im Jahr 1993 als solche bezeichnet.
Spam verursacht im System der weltweiten Kommunikation erheblichen Schaden. Dieser ist vor allem auf die zusätzliche Datenmenge und den Aufwand der damit verbundenen Bearbeitung zurückzuführen.
Da Unternehmen und Internetdienstanbieter ihre Leitungen typischerweise nicht nach Zeit, sondern nach übertragener Datenmenge abrechnen, entstehen Kosten für jedes Byte Spam, das übertragen wird.
Die Bearbeitung der Mails kann zu einem Ausfall oder zu einer Verlangsamung des erwünschten Mailverkehrs führen. Die Kompensation der Belastung erzeugt wiederum Kosten für neue leistungsfähigere Hardware.
Bei Fax-Spam können Kosten durch den Verbrauch von Papier und Tinte beziehungsweise Toner entstehen.Durch Spam entsteht allein in den Vereinigten Staaten pro Jahr ein Schaden von 22 Milliarden US-Dollar. Nach einer 2009 erstellten Studie verbrauchen 62 Billionen Spam-Mails jährlich zirka 33 Milliarden Kilowattstunden Energie sowie 100 Milliarden Stunden Arbeitszeit zum Sichten und Löschen der Spam-Mails. Demnach macht Spam mittlerweile je nach Schätzung 89 bis 97 % des gesamten E-Mail-Volumens aus.
An erster Stelle ist wegen des großen Umfangs und des daraus resultierenden Bekanntheitsgrades die Unverlangte Massen-E-Mail (kurz UBE, von englisch „Unsolicited Bulk E-Mail“) zu nennen. Es handelt sich dabei um E-Mails, die an eine große Anzahl Empfänger verschickt werden. Häufig handelt es sich dabei um E-Mail-Marketing-Aktionen – missionierende oder volksverhetzende E-Mails und Kettenbriefe gehören aber ebenfalls in diese Kategorie.
Durch die Fülle der Varianten haben sich für einige besonders häufige Typen eigene Begriffe herausgebildet, wie Scam, Phishing, Joe-Job, Hoax und Aktienspam.
Die unverlangte kommerzielle E-Mail (kurz UCE, von englisch „Unsolicited Commercial E-Mail“) ist eine E-Mail mit kommerziellen Inhalten, die an Empfänger (auch wenige oder einzelne) verschickt werden. UCE ist in Deutschland unter bestimmten Umständen legal, siehe Rechtslage in Deutschland. Typische Beispiele für UCE sind dubiose oder besonders günstig erscheinende Angebote für Sex, Potenzmittel, Pornographie, Penisvergrößerung, illegale Online-Glücksspiel-Casinos, gefälschte Uhren, Lebensverlängerung, Software, Markenprodukte, Finanzdienstleistungen oder Medikamente usw.
Als kollateraler Spam oder Backscatter werden E-Mails bezeichnet, die als Antwort auf eine eingehende E-Mail erzeugt und einem unbeteiligten Dritten zugestellt werden.
Auslöser von kollateralem Spam sind besonders häufig Malware- oder Spam-Mails, da hier in der Regel gefälschte Absender benutzt werden.
Wenn E-Mails mit gefälschter Absender-Adresse (der Adresse des unbeteiligten Dritten) verschickt werden, das empfangende E-Mail-System diese E-Mail zunächst annimmt und daraufhin eine Unzustellbarkeitsnachricht, eine Abwesenheitsnachricht oder ähnliches an den vermeintlichen Absender schickt.
Kollateraler Spam wird auch von Empfängern erzeugt, die in Verkennung der Lage den vermeintlichen Absender einer Spam-Mail oder eines Virus mit Beschwerde-E-Mails oder E-Mail-Bomben eindecken.Siehe auch: Backscatter (E-Mail)
Mitte der 1990er Jahre, als nur die wenigsten Menschen und Unternehmen eine E-Mail-Adresse hatten und schon allein daher massenhafter E-Mail-Versand noch nicht möglich war, fand das Wort Spam seinen Weg ins Usenet. Es sollte die Tätigkeit Einzelner bezeichnen, ihre immer gleichlautende Werbung in tausende von Newsgroups zu posten, ohne sich um die thematische Zweckentfremdung zu scheren oder sich für die nachfolgenden Diskussionen zu interessieren.
Der allererste Spam, der extrem viele Newsgroups verunreinigte, war 1994 eine Werbekampagne des Rechtsanwaltsbüros Canter & Siegel (USA), die dafür warb, bei der Teilnahme an der Verlosung von Green Cards behilflich zu sein.
Beim Suchmaschinen-Spamming wird versucht, über Einträge in eigenen oder fremden Webseiten die Bewertungs-Algorithmen von Suchmaschinen positiv zu beeinflussen.
Referrer-Spam ist eine Sonderform des Suchmaschinen-Spamming. Hierbei werden Webseiten massenhaft aufgerufen, damit diese als Referrer in den Logdateien von Webservern der angegriffenen Webseiten auftauchen.
Als Spam over Internet Telephony, kurz SPIT, werden unerwünschte und automatisch eingespielte Anrufe per VoIP bezeichnet. Dabei wird (bei Verwendung des SIP-Protokolls mittels INVITE-Nachrichten) versucht, automatisiert Telefonverbindungen aufzubauen. Sofern der angerufene Teilnehmer antwortet, werden die Audiodaten (z. B. eine aufgezeichnete Werbenachricht) über das RTP-Protokoll eingespielt.
Auch die Kommunikation per Mobiltelefon wird von Spam beeinträchtigt. Unerwünschte Kurzmitteilungen oder Anrufe werden als (Mobile) Phone Spam, teils auch als Spam over Mobile Phone (SPOM) bezeichnet. Durch verstärkten Einsatz von Mobile-Marketing zur Marktforschung und durch unerwünschte Kurzmitteilungen erreicht Mobile Phone Spam in Japan bereits bis zu 90 % des elektronischen Nachrichtenaufkommens. Eine Variante sind sogenannte Spam-  oder Ping-Anrufe, die nur Sekundenbruchteile dauern und den Angerufenen zum teuren Rückruf eines Mehrwertdienstes verleiten sollen.
Weitere Formen von Spam sind Spam over Instant Messaging, kurz SPIM, das Protokolle wie z. B. IRC, ICQ oder den Windows-Nachrichtendienst benutzt oder Spam over Presence Protocol, kurz SPPP.
Um unerwünschte E-Mail-Werbung zu versenden, wird lediglich ein E-Mail-Programm benötigt, das Spam-Mails mit den Empfängeradressen versieht, sowie ein SMTP-Relay-Server, der diese Mails dann an die Empfänger versendet. Da jede E-Mail viele Adressdaten enthalten kann, wird für die Übertragung an den Relay-Server nur eine vergleichsweise geringe Bandbreite benötigt, ein einfacher Internetzugang und ein durchschnittlicher Rechner reichen.
In der Vergangenheit wurden häufig offene Mail-Relays als Relay-Server verwendet, also schlecht konfigurierte Mailserver missbraucht. Dieses Vorgehen hat für den Spammer zwei wesentliche Vorteile: Der Absender wird verschleiert und die eigenen Ressourcen werden geschont. Dank Realtime Blackhole Lists hat die Zahl offener Mail-Relays inzwischen stark abgenommen.
Damit der Benutzer nicht von unerwünschten Personen belästigt wird, bieten die meisten Instant Messenger die Möglichkeit, nur Nachrichten von genehmigten Personen zuzustellen. Auch Chaträume können so eingestellt werden, dass nur Mitglieder sich unterhalten dürfen.
Da das Versenden unerwünschter Werbung in den meisten Ländern wie auch in Deutschland illegal ist, kann der Versender verklagt werden, wenn er rückverfolgbar ist. So lässt sich über die IP-Adresse des Versenders die Identität des Versenders herausfinden, wenn der Internetprovider die zugeteilte IP-Adresse mit Zuordnung des Kunden speichert.
Generell können an jeder Instanz, die Spam erzeugt oder transportiert, Maßnahmen ergriffen werden, die das Spam-Aufkommen verringern.
Derzeit wird Spam hauptsächlich durch Spam-Filter bekämpft. Neuere Verfahren schlagen vor, Spam durch Korrekturen im Simple Mail Transfer Protocol (SMTP) oder im Domain Name System (DNS) zu bekämpfen. Vereinzelt finden sich auch Vorschläge, Spammern das Sammeln der Empfängeradressen zu erschweren, was aber aufgrund des existierenden Adresshandels nur das Sammeln von Adressen im Internet verhindert, nicht aber die Nutzung von aus anderen Quellen (zum Beispiel Preisausschreiben oder Online-Bestellungen) stammenden Adressen. Eine wirksame Methode stellt auch das sogenannte Greylisting dar. Hierbei kann Spam um etwa 95 % reduziert werden, der Nachteil liegt aber darin, dass eingehende Nachrichten nur mit Verzögerung eintreffen.
Ist zur Anmeldung bei einem Dienst die Angabe einer E-Mail-Adresse erforderlich, kann eine Wegwerf-E-Mail mit einem internen Zähler (zum Beispiel von Spamgourmet) und einer zeitlich beschränkten Gültigkeit oder auch eine Alias-Adresse verwendet werden, um die eigene Haupt-E-Mail-Adresse zu verbergen.
Da die meisten E-Mail-Adressen aus dem Internet von sogenannten Address-Harvestern automatisch aus den Newsgroups und Webseiten extrahiert werden, verspricht es einigen Erfolg, dort keine Adressen zu nennen oder die Adressen so zu verschleiern, dass sie von den Address-Harvestern nicht gefunden werden.
Dazu werden die Adressen so verändert, dass sie nur von Menschen, nicht aber von Maschinen verstanden werden. Beispielsweise wird statt Paul@example.org die Adresse PaulXYZ@example.org (entferne XYZ) angegeben. Einfache Robot-Programme erkennen die Manipulation nicht – die E-Mail-Adresse Paul@example.org bleibt UBE-frei.
Fälschungen im Domainteil einer E-Mail-Adresse (also hinter dem @-Zeichen) sind auch möglich. Um absichtlich, und für Postmaster leichter zu erkennen, eine ungültige Adresse zu verwenden, wurde die Top Level Domain (TLD) .invalid erfunden.
Häufig wird auch einfach eine Mitarbeiterliste geführt und oben auf der Seite ein Hinweis angebracht: die E-Mail-Adressen der jeweiligen Personen entsprechen dem Schema Vorname.Nachname@example.com.
Die häufig empfohlene Unicode-Kodierung der Zeichen in der Form &#x61;&#x40;&#98;&#x2e;&#x63; und auch das Ersetzen des @ durch (at), [at] oder ähnlichen stellt für Adresssammler kaum ein Hindernis dar, da beispielsweise der Kommandozeilen-fähige Browser Lynx die Adressen korrekt auslesen (lynx -dump <url> | grep @) bzw. nach verschiedenen Schreibweisen des @ suchen kann. Allerdings hilft es gegen viele einfach gestrickte Adress-Sammler.
Problematisch bei diesen Maßnahmen ist, dass viele Benutzer Mailprogramme verwenden, die ein einfaches Löschen von XYZ aus der Mailadresse nicht zulassen.
Im Header von Usenet-Artikeln, das heißt in den Einstellungen des Newsreaders, verstößt diese Maßnahme gegen RFC 1036 und RFC 2822.
Zudem wird berechtigt die Ansicht vertreten, das Verfälschen von E-Mail-Adressen bekämpfe nicht die Ursachen von Spam, sondern treffe lediglich Anwender und unbeteiligte Dritte: Der antwortende Empfänger hat zusätzlichen Aufwand zum Ändern der Adresse, zudem können Dritte belästigt werden, wenn die manipulierte Adresse real existiert (aber einem Dritten gehört, nicht dem Absender).
Häufig wird auch eine Verschlüsselung mittels JavaScript vorgeschlagen. Um dies zu umgehen, muss der Harvester einen JavaScript-fähigen Browser integrieren. Dies schließt allerdings Nutzer von Browsern, die kein JavaScript unterstützen, oder mit deaktiviertem JavaScript aus. Hier ein Beispiel
<a href="javascript:location='mailto:'+'info'+ /* Kommentar */ '@ex' + 'ample.org'">E-Mail senden</a>
Eine mit modernen Browsern wesentlich einfachere Verschlüsselung zumindest in Webseiten kann über CSS erfolgen, zum Beispiel in der Form:
Die erzeugte Darstellung ist auch für Menschen ohne Browser mit CSS leicht lesbar, während Adresssammler bisher aus Effizienzgründen auf die Auswertung von CSS verzichten und daher nicht die korrekte Adresse herausfiltern.
Des Weiteren ist es möglich, die Mailadresse graphisch, zum Beispiel in Form eines Bildes zu speichern. Auch hier ist sie für die meisten Menschen lesbar, automatische Sammler dagegen lesen – bislang – noch keine Adressen aus Schrift in Bildern. Zu beachten ist dabei, dass Textreader, wie sie zum Beispiel von Sehbehinderten verwendet werden, so dargestellte Adressen nicht lesen können (siehe dazu auch: Barrierefreies Internet).
Es gibt noch weitere Möglichkeiten, die teilweise größere Sicherheit oder Flexibilität als obige Lösungen bieten, sich jedoch aufgrund der technischen Voraussetzungen nur in Einzelfällen anwenden lassen. Beispiel: Hat man administrativen Zugriff auf den Mailserver und ist die Webseite dynamisch generierbar, kann man beim Aufruf der Website eine zufällige, nicht vorhersagbare E-Mail-Adresse generieren, zum Beispiel djfh7c36544563@example.com, und diese temporär im Mailserver als gültig eintragen. Nach einer vorgegebenen Zeit wird diese wieder automatisch aus dem Mailserver entfernt. Somit kann ein Besucher der Website über die barrierefrei angebotene E-Mail-Adresse Kontakt aufnehmen, eine spätere Zusendung von Spam wird jedoch scheitern, da die Adresse nicht mehr gültig ist.
Mailhide von Google bietet einen Dienst namens reCAPTCHA an, der erst nach Eintippen zweier Worte, an denen herkömmliche OCR-Software vorher gescheitert war, die Adresse freigegeben wird. Die Richtigkeit wird statistisch und durch Mischen von neuen mit bereits gelösten Wörtern ermittelt. Das Projekt hilft nebenbei bei der Digitalisierung alter Bücher. Es werden auch für blinde Menschen Rätsel angeboten, bei denen alte Tonbänder digitalisiert werden.
Eine hohe Sicherheit bieten sogenannte Captchas, mittels derer Menschen von Maschinen unterschieden werden sollen. So wird vorgeschlagen, die E-Mail-Adresse in einem Bild anzugeben oder in einer Audio-Datei zu buchstabieren. Allerdings sind diese Lösungen weder besonders komfortabel noch barrierefrei. Auch bei einer Angabe als Audio-Datei und Bild sind sie zum Beispiel für Taubblinde unverständlich, und selbst für Sehende sind diese Bilder aufgrund von Sehfehlern oder Farbwahrnehmungsstörungen nicht immer lesbar.
Als Alternative zu Captchas bieten sich unter anderem Honeypots und Zeitmessungen an, um Spam zu bekämpfen und trotzdem barrierefreie Angebote zu betreiben.Im Usenet und auf Mailinglisten kann auch im From-Header eine nicht gelesene Müll-Adresse und im Reply-To die eigentliche Adresse eingetragen werden. Damit kommen Antworten an der korrekten Adresse an, die Täter scannen aber normalerweise nur die From-Adressen.
Auf Webseiten stellen Kontaktformulare (CGI oder PHP) eine Alternative zur Angabe der E-Mail-Adresse dar. Sie bieten dem Leser eine Möglichkeit zur Kontaktaufnahme mit dem Ansprechpartner, ohne dass eine E-Mail-Adresse angegeben und somit das Ernten der E-Mail-Adresse umgangen wird. Dies geschieht auf Kosten des Komforts, da der Sender nicht seine gewohnte Schreibumgebung verwenden kann (z. B. Tastaturkürzel, Textblöcke, Kopie an sich selbst oder andere Empfänger).
Um E-Mail-Adressen nicht unnötig zu verbreiten, kann es sinnvoll sein, E-Mails, die für eine geschlossene Gruppe von Empfängern bestimmt sind, an niemanden (Undisclosed recipients) oder an sich selbst zu adressieren und die eigentlichen Empfänger in das BCC-Feld zu setzen. Diese erhalten dann eine sogenannte Blindkopie (BCC, Blind Carbon Copy). Die Adressen im BCC-Feld werden den Empfängern nicht übermittelt.Allerdings hat diese Methode auch Nachteile. Einige Spamfilter bewerten Mails, die den Empfänger per BCC erreichen, negativ, das heißt, sie sehen den Versand via Blind Carbon Copy als ein mögliches Kriterium für Spam. Wer regelmäßig Mails an einen großen Empfängerkreis schickt, sollte daher die Einrichtung einer Mailingliste erwägen.
Kann der einzelne Benutzer nur verhindern, dass er selbst UBE erhält, bietet sich für Administratoren von Mailservern die Möglichkeit, die Verbreitung von UBE einzuschränken. Dies beginnt bei der richtigen Konfiguration des Mailservers, der es nur autorisierten Benutzern gestatten sollte, E-Mails zu verschicken.
Auf der Gegenseite kann der Mailserver den Empfang von E-Mails, die von sogenannten Open Relays stammen, über die jeder unautorisiert Mails einliefern kann, ablehnen. Mehrere Organisationen, zum Beispiel die Open Relay Database, bieten Listen solcher fehlkonfigurierter Mailserver an (RBL), die der Serveradministrator zur Überprüfung nutzen kann. Da sich offene Relais immer seltener finden, ist eine mittlerweile weitaus effektivere Möglichkeit, das Anliefern durch Einwahlzugänge nur nach Authentifizierung zu gestatten. Auch hierfür gibt es öffentliche Datenbanken (DialUp Lists (DUL)).
Eine breite Unterstützung von SMTP-Message-Submission durch Mailserverbetreiber könnte mittelfristig helfen, die Verbreitung von Spam und Malware über Botnetze einzudämmen. Wenn die Mailprogramme der Endanwender ihre Mail ausschließlich über den Message-Submission-Port 587 einliefern dürfen, können die Betreiber von Mailservern (ISPs) die Einlieferung von Spam erheblich erschweren: Von extern via SMTP (TCP Port 25) eingehender Mail-Datenverkehr kann stark gefiltert werden, insbesondere kann Mail von Dial-Up-Netzen (vorwiegend von trojanisierten PCs) abgewiesen werden. Gegen den Spamversand von trojanisierten PCs selbst wird diese Maßnahme nur kurzfristig helfen, da die Botnetze bei Bedarf auch über Message-Submission versendet werden können. SMTP-Message-Submission wird bereits von vielen Mailserverbetreibern unterstützt.
Sogenannte Teergruben können das Abliefern von UBE nicht verhindern, bieten aber eine Gegenmaßnahme gegen den Versandmechanismus der Täter, indem sie mit äußerst langsamen Reaktionen eine UBE-versendende Gegenstelle bei der Arbeit aufhalten. Die Kommunikation zwischen dem empfangenden System und dem UBE-Sendesystem wird quasi zähflüssig wie Teer, anstatt nur Sekundenbruchteile dauert der Versandvorgang mehrere Minuten und macht es damit unmöglich, in kurzer Zeit sehr viele Spam-Mails auszuliefern.
Bei automatischen White/Blacklist-Filtern antwortet das Mailsystem des Empfängers zunächst allen unbekannten Versendern und fordert diese auf, sich beim Mailsystem zu registrieren. Durch eine Aktion (z. B. eine Zahl aus einem generierten Bild abschreiben) bestätigt der Sender, dass er ein Mensch ist und ernsthaftes Interesse hat. Wenn er korrekt antwortet, bekommt der Empfänger die bis dahin aufgehobene Mail zugesandt. Der Versender wird daraufhin in die Whitelist aufgenommen. Handelt es sich um Spam, kann der Absender nachträglich aus der Weißen Liste auf die Schwarze Liste verschoben werden.
Es gibt noch weitere Registrierungsmöglichkeiten im W/B-Filter-Verfahren, zum Beispiel über einen URL mit ID (Beispiel: http://www.example.com/mail.php?ID=20032311-021).
Systeme der Art, die die Reaktion des Sendenden erfordert, werden auch als Challenge-Response-System bezeichnet. Viele Anwender und (vor allem) Administratoren sehen sie jedoch als kein zweckdienliches System zur UBE-Vermeidung an, und zwar aus folgenden Gründen:
Die Absenderadresse einer UBE wird im günstigsten Fall mit einer ungültigen Adresse, im Normalfall mit der Adresse eines Unbeteiligten versehen. Im Falle einer ungültigen Adresse führt der Versuch der Zustellung der Challenge-Mail zu einem Bounce, damit also zu einer Ressourcenverschwendung. Ist die Adresse gültig, so wird dieser vom Challenge-Response-System belästigt, womit der Benutzer des Systems technisch selbst zum Täter wird (kollateraler Spam).
Versendet der Benutzer eines Challenge-Response-Systems selbst eine Mail an ein Challenge-Response-System (z. B. eine Mailingliste mit Confirmed Opt-in), kommt es zu dem Effekt, dass beide Systeme jeweils auf die Antwort des anderen Systems warten (die Mailliste auf die explizite Bestätigung, dass die E-Mail-Adresse in die Liste aufgenommen werden soll, das System des Benutzers, dass sich die Mailliste als regulärer Benutzer authentifiziert). Die Aufnahme eines solchen Benutzers erfolgt dann meist durch manuelles Bearbeiten des Maillistenbetreibers, was für diese einen entsprechenden Mehraufwand bei der Administration zur Folge hat.
Ein Benutzer eines CR-Systems, der an einer Mailliste teilnimmt, verursacht im Allgemeinen eine Vielzahl von Challenge-Mails, da die Absenderadresse bei Mails an die Mailliste im Allgemeinen nicht verändert wird. Dies hat zur Folge, dass sich jeder Maillistenbeteiligte bei jedem einzelnen Benutzer eines solchen Systems authentifizieren muss, damit dieser die jeweilige Mail von der Mailliste erhalten kann. Da dies ab einer gewissen Anzahl von Benutzern von CR-Systemen innerhalb einer Mailliste die Akzeptanzschwelle vieler Benutzer überschreitet, führt dies meist dazu, dass sich die Benutzer solcher Systeme früher oder später aus den Diskussionen ausschließen.
RBL-Server sind Server, auf denen die Adressen bekannter Spamversender in Echtzeit gesammelt werden. Der Server für eingehende Mail kann diese Server anfragen, bevor er eine Mail annimmt. Wenn sich der Absender in einem IP-Bereich befindet, aus dem häufig Spam versendet wird, wird die Annahme verweigert. Dies geschieht oft unabhängig davon, ob von der speziellen Absenderadresse tatsächlich Spam versendet oder versandt wurde. Ein bekannter, frei zugänglicher RBL-Server ist www.spamhaus.org.
Graue Listen nutzen die Tatsache aus, dass Spamschleudern häufig das Mailprotokoll nicht korrekt einhalten. Wenn eine Mail eingeht, wird die Annahme zunächst mit einer vorgetäuschten Fehlermeldung verweigert und die Absenderadresse kommt vorübergehend auf eine graue Liste. Wenn der Absender nach einer bestimmten Zeit die Sendung wiederholt, gilt er als konform und wird von der grauen Liste entfernt; anderenfalls wird er ignoriert. Auf Wunsch kann ein einmal als konform erkannter Absender in eine weiße Liste eingetragen werden und wird in Zukunft direkt akzeptiert. Es kann allerdings auch passieren, dass seriöse Absender bei diesem Verfahren durchfallen, wenn deren Mailserver falsch konfiguriert sind.
Inzwischen gibt es eine Vielzahl verschiedener Spamfilter-Techniken zur automatischen Erkennung und Entfernung von Spam im Postfach. Einige E-Mail-Programme wie z. B. der Mozilla Thunderbird oder Microsoft Outlook haben integrierte Spamfilter, die Werbemails von vornherein aussortieren.
Allerdings leiden die Filter unter ihren Fehlerraten: So werden häufig Spam-Mails nicht zuverlässig erkannt und gelangen trotzdem in den Posteingang, man spricht von false negatives. Auch der umgekehrte Fehler ist möglich: Erwünschte Mails können durch zu strenge Filter als Spam eingestuft werden (sogenannte false positives) und erreichen so den Empfänger unter Umständen nicht oder nur verzögert.
Lediglich gut konfigurierte Spamfilter, die individuell auf den Benutzer oder eine Benutzergruppe zugeschnitten sind, haben hohe Erfolgsquoten. In solchen Fällen lassen sich false positives fast vollständig ausschließen und false negatives auf 10 % bis unter 1 % drücken. Allerdings ist der Einmalaufwand dafür hoch und erfordert eine gewisse Erfahrung. Zudem muss der Filter ständig durch immer neue und verbesserte Methoden an die immer neuen Methoden der Spammer angepasst werden.
Filter haben das Manko, dass durch die besprochenen Fehlerraten (die immer vorhanden sind) der Benutzer die E-Mails, die herausgefiltert wurden, im Zweifelsfall noch einmal kontrollieren muss und sich damit der eigentliche Zweck des Filters darauf beschränkt, eine Vorauswahl für den Benutzer darzustellen. Umgekehrt muss dem Empfänger klar sein, dass auch die manuelle Filterung von E-Mails ein erhebliches Potenzial für false positives aufweist. Es kann – speziell bei hohem E-Mail-Aufkommen – effizienter sein, sich auf einen guten Spamfilter zu verlassen, als von Hand zu filtern.
Rechtlich ist das Filtern unter bestimmten Umständen kritisch: Filtert der Provider oder Arbeitgeber ohne Einwilligung des Empfängers, ist dies nach verbreiteter Rechtsprechung ein Straftatbestand (siehe dazu unten die rechtswissenschaftliche Literatur). Dieses Problem lässt sich in gewissen Grenzen umgehen, indem als Spam erkannte E-Mails bereits beim Empfang abgewiesen werden. Die E-Mail gilt dann nach überwiegender Auffassung als nicht zugestellt, der Absender bekommt eine Unzustellbarkeitsnachricht und kann somit das Problem beheben, umgehen oder den Empfänger auf andere Weise kontaktieren.
Wenn die direkte Beschwerde beim Spammer nichts bewirkt, so kann man sich beim Provider des Spammers beschweren. Sollte auch danach die gewünschte Wirkung ausbleiben, bleibt nur der Rechtsweg: Durch die entstehenden Verfahrenskosten und zu zahlenden Ordnungsgelder kann der Versand von Spam möglicherweise langfristig teuer werden. Allerdings verursacht dieser Weg bei manueller Bekämpfung sehr viel Arbeit.
Weniger Arbeit hat man, wenn man die Beschwerden so gut es geht automatisiert, um möglichst viele pro Tag zu bearbeiten. Kritiker (halb-)automatisierter Spam-Reports weisen allerdings zu Recht darauf hin, dass automatisierte oder über entsprechende Dienstleister/Software erzeugte Beschwerden nicht nur oft fehlerhaft sind und daher nicht selten Unbeteiligte treffen, sondern von den Beschwerde-Bearbeitern vieler Provider auch pauschal gelöscht werden. Der Provider kann solche Meldungen auch als Spam einstufen, wenn ihn zu viele von einem Absender erreichen.
Die sinnvollere Alternative kann daher eine von Hand geschriebene Beschwerde sein, die sich auf mehrere zeitnah empfangene Spamsendungen aus gleicher Quelle bezieht.
Zu analysieren ist der Header der E-Mail, der von vielen Mail-Clients nicht automatisch angezeigt wird. Darin ist alles leicht zu fälschen außer den IP-Adressen der MTAs (Mailserver), die die E-Mail transportiert haben. Diese stehen in Headerzeilen, die mit dem Schlüsselwort Received anfangen. Zu welchem Provider die IP-Adresse gehört, kann man mit dem Unix-Befehl whois und dem Whois-Server der zuständigen Registry ermitteln.
Das Format, mit dem die einzelnen Whois-Server antworten, ist nicht einheitlich. Da jeder selber eigene Server aufsetzen kann, können Provider und Täter identisch sein.
Die meisten Provider haben eine eigene Beschwerde-Adresse, die jedoch nicht immer im Whois-Server eingetragen ist. Um zu ermitteln, welches die richtige Beschwerde-Adresse zu einer bestimmten Domain ist, kann man Abuse.net nutzen und dort beim Provider nachsehen.
Möglichkeiten zur Automatisierung dieses Ermittlungs- und Beschwerdeprozesses bieten Dienstleister wie beispielsweise SpamCop.
Seit dem 1. Januar 2007 nimmt die Internet-Beschwerdestelle des Bündnispartners eco-Verband Spam-Beschwerden an, und für Rechtsberatungen zum Thema Spam stehen seitdem die Verbraucherzentralen zur Verfügung.
Bis zum 31. Dezember 2006 hatte sich der Verbraucherzentrale Bundesverband e. V., bis zum 31. Dezember 2006, in einem Versuchsprojekt um die Verfolgung und Ahndung unerwünschter E-Mails gekümmert. Am 1. Juli 2005 hatte das vom Bundesministerium für Verbraucherschutz, Ernährung und Landwirtschaft (BMVEL) zusammen mit dem Verbraucherzentrale Bundesverband e. V. ein mittlerweile wieder eingestelltes Projekt einer Beschwerdestelle zur Bekämpfung von Spam gestartet. Unter beschwerdestelle@spam.vzbv.de konnten Verbraucher dem VZBV per Mail unerwünscht eingetroffenen Spam übermitteln. Der VZBV überprüfte diese Fälle und ging in geeigneten Fällen juristisch gegen Spam-Versender und deren Auftraggeber vor. Der VZBV arbeitete hierzu mit anderen Verbraucherzentralen auf der ganzen Welt zusammen. Er hatte sich zum Ziel gesetzt, Spam mit allen juristischen Mitteln unprofitabel zu machen. Der Service war kostenlos und war nur für Privatpersonen gedacht. Eine Registrierung war nicht nötig. Die Sache zeigte Wirkung. Besonders Spammer aus Deutschland und dem Rechtsgebiet der EU konnten sich nicht mehr in der scheinbaren Anonymität des WWW verstecken. Doch auch international wurde der VZBV dank mehreren Kooperationen tätig. Der Vorteil gegenüber Spam-Filtern lag hierbei darin, dass die Versender von Spam belangt werden, Spammen illegalisiert wird und somit langfristig das Versenden von Spam zurückging. Der Nachteil war der, dass die Spam-Mails vorerst weiter im Postfach landeten und das Weiterleiten inklusive des erweiterten Headers zeitaufwendig war.
Auch eBay oder PayPal verfolgen – natürlich primär im eigenen Interesse – Spam-Versender. Diese werden auf Unterlassung verklagt, mit dem Ziel, dass es keine Spam-Mails über die Firma mehr gibt. eBay und PayPal gehen jedem Hinweis nach und verfolgen die Versender von Spam-Mails weltweit. Dazu muss man nur Spam-Mails, die sich für eBay bzw. PayPal ausgeben bzw. darauf berufen, mit dem erweiterten Header an folgende Adresse weiterleiten: spoof@ebay.de oder spoof@paypal.de. Man erhält dann eine Antwort, ob die Mail echt war oder nicht, sowie allgemeine Informationen zum Thema.
Neben technischen Möglichkeiten gibt es noch weitere Methoden, den Täter an der Ausführung seiner Geschäfte zu hindern. So können Empfänger von UCE z. B. zum Schein mit falschen persönlichen Daten auf die angebotenen Geschäfte eingehen. Dies bewirkt beim Händler, dem der Täter zuarbeitet, eine Flut von Fehlern bei Bestellungen von Kunden, die vom Täter angeworben wurden. Das führt möglicherweise sogar zur Beendigung des Geschäftsverhältnisses. Dieses Vorgehen lässt sich automatisieren (beispielsweise mit Proxys), ist rechtlich aber höchst fraglich. Absendern von Phishing-E-Mails kann man auch die Arbeit erschweren, indem man falsche Bankkontodaten eingibt.
Absender von Nigeria-Connection-Mails (Vorschussbetrüger) kann man einfach durch Antworten und das Führen zielloser Diskussionen beschäftigen, das sogenannte Scambaiting. Dies bindet beim Täter Zeit, ist aber unter Umständen gefährlich, da man Kriminelle stört, die in der Regel über Verbindungen nach Europa und Nordamerika verfügen. Scambaiting sollte nur von erfahrenen Personen oder unter ihrer Anleitung durchgeführt werden, um die Übermittlung von Daten, die zur Identifizierung führen können, zu vermeiden. Hierzu zählen insbesondere persönliche Daten wie Name, Adresse und Bankverbindung, aber auch eigene Bilder, Nicknames in Foren und Chats, IP- oder E-Mail-Adressen und Telefonnummern.
Australien hat eine sehr komfortable und effektive Methode zur Bekämpfung entwickelt. Den Australiern wird das kostenlose Programm SpamMatters zur Verfügung gestellt, welches die vom Benutzer als Spam gekennzeichneten E-Mails an die australische Telekommunikationsbehörde ACMA übermittelt. Diese identifiziert den Spammer und informiert die zuständigen Strafverfolgungsbehörden.
Das unmittelbarste und wirksamste Instrument ist das Canceln. Damit veranlasst man alle entsprechend konfigurierten Newsserver, den Spam zu löschen. Diese Maßnahme greift umso erfolgreicher, je schneller sie auf Spam reagiert, weil sie nur denjenigen zugutekommt, die den Spam noch nicht mit dem Newsreader vom Newsserver heruntergeladen haben. Das Canceln von Spam erfordert die sorgfältige Einhaltung vielfältiger Regeln, man kann dabei sehr viel falsch machen.
Beschwerden an die Newsprovider der Spammer können bewirken, dass diesen die Nutzungsmöglichkeit des jeweiligen Newsservers entzogen wird.
Sehr selten werden Newsprovider, die auf Beschwerden nicht reagieren, mit einem Usenet Death Penalty (UDP) belegt, welches in zwei Formen geschehen kann:
Passives UDP: Die Administratoren der wichtigsten Newsserver einigen sich darauf, dass alle Usenet-Artikel, die über die Newsserver des schwarzen Schafes gelaufen sind, nicht weitergeleitet werden und damit verschwinden.
Aktives UDP: Die Spam-Canceller verständigen sich darauf, alle Artikel, die von den Newsservern des schwarzen Schafes aus ins Usenet gelangt sind, zu canceln, so als seien sie Spam.
Newsgroups, die sex in ihrem Namen tragen, lassen sich umbenennen. Dies ist sehr erfolgreich mit der ehemaligen Newsgroup de.talk.sex geschehen, die heute de.talk.liebesakt heißt und damit kaum noch Spam anlockt.
NoCeM als Alternative zum Canceln: Während das Canceln erfordert, jedem einzelnen Spam-Artikel eine eigene Cancel-Message hinterherzuschicken, kommt dieses Verfahren mit Steuernachrichten aus, die gleich ganze Listen von Spam-Artikeln enthalten. Diese NoCeM-Steuernachrichten werden allerdings nur von speziellen Clients verstanden, die nicht besonders weit verbreitet sind, und sind im Gegensatz zu Cancel-Messages nicht imstande zu vereiteln, dass als Folge von Spam Diskussionen über den Spam, die zum Thema der jeweiligen Newsgroup gar nicht passen, die Newsgroup unleserlich machen.
Moderierte Newsgroups: Die Beiträge gelangen nicht unkontrolliert ins Usenet, sondern werden von einem Moderator abgefertigt, der Spam abfangen kann. Es gelingt nicht immer, einen Freiwilligen für dieses Amt zu finden. Die ehemals sehr erfolgreiche Stellenanzeigen-Newsgroup misc.jobs.offered musste aus diesem Grund abgeschafft werden.
Serverseitige Maßnahmen: Newsserver-Software lässt sich mit Add-Ons ergänzen, die Spam erkennen und zurückweisen. Dazu gehört z. B. die Software Cleanfeed.
Clientseitige Maßnahmen: Die meisten Newsreader verfügen über ein sog. Killfile, das steuert, was man zu sehen bekommt. Der Bayessche Filter sortiert erwünschte und unerwünschte E-Mails, nach einem Training durch den Benutzer des E-Mail-Clients.
Wegwerf-E-Mail-Adressen: Bei der Verwendung von Wegwerf-E-Mail-Adressen gibt der Benutzer anstelle seiner eigenen Adresse eine temporäre, gültige E-Mail-Adresse an. Der Benutzer hält seine eigentliche Adresse somit anonym und verhindert, dass sein E-Mail-Konto mit Spam zugedeckt wird.
Eine Haftungsfrage für den Versand von E-Mail-Würmern und Trojanern, die den größten Anteil an der UBE nach UCE ausmachen dürften, ist in Deutschland noch umstritten. Unter sehr eingeschränkten Bedingungen sehen einige Autoren zumindest Unternehmen als haftbar an, für Privatpersonen verneint die Literatur überwiegend eine Haftungsverpflichtung. Ein Unterlassungsanspruch gegen versehentliche Wurmversender wurde bislang noch nicht durchgesetzt. Strafrechtlich ist das Erstellen und Verbreiten von Würmern, Viren und Trojanern als Computersabotage relevant. Im Jahr 2005 wurde in Deutschland deswegen ein Schüler als Autor von Netsky und Sasser zu einem Jahr und neun Monaten Haft auf Bewährung verurteilt.Aus unerwünschter E-Mail-Werbung kann sowohl ein wettbewerbsrechtlicher als auch ein privatrechtlicher Unterlassungsanspruch des Empfängers an den Versender erwachsen. Es ist dabei unerheblich, ob und wie häufig der Spammer schon spammte: Ein Unterlassungsanspruch entsteht ab der ersten E-Mail.
Nach ständiger Rechtsprechung der Instanzgerichte und mittlerweile auch des BGH (BGH, Urteil vom 11. März 2004, AZ: I ZR 81/01) zum alten Gesetz gegen den unlauteren Wettbewerb (UWG) ist eine Zusendung von unerwünschten Werbe-E-Mails nach den gleichen Grundsätzen sitten- und damit wettbewerbswidrig, die schon auf die Werbung per Telex, Telefax und Telefon angenommen wurden.
Demzufolge ist es dem Empfänger nicht zuzumuten, Werbung zu tolerieren, in deren Empfang er nicht eingewilligt hat, wenn dadurch auf Seiten des Empfängers Kosten und/oder eine sonstige Störung entstehen.
Das neue UWG (seit 2004) regelt unmissverständlich die Ansprüche, die an E-Mail-Werbung gestellt werden, damit sie wettbewerbsrechtlich einwandfrei ist. Dazu gehört insbesondere, dass der Empfänger in die Zusendung von Werbung per E-Mail vorher eingewilligt hat. Unterlassungsansprüche aus dem UWG stehen allerdings nur Wettbewerbern des Spammers zu, auch wenn der Begriff Wettbewerber weit ausgelegt wird. Dafür wirkt ein wettbewerbsrechtlicher Unterlassungsanspruch auf den gesamten geschäftlichen Verkehr. Der Spammer darf also auch keinem Dritten mehr unerwünschte Werbung zusenden. Würde er dabei erwischt, droht ihm die Zahlung eines Ordnungsgeldes an die Staatskasse oder sogar Ordnungshaft. Tatsächlich wurden schon Ordnungsgelder gegen Spammer verhängt, wenn sie gegen eine gerichtliche Unterlassungsverfügung verstoßen haben.
Weniger umfassend, dafür individuell schützend und ohne Wettbewerber-Position lässt sich auch aus dem allgemeinen Haftungsrecht ein Unterlassungsanspruch gegenüber dem Spammer herleiten. Er konstruiert sich, wie jeder Unterlassungsanspruch in diesem Bereich, aus den §§ 1004 analog und 823 Abs. 1 BGB.
Für Privatanwender wird dann auf das allgemeine Persönlichkeitsrecht, das sich aus dem Grundgesetz herleitet, rekurriert, der geschäftliche Anwender sieht einen ebenfalls grundrechtlich geschützten Eingriff in das Recht am eingerichteten und ausgeübten Gewerbebetrieb. Beides sind sonstige Rechte im Sinne des § 823 Abs. 1 BGB.
Vermehrt wird in letzter Zeit auch diskutiert, den Absender von unerwünschter Werbe-E-Mail strafrechtlich zu verfolgen. Einen Ansatz lieferte dazu die Dissertation Zur strafrechtlichen Bewältigung des Spamming von Thomas Frank. Eine Zusammenfassung davon war in Computer und Recht 2/2004, S. 123ff. abgedruckt. Allerdings ist die Rechtsprechung dazu noch uneinheitlich, insbesondere sehen die Staatsanwaltschaften derzeit noch keinen Handlungsbedarf, da es die Gesetzeslage der Staatsanwaltschaft nicht erlaubt, strafrechtlich ohne Gesetz gegen Spam vorzugehen.
Das seit 1. März 2007 in Kraft getretene Telemediengesetz verbietet in § 6 Abs. 2 das Verschleiern oder Verheimlichen des Absenders und des kommerziellen Charakters der Nachricht. Der Verstoß gegen das Verbot wird als Ordnungswidrigkeit mit einem Bußgeld geahndet.
Der Deutsche Bundestag hatte am 17. Februar 2005 in erster Lesung den Entwurf eines Anti-Spam-Gesetzes beraten. Das Anti-Spam-Gesetz soll das Teledienstegesetz um folgende Regelung erweitern:
„Werden kommerzielle Kommunikationen per elektronischer Post (E-Mail) versandt, darf in der Kopf- und Betreffzeile weder der Absender noch der kommerzielle Charakter der Nachricht verschleiert oder verheimlicht werden. Ein Verschleiern oder Verheimlichen liegt insbesondere dann vor, wenn die Kopf- oder Betreffzeile absichtlich so gestaltet ist, dass der Empfänger vor Einsichtnahme in den Inhalt der Kommunikation keine oder irreführende Informationen über die tatsächliche Identität des Absenders oder den kommerziellen Charakter der Nachricht erhält.“Ein Verstoß gegen diese Regelung soll als Ordnungswidrigkeit mit einer Geldbuße bis zu 50.000 Euro geahndet werden. Die Regelung würde allerdings nur die Irreführung über Absender und Inhalt der Mail verbieten, nicht aber das unverlangte Zusenden von Werbe-E-Mails selbst.
Das Gesetz wurde in der 15. Legislaturperiode des Deutschen Bundestages nicht mehr verabschiedet und konnte in der 16. Legislaturperiode als eigenständiges Gesetz nicht in Kraft treten. Stattdessen wurde eine ähnliche Regelung im neuen Telemediengesetz als § 6 Abs. 2 eingeführt, vgl. den vorangehenden Abschnitt Strafrecht.
Das Gesetz gegen den unlauteren Wettbewerb schützt Verbraucher seit 2004 aber auch unabhängig umfassend vor Belästigung durch unerwünschte Werbung. und wird in Ratgebern zum rechtskonformen E-Mail-Versand als Bezug genannt.
In Österreich war von 1999 bis 2003 für das Versenden von Massen- oder Werbe-E-Mail nach § 101 Telekommunikationsgesetz (TKG) 1997 die vorherige Zustimmung des Empfängers erforderlich (Opt-in), UCE und UBE somit verboten. Die Nachfolgeregelung, § 107 TKG 2003, erlaubte UCE an Unternehmen oder Behörden, mit Einschränkungen auch bei bestehenden Privatkundenbeziehungen, wenn diese weitere Nachrichten ablehnen können (Opt-out). Massen- oder Werbe-E-Mail an Privatpersonen bedarf weiterhin der vorherigen Zustimmung des Empfängers (Opt-in). Seit März 2006 ist der Versand von UCE und UBE (ohne vorherige Zustimmung des Empfängers) wieder generell verboten. Auch eine Mail oder ein Anruf um eine solche Zustimmung einzuholen erfüllt den Tatbestand nach § 107 TKG.
Durch das unerbetene Tätigen eines Anrufs, das unerbetene Schicken eines Telefax oder die Zusendung unerbetener elektronischer Post begeht der Absender eine Verwaltungsübertretung und ist mit einer Geldstrafe in der Höhe von bis zu € 37.000,- zu bestrafen. Für die Anzeige einer Übertretung des § 107 TKG sind die regionalen Fernmeldebüros zuständig. Darüber hinaus können Verletzungen der Impressums- und Offenlegungspflichten sowie Verstöße gegen das Kennzeichnungsgebot von (Direkt-)Werbung mit bis zu 2.180 Euro bzw. 3.000 Euro geahndet werden.
Die Rechtsprechung legt belästigende Werbung als Verstoß gegen das Bundesgesetz gegen unlauteren Wettbewerb (UWG) aus. Aufgrund unerbetener Kommunikation kann daher auf Unterlassung und Schadenersatz geklagt werden.
Eine vorherige Zustimmung für elektronische Post ist ausnahmsweise nicht notwendig, wenn die folgenden fünf Voraussetzungen vorliegen:
der Absender hat die Kontaktinformation für die Nachricht im Zusammenhang mit dem Verkauf oder einer Dienstleistung an seine Kunden erhalten und
der Kunde hat die Zusendung nicht im Vorhinein abgelehnt. Insbesondere darf nicht an Empfänger, die in die Robinsonliste eingetragen sind, gesendet werden. Diese Liste wird bei der Regulierungsbehörde für Telekommunikation und Rundfunk geführt und ist vom Absender immer zu beachten, wenn keine Zustimmung vorliegt.Diese Ausnahme gilt nur für elektronische Post, nicht aber für Telefonate und Faxe. Für diese gilt uneingeschränkt das Zustimmungsgebot.
Laut Gesetz gilt elektronische Post mit mehr als 50 Empfängern als Massensendung, auch wenn sie keine Werbung zum Inhalt hat. Nicht jede Massenmail ist rechtswidrig. Für Interessenvertretungen gibt es gesetzliche Sonderbestimmungen.
Auch die massenhafte Versendung an einen einzigen Empfänger gilt als Massensendung (z. B. Massen-Mails an mehrere Dienststellen eines Empfängers).
Die Zustimmung kann ausdrücklich vom zukünftigen Empfänger erteilt werden, indem er eine Erklärung unterschreibt.
Jede (Direkt-) Werbung in elektronischer Kommunikation muss als solche gekennzeichnet werden. Die Kennzeichnung kann zum Beispiel in der Betreffzeile einer E-Mail vorgenommen werden. Die Worte können frei gewählt werden, jedoch sollte für den Empfänger ersichtlich sein, dass es sich um Direktwerbung handelt.
Nach dem Unternehmensgesetzbuch (§ 14 UGB) und der Gewerbeordnung (§ 63 GewO) müssen alle E-Mails ein Impressum enthalten.
Für Aussendungen, die mindestens vier Mal im Kalenderjahr in vergleichbarer Gestaltung elektronisch verbreitet werden (z. B. E-Mail-Newsletter), ist nach dem Mediengesetz direkt im Newsletter ein Impressum anzugeben. Darüber hinaus ist eine Offenlegung im Newsletter selbst oder per Link auf eine Website anzuführen.
Im übrigen Europa ist die Rechtslage durch die Richtlinie des Europäischen Parlaments und des Rates über die Verarbeitung personenbezogener Daten und den Schutz der Privatsphäre in der elektronischen Kommunikation (2002/58/EG) vom 12. Juli 2002, die bis Ende 2003 von den EU-Mitgliedstaaten in nationales Recht umzusetzen war, im Ergebnis vergleichbar:
Die Zusendung von E-Mail-Werbung ist nur dann erlaubt, wenn der Empfänger vorher eingewilligt hat. Die konkrete Umsetzung in das jeweilige nationale Recht ist in den jeweiligen Ländern unterschiedlich. Eine Übersicht dazu liefert die Dissertation von Björn Bahlmann, Möglichkeiten und Grenzen der rechtlichen Kontrolle unverlangt zugesandter E-Mail-Werbung. Internationale Regelungen und alternative Lösungsmöglichkeiten, die nur direkt beim Verlag erhältlich ist.
In der Schweiz ist der Versand von Spam seit dem 1. April 2007 verboten. Das Fernmeldegesetz verbietet unter Androhung von Geld- oder Freiheitsstrafe, über E-Mail, SMS oder andere Telekommunikationskanäle unaufgefordert Massenwerbung zu versenden oder solche in Auftrag zu geben. Zudem sind die Provider und Telefongesellschaften verpflichtet, so weit bekannt, Namen und Adressen der Absender bekannt zu geben, damit die Opfer Klage einreichen können.In den USA wurde durch den CAN-SPAM-Act Spam im Prinzip verboten. Mittlerweile wurden die ersten Spammer bereits verhaftet, 2004 wurde in den USA ein Spammer zu einer Freiheitsstrafe von neun Jahren verurteilt, jedoch nicht wegen des Versendes von Spam, sondern anderer Delikte wie Computerbetrug oder Identitätsdiebstahl.
Australien war Vorreiter in Sachen Anti-Spam-Gesetze und bestrafte Spamming als erstes Land hart. Allerdings hielt sich die Regierung ein Schlupfloch offen: Parteienwerbung ist, anders als in Deutschland (siehe E-Card), dort erlaubt.
Vardan Vardanovich Kushnir – gestorben am 24. Juli 2005. Der notorischste Spammer Russlands wurde erschlagen in seiner Wohnung aufgefunden.
James McCalla: wurde am 6. Januar 2006 zu einer Schadensersatzzahlung von 11,2 Milliarden US-Dollar an einen lokalen Internet Service Provider verurteilt – die bisher höchste Geldstrafe für einen Spammer.
Daniel Lin: der 30-jährige US-Spammer ist am 6. September 2006 in den Vereinigten Staaten zu drei Jahren Haft und einer Geldstrafe von 10.000 US-Dollar (7.500 Euro) verurteilt worden. Damit geht eines der ersten Verfahren nach dem im Jahr 2003 eingeführten US-Anti-Spam-Gesetz (Can-Spam Act) zu Ende.
Robert Soloway, einer der weltweit aktivsten Spammer, von den Strafverfolgungsbehörden „Spam King“ genannt, wurde am 30. Mai 2007 wegen Bundesverbrechen in den USA verhaftet, im Juli 2008 zu 47 Monaten Gefängnis und einer Zahlung von über 700.000 US-Dollar verurteilt. Seit März 2011 ist er unter Bewährungsauflagen auf freiem Fuß.
Christopher Smith: gegen den zum Zeitpunkt der Verurteilung 27-jährigen wurde im August 2007 eine Gefängnisstrafe von 30 Jahren verhängt, allerdings nicht wegen Verstoßes gegen den CAN-SPAM Act, sondern wegen neun anderer Delikte.
Sanford Wallace: wurde gemeinsam mit Walter Rines von einem US-Bundesgericht zu einer Schadensersatzzahlung von 230 Millionen US-Dollar an den Internet-Foren-Betreiber Myspace verurteilt.
Oleg Nikolaenko: wurde im November 2010 durch das FBI verhaftet, von den Strafverfolgungsbehörden auch „King of Spam“ genannt. War für ein geschätztes Drittel des gesamten weltweiten Spam-Aufkommens verantwortlich.
Im Kampf um/gegen UBE wird von beiden Seiten ein immer größer werdender Aufwand getrieben. Das UBE-Aufkommen stieg in den letzten Jahren exponentiell an. Im Jahr 2003 überstieg das UBE-Aufkommen erstmals die Menge der regulären Mails, so eine Meldung von www.spamhaus.org Ende des Jahres.
Im November 2008 wurde der kalifornische Webhosting-Provider McColo vom Netz getrennt, dessen Hosting-Angebote von Kriminellen zum Steuern von Bot-Netzen missbraucht worden waren. Daraufhin sank das weltweite Spam-Aufkommen auf ein Drittel bis ein Viertel.
Im Folgenden werden die bekanntesten Maßnahmen gegen UBE und die daraus erfolgten Reaktionen der Spammer gegen neue Filter- und andere Techniken zu dessen Vermeidung beschrieben. Dies zeigt deutlich den erhöhten Aufwand auf beiden Seiten.
Die Überprüfung der Gültigkeit von Absenderadressen führte zur Verwendung gültiger Adressen. Dies hatte den Effekt, dass Unschuldige mit Tausenden bis zu Millionen von Bounces überschüttet wurden.
Die Einführung von Filtern, die Mails auf bestimmte Begriffe überprüften, führte zu Mails, die absichtliche Schreibfehler enthielten (beispielsweise V1a9ra statt Viagra) oder durch ungültiges HTML (das von HTML-darstellenden Mailreadern ignoriert wird) den wahren Inhalt verschleierten.
Den immer besser werdenden Textfiltern gegen Spam wird dadurch entkommen, dass Werbe-Spam in Form von GIF-Bildern verschickt wird und so nicht einfach gefiltert werden kann. Zusätzlich werden diese Bilder durch einfache Algorithmen leicht von Exemplar zu Exemplar modifiziert, ohne ihre Lesbarkeit einzuschränken. Dadurch sind sie noch schwerer per Filter aufzuspüren. Statt Bildern kommen auch PDF-Anhänge vor.
Das Sperren bekannter offener Relays und bekannter UBE-versendender Server führte zur Verbreitung von Trojanischen Pferden, die die Rechner von regulären Benutzern als UBE-Versender umfunktionierten.
Das Einführen von zentralen Listen, die Informationen über offene Relays und Anderes verbreiteten und immer öfter von Mailbetreibern genutzt werden, führte zu DoS-Angriffen gegenüber den Betreibern der jeweiligen Liste und deren ISPs.
Es wird vermutet, dass das 2003 vermehrte Aufkommen von Würmern auf die Verbreitung und Durchsetzung von statistischen Analysetools (z. B. Bayes-Filtern) zurückzuführen ist.
Einige Provider gehen dazu über, den Port 25 zu überwachen oder ganz zu sperren, um eventuell vorhandenen Viren die Möglichkeit zu nehmen, auf diesem Port E-Mails zu verschicken.
Die Verwendung neuer Übertragungsmethoden von Mail, die eine Authentifizierung der beteiligten Mailserver erlauben, sollen das bisherige System (SMTP) ablösen.
Erstellt wird ein neuer Standard von Seiten der IETF, gleichzeitig arbeiten große Mailanbieter an eigenen Lösungen. Das Sender Policy Framework (SPF) ist ein Konzept, das das Fälschen von E-Mail-Absenderadressen erschwert und auf einem zusätzlichen DNS-TXT-Eintrag basiert. Es werden bereits Patches für viele populäre sogenannte MTAs (Mail Transfer Agents) angeboten. Allerdings werden durch dieses Verfahren Mailweiterleitungen erschwert.
Ein weiterer Ansatz ist die Einführung von virtuellen Briefmarken, den beispielsweise Hashcash verfolgt. Dabei muss der Versender pro abgeschickter E-Mail einige Sekunden Rechenzeit investieren, um eine solche virtuelle Briefmarke, die nur für begrenzten Zeitraum und für eine bestimmte Empfängeradresse gültig ist, zu erstellen. Auf der Empfängerseite werden dann E-Mails von unbekannten Absendern von einem Filterprogramm wie SpamAssassin nur dann akzeptiert, wenn sie mit gültigen Briefmarken versehen sind. Das hat zur Folge, dass das massenhafte Versenden von E-Mails erheblichen Mehraufwand bedeuten würde, während der gelegentliche Versender kaum beeinträchtigt ist. Ein Vorteil dieser Methode ist, dass das Überprüfen der Gültigkeit einer virtuellen Briefmarke mit (im Vergleich zum Erzeugen der Briefmarke) sehr wenig Rechenaufwand geschehen kann. Ein Schwachpunkt ist, dass Täter ohnehin nicht mehr ihre eigenen Rechner benutzen und daher auch mehr Rechenleistung zur Verfügung haben.
Thoms Fassung von Framstags freundlichem Folterfragebogen, ein Standardtext für Spam-Empfänger jeglicher Art
Björn Bahlmann: Möglichkeiten und Grenzen der rechtlichen Kontrolle unverlangt zugesandter E-Mail-Werbung. Internationale Regelungen und alternative Lösungsmöglichkeiten. Verlag Dr. Kovac, ISBN 3-8300-1276-4.
Wendlandt: Europäische, deutsche und amerikanische Regelungen von E-Mail-Werbung – Überlegungen zum Nutzen des CAN-SPAM Act. In: MMR. 2004/06, Rd.-Nrn. 365 ff.
Kommission der europäischen Gemeinschaften: MITTEILUNG DER KOMMISSION AN DAS EUROPÄISCHE PARLAMENT, DEN RAT, DEN EUROPÄISCHEN WIRTSCHAFTS- UND SOZIALAUSSCHUSS UND DEN AUSSCHUSS DER REGIONEN über die Bekämpfung von Spam, Späh- und Schadsoftware. Brüssel, 15. November 2006. (PDF-Volltext)
Juristische Aspekte beim Einsatz von Spam- und Virenfiltern. In: Peter Eisentraut, Alexander Wirt: Mit Open Source-Tools Spam & Viren bekämpfen. O'Reilly, Beijing/ Cambridge/ Farnham/ Köln/ Paris/ Sebastopol/ Taipei/ Tokyo 2005, ISBN 3-89721-377-X, S. 313–317. (PDF; 355 kB)
Thomas Frank: Zur strafrechtlichen Bewältigung des Spamming. Kognos Verlag, Berlin 2004, ISBN 3-8325-0491-5.
Thomas Frank: You've got (Spam-)Mail. Zur Strafbarkeit von E-Mail-Werbung. In: Computer und Recht 2/2004, S. 123 ff.
Thomas Engels (Rechtsanwalt): Werbung per Telefax – Wie man es NICHT machen sollte… – Beitrag zur Rechtslage bei Telefax-Spam nach altem und neuem UWG. aufrecht.de/3991
Peter Sester, Sibylle Mutschler: Neue Kooperationen und rechtliche entwicklungen im Kampf gegen Spam. In: Informatik Spektrum. Heft 1, 2006.
Gerald Spindler, Stefan Ernst: Vertragsgestaltung für den Einsatz von E-Mail-Filtern. In: Computer und Recht. (CR) 6/2004, S. 437.
LG Hamburg: Haftung von Faxabrufdienstbetreiber für Spam. Urteil vom 17. November 2004, Az. 304 S 82/03 (rechtskräftig), In: Computer und Recht. (CR) 7/2005, S. 496.
Max W. Mosing, Gerald Otto: Spam: neuerliche Irrfahrt?! In: Medien und Recht (MR). 6/2005, S. 359ff.
Gerald Otto, Martin Parschalk: Spam- und Virenfilter – eine Notwendigkeit im Graubereich des Rechts. In: wirtschaftsrechtliche blätter (wbl). 1/2005, S. 10ff.
Center for Democracy and Technology: Why am I getting all this spam? (Memento  vom 8. Oktober 2006 im Internet Archive) (PDF; 311 kB), 2003.
Tobias Eggendorfer: No Spam! Besser vorbeugen als heilen. Software und Support Verlag, Frankfurt 2005, ISBN 3-935042-71-X.
Tobias Eggendorfer: Privatadresse. Homepages spamsicher gestalten. In: Linux User. 05/2004, Linux New Media, München 2004.
Tobias Eggendorfer: Ernte – nein danke. E-Mail-Adressenjägern auf Webseiten eine Falle stellen. In: Linux Magazin. 06/2004, Linux New Media, München 2004.
Tobias Eggendorfer: Spezialfilter. Anti-Spam-Appliance mit Langzeitwirkung. In: Linux Magazin. 09/2004, Linux New Media, München 2004.
Peer Heinlein: Verzögerungstaktik. Greylisting schützt vor Wurm-generierten E-Mails. In: Linux-Magazin. 09/2004, Linux New Media, München 2004
Jo Bager: Wider die E-Mail-Massen. Neue Verfahren gegen Spam. In: c’t. 15/2004, Heise Verlag, Hannover 2004.
Moritz Mertinkat: Spam und SPIT – Aktuelle Schutzmöglichkeiten und Gegenmaßnahmen. (PDF; 256 kB), 2007
Antispam - Strategien. Unerwünschte E-Mails erkennen und abwehren – BSI-Studie 3/2005, abgerufen am 13. Januar 2016
Schlechte Nachrichten (Memento  vom 30. Juni 2007 im Internet Archive) – Artikel von iX (Ausgabe 7/2007) – Thema: aktuelle Spam-Wellen und Greylisting als Gegenmaßnahme
private Seite Juristische Einordnung von unerwünschten E-Mails - just law Rechtsanwälte, Groner-Tor-Straße 8, 37073 Göttingen

Die Spandau-West–Hennigsdorfer Kleinbahn war eine straßenbahnähnliche Kleinbahn, die von 1923 bis 1945 existierte und Bestandteil des Berliner Straßenbahnnetzes war. Die von der Linie 120 bediente Verbindung führte vom Bahnhof Spandau-West durch die Spandauer Neustadt über Johannesstift und Nieder Neuendorf nach Hennigsdorf. Konzessionär des Unternehmens war die AEG, auf deren Betreiben die Bahn auch eingerichtet wurde. Die Betriebsführung oblag der Berliner Straßenbahn und deren Nachfolgern. Auf einem rund fünfeinhalb Kilometer langen Abschnitt befuhr die Linie die Bötzowbahn der Osthavelländischen Kreisbahnen (OHKB, ab 1943 Osthavelländische Eisenbahn).
Die AEG nahm im Jahr 1911 eine Fabrik zur Fertigung von Porzellanisolatoren in Hennigsdorf in Betrieb. 1913 begann der Bau von Elektrolokomotiven. Der Ort war seit 1893 über die Kremmener Bahn an Berlin angeschlossen. Die in Spandau wohnhaften Teile der Belegschaft mussten für ihren Arbeitsweg eine umständliche Fahrt mit Umstieg am Bahnhof Berlin Gesundbrunnen in Kauf nehmen. Bereits 1911 plante die AEG die Verlängerung der Spandauer Straßenbahn nach Hennigsdorf. Das Unternehmen wollte seinen Mitarbeitern dadurch die Möglichkeit bieten, in Spandau zu wohnen oder ihre Kinder auf die dort ansässigen höheren Schulen zu schicken. Um dem Vorhaben zusätzlich Auftrieb zu verleihen, wollte die AEG eine Grundauslastung der Bahn garantieren. Durch den Ersten Weltkrieg ließ man das Vorhaben zunächst fallen.Im Jahr 1921 nahm die AEG in ihrer Lokomotivfabrik den Bau von Dampflokomotiven auf. Im Folgejahr betrug die Belegschaft 7200 Personen, von denen ein Teil in Spandau wohnte. Im Juni 1922 stellte das Unternehmen daher beim Regierungspräsidenten in Potsdam, Franz Schleusener, den Antrag zur Inbetriebnahme einer Kleinbahn vom Bahnhof Spandau-West (in der Lage des heutigen Bahnhofs Berlin-Spandau) nach Hennigsdorf. Die Bahn sollte in Teilen die Gleise der Osthavelländischen Kreisbahnen nutzen.Die Strecke gliederte sich in vier Abschnitte:
4,3 Kilometer auf den Gleisen der Berliner Straßenbahn von Bahnhof Spandau-West zum Bahnhof Johannesstift
5,5 Kilometer auf den Gleisen der Osthavelländischen Kreisbahnen vom Bahnhof Johannesstift zum Bahnhof Nieder Neuendorf
1,0 Kilometer neu zu errichtendes Gleis bis zum Eingang der LokomotivfabrikDie Berliner Straßenbahn erteilte 1922 der AEG die Genehmigung zur Benutzung ihrer Gleisanlagen, der Magistrat von Spandau stimmte dem Vorhaben im September 1922 zu. Mit der OHKB handelte die AEG einen Vertrag über Mitnutzung ihrer Gleisanlagen sowie der Herstellung einer Gleisverbindung im Bahnhof Johannesstift aus. Gleichzeitig veranlasste sie die Sanierung des Anschlussgleises. Ende 1922 lag die vorläufige Genehmigung zum Betrieb der Bahn vor. Die endgültige Genehmigungsurkunde lag am 28. Juli 1925 vor. Darin wird die Bahn als Spandau-West–Hennigsdorfer Kleinbahn bezeichnet. Genehmigungsinhaberin war die AEG-Bahnabteilung, Betriebsführerin war die Berliner Straßenbahn-Betriebs-Gesellschaft. Die Genehmigung wurde bis zum 31. März 1945 erteilt.Da die AEG zunächst nur während der Schichtwechselzeiten von einem größeren Fahrgastaufkommen ausging, erfolgte die Bedienung durch Benzol- anstelle von elektrischen Triebwagen, wodurch die Kosten für den Unterhalt der Oberleitungen und anderer Einrichtungen entfielen. Die AEG stellte zunächst zwei Triebwagen, die Berliner Straßenbahn vier Beiwagen. Die Wagen waren im Straßenbahnbetriebshof Spandau beheimatet. In Hennigsdorf existierte an der Rathenaustraße eine beheizbare Wagenhalle mit zwei Ständen, die zum Warten und Betanken der Benzolwagen diente.Am 1. Januar 1923 nahm die Berliner Straßenbahn den Betrieb auf Grundlage der vorläufigen Genehmigung auf. Die offizielle Abnahmefahrt fand eine Woche später am 8. Januar 1923 statt. Sie verlief anstandslos. Gleichzeitig gingen die Haltestellen Wichernstraße (zur Anbindung der ab 1914 entstandenen Waldsiedlung Hakenfelde) und Kraftwerk in Betrieb. Die gewählte Liniennummer 120 war an die Linie 20 angelehnt, die von Spandau Hauptbahnhof (heute: Stresow) bis Johannesstift fuhr und deren Fahrweg zwischen der Spandauer Altstadt und Johannesstift mit dem der Linie 120 identisch war. Die Berliner Straßenbahn stellte die Linie 20 im September 1923 ein. Den Streckenast nach Johannesstift bedienten in den Folgejahren unter anderem die Linien 54, 154 und 58.Die Linie soll in den 1920er-Jahren zweigeteilt betrieben worden sein. Zwischen Spandau-West und Johannesstift verkehrten elektrische Triebwagen, während die Benzoltriebwagen auf dem Abschnitt von Johannesstift bis Hennigsdorf fuhren. Vom 25. April 1926 bis 16. Januar 1928 führte die Berliner Straßenbahn die Wagen des elektrischen Pendelbetriebs als Linie 120E. Bereits 1924 soll eine Linie 120E zwischen Spandau, Seegefelder Straße Ecke Nauener Straße und Spandau, Markt bestanden haben.Die Bahn stellte sich als Erfolg heraus, sodass die Anzahl der Fahrten schrittweise erhöht wurden. Auf der parallel verlaufenden Strecke der OHKB zwischen Spandau-West und Johannesstift nahm der Verkehr hingegen rapide ab, sodass sich die Kreisbahnen zur Reduzierung des Angebots auf täglich zwei Zugpaare entschlossen.1928 gab es Pläne seitens der Berliner Straßenbahn-Betriebs-Gesellschaft, die Linie nicht mehr über die Schönwalder Straße zum Johannesstift zu führen, sondern über die Streitstraße verkehren zu lassen. Von der 1928 eingerichteten Wendeschleife an der Endhaltestelle Hakenfelde am Eschenweg sollte die Trasse am Ostrand der Waldsiedlung entlang des heutigen Lichtwarkweges bis zur Bötzowbahn führen und in Höhe des Rustwegs in die OHKB-Strecke einfädeln. Die vorgesehene Trasse war über mehrere Jahre freigehalten worden. Als möglicher Hinderungsgrund wird die Einrichtung einer mit einem Fahrdienstleiter besetzten Abzweigstelle am Treffpunkt beider Strecken vermutet. Zudem wünschte sich das Unternehmen die baldige Elektrifizierung der Bahn. Die OHKB ihrerseits war an einer Auflösung des Mitbenutzungsvertrages interessiert, da die Fahrgäste zunehmend auf die Straßenbahn abwanderten.Am 1. Januar 1929 ging der Streckenabschnitt von Nieder Neuendorf nach Hennigsdorf in das Eigentum der neu gegründeten BVG als Nachfolger der Berliner Straßenbahn-Betriebs-Gesellschaft über. Gleichzeitig wurde die BVG Konzessionsinhaberin der Bahn. Im Juni schlossen BVG und OHKB einen Vertrag über die Elektrifizierung der Linie ab. Der Vertrag war bis zum 31. März 1950 gültig und wurde stillschweigend um jeweils zwei Jahre verlängert, sollte einer der beiden Vertragspartner nicht die Auflösung wünschen. In dem Vertrag erklärte sich die BVG bereit, jährlich eine Trassennutzungsgebühr von pauschal 9000 Reichsmark zuzüglich drei Reichspfennig je Nutzwagenkilometer an die OHKB zu entrichten. Ferner überließ sie der OHKB zwei Beiwagen der ehemaligen Schmöckwitz–Grünauer Uferbahn (Bw 1543II und 1544II). Die AEG überließ ihrerseits der OHKB ihre drei Triebwagen. Nachdem im Juli 1929 die Genehmigung seitens der OHKB zur Aufstellung der Oberleitungsmasten vorlag, begannen die Arbeiten zur Elektrifizierung. Am 11. November 1929 konnte der elektrische Betrieb aufgenommen werden. Die Oberleitung war für den Betrieb mit Scherenstromabnehmern ausgelegt und nicht für die in Berlin damals üblichen Rollenstromabnehmer. Die Streckenabschnitte vom Bahnhof Spandau-West nach Johannesstift und die Gleise zum Betriebshof Spandau mussten daher für den Mischbetrieb eingerichtet werden.Am 23. Juli 1931 wurde die Linie um 900 Meter von der Lokomotivfabrik zum Bahnhof Hennigsdorf verlängert. Die Wagenhalle musste zuvor abgebrochen werden.Während des Zweiten Weltkrieges kam es zunächst zu keinen größeren Betriebseinschränkungen. Luftangriffe auf Spandau und Hennigsdorf führten zur Beschädigung der Oberleitung. Beim Wiederaufbau soll der Stadtabschnitt lediglich für den Betrieb mit Rollenstromabnehmern wiederaufgebaut worden sein, sodass erneut eine Teilung der Linie in Johannesstift erfolgte. Ab dem 25. Januar 1945 verkehrte die Linie 120 planmäßig nur noch zwischen Johannesstift und Hennigsdorf. Die vollständige Betriebseinstellung war vermutlich zwischen dem 12. April und 21. April 1945. Eine Wiederinbetriebnahme fand nach Kriegsende nicht statt. Zwischen Sommer 1945 und August 1950 bedienten die Osthavelländischen Eisenbahnen den Abschnitt nach Hennigsdorf. Bis 1951 fuhren vereinzelt auch Züge von Bötzow mit Fahrtrichtungswechsel in Nieder Neuendorf nach Hennigsdorf. Der ehemalige Benzoltriebwagen 8001 soll hierbei auch zum Einsatz gekommen sein.Bis etwa 1951/1952 nutzte die BVG-Ost die Strecke zudem für Überführungsfahrten zum LEW „Hans Beimler“ Hennigsdorf. Die Überführungsfahrten besorgten die BVG-Ost und LEW. Hierzu schleppte der Arbeitstriebwagen A277II die zu überführenden Wagen durch den Westteil der Stadt bis zum Bahnhof Spandau-Johannesstift. Da zu dieser Zeit die Umstellung der Fahrleitungsanlagen der BVG-West auf Schleifbügel erfolgte, war der Triebwagen mit zwei Rollenstromabnehmern und einem Scherenstromabnehmer ausgerüstet. In Johannesstift wurden die nicht eisenbahntauglichen Wagen auf einen vierachsigen Niederbordwagen der LEW verladen und mittels einer Akkulokomotive nach Hennigsdorf gezogen. Die Verbindungskurve am Bahnhof Johannesstift wurde 1952 ausgebaut. Den zwischen Nieder Neuendorf und Spandauer Allee gelegenen Abschnitt rüstete LEW danach zur Drei-Kilovolt-Prüfstrecke und betrieb diese bis in die 1970er-Jahre.
Obwohl sowohl die Berliner Straßenbahn als auch die Osthavelländischen Kreisbahnen als Kleinbahnen nach dem Preußischen Kleinbahngesetz von 1892 konzessioniert waren, mussten die Bahn und ihre Fahrzeuge auch die Anforderungen der Eisenbahn-Bau- und Betriebsordnung (EBO) von 1904 und 1928 erfüllen. Hintergrund war, dass die OHKB in Nauen, Velten und Spandau an die Strecken der Deutschen Reichsbahn angeschlossen war und ein Wagenübergang zwischen beiden Bahnen stattfand. Daher kamen die Abschnitte II. Bahnanlagen und III. Fahrzeuge der EBO auch zur Geltung.Die Führer der Triebwagen mussten in zwei Systemen ausgebildet sein. Von Spandau-West beziehungsweise dem Betriebshof an der Pichelsdorfer Straße bis zur Kehranlage am Johannesstift und auf der Neubaustrecke zum Bahnhof Hennigsdorf wurde der Verkehr als Straßenbahnbetrieb abgewickelt, die Fahrzeugführer waren Straßenbahnfahrer im Sinne des Kleinbahngesetzes beziehungsweise ab 1938 der Straßenbahn-Bau- und Betriebsordnung (BOStrab). Auf den Gleisen der Osthavelländischen Kreisbahnen zwischen dem Bahnhof Johannesstift und dem Bahnhof Nieder Neuendorf handelte es sich um Zugfahrten im Blockabstand nach der Eisenbahn-Bau- und Betriebsordnung, der Wagenlenker war Triebfahrzeugführer. Die Strecke zwischen dem Bahnhof Nieder Neuendorf und der Neubaustrecke galt als Rangierfahrt auf einer Anschlussbahn.
Zwischen Spandau-West und der Straßenbahnhaltestelle Johannesstift fuhren die Züge als Straßenbahn auf Sicht. Die zweigleisige Strecke lag in der Straßenmitte, die Züge befuhren die Seegefelder Straße, Potsdamer Straße (seit 1939: Carl-Schurz-Straße), Neuendorfer Straße, den Hafenplatz, die Schönwalder Straße bis zum Fehrbelliner Tor, von dort auf eigenem Bahnkörper rechts von der Schönwalder Allee zweigleisig zum Stadtpark (heute Cautiusstraße) und eingleisig weiter bis Johannesstift, wo das Verbindungsgleis zur Strecke der OHKB begann. Die Bahnen hielten zum Ein- und Aussteigen an der Straßenbahnendhaltestelle Johannesstift; die Bahnsteige des Bahnhofs Johannesstift lagen weiter südlich jenseits der Straße nach Schönwalde und wurden nicht berührt. Im Verbindungsgleis meldete sich das Fahrpersonal über einen Streckenfernsprecher beim Fahrdienstleiter der OHKB im Stellwerk „Jr“, der die Zustimmung zur Weiterfahrt in Richtung Nieder Neuendorf durch Fahrtstellung des Einfahrsignals C erteilte. Der Zug fuhr dann zunächst auf das städtische Industriegleis nach Hakenfelde und wechselte über eine Weichenverbindung ohne weiteres Signal auf das Streckengleis der Bötzowbahn. In der Gegenrichtung fuhr der Zug auf das dreiflügelige Einfahrsignal A zum Bahnhof Johannesstift zu.
Im Bahnhof Nieder Neuendorf war der dort ansässige Fahrdienstleiter für die Abfertigung in beiden Richtungen zuständig. Das Gleis aus Richtung Hennigsdorf in den Bahnhof war mit einem Sperrsignal versehen. Die Bahn wechselte hier auf das ehemalige Anschlussgleis der Munitionsfabrik Hennigsdorf, das bis zum Haltepunkt Nieder Neuendorf Forsthaus parallel zur Strecke der OHKB verlief. Anschließend folgte die Strecke der Trappenallee und Spandauer Allee bis zur Endhaltestelle an der Rathenaustraße vor dem Haupteingang der Lokomotivfabrik. Etwa 200 Meter dahinter stand die Triebwagenhalle. Die Verlängerung zum Bahnhof Hennigsdorf führte etwa 900 Meter entlang der Rathenaustraße bis zum Bahnhofsvorplatz, wo sich eine Wendeschleife befand.
Die auf das OHKB-Netz übergehenden Straßenbahnwagen mussten sowohl den Anforderungen des Kleinbahngesetzes als auch der Eisenbahn-Bau- und Betriebsordnung sowie der Eisenbahn-Signal-Ordnung genügen. Dies betraf insbesondere die Anpassung der Räder nach § 31 EBO hinsichtlich Breite und Stärke der Radreifen sowie Höhe und Stärke der Spurkränze, die Ausrüstung der Fahrzeuge mit Signalstützen und die Ausrüstung der Triebwagen mit geeigneten Vorrichtungen zum Abgeben hörbarer Signale. Die bei der Berliner Straßenbahn üblichen Umsetztüren mussten durch Falt- oder Schiebetüren ersetzt werden, die Druckluftbremse wurde durch eine elektrische Bremse ersetzt. Hinsichtlich der Radbreite galt es einen Kompromiss zwischen den 80 Millimeter breiten Rädern der Straßenbahn und den 135 Millimetern Breite der Eisenbahn zu finden. Ein zu schmales Maß hätte zur Folge, dass die Räder beim Befahren von Eisenbahnweichen in die Herzstücke geraten können. Bei einem zu breiten Maß wären die Räder teilweise auf dem Straßenpflaster gelaufen. Die auf der Kleinbahn einzusetzenden Wagen hatten daher 96 Millimeter breite Radreifen. Es zeigte sich allerdings, dass bei ausgefahrenen Herzstücken trotzdem eine Entgleisungsgefahr bestand. Die entsprechenden Herzstücke wurden anfangs aufgefüllt, sodass die Räder beim Befahren ein kurzes Stück auf dem Spurkranz liefen und anschließend wieder auf den Schienenkopf aufkletterten. Später ging man dazu über, die Herzstücke mit beweglichen Knieschienen auszurüsten. Eine Weiche im Bahnhof Johannesstift war abweichend davon mit einem beweglichen Herzstück ausgestattet.Die Geschwindigkeit betrug zwischen Spandau-West und Johannesstift 25 Kilometer in der Stunde und ab Johannesstift 40 Kilometer je Stunde. In der Spandauer Altstadt war die Höchstgeschwindigkeit auf 16 Kilometer je Stunde begrenzt. Die eingangs erwähnten Weichen durften mit höchstens acht Kilometer in der Stunde befahren werden.Während der Fahrt auf der OHKB-Strecke hatten die Wagen die nach der Eisenbahn-Signalordnung vorgeschriebenen Zugsignale zu führen, insbesondere das vereinfachte Zugschlusssignal bestehend aus einer weiß umrandeten, roten Scheibe am letzten Wagen beziehungsweise als Nachtzeichen ein rotes Licht. Zudem hatte jeder Triebwagen zwei Haltscheiben und vier rot abblendbare Petroleumlampen mitzuführen, um einen liegengebliebenen Zug absichern zu können.Die AEG stellte zunächst zwei Benzoltriebwagen (6001 und 6002, ab 1925 Triebwagen 8001 und 8002) für den Betrieb, ein dritter Benzoltriebwagen wurde 1925 noch als 6003 ausgeliefert und kurz darauf in 8003 umbezeichnet. Die Umzeichnung erfolgte mit Rücksicht auf die zeitgleiche Auslieferung der Wagen der Bauart 1924 (Serie 5701–6200). Die Berliner Straßenbahn stellte vier Beiwagen, die von der ehemaligen Spandauer Straßenbahn stammten (1526–1530, später 1482II–1485II). Neben den für den Eisenbahnbetrieb erforderlichen Anpassungen unterschieden sich die Überland-Beiwagen auch durch eine größere Wagenlänge infolge der vergrößerten Plattformen von den Stadtfahrzeugen der gleichen Serie. Mit der Umrüstung auf elektrischen Betrieb verkaufte die BVG die Triebwagen an die OHKB.Für den elektrischen Betrieb rüstete die BVG vier Triebwagen (4317–4320) der ehemaligen Teltower Kreisbahnen um. Die Wagen besaßen Scheren- anstelle der damals in Berlin üblichen Rollenstromabnehmer. Triebwagen 4315 und 4316 waren entgegen älteren Quellen nicht vom Umbau betroffen. Triebwagen 4317 wurde um 1951 verschrottet. Die übrigen drei Triebwagen sind vermutlich während des Zweiten Weltkrieges verloren gegangen.1939 wurden weitere Beiwagen des Typs B 26, nun als B 26 S bezeichnet, für den Einsatz auf der Linie 120 umgerüstet. Je nach Quelle werden zwei (Bw 1238II und 1239II), drei (zzgl. Bw 1236II) oder vier Beiwagen (zzgl. Bw 1237II) genannt. Die Wagen wurden 1929 für die ehemalige Flachbahn der Hochbahngesellschaft gebaut. Die Spandauer Beiwagen sollen danach wieder im Stadtverkehr eingesetzt worden sein. Die ehemaligen Flachbahnwagen verfügten bereits über Schiebetüren, deren Handhabung einfacher war als die der Falttüren der Spandauer Wagen. Beiwagen 1236II schied während des Krieges aus, Beiwagen 1237II kam nach 1949 zur BVG-Ost, Beiwagen 1238II und 1239II zur BVG-West. Die BVG musterte die Wagen bis spätestens 1966 aus.Zusätzlich zu den Fahrzeugen für den Personenverkehr rüstete die Berliner Straßenbahn-Betriebs-Gesellschaft zwei ehemalige Personenwagen als Arbeitsfahrzeuge für den Mischbetrieb um. Arbeitswagen A49 entstand um 1924 aus dem Triebwagen 3877, der 1908 als Triebwagen 28 an die Neue Berliner Straßenbahnen Nordost AG geliefert wurde. Der Hilfsgerätewagen H28A entstand um 1925 aus dem Triebwagen 4166, der 1911 als Triebwagen 105 der Spandauer Straßenbahn in Betrieb ging.Die auf der Linie 120 eingesetzten Wagen führten offiziell die 3. Klasse und waren mit Holzsitzen ausgestattet. Die Triebwagen durften höchstens zwei Beiwagen ziehen.
Der erste Fahrplan von 1923 sah täglich Zugpaare vor. Die Fahrzeit betrug zunächst 40 Minuten und wurde 1928 auf 33 Minuten reduziert. In den folgenden Jahren nahm der Ausflugsverkehr einen größeren Stellenwert als der Berufsverkehr ein, was sich in der Anzahl der Fahrten widerspiegelte. Der Fahrplan vom 15. Mai 1929 führte bereits elf Zugpaare an Werktagen sowie 13 Zugpaare an Sonntagen auf. Nach der Aufnahme des elektrischen Betriebs wurde die Anzahl auf 21 Fahrten je Tag und Richtung erhöht. Ab 1931 betrug die Fahrzeit infolge der Streckenverlängerung 35 Minuten. In den Jahren 1933 und 1934 lag sie bei 15 Fahrten (sonntags: 18), 1936 bei 16 Fahrten (sonntags: 18). Im Fahrplan vom 16. Juli 1938 werden montags bis freitags 14 Fahrten, sonnabends 16 Fahrten und sonntags 18 Fahrten gezählt. Zugkreuzungen fanden in der Regel im Bahnhof Johannesstift, vereinzelt auch im Bahnhof Nieder Neuendorf statt.
Im Gegensatz zum sonst gültigen Einheitstarif galt auf der Linie 120 über die meiste Zeit ein entfernungsabhängiger Tarif. Es wurden Fahrkarten über die gesamten Distanz sowie ausgewählten Teilstrecken und Wochenkarten ausgegeben. Die Teilstrecke von Spandau-West nach Johannesstift entsprach im Fahrpreis dabei einer Einzelfahrt mit einer normalen Straßenbahnlinie, zumal diese ebenfalls bis Johannesstift fuhren. Ab dem 1. September 1933 gab es zwischen Johannesstift und Spandau-West weitere Teilstrecken zum Preis von 10 Pfennig. Fahrscheine mit Umsteigeberechtigung auf die anderen Straßenbahnlinien gab es ab 1934 zum Preis von 40 Pfennig. Ab 1938 gab es zusätzlich auch Einzelfahrscheine für Schüler zum Preis von 15 Pfennig ohne und 20 Pfennig mit Umsteigeberechtigung. Ab dem 1. September 1944 bis zur Einstellung galt der Kriegseinheitstarif auf allen Linien der BVG. Der Preis für eine Einzelfahrt ohne Umsteigeberechtigung lag hier bei 20 Pfennig.
Wolfgang Hellmuth Busch: Linie 120. Eine Berliner Überlandstraßenbahn 1923 bis 1945. In: Berliner Verkehrsblätter. Heft 11, November 1999. 
Hans-Jürgen Kämpf: Die Linie 120 – eine ganz besondere Straßenbahnlinie. In: Heimatkundliche Vereinigung Spandau 1954 e.V. (Hrsg.): Die Straßenbahn in Spandau und um Spandau herum. Berlin 2008, ISBN 978-3-938648-05-6, S. 185–234. 
Wolfgang von Linstow: Die „Benzolbahn“ der Berliner Straßenbahn. In: Straßenbahn Magazin. Heft 5, Mai 1972. 
Siegfried Münzinger: Die Spandau West-Hennigsdorfer Kleinbahn. In: Berliner Verkehrsblätter. Heft 1, Januar 1962. 
Reinhard Richter: Kleinbahnjubiläen 2004. In: Die Museumseisenbahn. Januar 2004 (PDF [abgerufen am 11. Januar 2010]).
Mit der Benzolbahn ins Umland. (Nicht mehr online verfügbar.) Ehemals im Original; abgerufen am 11. Januar 2010.@1@2Vorlage:Toter Link/deutschland-im-internet.de (Seite nicht mehr abrufbar, Suche in Webarchiven) 

Die spanische Eroberung Mexikos von 1519 bis 1521 unter Hernán Cortés führte zum Untergang des Reiches der Azteken und begründete die Herrschaft der Spanier über Mesoamerika. Entscheidend für den Erfolg der Spanier waren dabei ihre überlegene Waffentechnik, die Anfälligkeit der indigenen Bevölkerung für die von den Eroberern eingeschleppten Krankheiten, insbesondere Pocken, Masern und Grippe, die dort vorher unbekannt waren, und die Ausnutzung innen- und außenpolitischer Schwachpunkte des aztekischen Reiches.
Nach der Eroberung des Aztekenreiches gründeten die Spanier das Vizekönigreich Neuspanien. In der Folge kamen viele Siedler aus Spanien nach Zentralmexiko, während die Religion der Azteken vom Christentum verdrängt und die örtliche Kultur zu einem großen Teil von den Spaniern ausgelöscht wurde.
Eine vollkommen objektive Betrachtung der Eroberung ist nicht mehr möglich. Es existieren lediglich zwei Berichte von Augenzeugen aus spanischer Sicht sowie die unter der Anleitung des spanischen Franziskaners Bernardino de Sahagún von aztekischen Schreibern in Nahuatl verfasste Historia general de las cosas de Nueva España, die unter anderem die aztekische Sicht der Eroberung schildert. Die moderne Forschung kann diesen Mangel an Augenzeugenberichten nur teilweise ausgleichen, auch deshalb, weil das alte präkolumbische Tenochtitlán nach der Niederlage der Azteken fast vollkommen zerstört wurde.
Das Becken von Mexiko war zu Beginn des 16. Jahrhunderts die Heimat zahlreicher Stadtstaaten. In den beiden Jahrhunderten vor der Ankunft der Europäer hatte sich der Aztekische Dreibund, ein lockeres Bündnis aus den drei Städten Tenochtitlán, Texcoco und Tlacopán, zur vorherrschenden Macht im Tal von Mexiko entwickelt. Dieser Städtebund hatte im Laufe der Zeit zahlreiche konkurrierende Staaten besiegt, die jedoch nicht direkt in den Bund eingegliedert, sondern einem ausgeklügelten Tributsystem unterworfen wurden. Von den Städten des Dreibundes war das auf mehreren Inseln im Texcoco-See errichtete Tenochtitlán die mit Abstand mächtigste, weshalb ihr Herrscher auch militärische Befehlsgewalt über Tlacopán und Texcoco verfügte, die sich ansonsten vollkommen eigenständig regierten.
Seit 1502 wurde Tenochtitlán von Moctezuma II. beherrscht, der in sein Amt gewählt worden war. Er war der Sohn eines früheren Herrschers mit dem Namen Axayacatl und vor seiner Wahl Hohepriester des Gottes Huitzilopochtli gewesen. Unter seiner Regentschaft verschärften sich die sozialen Spannungen mehr und mehr, denn die Schere zwischen Arm und Reich vergrößerte sich zusehends. Zudem war die Zuverlässigkeit Texcocos als Bündnispartner in Frage gestellt, seit Moctezuma 1515 offen in einem Thronfolgestreit interveniert hatte und nur mühsam ein Kompromiss zwischen den beiden um den Thron kämpfenden Parteien erreicht werden konnte. Dennoch war der Dreibund immer noch der stärkste Machtfaktor im Tal von Mexiko. Eine größere Bedrohung stellten nur noch das im heutigen mexikanischen Bundesstaat Michoacán, westlich des Tales gelegene Reich der Tarasken sowie ein loses Bündnis von vier Städten östlich des Tales dar, deren stärkstes Mitglied die Stadt Tlaxcala war. Im Vergleich zu den Kulturen des eurasischen Kontinents waren alle Staaten Zentralmexikos technologisch weit unterlegen; die Verarbeitung von Eisen, eine ausgearbeitete Schrift sowie der Einsatz des Rades zu Transportzwecken beispielsweise waren ihnen unbekannt.
Nach der Entdeckung Amerikas im Jahre 1492 durch Christoph Kolumbus besetzten die Spanier in der Karibik mehrere Inseln. Die erste dauerhafte Siedlung war La Isabela auf der Insel Hispaniola, das 1493 gegründet wurde. Nach anfänglich freundschaftlichen Beziehungen kam es zu offenen Kämpfen mit den einheimischen Taíno, da die Spanier immer häufiger deren Frauen raubten und nach Gold verlangten. Die Taíno wurden nach ihrer schnellen Niederlage und der Tötung ihrer Häuptlinge von den Spaniern zur Zwangsarbeit herangezogen, was viele von ihnen nicht überlebten, da die Feldarbeit wegen des Arbeitskräftemangels vernachlässigt wurde. In den ersten beiden Jahrzehnten des 16. Jahrhunderts erlangten die Spanier zusätzlich noch die Kontrolle über die Inseln Puerto Rico (1508), Jamaika (1509) und Kuba (1511), deren indigene Bevölkerung ähnlich wie die Taíno auf Hispaniola Zwangsarbeit leisten musste. Zusätzlich gründeten sie eine Siedlung in Darién im heutigen Panama, wo bereits Christoph Kolumbus auf seiner vierten Reise an Land gegangen war.
Die Krone schenkte den Kolonien nur wenig Beachtung. Folglich hatten die spanischen Konquistadoren weitestgehend freie Hand bei ihren Vorhaben. Viele von ihnen waren jüngere Söhne von Hidalgos, die an den Kämpfen der Reconquista beteiligt gewesen waren. Da nur der erstgeborene Sohn den Besitz des Vaters erben konnte, waren die jüngeren Söhne gezwungen, anderweitig für ihren Lebensunterhalt zu sorgen. Die zu Beginn des 16. Jahrhunderts in Spanien herrschende Wirtschaftskrise verstärkte den Druck zur Auswanderung weiter, nachdem auch noch erste Nachrichten von Goldfunden in Spanien eingetroffen waren. Eine andere Motivation stellte der christliche Glaube dar. 1493 wurde die päpstliche Bulle Inter caetera erlassen, die auf einer früheren mit Portugal beschlossenen Bulle beruhte und mit der alles neu entdeckte Land westlich einer Demarkationslinie, die knapp fünfhundert Kilometer westlich der Kanarischen Inseln verlief, Spanien zu übereignen sei. Die Kirche war in Spanien ohnehin vergleichsweise kämpferisch eingestellt, was aus der Reconquista resultierte. Da die Überfahrten nach Amerika aber kaum von der spanischen Krone gefördert wurden, befanden sich um 1520 vermutlich lediglich rund 27.000 Spanier in den Kolonien, davon der Großteil auf Hispaniola.
Bereits um 1502 verbreiteten sich in Zentralmexiko Gerüchte über bärtige, hellhäutige Männer, denen große Grausamkeit zugeschrieben wurde. In den folgenden Jahren kamen diese Gerüchte einige Male erneut auf. Woher sie stammten, ist unsicher. Als mögliche Quellen kommen die Expedition des Kolumbus, die 1502 die Küste des mittelamerikanischen Festlandes von Honduras bis nach Panama erkundet hatte, die Erfahrungen der indigenen Bevölkerung mit Spaniern in Darién sowie ein Kanu, das 1512 mit einheimischen Insassen von Jamaika nach Yucatán abgetrieben wurde, in Frage. Zudem existiert ein auf April 1514 datierter Brief von Diego Velázquez de Cuéllar, dem Eroberer und zu diesem Zeitpunkt amtierenden Gouverneur Kubas, in dem von gelegentlich in Kanus ankommenden Indios die Rede ist, die „eine Reise von fünf oder sechs Tagen“ hinter sich hätten, was der Distanz zwischen der Halbinsel Yucatán und Kuba entspräche.Die ersten Spanier, die die heutige mexikanische Küste erreichten, waren Angehörige der unter dem Kommando von Francisco Hernández de Córdoba stehenden Expedition, die am 8. Februar 1517 von Santiago de Cuba aus aufbrach. Diego Velázquez de Cuéllar hatte Hernández de Córdoba den Auftrag gegeben, neue Länder zu entdecken und Sklaven zu fangen. Die Expedition stieß schon bald auf die Halbinsel Yucatán und somit – freilich ohne es zu wissen – auf das mittelamerikanische Festland, wo Hernández de Córdoba auf einen Stamm der Maya traf und seine Männer nur knapp vor deren Angriffen retten konnte. Die Expedition segelte in Richtung Westen bis zur heutigen Stadt Champotón (südlich von Campeche), wo die Spanier in einen Hinterhalt gerieten und die Hälfte ihrer Männer verloren. Hernández de Córdoba, selbst schwer verwundet, befahl daraufhin die Rückkehr nach Kuba, wo er den Folgen der Verwundungen erlag.
Aufgrund der Berichte der Überlebenden über ein sehr reiches Volk auf dem Festland rüstete Velázquez de Cúellar eine weitere Expedition aus. Zum Kommandanten ernannte er seinen Neffen Juan de Grijalva. Sein genauer Auftrag ist nicht überliefert. Grijalvas Schiffe brachen Ende Januar 1518 von Santiago de Cuba auf und erreichten nach mehreren Zwischenstopps Ende Mai die Insel Cozumel vor der Küste Yucatáns. Nach einigen Scharmützeln mit den Einheimischen segelte Grijalva weiter nach Süden bis zu einer Bucht, der er den Namen Bahia de la Ascensión („Bucht der Himmelfahrt“) gab, um wieder nach Cozumel zurückzukehren und von dort aus der Route der vorhergegangenen Expedition bis zu einem Ort in der Nähe von Champotón. Dort brachen erneut Kämpfe aus, nachdem die Spanier von den Einheimischen Gold gefordert hatten, doch konnten Grijalvas Männer ihre Stellung halten und an der Mündung des nach dem Expeditionskommandanten neu benannten Río Grijalva vorbei zu einer Insel in der Nähe des heutigen Veracruz fahren, die sie am 17. Juni erreichten. Der Ort der Landung wurde von ihnen San Juan de Ulúa genannt. Dort stießen die Spanier auf das Volk der Totonaken, das den Azteken tributpflichtig war. Dadurch kamen sie erstmals mit einheimischen Kulturelementen in Kontakt, einschließlich der Opferung von Menschen. Nachdem die Spanier von den Totonaken freundlich aufgenommen worden waren und Tauschhandel mit ihnen getrieben hatten, nahm auch der örtliche Gesandte des Moctezuma Kontakt zu den Spaniern auf, der diesem später darüber Bericht erstattete. Später schickte Grijalva seinen Untergebenen Pedro de Alvarado mit einigen Männern und den bislang erhandelten Wertgegenständen zurück nach Kuba, bevor er selbst nach Norden weitersegelte. Jedoch zwangen ihn schlechte Windverhältnisse und Schäden an seinen Schiffen bald ebenso zur Rückkehr. Anfang Oktober kehrte er mit seinen Männern nach Kuba zurück; Alvarado war zuvor bereits angekommen.
Nach Grijalvas Rückkehr erteilte der Gouverneur von Kuba, Diego Velázquez de Cuéllar, dem Alkalden (Bürgermeister) von Santiago de Cuba, Hernán Cortés, den Befehl zu einer dritten Expedition. Die Mittel, die Cortés in die Unternehmung steckte, sowie dessen Eifer beunruhigten Velázquez sehr, denn er befürchtete, dass Cortés die vereinbarte Gewinnbeteiligung nicht einhalten werde. Er versuchte ihn von seinem Posten als Kapitängeneral zu entfernen, doch Cortés bestieg kurzerhand mit 400 bis 600 Mann die für die Reise vorgesehenen Schiffe und stach am 10. Februar 1519 in See.Die Spanier landeten zunächst am 18. Februar – so die Angabe des Chronisten Francisco López de Gómara – auf der vor der Küste von Yucatán gelegenen Insel Cozumel, wo ihnen die Einheimischen von zwei Christen berichteten, die seit einigen Jahren bei den Maya lebten. Einer von beiden, Gerónimo de Aguilar, konnte nach kurzer Suche gefunden werden und schloss sich ihnen begeistert an, doch der andere, Gonzalo Guerrero, hatte es bei den Maya zu hohen Ehren gebracht und weigerte sich standhaft, seine neue Heimat zu verlassen. Aguilar war für Cortés von großer Wichtigkeit, da er sich mit den Maya in ihrer Muttersprache verständigen konnte.
Am 12. März 1519 fand in der Nähe von Potonchán am Fluss Tabasco eine zweite Landung statt, wo die Spanier nach einem Kampf mit den dort ansässigen Chontal-Maya als Zeichen der Ehrerbietung von Tabscoob dem Halach Huinik von Potonchán zwanzig Sklavinnen als Geschenk erhielten. Unter ihnen befand sich eine junge Frau, die von den Spaniern Doña Marina oder auch Malinche genannt wurde. Sie war den Spaniern zusammen mit Gerónimo de Aguilar durch ihre Kenntnis der Sprachen der Maya und derjenigen der Azteken, Nahuatl, sehr hilfreich und wurde später Cortés’ Geliebte.
Nach dem Intermezzo bei den Maya fand Hernán Cortés Juan de Grijalvas Landeplatz und ging dort mit seinen Männern an Land. Schon bald nach der Landung erschienen aztekische Gesandte, doch Cortés weigerte sich trotz der von den Azteken gebrachten Geschenke, das aufgeschlagene Lager wieder abzubrechen. Nach dem Rückzug der Gesandten nahm der König der Totonaken, eines aztekischen Vasallenvolkes, Kontakt mit den Spaniern auf und schloss ein Bündnis mit ihnen. Obwohl der Auftrag des Vizekönigs Velázquez, nämlich die Erkundung des Gebietes, mit der Landung erfüllt war, verweigerte Cortés seinen Soldaten die Rückkehr nach Kuba und gründete stattdessen die Siedlung Villa Rica de la Vera Cruz, nur einige Kilometer von der heutigen Stadt Veracruz entfernt. Sogleich setzte er selbst einen Stadtrat ein, der ihn zum Kapitängeneral ernannte, als solchen direkt der Krone unterstellte und ihn somit von den Pflichten Velázquez gegenüber entband. Um die Krone für seine Sache zu gewinnen, schickte er ein Schiff mit allem Gold, das die Neuankömmlinge auftreiben konnten, nach Spanien; alle anderen Schiffe ließ er aus Furcht vor Desertionen versenken. Dann marschierte er mit etwa 300 Soldaten ins Landesinnere.
Bald darauf gelangten die Spanier in die Nähe von Tlaxcala, einer mit den Azteken verfeindeten mächtigen Stadt. Die Bewohner der Stadt griffen die Eindringlinge mehrmals an, wobei die Spanier nur durch ihre Schießpulverwaffen vor einer Niederlage bewahrt wurden. Da ihnen langsam aber sicher die Vorräte ausgingen, machte Cortés den Tlaxcalteken mehrfach Friedensangebote, die trotz Forderungen auf tlaxcaltekischer Seite nach einer Fortsetzung des Kampfes schließlich auf Betreiben des Kaziken Xicoténcatls des Älteren angenommen wurden. Beide Parteien erkannten schon bald darauf den Wert der jeweils anderen Seite für den Kampf gegen die Azteken. Die Stadt Tlaxcala verfügte rein zahlenmäßig über zu wenige Soldaten, um die Azteken entscheidend zu schlagen. Die Feuerkraft der spanischen Truppe verschaffte ihnen hingegen einen entscheidenden taktischen Vorteil. Die Spanier ihrerseits erkannten, dass ihre Unternehmung ohne die Unterstützung der Tlaxcalteken zum Scheitern verurteilt war. Somit schlossen die beiden Parteien ein Bündnis gegen die Azteken.
Nach einem Aufenthalt von sechzehn Tagen in Tlaxcala zogen die Spanier weiter nach Cholula. Nachdem sie einen Großteil der dortigen Führungsschicht der Stadt am Tempel des Quetzalcoatl beseitigt hatten – vermutlich auf Betreiben der Tlaxcalteken – töteten sie den König, der den Tlaxcalteken gerade das Bündnis aufgekündigt hatte. Es wurde danach ein loyaler Marionettenkönig eingesetzt, der sich ebenfalls mit den Spaniern verbündete. Somit wusste Hernán Cortés eine große Armee von Soldaten einheimischer Völker hinter sich, als er nach zweiwöchigem Marsch am 8. November 1519 die aztekische Hauptstadt Tenochtitlán erreichte. Die Spanier waren überwältigt vom Anblick der riesigen Stadt, deren König Moctezuma II. sie mit reichen Geschenken überhäufte. Dass Moctezuma in Cortés den Gott Quetzalcoatl erblickt habe, dessen Wiederkehr in einer aztekischen Prophezeiung angekündigt werde, und ihm in einer Rede formell die Herrschaft übergeben habe, wird in der neueren Forschung als von den Spaniern konstruierter Geschichtsmythos angesehen, mit dem sie ihr Vorgehen gegenüber König Karl I. zu rechtfertigen versuchten.
Hernán Cortés erkannte binnen einer Woche seine Lage. Tenochtitlán war auf einigen Inseln im Texcoco-See erbaut worden und mit dem Festland nur durch drei Dämme verbunden. Bei einem falschen Schritt hätte er mit seinen Männern keine Chance gehabt, aus der Stadt zu entkommen. Cortés bemerkte jedoch die Bedeutung des Moctezuma für seine Untertanen und schloss daraus, dass eine Bemächtigung seiner Person die einzige Möglichkeit für die Machtübernahme im Aztekenreich sei. Mithilfe der unterschwelligen Drohung, die er durch die Stationierung einiger Soldaten in Moctezumas Palast erzeugte, brachte er den Aztekenherrscher dazu, sein Quartier im selben Palast zu beziehen, den dieser Cortés erst kurz zuvor zur Verfügung gestellt hatte. Auf diese Weise übte Hernán Cortés über Moctezuma in den nächsten acht Monaten die Herrschaft über das Aztekenreich aus. Als erstes gab Moctezuma den Spaniern die Erlaubnis zum Bau einer kleinen Kapelle in der Stadt und überließ ihnen den Schatz, der in seinem Palast lagerte, als Geschenk an das spanische Königshaus. Bald darauf zog Hernán Cortés den Zorn der aztekischen Bevölkerung auf sich, als er sogar gegen den Widerstand des Moctezuma auf der Plattform des Großen Tempels Kreuze sowie ein Bild der Jungfrau Maria aufstellen ließ, während das Gold und die Juwelen des Tempels fortgeschafft wurden.
Im Mai 1520 erreichte Cortés die Nachricht, dass Diego Velázquez mehr als tausend Soldaten unter Pánfilo de Narváez nach Veracruz geschickt hatte, um ihn für seine Machenschaften zur Verantwortung zu ziehen. Cortés ernannte Pedro de Alvarado zu seinem Stellvertreter in Tenochtitlán und marschierte mit etwa 250 Soldaten an die Küste, wo er Narváez durch einen Überraschungsangriff besiegte und ihn gefangen nahm. Anschließend übernahm er Narváez’ Soldaten und auch dessen Tross. Auf dem Weg nach Tenochtitlan wurde der Tross von Kriegern aus Texcoco überfallen. Die etwa 550 Gefangenen wurden nach Zultepec gebracht und dort den Göttern geopfert.
Unterdessen wurden die circa achtzig Spanier, die in Tenochtitlán zurückgeblieben waren, immer unruhiger. Pedro de Alvarado war auch bei den Spaniern als grausamer Mann bekannt; in Tenochtitlán ließ er während Cortés’ Abwesenheit zwei örtliche Häuptlinge töten. Dies steigerte die Unruhe in der aztekischen Bevölkerung, was sich in einer erhöhten Militärpräsenz der Azteken an den Stadttoren niederschlug. Am Tag des aztekischen Frühlingsfestes versammelte sich dann eine große Zahl Adeliger und Priester – verschiedenen Quellen zufolge mindestens sechshundert, möglicherweise jedoch sogar acht- bis zehntausend Menschen – im Hof des Großen Tempels von Tenochtitlán. Noch während der Prozession postierten sich die Spanier an den vier Eingängen des Tempels und töteten alle anwesenden Azteken, vermutlich aufgrund einer Panikreaktion von Pedro de Alvarado, die durch die Nachricht von dieser Versammlung ausgelöst wurde. Was dieser später als Prävention für einen Angriffsplan der Azteken angeben würde, war der sprichwörtliche Tropfen, der das Fass zum Überlaufen brachte. Ein aufgebrachter Mob tötete sieben Spanier, trieb die übrigen in ihre Quartiere und belagerte sie dort. Cortés eilte so schnell wie möglich zurück in die Stadt. Dort zog er mit seinen Männern am 24. Juni ein, doch als die Azteken kurz nach seiner Ankunft die Zugbrücken ihrer Dämme hochzogen, saß auch er in der Falle.
Da die Lage der Spanier immer aussichtsloser wurde und ihnen die Vorräte ausgingen, ließ Cortés Moctezuma auf das Dach des Palastes bringen, um die wütende Menge zu beruhigen. Mit diesem Schachzug hatte er jedoch keinen Erfolg; Moctezuma wurde von seinen eigenen Untertanen durch Steinwürfe angegriffen. Einige Tage später starb er, wobei wegen der widersprüchlichen Quellenlage ungeklärt bleibt, ob er den durch die Steine beigebrachten Verletzungen erlag oder ob er durch das Schwert der Spanier getötet wurde. Nach dem Tod von Moctezuma II. ließ der spanische Anführer Holzplanken herbeischaffen, um über die Dammlücken flüchten zu können. Die Flucht begann kurz vor Mitternacht am 30. Juni und endete für die Spanier in einem Desaster. Fast drei Viertel der zum Teil schwer mit Beute beladenen Spanier und eintausend Tlaxcalteken wurden beim Versuch, die Stadt zu verlassen, getötet, da sie von den Azteken schnell entdeckt worden waren. Etwa 270 Spanier, die in einem anderen Stadtteil untergebracht worden waren, wurden nach Cortés’ Flucht aufgegriffen und den aztekischen Göttern geopfert. Diese Niederlage der Spanier ging später als Noche triste („die traurige Nacht“ oder „die Nacht der Tränen“) in die Geschichte ein.
Mit der anschließenden Schlacht von Otumba am 14. Juli 1520 versuchten die Azteken Cortés samt den verbündeten Tlaxcalteken nach der Flucht von Tenochtitláns auf dem Weg zur Küste aufzuhalten, erlitten jedoch eine Niederlage.
Nur rund 440 Spanier hatten die „traurige Nacht“ überlebt, darunter auch Cortés und Alvarado. Sie hätten wohl kaum noch eine Chance gehabt, zu überleben, wäre nicht eine Pockenepidemie unter den Einheimischen ausgebrochen. Diese Krankheit, die von den Spaniern unwissentlich selbst eingeschleppt worden war, raffte innerhalb eines Jahres vierzig Prozent der indigenen Bevölkerung dahin und tötete auch den neuen aztekischen König Cuitláuac, der als Nachfolger seines Bruders nur achtzig Tage regiert hatte. Ihm folgte sein Neffe Cuauhtémoc auf den Thron. Es entstand dadurch bei den Azteken ein Klima der politischen Instabilität, das den Invasoren Zeit gab, sich von der schweren Niederlage zu erholen, zumal die Pocken – als in Europa seit langem endemische Krankheit – ihrem Immunsystem weitaus weniger zusetzten. Cortés zog sich nach Tlaxcala zurück und veranlasste dort Zimmermannsarbeiten für den Bau von dreizehn Brigantinen. Als neue spanische Soldaten seine Armee vergrößert hatten, zog er mit diesen und etwa zehntausend Tlaxcalteken erneut ins Tal von Mexiko, diesmal jedoch nach Texcoco. Die Einzelteile der Brigatinen wurden von indianischen Trägern bis zum See getragen und die Schiffe am Ufer montiert. Unmittelbar nach deren Fertigstellung begann die Belagerung und Aushungerung von Tenochtitlán.
Texcoco war eine der drei Städte im aztekischen Dreibund, dem neben Tenochtitlán außerdem noch das Volk von Tlacopán angehörte. Seit 1515 hatte es in Texcoco Streitereien um die Thronfolge gegeben. Der zu dieser Zeit herrschende König Cacama hatte nur dank der Unterstützung der Azteken König werden können. Daneben gab es noch einen anderen Mann, Ixtlilxochitl, der ebenfalls Anspruch auf den Thron von Texcoco erhob und zur Zeit der Invasion der Spanier ein Gebiet nördlich der Stadt kontrollierte. Als die Spanier in der Nähe von Texcoco erschienen, nutzte Ixtlilxochitl die Gelegenheit, um sich mit den europäischen Eindringlingen zu verbünden und Cacama zu vertreiben. Dadurch verschafften sich die Spanier eine gute Ausgangsstellung für einen erneuten Angriff auf Tenochtitlán. Bis jedoch das Holz für die Schiffe, die für die Kontrolle des Texcoco-Sees nötig waren, von Tlaxcala nach Texcoco geschafft war – was Anfang Februar 1521 geschah – und mit dem Bau der Brigantinen begonnen werden konnte, eroberte Cortés einige den Azteken tributpflichtige Städte.
Am 28. April 1521 war es dann soweit: Die dreizehn Brigantinen waren fertiggestellt. Von nun an riegelten sie Tenochtitlán durch eine Seeblockade von der Außenwelt ab und unterbrachen auf diese Weise die Lebensmittelversorgung der Stadt. Ab Mitte Mai unterwarfen dann Pedro de Alvarado, Cristóbal de Olid und Gonzalo de Sandoval die Städte am Ufer des Texcoco-Sees, darunter auch Tlacopán, und vollendeten so die Isolation der aztekischen Hauptstadt. Schließlich begannen die Spanier mit der stückweise erfolgenden Eroberung von Tenochtitlán.
Die Azteken hatten zuvor bereits Barrikaden auf den drei Dämmen errichtet, die die Stadt mit dem Festland verbanden und die nun die Spanier für einen direkten Angriff nutzen wollten. Die Kampftaktik der Azteken bestand darin, Barrikaden zu bauen, Teile der Dämme zu zerstören und Öffnungen in den Dammbrücken zu schaffen. Sobald die Spanier über diese Lücken setzten, wurden sie von den Azteken sofort eingekesselt. Diese Taktik hätte es ihnen am 30. Juni, auf den Tag genau ein Jahr nach der La Noche Triste, fast ermöglicht, Hernán Cortés zu töten. Es gelang ihnen nur dank des beherzten Eingreifens des Cristóbal de Olea nicht, der dafür mit seinem Leben bezahlte. Die Azteken verteidigten sich trotz der aussichtslosen Lage erbittert und lieferten den Spaniern in der Stadt selbst einen erbarmungslosen Häuserkampf.
Trotz der erbitterten aztekischen Verteidigung war der Fall von Tenochtitlán nur eine Frage der Zeit. Am 13. August 1521 durchbrachen die Spanier und ihre tlaxcaltekischen Verbündeten im Stadtteil Tlatelolco, der Jahrzehnte zuvor noch eine eigenständige Stadt gewesen war, die letzten Verteidigungslinien der Azteken. Die Stadt Tenochtitlán wurde danach vier Tage lang geplündert und ihre Einwohner zu Tausenden getötet.
Cuauhtémoc, der letzte Herrscher der Azteken, wurde noch am Tag des Falles der Stadt aufgegriffen und festgenommen. Er wurde zunächst am Leben gelassen, jedoch am 28. Februar 1525 hingerichtet, nachdem man ihn einer Verschwörung gegen Hernán Cortés bezichtigt hatte.
Auf den Trümmern Tenochtitláns entstand bald darauf eine neue Siedlung. Die Spanier ließen die alten Wasserleitungen, die in die Stadt führten, reparieren und Aufräumarbeiten durchführen. Nach zwei Monaten wurde den Einheimischen die Rückkehr in einige Stadtviertel erlaubt; das alte Zentrum wurde jedoch weiter von den Eroberern bewohnt, die sich aus den Überresten der alten aztekischen Gebäude neue, eigene Gebäude errichteten. So entstand der spätere Palast des Vizekönigs am Ort des aztekischen Königspalastes und auf den Trümmern der Tempel wurden christliche Kirchen errichtet. Den Texcoco-See legten die Spanier im Lauf der Jahrzehnte schrittweise trocken. 1535 wurde Tenochtitlán in Mexiko-Stadt umbenannt und zum Verwaltungssitz des neugegründeten Vizekönigreichs Neuspanien.
In den Jahren und Jahrzehnten nach dem Untergang der einheimischen Staaten wurden auch alle anderen Stämme und Völker Mexikos von den Spaniern unterworfen. Zudem strömten immer mehr spanische Siedler ins Land. Die überlebenden Angehörigen der lokalen Völker wurden zwangsweise christianisiert und zur Arbeit gezwungen; viele von ihnen hatten unter den von den Siedlern eingeschleppten Krankheiten und härtester körperlicher Arbeit zu leiden. Als einziger indigener Stamm genossen dagegen die Tlaxcalteken gegenüber den anderen Völkern einige Privilegien, die sie von den Spaniern aufgrund ihrer Unterstützung erhalten hatten.
Einige der ins Land gekommenen Missionare versuchten, die Kultur der ansässigen Völker zu verstehen und Nahuatl zu lernen, um der Bevölkerung das Christentum besser vermitteln zu können. Besonders Bernardino de Sahagún (* um 1499; † 23. Oktober 1590 in Mexiko-Stadt, Mexiko) ist hier zu nennen, der 1529 nach Mexiko kam und den Azteken dort Spanisch und Latein beibrachte, wobei er selbst Nahuatl lernte. Das Werk Historia general de las cosas de Nueva España, das unter seiner Anleitung von seinen aztekischen Schülern verfasst wurde, gibt unter anderem die aztekische Sicht der Eroberung wieder. Ein anderer Augenzeugenbericht – aus spanischer Sicht – wurde von Bernal Díaz del Castillo verfasst, der selbst Soldat unter Cortés gewesen war. Die Wahrhafte Geschichte der Eroberung von Neuspanien, spanisch Historia verdadera de la conquista de la Nueva España, wurde von ihm 40 Jahre nach Abschluss der Eroberung geschrieben und ist neben Cortés’ Briefen an König Karl V. der einzige bekannte erhaltene spanische Augenzeugenbericht.
Schätzungen zufolge sank die Einwohnerzahl der mexikanischen Urbevölkerung zwischen 1519 und 1565 von 25 Millionen auf 2,5 Millionen, vor allem aufgrund der von den Spaniern eingeschleppten Krankheiten, deren Auswirkungen durch die Zwangsarbeit auf den nach dem Encomienda-System arbeitenden spanischen Latifundien oder in den mexikanischen Minen noch verstärkt wurden. Die einheimische Kultur wurde dennoch nicht völlig von der spanischen Kultur verdrängt. So ist zum Beispiel die Sprache der Azteken, Nahuatl, noch sehr lebendig. Heutzutage wird das aztekische Vermächtnis in der mexikanischen Bevölkerung allgemein hochgehalten.
Bernardino de Sahagún: Historia general de las cosas de la Nueva España. Estudio introductorio, paleografía, glosario y notas de Alfredo López Austín y Josefina García Quintana. Drei Bände. Conaculta, México 2000, ISBN 970-18-4106-9 (Cien de México)
Bernal Díaz del Castillo: Geschichte der Eroberung von Mexiko. Herausgegeben und bearbeitet von Georg A. Narciß. 7. Auflage. Insel-Verlag, Frankfurt am Main u. a. 2009, ISBN 978-3-458-32767-7 (Insel-Taschenbuch 1067), (spanischer Originaltitel: Historia verdadera de la conquista de la Nueva España)
Arthur Schurig (Hrsg.): Die Eroberung von Mexiko durch Ferdinand Cortes. Mit den eigenhändigen Berichten des Feldherrn an Kaiser Karl V. von 1520 und 1522. Insel-Verlag, Leipzig 1923
José de Acosta: Das Gold des Kondors – Berichte aus der Neuen Welt 1590 und Atlas zur Geschichte ihrer Entdeckung Herausgegeben und übertragen von Rudolf Kroboth und Peter H. Meurer. Edition Erdmann in K. Thienemanns Verlag, Stuttgart u. a. 1991, ISBN 3-522-60750-3 (Originalausgabe: America, Oder wie mans zu Teutsch nennet Die Neuwe Welt/ oder West India. Von Herrn Josepho De Acosta in Sieben Büchern/ eins theils in Lateinischer/ und eins theils in Hispanischer Sprach/ Beschrieben. Sutorius, Ursel 1605. Nach dem Exemplar der Staatsbibliothek Preußischer Kulturbesitz, Berlin).
Maurice Collis: Cortés and Montezuma. New Directions, New York NY 1999, ISBN 0-8112-1423-0 (New Directions Paperbook 884 A New Directions Classic)
Serge Gruzinski: Drache und Federschlange. Europas Griff nach Amerika und China 1519/20. Campus, Frankfurt am Main 2014.
Ross Hassig: Mexico and the Spanish Conquest. 2nd edition. University of Oklahoma Press, Norman OK 2006, ISBN 0-8061-3793-2
Felix Hinz: „Hispanisierung“ in Neu-Spanien 1519–1568. Transformation kollektiver Identitäten von Mexica, Tlaxkalteken und Spaniern. 3 Bände. Kovač, Hamburg 2005, ISBN 3-8300-2070-8 (Studien zur Geschichtsforschung der Neuzeit 45), (Zugleich: Köln, Univ., Diss., 2004)
William Hickling Prescott: History of the Conquest of Mexico, with a preliminary view of the ancient Mexican civilization, and the life of the conqueror, Hernando Cortés. Bentley, London 1843, online, (Deutsche Übersetzung: Die Eroberung von Mexiko. Der Untergang des Aztekenreiches. C. H. Beck, München 1984, ISBN 3-406-09050-8)
Miguel León-Portilla: Visión de los vencidos Relaciones indígenas de la Conquista. introd., selección y notas: Miguel León-Portilla, Versión de textos nahuas: Ángel Ma. Garibay, 12ª. Edición, México, UNAM, 1989. http://www.biblioweb.dgsca.unam.mx/libros/vencidos/ (Memento  vom 18. November 2010 im Internet Archive)
Werner Stenzel: Das kortesische Mexiko. Die Eroberung Mexikos und der darauf folgende Kulturwandel. Lang, Frankfurt am Main u. a. 2006, ISBN 3-631-55208-4.
Hugh Thomas: Die Eroberung Mexikos – Cortés und Montezuma. Fischer, Frankfurt am Main 1998, ISBN 3-10-078003-5
Tzvetan Todorov: Die Eroberung Amerikas. Das Problem des Anderen. Aus dem Frz. übersetzt von Wilfried Böhringer. 8. Druck. Suhrkamp, Frankfurt am Main 2002, ISBN 3-518-11213-9 (Edition Suhrkamp 1213 = NF 213)

Die Spanische Grippe war eine Pandemie, die durch einen ungewöhnlich virulenten Abkömmling des Influenzavirus (Subtyp A/H1N1) verursacht wurde und zwischen 1918 und 1920 mindestens 25 Millionen, nach einer Bilanz der Fachzeitschrift Bulletin of the History of Medicine vom Frühjahr 2002 sogar knapp 50 Millionen Todesopfer forderte.Die Auswirkung der Pandemie ist damit in absoluten Zahlen mit dem Ausbruch der Pest von 1348 vergleichbar, der damals mehr als ein Drittel der europäischen Bevölkerung zum Opfer fiel. Eine Besonderheit der Spanischen Grippe war, dass ihr vor allem 20- bis 40-jährige Menschen erlagen, während Influenzaviren sonst besonders Kleinkinder und alte Menschen gefährden.
Varianten des Subtyps A H1N1 verursachten 1977/1978 den Ausbruch der Russischen Grippe und 2009 der „Schweinegrippe“-Pandemie.
Der Name Spanische Grippe entstand, nachdem die ersten Nachrichten über die Seuche aus Spanien kamen; als neutrales Land hatte Spanien im Ersten Weltkrieg eine relativ liberale Zensur, sodass dort im Unterschied zu anderen betroffenen Ländern Berichte über das Ausmaß der Seuche nicht unterdrückt wurden: Nachrichtenagenturen meldeten Ende Mai 1918, dass in ganz Spanien acht Millionen Menschen infiziert waren; in Madrid erkrankte jeder Dritte. Büros und Geschäfte mussten geschlossen werden. Die Straßenbahnen stellten ihren Dienst ein. Unter den Erkrankten waren auch der spanische König Alfons XIII. und einige seiner Kabinettsmitglieder. Die Agencia Fabra kabelte an Reuters in London:
In den anschließenden Presseberichten wurde die Bezeichnung „Spanische Grippe“ gebraucht. In der deutschen Presse durfte zwar nicht über Erkrankungen an der Front berichtet werden, wohl aber ab Anfang Juni 1918 – auch auf den ersten Seiten der Zeitungen – über zivile Opfer. In Deutschland wurde sie gelegentlich „Blitzkatarrh“ oder „Flandern-Fieber“ genannt, amerikanische Soldaten nannten sie „three-day fever“ (Drei-Tage-Fieber) oder „purple death“ (wegen der Hautverfärbungen), britische Soldaten bezeichneten sie als „flu“ oder „flandrische Grippe“, die Franzosen als „la grippe“ oder „bronchite purulente“ (eitrige Bronchitis) und die Italiener – fälschlicherweise – als „Sandfliegen-Fieber“. In Spanien hatte sich die Bezeichnung „gripe“ eingebürgert.
Die Spanische Grippe trat in drei Wellen auf: im Frühjahr 1918, im Herbst 1918 und in vielen Teilen der Welt noch einmal 1919. Die erste Ausbreitungswelle im Frühjahr 1918 wies keine merklich erhöhte Todesrate auf. Erst die Herbstwelle 1918 und die spätere, dritte Welle im Frühjahr 1919 waren mit einer außergewöhnlich hohen Letalität verbunden. Zum Höhepunkt der „Herbstwelle“ schätzten die preußischen und die Schweizer Gesundheitsbehörden, dass zwei von drei Bürgern erkrankt waren.
Ab Herbst/Winter 1918 starben weltweit zwischen 25 Millionen und 50 Millionen Menschen; manche schätzen die Zahl der Verstorbenen deutlich höher auf etwa 70 Millionen Opfer. Die genaue Zahl lässt sich nicht mehr ermitteln, da auch entlegene Regionen davon betroffen waren und in Ländern wie etwa Russland aufgrund der Nachkriegswirren die Zahl der an der Grippe Verstorbenen nicht zuverlässig erfasst wurde. Die US-amerikanische Armee verlor etwa so viele Infanterie-Soldaten durch die Grippe wie durch die Kampfhandlungen während des Ersten Weltkrieges. Allein in Indien sollen mehr als 17 Mio. Menschen an der Spanischen Grippe gestorben sein, was durch die nachfolgende Volkszählung von 1921 gut belegt erscheint.
Die Zeitspanne von nur einem Jahr für das Auftreten von drei pandemischen Wellen ist eine Besonderheit der Spanischen Grippe. Bei anderen Influenza-Pandemien, wie 1889/90, wurden Abstände von acht bis neun Monaten zwischen den einzelnen Wellen beobachtet. Die Ursache dieser „komprimierten“ Wellen ist unklar. Zahlreiche anekdotische Berichte sowie statistische Daten aus Spanien weisen darauf hin, dass Menschen, die während der ersten Welle erkrankt waren, in der zweiten Welle einen relativen Schutz gegen eine erneute Erkrankung genossen.
Die Letalität dieser Form des Influenzavirus bleibt unklar, da es keine exakten Daten zur Zahl der Erkrankten gibt; sie wird deutlich höher als 2,5 % vermutet. Andere Influenza-Pandemien wiesen eine Letalität unter 0,1 % auf.
Wo die Spanische Grippe sich zuerst manifestierte, ist nicht völlig gesichert. Dies ist weitgehend vor dem Hintergrund des Ersten Weltkrieges zu sehen. In den Schützengräben in Europa starben zu dieser Zeit wöchentlich tausende von Soldaten. Sowohl die Presse als auch die lokalen Gesundheitsbehörden konzentrierten sich daher wenig auf die ersten Grippefälle im Frühjahr 1918, zumal während der ersten Welle nur wenige Menschen der Krankheit erlagen.
Die These, dass es zu den ersten virulenten Grippeausbrüchen in den USA kam und sie von dort aus durch Truppenbewegungen weltweit verbreitet wurde, ist schon in den 1970er Jahren durch den australischen Medizin-Nobelpreisträger Frank Macfarlane Burnet aufgestellt worden. Heute vermutet eine Reihe von Wissenschaftlern, dass die Grippewelle in Haskell County im US-Bundesstaat Kansas ihren Ausgang nahm. Zum Jahresanfang 1918 behandelte dort der Landarzt Loring Miner zahlreiche Patienten, deren Grippesymptome das bisher Bekannte an Heftigkeit erheblich übertrafen. Den Krankheitsverlauf schilderte Miner als rasend schnell und gelegentlich tödlich. Miner war über diesen Krankheitsausbruch so beunruhigt, dass er sich an den U.S. Public Health Service wandte, wo man jedoch auf seine Bitte um Unterstützung nicht reagierte. Seine Warnung vor einer Grippeform mit ungewöhnlich heftigem Verlauf wurde dennoch im Frühjahr 1918 im Public Health Report veröffentlicht. Dank dieses Berichts konnte die Medizingeschichte einen möglichen Ansteckungsverlauf rekonstruieren. Belegt ist, dass mindestens drei Personen aus Haskell County Ende Februar in das US-Army-Ausbildungslager Camp Funston eingezogen wurden. Am 4. März erkrankte ein Koch namens Albert Gitchell an der Grippe, drei Wochen später waren in dem Ausbildungslager, in dem sich durchschnittlich 56.000 Rekruten befanden, 1100 Schwerkranke und 38 Todesfälle zu beklagen. Die Soldaten bezeichneten die Erkrankung als three-day fever oder knock-me-down fever. Von dem zur Militärbasis Fort Riley gehörenden Ausbildungslager breitete sich die Krankheit sehr schnell weiter aus. Am 18. März wurden Grippefälle auch in zwei Ausbildungslagern in Georgia gemeldet.
Der Krankheitsverlauf war grundsätzlich heftig und kurz und ging mit starkem Fieber, Kopf- und Gliederschmerzen einher. Den meisten Erkrankten ging es nach wenigen Tagen wieder besser. Todesfälle waren meist bei zusätzlicher Komplikation auf eine Lungenentzündung in Form einer Primärpneumonie durch die Grippeviren beziehungsweise in Form einer Sekundärpneumonie durch bakterielle Superinfektionen zurückzuführen.
In den beengten Verhältnissen der amerikanischen Ausbildungslager erkrankten bis zu 90 Prozent der dort versammelten Männer. Die Krankheit griff außerdem, ausgehend von den Lagern, auf die Zivilbevölkerung über. In den Ford-Werken in Detroit fielen im Frühjahr zeitweise bis zu 1000 Arbeiter wegen einer Erkrankung an der Grippe aus. Von den 1900 Insassen im kalifornischen Gefängnis San Quentin erkrankte jeder vierte; drei Häftlinge starben. Insgesamt hatten 30 der 50 größten US-amerikanischen Städte, von denen die meisten sich in der Nähe von Ausbildungslagern befanden, im April 1918 eine erhöhte Sterberate. Dieser Anstieg war jedoch nicht so signifikant, dass er von der Öffentlichkeit oder den Gesundheitsbehörden zu dieser Zeit wahrgenommen wurde. Erst Untersuchungen durch Epidemiologen nach Abklingen der Grippewelle deckten dieses statistische Muster auf.
Mit US-amerikanischen Truppentransportern gelangte die Krankheit offenbar nach Frankreich. Für Anfang April 1918 sind Grippeerkrankungen aus der französischen Hafenstadt Brest belegt, von wo sie sich sowohl in der Zivilbevölkerung als auch unter den Soldaten ausbreitete. In den französischen Lazaretten wurden die ersten grippeerkrankten Soldaten am 10. April eingeliefert. Ende April hatte die Grippewelle Paris erreicht. In den ersten zwei Wochen im Mai 1918 meldete die britische Marine über 10.000 Krankheitsfälle und sah sich außerstande, auszulaufen. Im Juni wurden zahlreiche Fälle aus Indien, China, Neuseeland und den Philippinen gemeldet. Auch in Deutschland lag der Höhepunkt der ersten Welle im Juni. Im Hafen von Manila erkrankten über zwei Drittel der Dockarbeiter, sodass Schiffe nicht mehr entladen werden konnten. Der deutsche General Ludendorff beklagte sich darüber, jeden Morgen die Krankheitsberichte seiner Heereskommandeure anhören zu müssen, und schob das Versagen der Sommeroffensive (die letzte deutsche Großoffensive vom 15. Juli 1918 bei Reims und in der Champagne verpuffte nahezu wirkungslos, trotz sehr starker Artillerievorbereitung) auf die niedrige Kampfmoral und den schlechten Zustand seiner Truppen. Als Ursache dafür nannte er die grassierende Grippewelle. Dänemark und Norwegen waren vor allem im Juli betroffen; in Holland und Schweden war der Höhepunkt der ersten Grippewelle im August. In Australien erkrankten 30 Prozent der Bevölkerung Sydneys im September an der Grippe.
Am 13. Juli 1918 erschien in der Ausgabe des britischen Medizinjournals The Lancet ein Artikel, in dem drei Ärzte spekulierten, dass es sich bei der aktuellen Epidemie möglicherweise nicht um Grippe handelte, weil der Verlauf so kurz und sehr häufig auch komplikationslos verlief. Ihnen war offenbar zu dem Zeitpunkt noch unbekannt, dass es bereits auffällige Ausnahmen von dem weitgehend harmlosen Verlauf gab. Ende Mai 1918 starben in einem kleinen französischen Militärlager fast fünf Prozent der dort stationierten Soldaten an der Grippe bzw. ihren Folgen. Und in Louisville, Kentucky, tauchte bereits das Muster auf, das aus heutiger Sicht eines der charakteristischen Merkmale der Spanischen Grippe ist. 40 Prozent der Todesopfer gehörten der Altersgruppe der 20- bis 35-Jährigen an.
Der Beginn der Herbstwelle lässt sich etwa auf die zweite Augusthälfte des Jahres 1918 terminieren. Die Krankheit brach mehr oder weniger gleichzeitig in der US-amerikanischen Stadt Boston, in der französischen Hafenstadt Brest an der Atlantikküste und in Freetown, der Hauptstadt des westafrikanischen Staates Sierra Leone, aus. Der Ausbruch in Freetown fällt zeitlich mit dem Einlaufen des britischen Kriegsschiffes HMS Mantua am 15. August zusammen. Bis Ende September waren zwei Drittel der Einwohner von Freetown an der Grippe erkrankt. Auf einhundert Erkrankte kamen drei Todesopfer.
In Boston war die Spanische Grippe mit dem aggressiveren Krankheitsverlauf das erste Mal am 27. August unter Marinesoldaten aufgetreten. Der erste zivile Erkrankte wurde am 3. September ins Boston City Hospital eingeliefert. Gut dokumentiert ist der Verlauf der Krankheit in der Militärbasis Camp Devens, die nur dreißig Kilometer westlich von Boston lag. Auf der eigentlich für 35.000 Soldaten ausgelegten Basis befanden sich zu der Zeit 45.000 Soldaten, 5000 Soldaten hatte man in einem Zeltlager auf dem Gelände der Basis untergebracht. Am 8. September erkrankte der erste Soldat so heftig, dass man zunächst eine Hirnhautentzündung bei ihm vermutete. Bereits am nächsten Tag war ein weiteres Dutzend Männer seiner Einheit erkrankt. Am 23. September lag die Krankenzahl bereits bei 12.604 Soldaten. 63 Soldaten starben an diesem Tag. Die Bedingungen, unter denen die Erkrankten gepflegt wurden, können als typisch für zahllose andere Lazarette und Krankenhäuser weltweit gelten, an denen die Spanische Grippe wütete. Obwohl die USA weniger unter den Folgen des Ersten Weltkrieges litten als die europäischen Staaten, fehlte es an Pflegepersonal. Man nutzte jeden verfügbaren Raum, um Krankenbetten aufzustellen. Frisches Bettzeug war Mangelware, sodass die Kranken in schmutzigen und blutbefleckten Laken lagen. Die Toten stapelten sich in den Gängen der Leichenhalle, und man kam mit ihrer Beerdigung kaum nach.
In dem Versuch, die Krankheit einzudämmen, versuchten leitende Militärärzte zu erreichen, dass nur die notwendigsten Schiffbewegungen zugelassen wurden. Vor dem Ablegen aus dem Hafen sollten die Schiffe eine Quarantäne durchlaufen, um zu verhindern, dass Kranke an Bord waren. Den Militärärzten gelang es jedoch nicht, diese Maßnahme durchzusetzen. Weder erhielten sie durch den Surgeon General of the United States Rupert Blue, der dem U.S. Public Health Service vorstand, irgendeine Form der Unterstützung, noch trafen sie auf irgendwelche Unterstützung innerhalb ihrer eigenen Organisation. Das US-amerikanische Militär wehrte sich erfolgreich gegen diese Maßnahme, da die in Europa kämpfenden Truppen dringend Verstärkung brauchten. Für die betroffenen Soldaten stellte dies allerdings eine Form von Lotterie dar. Von 100 Soldaten, die an Bord eines Truppentransporters auf dem Weg nach Europa erkrankten, starben sechs. Damit war die Letalität mehr als doppelt so hoch wie die der an Land Erkrankten.
Trotz der eingeleiteten Quarantänemaßnahmen breitete sich die Krankheit sehr schnell aus. Die Anzahl der Toten in den USA, die auf die Grippewelle zurückzuführen war, stieg von 2800 im Monat August auf mindestens 12.000 Tote im September. Ärzte aus den bereits betroffenen Städten im Osten Nordamerikas schickten ihren Kollegen im Westen düstere Mahnungen:
In weniger als vier Wochen hatte sich die Krankheit bis nach New Orleans, Seattle und San Francisco ausgebreitet. Der Ausbruch der Grippe konnte dabei sehr schnell erfolgen. In einer Militärbasis in Georgia wurden an einem Tag im September 1918 nur zwei Erkrankungen gemeldet, am nächsten Tag bereits 716. Eine der am stärksten betroffenen Städte der USA war Philadelphia, wo an einem einzigen Tag im Oktober 1918 711 Menschen der Krankheit zum Opfer fielen. Da die städtische Leichenhalle für maximal 36 Tote ausgelegt war, musste man die Toten vierreihig in Korridoren und Räumen lagern. Im kanadischen Montreal, wo am 21. Oktober 201 Menschen der Grippe erlagen, erteilten Priester die Sterbesakramente auf offener Straße.
So wie in Nordamerika verbreitete sich die Krankheit weltweit. Die Auswirkungen in Europa wurden dabei weniger aufmerksam verfolgt. Mehr im Fokus der Presse und der öffentlichen Aufmerksamkeit stand nach wie vor der Erste Weltkrieg. Stark betroffen waren aber auch Südamerika, Asien, Afrika und die pazifischen Inseln. In Indien war die Letalitätsrate mit geschätzten fünf Toten je hundert Erkrankten besonders hoch. Verstärkend trat hier hinzu, dass Indien zu dieser Zeit von einer Hungersnot heimgesucht wurde. Aus den ländlichen Regionen zogen viele in die größeren Städte, weil sie dort auf eine bessere Versorgung hofften. In den beengten Verhältnissen war die Ansteckungsgefahr besonders hoch. Auch in den durch vier Jahre Kolonialkrieg geschädigten Gebieten des heutigen Tansania, Sambia und Mosambik wütete die Seuche sehr stark.
Neuseeland wurde von der Grippewelle vor allem im November 1918 heimgesucht, als die ersten Truppen zurückkehrten. In Neuseeland starben 8600 Menschen an der Krankheit, mehr als doppelt so viele, wie neuseeländische Soldaten im Ersten Weltkrieg gefallen waren. Auf dem Höhepunkt der Krise kam das gesamte öffentliche Leben zum Erliegen. Von der Grippewelle besonders stark betroffen waren die Māori. In den entlegenen Māori-Gemeinschaften kam der Krankheitsausbruch meist ohne jede Vorwarnung. Häufig waren so viele betroffen, dass niemand mehr zur Verfügung stand, der die Kranken pflegte oder die Toten begrub. Ähnlich dramatisch war der Verlauf auf Samoa, wo ein Fünftel der Bevölkerung oder 7500 Menschen starben.
Auch die Berichte von kanadischen Missionsstationen, die der Sachbuchautor Pete Davies in seinem Buch Catching Cold zitiert, lassen darauf schließen, dass indigene Völker stärker als andere Bevölkerungsgruppen von der Grippewelle betroffen waren: Am 31. Oktober 1918 traf der Reverend Henry Gordon in der Inuit-Siedlung Cartwright im kanadischen Labrador ein. Er fand die Siedlung seltsam leer und verlassen und musste feststellen, dass von den 100 dort lebenden Personen 96 an der Grippe litten. In vielen Familien waren alle Mitglieder so stark erkrankt, dass sie nicht mehr in der Lage waren, sich um Nahrung oder um das Feuer zu kümmern. Bis Ende November waren in der Siedlung Cartwright 26 der 100 Personen verstorben. Gordon, der selber Anfang November an der Grippe erkrankt war, brach Ende November mit vier weiteren Männern auf, um weitere Inuit-Siedlungen aufzusuchen. In der Siedlung Okak hatten nur 59 von einstmals 266 Einwohnern überlebt. Zwei Wochen benötigte Gordon, um mit seinen vier Begleitern ein Massengrab zu schaufeln, in dem sie 114 Leichen beisetzten. In der Siedlung Hebron, wo von einer Gemeinschaft von einstmals 220 Inuit nur jeder dritte überlebte, beschwerten sie die Toten mit Steinen und ließen sie durch Eislöcher ins Meer gleiten. Selbst wer die Krankheit überlebte, war der Gefahr ausgesetzt, zu verhungern oder zu erfrieren. In Okak fanden die Missionare ein achtjähriges Mädchen, das fünf Wochen bei −30 Grad Celsius neben vier Leichen überlebte, indem es mit Weihnachtskerzen Schnee schmolz, um Trinkwasser zu gewinnen.
Die Krankheit verlief während der Herbstwelle oft sehr schnell, mit plötzlich einsetzendem hohen Fieber samt Schüttelfrost, starken Kopf- und Gliederschmerzen, Husten und starken Reizungen im Hals- und Rachenbereich. In manchen Fällen wurde auch Nasenbluten beobachtet. Während manche Patienten nur schwache Symptome entwickelten und sich ohne Komplikationen erholten, verstarben andere binnen Stunden an einer sich schnell entwickelnden hämorrhagischen, also von starken Blutungen begleiteten Lungenentzündung. Oftmals zeigten sich auch Symptome einer normalen Influenza, bei der sich jedoch eine bakterielle Lungenentzündung bildete, die ebenfalls häufig zum Tode führte. Häufig wurde eine begleitende, bläulich-schwarze Verfärbung der Haut beobachtet, die vom Mangel an Sauerstoff rührte.
Überlebende waren oft Wochen von starker Müdigkeit und chronischer Erschöpfung gezeichnet, nicht selten traten auch Depressionen als Folgeerscheinung auf. Wer eine Lungenentzündung überlebte, dem stand gar eine langwierige und mühsame Rekonvaleszenz bevor.
„Dritte Welle“ ist als einheitliche Bezeichnung problematisch, weil sie in den verschiedenen Regionen der Welt unterschiedlich verlief. In Deutschland kam es beispielsweise zum Jahresanfang 1919 zu einer weiteren, also dritten Welle, die erst im März oder April auslief. Generell scheint diese dritte Welle, abgesehen vom chinesischen Raum, allgemein schwächer verlaufen zu sein als die Herbstwelle. Deshalb handelte es sich mit hoher Wahrscheinlichkeit um eine echte Nachepidemie. Im Übrigen kam es in den meisten Teilen der Welt bis weit in die 1920er Jahre hinein zu weiteren Nachepidemien, die sich dann aber wieder an den „normalen“ jahreszeitlichen Rhythmus hielten, also in Europa beispielsweise im Winter auftraten. Man könnte daher im Anschluss an die Herbstwelle im Grunde statt von einer einheitlichen „dritten Welle“ auch von diversen aufeinanderfolgenden, voneinander unabhängigen Wellen sprechen.
Wegen des fulminanten Krankheitsverlaufs bezweifelten anfangs einige Forscher, dass es sich bei der Spanischen Grippe überhaupt um eine Form der Influenza handele. Unter anderem wurde als Auslöser der Pandemie eine Form der Lungenpest vermutet. Schon zu dieser Zeit wurde im Übrigen im medizinischen Schrifttum von „Viren“ geredet, damals allerdings im Sinne von „Krankheitserreger“ oder „Gift“. Dass es sich im heutigen Sinne um Viren handelte, wurde erst durch die Isolation von Influenza-Viren im Jahr 1933 festgestellt.
In der Öffentlichkeit kursierten eine Reihe unterschiedlicher Gerüchte über die Entstehung der Krankheit. Eine weit verbreitete Hypothese der damaligen Zeit besagte, die Grippe sei durch Konservendosen aus Spanien importiert worden, diese wären von den Deutschen vergiftet worden, welche die spanischen Konservenfabriken unter ihre Kontrolle gebracht hätten. Einer weiteren Theorie zufolge soll die Krankheit im US-Gefängnis Sing Sing ausgebrochen und von amerikanischen Soldaten nach Europa eingeschleppt worden sein. Sogar klimatische Faktoren sollen eine Rolle gespielt haben; Soldaten schlafen sehr oft unter freiem Himmel, und über den Tau seien sie mit dem Grippevirus in Berührung gekommen.US-Amerikaner vermuteten hinter dem Grippeausbruch den Verzehr von Fisch, der vom deutschen Kriegsgegner vergiftet worden sei, sahen den Staub ebenso als Krankheitsursache wie unsaubere Pyjamas oder zu leichte Kleidung, zogen geschlossene Fenster genauso in Erwägung wie offene oder den unvorsichtigen Umgang mit alten Büchern und schlossen auch kosmischen Einfluss nicht aus. Das Gerücht, dass Deutsche beigetragen hätten, die Krankheit in den USA zu verbreiten, wurde dabei sogar von offizieller Seite unterstützt. Am 17. September verkündete der Leiter der US-amerikanischen „Health and Sanitation Section of the Emergency Fleet Corporation“ Lt. Col. Philip Doane offiziell, dass nach seiner Ansicht Deutsche die Krankheit verursacht hätten:
Bereits sehr frühzeitig waren in einigen Ländern von den Gesundheitsbehörden Quarantänemaßnahmen eingeleitet worden. Bereits in der zweiten Augusthälfte 1918 hatte der Surgeon General of the United States angeordnet, dass die Gesundheitsbehörden in den USA in allen Häfen Schiffe mit Erkrankten an Bord unter Quarantäne stellen sollten. Aufgrund der Kriegsanstrengungen erwies sich das jedoch als kaum durchführbar. In Toronto veröffentlichte Dr. Hastings, ein Angestellter der Gesundheitsbehörde, Ratschläge, wie eine Ansteckung zu vermeiden sei. Dazu gehörte die Empfehlung, Menschenmengen zu meiden, Mund, Haut und Kleider immer reinlich zu halten und die Fenster möglichst viel geöffnet zu lassen. Man sollte sich kühl halten, wenn man zu Fuß unterwegs war, und warm, wenn man fuhr oder schlief. Hände sollten vor dem Essen gewaschen und das Essen gut gekaut werden. Die Ansammlung von Verdauungsprodukten im Körper sollte vermieden werden, nach dem Aufstehen sollte man direkt ein oder zwei Gläser Wasser trinken. Handtücher, Servietten und Besteck, das von anderen benutzt wurde, sollte man meiden. Ebenso sollte man auf zu enge Kleidung, Schuhe oder Handschuhe verzichten. (zitiert nach Davies, S. 115)
In New York stellte man das Spucken auf der Straße unter Strafe. Etwa 500 Personen wurden verhaftet, weil sie dagegen verstießen. Andere Städte ordneten das Tragen von Masken an und drohten mit Geldstrafen denen, die dagegen verstießen. In Atlanta wies eine gewisse Mrs. Hunnicutt darauf hin, dass schwere Seidenschleier einer Frau viel besser stünden. Werbungen in den Zeitungen priesen Feigensirup oder Eukalyptussalben als Heilmittel an. Antiseptische Sprays sollten Mund und Nase rein halten.
In Deutschland und anderen kriegsführenden Staaten wurde die Influenza häufig als Kriegsseuche interpretiert, sie war jedoch auch in neutralen Staaten, wie Spanien oder der Schweiz, weit verbreitet. Auch die Ernährungslage scheint nicht ausschlaggebend gewesen zu sein; so fiel zeitgenössischen Ärzten auf, dass gerade wohlgenährte Menschen besonders gefährdet waren.
Zwischen Juli 1918 und Ende Juni 1919 starben in der Schweiz gemäß offizieller Statistik 24.449 Menschen an der Spanischen Grippe. Das entspricht 0,62 Prozent der gesamten Bevölkerung im Jahre 1918. Mangels ärztlicher Meldepflicht geht man von einer großen Dunkelziffer aus.
Die nicht medikamentösen Behandlungsmethoden, die diätetische, physikalische und naturheilkundliche Maßnahmen umfassten, dazu Heißluft und elektrische Lichtbäder, Schwitz- und Prießnitzkuren, Bäder, Packungen und Umschläge, waren meist wirkungslos. Gleiches galt für die Vielzahl fragwürdiger Arzneimittel, die der Ärzteschaft in zahlreichen Aufsätzen empfohlen wurden: Malafebrin, Vioform, Sublimat, Kreosot. Angesichts der kulminierenden Gesundheitskatastrophe war für kritische Arzneimittelprüfungen keine Zeit.
Da wirkungsvolle spezifische Heilmittel nicht zur Verfügung standen, konzentrierten sich die Ärzte auf die Linderung der Symptome: Schon bei der Influenza anno 1889/90 stand die antipyretische Therapie im Vordergrund, und so manches Fiebermittel erlebte 1918/19 eine erstaunliche Renaissance, nicht nur das altvertraute Chinin, sondern auch die Pharmaspezialitäten Antipyrin, Salipyrin, Antifebrin und Phenacetin. Seit den 1890er Jahren vollzog sich der Siegeszug des Pyramidons und des Aspirins. Letzteres wurde nun zum zentralen Arzneimittel im Kampf gegen die Spanische Grippe. War in schweren Fällen eine stark sedative und antineuralgische Wirkung erwünscht, griffen die Ärzte des Jahres 1918 zu Substanzen wie Opium, Morphium, Heroin oder Kokain. 
Gegen den lästigen, oft quälenden Reizhusten standen narkotisierende und anästhesierende Hustenmittel parat, wie Kodein, das subkutan injizierbare Opium-Extrakt Pantopon oder Glyzerinanpräparate zur Inhalation. Als Expektorantien wurden Kampferbenzoe, Eukalyptusöl oder Ipecacuanhapulver verschrieben. Ein Leitmotiv der symptomatischen Grippebehandlung war die Aufrechterhaltung der Herz- und Kreislauffunktion, besonders bei lebensbedrohlichen Lungenentzündungen, wozu Digitalis, Strophantin, Koffein, Strychnin und Kampfer zum Einsatz kamen, und in besonderen Fällen Adrenalin. Zur Linderung der Atemnot boten sich Sauerstoffinhalationen an, doch die damit einhergehenden Nebenwirkungen begrenzten dessen Wert.
Große, aber letztlich vergebliche Hoffnungen setzte man in spezifisch wirksame Chemotherapeutika wie das Syphilismittel Salvarsan und dessen Nachfolger Neosalvarsan, ferner in kolloidale Silberpräparate wie Kollargol, Septargol, Elektrargol und Fulmargin und schließlich das in der Urologie bewährte Diuretikum Urotropin, dessen keimtötende Wirkung bei Staphylo- und Streptokokkeninfekten außer Streit stand. Weitverbreitet war der Einsatz der Chininderivate Eukupin, Optochin und Vuzin, freilich auch das ohne durchschlagenden Erfolg.
Die Letalität bei Influenza-Erkrankungen ist üblicherweise eine U-förmige Kurve, deren Maxima in den sehr jungen und sehr alten Bevölkerungsschichten liegen. Die Letalität der Spanischen Grippe ist hingegen eine W-förmige Kurve, eine Eigenart, wie sie auch schon bei der Pandemie von 1889/90 beobachtet wurde. Das dritte, atypische Maximum liegt im Bereich der 20- bis 40-Jährigen. Insgesamt werden die Todesfälle der 20- bis 40-Jährigen auf nahezu die Hälfte der gesamten Pandemietoten geschätzt. Als weitere Einzigartigkeit der Spanischen Grippe lag die Letalität bei Personen unter 65 Jahren deutlich höher als bei der Bevölkerung über 65; etwa 99 % der Toten entfielen auf die erste Gruppe, gegenüber 36 % und 48 % bei den Pandemien von 1957 und 1968.
Eine mögliche Erklärung dieser Anomalien ist ein vor 1889 kursierendes Virus, welches eine partielle Immunisierung bewirkte. Ein Problem dieser Vermutung ist jedoch, dass dieses Vorgänger-Virus um 1889 verschwunden, knapp 30 Jahre später jedoch wieder aufgetaucht sein müsste.
Gemäß Gibbs et al. in Spektrum der Wissenschaft vom Januar 2006 war ein Faktor bei der ungewöhnlichen Verteilung auch die durch das Virus induzierte atypisch starke Zytokin-Aktivität. Die Überreaktion des Immunsystems in Form eines Zytokinsturms veranlasst Abwehrzellen zu einem Angriff auf das Lungengewebe. Da gerade die Gruppe der 20- bis 40-Jährigen über ein besonders aktives Immunsystem verfügt, ist hier die Ausprägung des Zytokinsturms besonders stark.
Als Folge der Influenza-Infektion litten viele Menschen für den Rest ihres Lebens an neurologischen Funktionsstörungen, unter anderem wurde eine nennenswerte Häufung von Fällen der Encephalitis lethargica (EL) beobachtet. Hierbei handelt es sich um eine Form der Gehirnentzündung, die Lethargie, unkontrollierte Schlafanfälle und eine temporäre, der Parkinson-Krankheit ähnliche Störung auslöst.
Ein direkter Zusammenhang der EL mit der Spanischen Grippe ist jedoch nicht bewiesen worden. In von McCall et al. 2001 sowie Lo und Geddes 2003 untersuchten Gewebeproben fanden sich keine Hinweise auf das Influenza-Virus. Im Spielfilm Awakenings (deutsch: Zeit des Erwachens) mit Robert De Niro und Robin Williams wurde diese Krankheit 1990 auf der Grundlage des gleichnamigen Buches von Oliver Sacks thematisiert. Hintergrund des Filmes waren die kurzzeitigen Therapieerfolge Anfang der 1970er Jahre gegen die neurologischen Spätfolgen der Pandemie bei einigen Patienten nach dem Einsatz von Levodopa.
Die Geschwindigkeit, in der die Spanische Grippe um sich griff, spiegelt sich in makabrer Weise in dem Kinderreim „A bird named Enza“ wider:
1951 hatte der damalige Doktorand und später als Pathologe tätige Johan Hultin Gewebeproben aus einem Massengrab von Grippeopfern im Permafrostboden von Alaska exhumiert, jedoch keine Influenzaviren nachweisen können. 1997 beschaffte er sich eine Erlaubnis der auf der Halbinsel Seward gelegenen Gemeinde zur erneuten Exhumierung. Von vier Toten wurden Proben aus dem Lungengewebe entnommen, aus einer davon konnten Bruchstücke der Grippevirus-Gene isoliert werden. Schließlich gelang es, das komplette Genom des Erregers der Spanischen Grippe zu sequenzieren. Dieselbe Forschergruppe des Instituts für Pathologie der US-Streitkräfte in Rockville isolierte zudem 1996 und 1997 unter der Leitung von Jeffery Taubenberger Teile des Grippevirus aus unterschiedlichen Gewebeproben, die von der US-Armee aus dem Ersten Weltkrieg aufbewahrt wurden.
Im Jahr 2003 konnte durch Reid et al. bestätigt werden, dass das Virus zu den Influenza-A-Viren gehörte. 2004 haben Gamblin et al. durch Strukturanalyse des Hämagglutinins H1 gezeigt, wie sich das Virus der Spanischen Grippe an menschliche Zellen bindet.Im Oktober 2005 berichteten US-amerikanische Wissenschaftler um Jeffery Taubenberger, dass sie in einem Hochsicherheitslabor der CDC (Centers for Disease Control and Prevention) in Atlanta das Virus von 1918 rekonstruiert haben. Ihre Forschungsarbeiten wurden am 6. Oktober 2005 zusammen mit der kompletten Gensequenz in den Fachzeitschriften Science und Nature veröffentlicht.Die Forscher kamen aufgrund ihrer Analysen zu dem Schluss, dass die RNA-Polymerase des menschlichen Influenza-Virus direkt von einem Vogelgrippevirus abstamme und der Übergang auf den Menschen wahrscheinlich erst unmittelbar vor Beginn der Pandemie stattgefunden habe. Aufgrund der großen Ähnlichkeit mit bekannten Varianten der Geflügelpest vertreten sie ferner die Meinung, dass das Virus seine Gefährlichkeit als Folge weniger Mutationen erzielt habe und nicht durch einen Austausch von Erbanlagen mit bereits zuvor vorhandenen Varianten der menschlichen Influenza, d. h. nicht durch eine Reassortierung (vgl. auch Antigenshift bei Influenzaviren).
In Tierversuchen erwies sich das rekonstruierte Virus (wie aufgrund der hohen Todesraten der Epidemie von 1918 zu erwarten war) als extrem aggressiv: Es tötete Mäuse rascher als jedes andere bisher bekannte Influenza-Virus des Menschen und war – im Unterschied zu den meisten menschlichen Influenza-Viren – auch für Hühnerembryos tödlich. Im Gegensatz zu anderen Experimenten mit Mäusen musste das rekonstruierte Virus hierzu nicht erst an Mäuse angepasst werden. Dies zeigt, dass die Proteine Hämagglutinin wie auch möglicherweise die Neuraminidase des Virus Virulenzfaktoren für Mäuse enthalten. Seine Polymerase-Gene ähnelten denen von A/H5N1 und anderer Geflügelpest-Viren. Außerdem erwies es sich als äußerst vermehrungsfreudig in Epithelzellen aus menschlichen Bronchien, was im funktionstüchtigen Organ zur Lungenentzündung führen würde. Zusätzlich ist es in der Lage, anders als heute kursierende Influenza-Viren, sich ohne Trypsin vermehren zu können, was einen bisher unbekannten Mechanismus der Neuraminidase voraussetzt, der die Abspaltung des Hämagglutinins vereinfacht.
Bisher war das aktive Virus nur für einen Wissenschaftler an den CDC zugänglich gemacht worden. Seit Ende Oktober 2005 verschicken die Centers for Disease Control and Prevention das Virus der Spanischen Grippe an alle interessierten Labore der Biologischen Schutzstufe 3.
Hubert Parry († 7. Oktober 1918 in Knight’s Croft, Rustington, Sussex), Komponist und Musikwissenschaftler
Francisco de Paula Rodrigues Alves († 16. Januar 1919 in Rio de Janeiro), ehemaliger und neu-gewählter Präsident von Brasilien, starb vor dem Amtsantritt
Francisco Marto († 4. April 1919 in Aljustrel, heute Teil von Fátima), portugiesischer Heiliger und Zeuge der Marienerscheinung von Fátima
Jacinta Marto († 20. Februar 1920 in Lissabon), portugiesische Heilige und Zeugin der Marienerscheinung von Fátima
Rosalia Lombardo († 6. Dezember 1920 in Palermo), wurde nach ihrem Tod zu einer sehr gut erhaltenen Mumie in der Kapuzinergruft von Palermo.
PopulärwissenschaftlichJohn M. Barry: The Great Influenza. The Epic Story of the Deadliest Plague in History. Penguin Books, New York 2004, ISBN 0-670-89473-7.
Alfred W. Crosby: America’s Forgotten Pandemic. The Influenza of 1918. 2. Aufl. Cambridge University Press, Cambridge 2003, ISBN 978-0-521-54175-6.
Pete Davies: Catching Cold – The Hunt for a Killer Virus. Penguin Books, London 1999, ISBN 0-14-027627-0.
Niall Johnson: Britain and the 1918–19 Influenza Pandemic, A Dark Epilogue. Routledge, London / New York 2006, ISBN 0-415-36560-0.
Gina Bari Kolata: Influenza. Die Jagd nach dem Virus (= Fischer. 15385). Fischer Taschenbuchverlag, Frankfurt a. M. 2001, 2003, ISBN 3-596-15385-9.
Matthias Kordes: Die Spanische Grippe von 1918 und das Ende des Ersten Weltkrieges in Recklinghausen. In: Vestische Zeitschrift. Band 101, Nr. 07, 2006, S. 119–126, ISSN 0344-1482.
Harald Salfellner: Die Spanische Grippe. Eine Geschichte der Pandemie von 1918. Vitalis, Prag 2018, ISBN 978-3-89919-510-1.
Dieter Simon: Die „Spanische Grippe“-Pandemie von 1918/19 im nördlichen Emsland und einigen umliegenden Regionen. In: Emsländische Geschichte. Nr. 13, Studiengesellschaft für Emsländische Regionalgeschichte, Haselünne 2006, S. 106–145, ISSN 0947-8582.
Laura Spinney: 1918 – Die Welt im Fieber: Wie die Spanische Grippe die Gesellschaft veränderte. Hanser, München 2018, ISBN 978-3-446-25848-8.
Jeffery Taubenberger, Ann H. Reid, Thomas G. Fanning: Das Killervirus der Spanischen Grippe. In: Spektrum der Wissenschaft. Nr. 4, 2005, S. 52–60, ISSN 0170-2971.
Manfred Vasold: Die Spanische Grippe. Die Seuche und der Erste Weltkrieg. Primus, Darmstadt 2009, ISBN 978-3-89678-394-3.
Stefan Winkle: Kulturgeschichte der Seuchen. Artemis & Winkler, Düsseldorf 1997, ISBN 3-933366-54-2, S. 1045 ff.
Wilfried Witte: Erklärungsnotstand. Die Grippe-Epidemie 1918–1920 in Deutschland unter besonderer Berücksichtigung Badens. Centaurus, Herbolzheim 2006, ISBN 3-8255-0641-X.
Wilfried Witte: Tollkirschen und Quarantäne. Die Geschichte der Spanischen Grippe. Wagenbach, Berlin 2008, ISBN 978-3-8031-2633-7.FachliteraturJeffery K. Taubenberger et al.: Initial Genetic Characterization of the 1918 „Spanish“ Influenza Virus. In: Science. Band 275, Nr. 5307, 1997, S. 1793–1796, doi:10.1126/science.275.5307.1793, ISSN 0036-8075.
Jefferey K. Taubenberger, David M. Morens: 1918 Influenza, the Mother of All Pandemics. In: Emerging Infectious Diseases. Band 12, Nr. 1, 2006, S. 15–22, ISSN 1080-6040 (Volltext (PDF)).
Jefferey K. Taubenberger et al.: Characterization of the 1918 influenza virus polymerase genes. In: Nature. Band 437, 2005, S. 889–893, doi:10.1038/nature04230, ISSN 0028-0836.
Terrence M. Tumpey et al.: Characterization of the Reconstructed 1918 Spanish Influenza Pandemic Virus. In: Science. Band 310, Nr. 5745, 2005, S. 77–80, DOI:10.1126/science.1119392, ISSN 0036-8075.
Ann H. Reid, Thomas A. Janczewski, Raina M. Lourens et al.: 1918 Influenza Pandemic Caused by Highly Conserved Viruses with Two Receptor-Binding Variants. In: Emerging Infectious Diseases. Band 9, Nr. 10, 2003, ISSN 1080-6040, doi:10.3201/eid0910.020789.
Howard Phillips, David Killingray (Hrsg.): The Spanish Influenza Pandemic of 1918–19. New Perspectives. In: Routledge Studies in the Social History of Medicine. Bd. 12, London/ New York 2003, ISBN 0-415-23445-X (Enthält Aufsätze von Virologen, Ethnologen, Geografen und Historikern und eine umfassende Bibliografie).
Guido Steinberg: The Commemoration of the „Spanish Flu“ of 1918–1919 in the Arab East. In: Olaf Farschid, Manfred Kropp, Stephan Dähne (Hrsg.): The First World War as Remembered in the Countries of the Eastern Mediterranean (= Beiruter Texte und Studien. Bd. 99). Beirut 2006, ISBN 3-89913-514-8, S. 151–162.
Marc Hieronimus: Krankheit und Tod 1918: zum Umgang mit der Spanischen Grippe in Frankreich, England und in dem Deutschen Reich. Lit, Münster 2006, ISBN 978-3-8258-9988-2.
Eckard Michels: Die „Spanische Grippe“ 1918/19. Verlauf, Folgen und Deutungen in Deutschland im Kontext des Ersten Weltkriegs. In: Vierteljahrshefte für Zeitgeschichte. Bd. 58, Nr. 1, München 2010, S. 1–33.
Wilfried Witte: Die Grippe-Pandemie 1918–1920 in der medizinischen Debatte. In: Berichte zur Wissenschaftsgeschichte. Band 29, Nr. 1, 2006, S. 5–20, ISSN 0170-6233.
Michael Lange: Tanz mit dem Teufel – Die rätselhafte Spanische Grippe, Deutschlandfunk – „Wissenschaft im Brennpunkt“ vom 4. März 2018
TV-Dokumentation von Peter Solfrank Versteckte Weltkatastrophe: Die Spanische Grippe vor 100 Jahren In: Bayerisches Fernsehen 2018, abgerufen am 20. Dezember 2018
Spanish Influenza in North America, 1918–1919. In: Harvard University Library, Open Collections Program, Contagion, Historical Views of Diseases and Epidemics
