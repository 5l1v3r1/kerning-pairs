
Un paquebot est un navire spécialisé dans le transport de passagers en haute mer, que son but soit d'assurer une liaison (paquebots de ligne, dont les plus célèbres sont les transatlantiques) ou bien un voyage d'agrément (paquebots de croisière). Le terme est issu de la francisation de l'anglais packet-boat qui désignait les navires transportant du courrier.
Les premiers grands paquebots sont apparus au début du XIXe siècle avec l'essor des migrations internationales et du phénomène colonial. De nombreuses innovations techniques telles que la machine à vapeur et les coques en acier ont permis aux paquebots de gagner en taille et en puissance, un développement accéléré par la compétition entre puissances, notamment entre le Royaume-Uni et l'Allemagne, dont ils faisaient l'objet. Après deux âges d'or, au tournant des deux siècles puis dans l'entre-deux-guerres, le règne du paquebot, au moins en tant que moyen d'assurer des liaisons, s'est achevé pour laisser place à celui de l'avion à la fin des années 1950. Aujourd'hui, il n'existe plus que des paquebots de croisière, évoluant en particulier en Méditerranée et dans les Caraïbes, car même le dernier transatlantique, à savoir le Queen Mary 2, ne sert que dans le cadre de voyages d'agrément.
Au cours de l'âge d'or des paquebots, dans la première partie du XXe siècle, beaucoup de navires offraient des voyages extrêmement luxueux  pour un riche public. Cependant, ces mêmes navires transportaient un grand nombre de passagers relativement pauvres, à l'étroit sur les ponts inférieurs. Les bateaux les plus anciens ont souvent offert aux immigrés des prix très bas.
Les paquebots ont laissé une forte empreinte dans la mémoire collective. Certains, comme le Queen Mary ou le France, sont ainsi devenus des symboles nationaux tandis que d'autres, comme le Titanic ou le Lusitania, ont connu des fins dramatiques qui ont durablement marqué les esprits. Le cinéma et la littérature ont également largement contribué à créer ce « mythe du paquebot ».
Après l'effondrement du trafic au cours des années 1960, beaucoup de liners ont continué à naviguer comme navires de croisière.
Le plus grand paquebot actuellement en service est Harmony of the Seas, long de 362 mètres. Le Mauretania de la Cunard a été considéré, à l'époque, comme le plus beau paquebot de sa génération. Quelques décennies plus tard, les mêmes compliments ont été adressés au Normandie.
Les premiers paquebots apparaissent au milieu du XVIIe siècle. Ces navires étaient chargés de transporter le courrier — des « packets » — entre la Grande-Bretagne et le continent. Ils prenaient parfois à leur bord quelques passagers. La marine britannique est alors la plus puissante du monde. Elle exploite de nombreuses routes commerciales de par le monde pour importer des matériaux exotiques. La mer du Nord et la mer Baltique deviennent également des routes commerciales majeures. Cependant, pendant la première moitié du XIXe siècle le service des « packets » disparaît au profit du paquebot moderne.
Au début du XIXe siècle, la révolution industrielle et la croissance des échanges commerciaux inter-continentaux — par le biais, notamment, des colonies — rend impérieux le développement de liaisons sûres entre ces terres. Au premier rang des puissances coloniales, le Royaume-Uni a besoin de routes maritimes stables pour relier les différentes parties de son empire : Extrême-Orient, Inde, Australie, etc. Depuis le XVIIIe siècle, les grandes puissances maritimes s'équilibrent, et le rêve du Hollandais Hugo Grotius selon lequel les mers appartiennent à tous se concrétise. La naissance du concept d'« eaux internationales » et l'absence de revendication qui s'ensuit, simplifie la navigation. Le début de ce XIXe siècle est l'ère des clippers, puissants voiliers au gréement imposant, capables d'atteindre des vitesses parfois supérieures à 20 nœuds. Ces navires long-courriers sont principalement américains et assurent de nombreuses liaisons ; ils peuvent ainsi rallier Macao à New York en 70 jours.
En 1818, des marchands américains fondent la Black Ball Line qui est la première compagnie à assurer une liaison transatlantique régulière. Rapidement, de nombreuses autres compagnies l'imitent de par le monde. Cependant, la domination reste américaine. La Guerre de Sécession freine le développement maritime : naufrages et faillites entament la suprématie américaine. La fin de la domination des clippers est définitive lorsqu'une nouvelle technologie apparaît : la navigation à vapeur.
En 1807, l'ingénieur Robert Fulton réussit à appliquer le principe de la machine à vapeur aux navires. Il construit la première embarcation mue par cette technologie, le Clermont, qui parvient à rallier New York à Albany en une trentaine d'heures, avant d'entamer un service régulier entre ces deux villes. D'autres vaisseaux reprennent rapidement cette innovation. En 1816, l'Elise est le premier navire à vapeur à traverser la Manche. 1819 voit un nouveau progrès important. Le Savannah devient le premier navire à vapeur à traverser l'Atlantique. Parti de la ville du même nom, il rallie Liverpool en 27 jours. Toutefois, la traversée s'effectue principalement à la voile alors que la vapeur ne sera pas utilisée plus de 72 heures au cours du voyage. L'engouement du public pour cette nouvelle technologie se réduit et, des 32 passagers qui avaient réservé une place à bord, aucun n'embarque sur le Savannah. À cette époque, la navigation à vapeur ne convainc toujours pas les professionnels qui ne voient en elle qu'une curiosité, au point qu'en 1820, le propriétaire du Savannah débarrasse son navire de sa machine à vapeur.
Les travaux sur cette technologie continuent et un nouveau pas est franchi en 1833. Le Royal Edward réussit à traverser l'Atlantique en ayant recours à la vapeur sur les trois quarts du parcours. La voile n'est alors utilisée que pour permettre de décrasser les parois des chaudières couvertes de sel. Cependant, les sceptiques sont toujours nombreux au point qu'en 1836, le physicien Dionysius Lardner déclare que « le voyage direct d'un bateau à vapeur entre Liverpool et New York est aussi chimérique qu'un voyage sur la Lune ».
La dernière étape vers la navigation à vapeur est franchie lorsque le Sirius, parti de Liverpool le 3 avril 1838, atteint New York 19 jours plus tard après une traversée mouvementée. En effet, trop peu de charbon ayant été prévu pour la traversée, l'équipage doit brûler voiles et mobiliers pour arriver à destination. Le voyage se fait à une vitesse de 6,7 nœuds. Il est rendu possible par la mise en œuvre de condenseurs alimentant les chaudières en eau douce, ce qui évite d'avoir à ôter le sel. L'exploit est de courte durée. En effet, le lendemain, le Great Western, de l'architecte visionnaire Isambard Kingdom Brunel arrive à son tour à New York. Parti de Liverpool le 8 avril, il pulvérise le record du Sirius avec une vitesse moyenne de 8 nœuds. La course à la vitesse est lancée et avec elle, la tradition du « Ruban bleu », remis au navire ayant effectué la traversée la plus rapide de l'Atlantique qui gagne ainsi le droit d'arborer un pavillon bleu à son mât.
Avec le Great Western, Brunel pose les bases des nouvelles techniques de construction navale. Il s'est rendu compte que la capacité d'un navire augmente plus rapidement que sa résistance à l'eau ; en d'autres termes, la proportion de charbon à transporter par rapport à la taille du navire diminue lorsque le navire est plus imposant. Construire de grands navires est donc plus rentable. De plus, durant les années 1830, les migrations à destination des Amériques augmentent énormément. Ces mouvements de population sont une manne financière pour les compagnies maritimes dont certaines parmi les plus grandes voient le jour à cette époque. C'est en particulier le cas de la P&O en 1822 au Royaume-Uni, et en France de la Compagnie générale maritime en 1855.
La vapeur permet également aux navires d'assurer un transport régulier que ne permettait pas la voile. Cet aspect séduit particulièrement les compagnies postales, qui louent les services de ces bâtiments afin de desservir des clients séparés par l'océan. Samuel Cunard fonde en 1839 la Cunard Line et devient le premier à dédier son activité au transport de courrier assurant dès lors un service régulier suivant un calendrier déterminé. Ses navires exploitent les routes entre le Royaume-Uni et les États-Unis et sont rapidement surnommés « paquet boats », termes à l'origine du mot « paquebot ». Progressivement, ces paquebots long-courriers abandonnent la roue à aubes, peu pratique en haute mer, au profit de l'hélice.
La taille des navires allant en augmentant, les coques en bois se fragilisent : l'apparition de coques en fer, à partir de 1845, puis en acier, remédie à ces problèmes. Le premier navire à double coque en fer est le Great Britain, nouvelle création de Brunel, qui est également le premier navire sur lequel une hélice remplace les roues à aubes ; une fois encore, le navire effraie la clientèle et sa carrière se révèle désastreuse : le navire s'échoue et est reconverti en entrepôt. Il est finalement transformé en musée plus de 150 ans après son inauguration. La compagnie américaine Collins Line suit une autre approche. Elle prend le parti du luxe avec des navires pourvus de chambres froides, de systèmes de chauffage et d'innovations diverses, mais l'opération est coûteuse. Les naufrages de deux de ses navires portent un coup fatal à la Collins Line qui est dissoute en 1858.
En 1858, Brunel construit son troisième et dernier géant, le Great Eastern. D'une longueur de 212 mètres, ce paquebot reste pendant près de 40 ans le plus grand objet flottant jamais construit. Il peut en théorie embarquer 5 000 personnes. Cependant, sa carrière est ruinée par une suite d'échecs et d'incidents : constructeur et armateurs font faillite, et la traversée transatlantique inaugurale maintes fois repoussée se fait presque à vide.
Entre l'Allemagne et les États-Unis également, beaucoup de navires sont au départ des grands ports allemands de Hambourg et de Brême avec des compagnies comme l'HAPAG et la Norddeutsche Lloyd. L'année 1858 est marquée par un accident majeur, le naufrage de l'Austria, le 13 septembre. Le navire construit à Greenock en 1857, qui reliait bi-mensuellement les deux villes de Hambourg et New York via Southampton subit un incendie accidentel qui fit succomber une bonne part des 534 passagers et membres d'équipage, au large de Terre-Neuve. 88 survivants témoigneront de la catastrophe.
Sur le marché britannique, la Cunard Line et la White Star Line se livrent une forte concurrence à partir du rachat de cette dernière par Thomas Ismay à la fin des années 1860. La lutte prend pour symbole l'obtention du Ruban bleu que les deux compagnies décrochent plusieurs fois à la fin du siècle et qu'elles partagent avec certains navires de l’Inman Line. Le luxe et la technologie des navires évoluent également. Les voiles auxiliaires deviennent obsolètes et disparaissent complètement à la fin du XIXe siècle tandis que les paquebots adoptent une silhouette plus moderne avec une proue tombant à angle droit. Un usage militaire possible des paquebots est envisagé et le Teutonic devient, en 1889, le premier croiseur auxiliaire de l'histoire : en cas de guerre, il peut facilement être équipé de canons et utilisé en cas de conflit. Le navire parvient à impressionner jusqu'à l'empereur allemand Guillaume II qui désire voir son pays doté d'une flotte moderne.
En 1897, l'Allemagne lance le Kaiser Wilhelm der Grosse de la Norddeutscher Lloyd suivi quelques années après par trois sister-ships. Ce navire long-courrier présente de nombreuses innovations : il est très luxueux et rapide et parvient à dérober le Ruban bleu aux Britanniques. Il est également le premier des quatorze paquebots à quatre cheminées qui voient le jour dans l'histoire de la marine. Le navire n'en aurait eu besoin que de deux, mais un plus grand nombre de cheminées donne aux passagers une impression de sécurité et de puissance sur laquelle les constructeurs jouent.
En 1900, la HAPAG réplique avec le Deutschland, paquebot de quatre cheminées construit pour la vitesse. Celui-ci ravit facilement le Ruban bleu à son compatriote, avant de le perdre au profit d'un de ses navires jumeaux . Cette course à la vitesse se fait cependant au détriment du confort et engendre de fortes vibrations, qui font perdre tout succès au navire une fois son record battu. Le Deutschland n'est utilisé que pendant dix années comme transatlantique avant d'être reconverti prématurément en navire de croisière. Jusqu'en 1907, le Ruban bleu reste entre les mains des Allemands, au grand dam des Britanniques.
Ceux-ci ont par ailleurs d'autres problèmes : le financier américain John Pierpont Morgan embrasse l'idée d'un empire maritime regroupant un grand nombre de compagnies. Il fonde l’International Mercantile Marine Company qui regroupe dans un premier temps des compagnies maritimes américaines, et à partir de 1902, la Leyland Line et la White Star Line. Par ailleurs, son trust signe des accords avec les plus grandes compagnies allemandes de telle façon que sa seule concurrente sérieuse est la Cunard Line. Le gouvernement britannique décide donc d'intervenir pour reprendre l'ascendant.
Bien que les Allemands dominent sur le plan de la vitesse, les plus gros navires battent pavillon britannique. L'Oceanic et les Big Four de la White Star Line sont en effet les premiers à surpasser largement le record établi quarante ans plus tôt par le Great Eastern. Cependant, leur propriétaire est américain. Face à cette importante concurrence, le gouvernement britannique propose à la Cunard Line de participer financièrement à la construction de deux paquebots à la taille et la vitesse inégalée. En contrepartie, ceux-ci pourront être convertis en croiseurs en cas de guerre. Le résultat de cet accord est la mise en service, en 1907, de deux jumeaux : le Lusitania et le Mauretania, qui reprennent le Ruban bleu dès leurs traversées inaugurales respectives. Le deuxième conserve cette distinction pendant plus de vingt ans. Leur vitesse est accrue par l'utilisation de turbines au lieu des classiques machines à expansion. D'autres navires cumulent les deux systèmes de propulsion pour plus d'efficacité.
La White Star Line réplique fin 1908 - début 1909. Elle lance la construction des navires de classe Olympic, dont la mise en service est prévue entre 1911 et 1914. Le premier, l’Olympic commence une carrière honorable, bien que ponctuée d'incidents. Ce n'est pas le cas de son jumeau, le tristement célèbre Titanic, qui fait naufrage lors de son voyage inaugural le 15 avril 1912. Cette catastrophe met en lumière les nombreux problèmes de sécurité dont souffrent les navires, notamment en termes de moyens de sauvetage, ce qui entraîne un durcissement de la réglementation.
À la même époque, la France tente de marquer sa présence avec l'arrivée de son seul « quatre-cheminée ». En 1912, le France est inauguré. C'est un bâtiment modeste par rapport à la concurrence. L'Allemagne réagit à l'offensive britannique. La HAPAG lance un trio de paquebots géants nettement plus grands que ceux de la White Star Line. L'Imperator est le premier à arriver en 1913. Il est suivi du Vaterland, peu avant le déclenchement de la Première Guerre mondiale. La construction du troisième géant, le Bismarck, est stoppée par les hostilités,.
La Première Guerre mondiale est une époque tourmentée pour les paquebots. Certains comme le Mauretania, l'Aquitania, le Britannic ou encore le France sont transformés en navires-hôpitaux durant le conflit,,,. D'autres deviennent des transports de troupes, tandis que certains, tels que le Kaiser Wilhelm der Grosse, participent en tant que navire de guerre aux hostilités. Ce dernier type d'opérations est cependant vite jugée inutile, les navires offrant des cibles trop faciles. Le transport est en revanche très prisé du fait de leur grande taille. Les navires sont peints de formes géométriques et colorées selon le système de camouflage Dazzle pour éviter leur torpillage par les sous-marins ennemis.
La guerre est cependant marquée par son lot de naufrages de paquebots ; ainsi, le Britannic sombre en 1916 en mer Égée après avoir heurté une mine. Nombre de torpillages ont lieu et un grand nombre de navires coule. Le Kaiser Wilhelm der Grosse sombre à la suite d'un épique combat au large des côtes africaines en 1914, tandis que son jumeau le Kronprinz Wilhelm participe à des traques en mer pour aborder et couler des navires marchands. Le torpillage du Lusitania, le 7 mai 1915, coûte pour sa part la vie à plusieurs dizaines de citoyens américains alors que les États-Unis sont encore neutres. Bien que d'autres facteurs entrent en jeu, l'impact de ce naufrage pousse fortement l'opinion américaine en faveur des Alliés et facilite l'entrée en guerre du pays.
Les pertes sont compensées par le traité de Versailles en 1919 qui conduit à la remise aux vainqueurs de nombreux paquebots allemands : le trio de la HAPAG est partagé entre la Cunard, la White Star et les United States Lines tandis que les trois navires survivants de la classe Kaiser sont réquisitionnés par la marine américaine dans le cadre du conflit, puis conservés. Des plus grands paquebots allemands, seul le Deutschland échappe à ce destin à cause de son mauvais état.
Après une période de reconstruction, les compagnies maritimes se remettent rapidement des dégâts causés par la guerre. Les navires commencés avant la guerre tel que le Paris de la Compagnie générale transatlantique sont achevés et mis en service. Les navires britanniques bien établis tels que l’Olympic et le Mauretania reprennent également du service et connaissent un grand succès au début des années 1920, tandis que des paquebots plus modernes font leur apparition. Ils sont tout d'abord français, avec l’Île-de-France, qui est mis en service en 1927. Les États-Unis ont pour leur part récupéré le Vaterland, qui, renommé Leviathan devient le fleuron de leur flotte marchande. Cependant, la Prohibition, qui s'applique sur les navires américains comme sur tout le territoire, les pénalise fortement au profit des navires européens.
L'Allemagne revient sur le devant de la scène en 1929 avec les navires-jumeaux Bremen et Europa qui ravissent le Ruban bleu au Mauretania et s'imposent par leur modernité. L'Italie n'est pas en reste avec l'arrivée, dans les années 1930, du Rex et du Conte di Savoia qui battent également des records de luxe et de vitesse. La France, enfin, marque l'âge d'or des paquebots avec le Normandie, en 1935 : summum du luxe, le paquebot est également rapide et gigantesque, et devient une fierté nationale.
Cependant, l'heure est également à la crise. Les États-Unis ayant drastiquement réduit leurs quotas d'immigrants, les compagnies maritimes perdent une grande partie de leurs revenus et doivent se reconvertir. La Grande Dépression joue également un rôle important et, les traversées rentables allant en diminuant, les compagnies redirigent une partie de leurs vaisseaux vers des croisières touristiques plus rentables. Au Royaume-Uni, Cunard et White Star sont très mal en point, et le Chancelier de l'Échiquier Arthur Neville Chamberlain propose de fusionner les deux compagnies pour créer une compagnie britannique forte. Toutes deux fusionnent en 1934, et lancent la construction du Queen Mary tout en envoyant progressivement leurs anciens navires à la casse. La construction d'un deuxième navire, le Queen Elizabeth, est perturbée par le début de la Seconde Guerre mondiale.
La Seconde Guerre mondiale est un conflit riche en événements concernant les paquebots. Dès le début des combats, les navires allemands sont réquisitionnés dans le cadre des hostilités, souvent comme casernes flottantes. C'est dans le cadre de cette activité que le Bremen prend feu et sombre en 1941. À l'inverse, le Queen Elizabeth, bien qu'inachevé, part pour les États-Unis, après une opération de désinformation conséquente visant à détourner le feu allemand du navire. Ce paquebot et le Queen Mary se distinguent alors pour les transports de troupes.
De terribles naufrages marquent le conflit : le Lancastria sombre, corps et biens, en 1940 avec 4 000 à 7 000 personnes à son bord,. En 1945, le bombardement du paquebot Cap Arcona tue 5 000 personnes, pour la plupart des prisonniers du camp de concentration de Neuengamme. À la même époque, le Wilhelm Gustloff est torpillé par les Soviétiques et coule avec plus de 9 000 réfugiés à son bord, ce qui en fait le naufrage le plus meurtrier de l'histoire.
D'autres naufrages sont nettement moins meurtriers mais causent la perte des navires : le Rex est ainsi bombardé et coulé, tandis que le Normandie prend feu durant sa transformation en transport de troupes dans le port de New York.
Après-guerre, certains navires sont à nouveau transférés des vaincus aux vainqueurs au titre des dommages de guerre. C'est le cas de l'Europa ainsi cédé à la France qui lui donne le nom de Liberté. Par ailleurs, les chantiers de La Ciotat achèvent la construction de La Marseillaise (qui devait à l'origine s'appeler Maréchal Pétain), destiné à desservir les colonies pour le compte des Messageries maritimes. Cependant, avec la décolonisation, ce type de navire devient inutile et est rapidement revendu à des compagnies de croisières.
Les États-Unis sont pour leur part fortement impressionnés par les états de service des deux Queen durant la guerre. Désirant un navire fiable et rapide en cas de guerre contre l'URSS, ils font construire en 1952 le United States, qui bat tous les records de vitesse établis et détient depuis lors le Ruban bleu. L'Italie construit peu après le prestigieux Andrea Doria qui fait naufrage en 1956.
La concurrence des avions de ligne se fait cependant de plus en plus pressante et, au début des années 1960, l'avion représente 95 % du trafic de passagers sur la traversée de l'Atlantique. Le règne du paquebot se termine. La France lance cependant un dernier transatlantique, le prestigieux France, qui, même s'il naviguera jusqu'en 1972, se montre  peu rentable. Il illustre parfaitement la mutation qui s'effectue alors, et une grande partie de son temps est destiné à des croisières en divers points du globe.
Les navires de croisières ne sont pas un fait nouveau. La P&O en organisait déjà dans les années 1840 et, lorsqu'il s'est montré peu rentable en tant que transatlantique, le Deutschland est également devenu un navire consacré aux croisières de luxe. À partir des années 1930, les grandes compagnies consacrent leurs transatlantiques à des croisières durant la morte saison. À partir des années 1950 et 1960, la croisière devient la première activité des paquebots, qui n'effectuent plus que quelques traversées transatlantiques. En 1979, le France, devenu Norway, est ainsi totalement transformé pour s'y consacrer à plein temps.
Au fur et à mesure, les navires deviennent de véritables hôtels flottants, la superstructure s'élevant de plus en plus haut. Les grandes sociétés de croisières sont désormais Costa Croisière (du groupe leader mondial des croisières Carnival Group) , MSC Croisières ou encore la Royal Caribbean Cruise Line, qui possèdent de nombreux navires. Celles-ci mettent à disposition de leur clientèle des restaurants, des salles de gymnastique, des piscines, des théâtres, des cinémas ou bien des casinos, ainsi que des offres de prestations de luxe comme des suites, de la balnéothérapie, du thermalisme entre autres. Le navire n'est plus un moyen de traverser une mer ou un océan, mais devient un but en soi.
Les paquebots assurant des liaisons transatlantiques régulières se réduisent rapidement en nombre. Le United States est ainsi retiré du service en 1967. La même année, le Queen Mary est transformé en musée à Long Beach en Californie. En tout état de cause, ces navires ne sont plus à proprement parler des paquebots de ligne dans la mesure où ils assurent avant tout un voyage d'agrément. C'est ainsi le cas, jusqu'en 2008, du Queen Elizabeth 2 qui continue à perpétuer la tradition des liners en effectuant plusieurs traversées transatlantiques annuelles. À partir de 2004, le Queen Mary 2, de la même compagnie Cunard Line, fait de même avec un certain succès auprès d'une clientèle désireuse de revivre cette expérience.
D'autres paquebots de ligne ont été conservés comme témoignage de cette époque : le United States repose le long d'un quai de Philadelphie en attente d'un repreneur, tandis que le Rotterdam, fleuron de la marine néerlandaise depuis 1959 est transformé en février 2010 en hôtel et musée flottants dans la ville dont il porte le nom.
Les paquebots de ligne doivent dans un premier temps répondre à une demande croissante. Les premiers paquebots sont par ailleurs souvent considérés comme des « bateaux-cercueils », car les conditions d'hygiène déplorables y entraînent des taux de mortalité dépassant les 25 %. Enrayer ce phénomène nécessite des navires plus grands, pour diminuer l'entassement des passagers, et plus puissants, pour réduire le temps de traversée. La vapeur et les coques en fer puis en acier qui apparaissent au cours du XIXe siècle permettent ces avancées. Ainsi, le Great Western, en 1838, mesure 65 mètres de long, tandis que le Great Eastern en mesure 212 vingt ans plus tard. Son record n'est par ailleurs battu que 40 ans plus tard, en longueur par l’Oceanic et en tonnage par le Celtic. Le tonnage croît par la suite de façon exponentielle : alors qu'il atteint pour la première fois les 20 000 tonneaux de jauge brute (tjb) en 1901 avec les Big Four de la White Star Line, les géants de classe Olympic atteignent les 45 000 tjb dix ans plus tard. Dans les années 1930, le Normandie est presque à 80 000 tjb. Le Queen Elizabeth est alors, en 1940, le plus gros navire jamais construit, avec plus de 83 000 tjb, et il le reste jusqu'en 1997. Au début du XXIe siècle, cependant, certains paquebots comme le Queen Mary 2 ou le Freedom of the Seas dépassent largement les 100 000 tjb.
Au début des années 1840, la vitesse moyenne d'un paquebot est d'à peine 10 nœuds (une traversée de l'Atlantique dure donc une douzaine de jours). Elle passe dans les années 1870 à 15 nœuds (environ une semaine de traversée) grâce aux progrès réalisés dans le mode de propulsion des navires : les chaudières à vapeur rudimentaires cèdent le pas à des machineries plus élaborées ; les roues à aubes disparaissent progressivement, remplacées d'abord par une hélice puis par deux. Au début du XXe siècle, les « lévriers des mers » de la Cunard, Mauretania et Lusitania atteignent 27 nœuds. Les records semblent alors imbattables, et la plupart des compagnies abandonnent la course à la vitesse au profit de la taille, du luxe et de la sécurité. L'apparition de navires alimentés par un moteur Diesel, ou dont les machines à vapeur préfèrent le mazout au charbon, comme le Bremen, au début des années 1930, relance la course au Ruban bleu, qui change plusieurs fois de mains au cours de la décennie avant d'être conquis par le Normandie en 1935 puis par le Queen Mary en 1938. Il faut attendre 1952 pour que le United States établisse un record de vitesse définitif, à 34,5 nœuds (3 jours et 12 heures de traversée). Par ailleurs, depuis 1935, le Ruban bleu est accompagné du Trophée Hales, remis au vainqueur. Cependant, le développement des croisières à partir des années 1960 diminue radicalement les besoins de vitesse.
Le nombre de cheminées d'un navire est pendant longtemps un facteur important. Jusque dans les années 1890, elles sont accompagnées d'une mâture fonctionnelle destinée à recevoir des voiles. Le gréement est peu à peu abandonné avec le temps, son utilité étant de moins en moins évidente. Jusqu'à cette époque, les paquebots arborent souvent des cheminées au nombre de deux. Outre leur fonction technique, les cheminées signalent également la compagnie à laquelle appartient le navire par le biais d'un code de couleurs : les cheminées de la White Star Line sont ainsi ocre brun à manchette noire ; celles de la Transat sont rouges à manchettes noires et ainsi de suite.
Avec le Kaiser Wilhelm der Grosse, les Allemands lancent la mode des paquebots à quatre cheminées : quatorze naviguent en tout, dont douze dans l'Atlantique Nord : cinq sont allemands, un français et les huit autres britanniques. Bien que non fonctionnelles, elles donnent une illusion de sécurité et de puissance qui attire les passagers. La mode passe cependant rapidement et l'Aquitania, en 1914, est le dernier paquebot à quatre cheminées. Par la suite, le nombre de cheminées se réduit, et celles-ci se font plus profilées et tassées. Certains navires, comme le Rotterdam en 1959, n'arborent même plus de cheminées.
Les premiers paquebots sont conçus pour transporter majoritairement des migrants. Les conditions sanitaires sont souvent déplorables et les épidémies sont fréquentes. En 1848, des lois maritimes imposant des règles d'hygiène sont adoptées et améliorent les conditions de vie à bord. Peu à peu, deux classes distinctes se développent, la classe cabine et l'entrepont. Les occupants de la première, minoritaires, sont des passagers fortunés qui bénéficient d'un certain confort. Les autres sont des migrants massés dans de grands dortoirs. Jusqu'au début du XXe siècle, ceux-ci ne disposent pas toujours de draps et de repas. Une classe intermédiaire de touristes et de passagers de classe moyenne apparaît peu à peu. Les navires sont alors divisés en trois classes. Au fil du temps, l'immigration disparaît au profit du tourisme. Les navires se dotent alors d'une classe « touriste » qui prend finalement le pas sur les autres classes à bord des navires de croisière.
Les installations offertes aux passagers se développent progressivement. Dans les années 1870, l'apparition sur l'Oceanic de baignoires et de lampes à huile fait sensation. Dans les années qui suivent, les équipements se multiplient dans les navires : fumoirs, salons et ponts-promenade. En 1912, le Titanic propose même des bains turcs et une piscine. Dans les années 1920, le Paris est le premier à proposer un cinéma. Les paquebots modernes suivent toujours cette tendance et sont équipés de piscines, de courts de tennis, de casinos et de boutiques.
Des spectacles sont par ailleurs organisés, et, durant l'âge d'or des paquebots, des célébrités voyageant à bord participent aux festivités. Ainsi, Tino Rossi improvise en 1953 un spectacle à bord de l'Île-de-France. Si les enfants ont longtemps été considérés comme « à charge », ils jouissent à partir des années 1920 de la pleine attention de l'équipage qui leur propose des salles réservées, des animations et des spectacles.
Les paquebots génèrent une forte animation et nécessitent un personnel nombreux. Jules Verne, à la suite d'une traversée à bord du Great Eastern, écrit ainsi un ouvrage témoignage intitulé Une ville flottante. Nombre de services sont en effet requis pour satisfaire au bien-être des passagers : l'équipage ne se contente pas du personnel navigant et de mécaniciens mais doit également faire avec une batterie de stewards, cuisiniers, grooms... En 1912, l'équipage du Titanic comporte ainsi près de 900 personnes et il n'est pas rare que certains navires excèdent les 1 000 membres d'équipage. Les cuisines d'un paquebot moderne sont donc souvent gigantesques, et les réserves transportées pour une traversée sont gargantuesques : le nombre de bouteilles se compte généralement en dizaines de milliers pour une traversée.
Sur les très grosses unités telles l’Oasis ou Allure of the Seas, on ne compte pas moins de 2 000 m2 de chambres réfrigérées (fruits, légumes, produits laitiers, boissons et conserves), et 700 m2 de chambres pour produits congelés à −25 °C (glaces, viandes, volailles et poissons). Contrairement à certaines idées reçues, les plats servis sur les navires de croisières sont entièrement réalisés à bord par un important personnel en cuisine, à partir des produits de base frais et congelés. Les impositions sanitaires sont draconiennes, et souvent conformes aux normes sanitaires américaines (USPHS), ce qui permet à ces navires de pouvoir toucher les plaques tournantes américaines (Miami et Fort Lauderdale notamment).
Outre la nourriture, l'équipage doit répondre à d'autres besoins, en particulier religieux. Si les paquebots du début du XXe siècle se contentent bien souvent d'un office donné par le commandant dans le grand salon, d'autres tels que le Normandie ou le France disposent de plusieurs lieux de cultes exclusivement consacrés à la pratique religieuse. La santé est également prise en compte et des hôpitaux apparaissent sur les paquebots. Il s'agit au départ de zones de quarantaine gérées par une équipe de médecins du bord mais les hôpitaux de navires plus modernes n'ont rien à envier aux cliniques terrestres. Certains disposent de blocs opératoires et de pharmacies, entre autres. En 1965, le France transmet des électrocardiogrammes d'un passager aux deux côtés de l'Atlantique, permettant une opération en direct.Il est fréquent d’avoir à gérer des décès de passagers, et chaque navire dispose d’une armoire mortuaire de conservation. Le Queen Mary 2 qui a vocation de faire voyager une clientèle âgée dispose, en plus de son important hôpital, d’une morgue pour 6 corps.
Les navires de croisière modernes doivent se montrer économiques en énergie, et respectueux de l’environnement. La chaleur générée par les moteurs diesel sert en partie à la production d’eau douce en quantité considérable. Toutes les eaux usées sont traitées dans des réacteurs biologiques, et ne sont rejetées à la mer qu’un fois conformes et stériles. Les ordures sont triées et débarquées à chaque rotation de passagers.[réf. nécessaire]
Les riverains des ports se plaignent de plus en plus de la pollution causée par les paquebots de croisière, qui utilisent comme carburant du fioul lourd, peu coûteux mais très polluant : en Méditerranée, la teneur en soufre autorisée pour les carburants maritimes est actuellement de 1,5 % pour les navires de passagers, soit 1500 fois plus que la limite tolérée du SOx dans le diesel des voitures ou des camionnettes. Dans les ports, ils doivent utiliser depuis 2015 un autre carburant, ne comptant que 0,1 % de soufre, stocké dans un autre réservoir. Ils produisent de plus des particules fines en grandes quantités : à Marseille, la part du maritime dans les particules en suspension dans l'atmosphère de la ville est estimée à 10 à 20 % ; un bateau à quai produit des rejets dans l'atmosphère équivalents à 10.000 à 30.000 véhicules, et en propulsion, 5 à 10 fois plus ; l' « Harmony of the Seas », fierté des chantiers STX de Saint-Nazaire, même au diesel marin dans les ports, pollue encore autant que 87000 voitures. Parmi les solutions envisagées :  le branchement électrique à quai des navires, pour qu'ils coupent leurs moteurs diesel en escale ( Göteborg, Los Angeles, Vancouver) et surtout le passage à des motorisations au GNL (gaz naturel liquéfié), qui réduit de 85 % les oxydes d'azote, annihile les émissions d'oxyde de soufre et l'essentiel des particules fines (95 % de moins que le fioul lourd) ; plusieurs armateurs ont commandé des paquebots alimentés au gaz.
Les Britanniques et les Allemands sont ceux qui se sont le plus illustrés dans la construction navale lors de la grande époque des paquebots. En Irlande, les chantiers Harland & Wolff de Belfast se montrent particulièrement innovants et réussissent à gagner la confiance de nombreuses compagnies au premier rang desquelles vient la White Star Line. Ces chantiers gigantesques emploient une forte partie de la population de la ville et construisent coques, machines, mobilier et même dispositifs de sauvetage. Parmi les autres chantiers britanniques réputés se trouvent Swann, Hunter Wigham Richardson, notamment constructeur du Mauretania et le chantier John Brown & Company, constructeur du Lusitania.
Les Allemands disposent pour leur part de nombreux chantiers en mer du Nord et sur la Baltique, notamment les chantiers Blohm & Voss de Hambourg et les chantiers AG Vulcan de Stettin. Fortement détruits pendant la Seconde Guerre mondiale, ces chantiers reprennent cependant une activité de construction intense.
La France n'est pas en reste et bénéficie également de grands chantiers. À Saint-Nazaire, les Chantiers de Penhoët sont à l'origine du prestigieux Normandie. Après leur fusion, ils forment les chantiers de l'Atlantique, à l'origine de plusieurs paquebots de classe internationale (comme le Queen Mary 2...), et ce même au début du XXIe siècle (STX France. Le pays dispose également de chantiers sur les bords de la Méditerranée.
L'Italie et les Pays-Bas disposent également de chantiers capables de construire des paquebots d'une certaine ampleur, bien que ceux-ci soient moins nombreux (Fincantieri, par exemple).
Les compagnies britanniques sont nombreuses mais deux se distinguent particulièrement : la Cunard et la White Star Line. Toutes deux sont fondées dans les années 1830 - 1840 et se livrent une forte concurrence, alignant au début du XXe siècle les plus gros et les plus rapides navires du monde. Il faut attendre 1934 pour que, la crise aidant, les deux fusionnent sous le nom de Cunard-White Star Line, avant que la Cunard ne rachète le tout en 1947. Bien qu'elle-même ait été rachetée par le Carnival Group, ses paquebots continuent à arborer son nom. La P&O occupe également une grande part de l'activité.
D'autres compagnies britanniques s'illustrent. La Royal Mail Steam Packet Company fait office de compagnie d'État, tant ses rapports avec le gouvernement sont étroits. Au cours de son histoire, elle englobe de plus en plus de compagnies (en particulier la White Star à la fin des années 1920), devenant l'une des plus grandes compagnies au monde avant que des ennuis judiciaires ne conduisent à sa liquidation en 1931. L’Union-Castle Line dessert pour sa part l'Afrique et l'océan Indien avec une flotte non négligeable.
L'Allemagne voit s'affronter deux compagnies rivales, la Hamburg America Line, souvent appelée HAPAG, et la Norddeutscher Lloyd. Toutes deux connaissent leur heure de gloire dans les années 1900 et 1910. Les deux guerres mondiales sont cependant autant de coups durs et les compagnies, à chaque fois contraintes de céder leurs navires aux vainqueurs, fusionnent pour former la Hapag-Lloyd.
La France est également partagée, mais les domaines sont mieux définis. La Compagnie générale transatlantique, également connue sous le nom de « Transat » ou French Line, dessert la route de l'Atlantique Nord avec de prestigieux paquebots tels que le Normandie ou le France, tandis que la Compagnie des messageries maritimes est affectée aux colonies d'Asie et d'Afrique. La décolonisation conduit cependant à une forte baisse de rentabilité et les deux compagnies fusionnent en 1975 pour former la Compagnie générale maritime.
D'autres compagnies maritimes d'importance s'illustrent dans le domaine des paquebots. Aux États-Unis, les United States Lines tentent de s'imposer sur la scène internationale mais ne parviennent pas à concurrencer leurs rivales européennes. En Italie, l’Italian Line est fondée en 1932 à la suite de la fusion de trois grandes compagnies et possède notamment le Rex et l'Andrea Doria.
La plus prestigieuse de toutes les lignes empruntées par les paquebots est la ligne transatlantique. Elle représente en effet une grande part de la clientèle, qui embarque dans les ports de Liverpool, Southampton, Hambourg, Le Havre, Cherbourg ou encore Cobh pour gagner les États-Unis. La prospérité de cette ligne vient donc également en grande partie de la migration vers le nouveau continent. Le besoin de vitesse se répercute également sur les navires et le prestigieux Ruban bleu récompense la traversée la plus rapide, bien que cela ne soit pas sans danger. Nombre de naufrages touchent en effet cette route, le plus célèbre étant celui du Titanic en 1912 : abordages entre navires, tempêtes et glaces sont en effet monnaie courante dans l'Atlantique Nord.
Cette ligne est la destination privilégiée des grandes compagnies et le théâtre des grandes compétitions : la Cunard, la White Star, la Norddeutscher Lloyd, la HAPAG et la Compagnie générale transatlantique s'y livrent une concurrence acharnée, parfois au détriment de la sécurité. Ce prestige s'illustre également par la présence sur cette route de douze des quatorze paquebots à quatre cheminées jamais construits.
L'Atlantique Sud est également un océan fort fréquenté par les navires à destination d'Amérique du Sud, mais aussi d'Afrique et parfois d'Océanie. La White Star Line dispose ainsi certains de ses navires sur la ligne Liverpool - Le Cap - Sydney, notamment le Suevic, qui a la particularité d'être coupé en deux en 1907 et de se voir construire une nouvelle proue. Cependant, ces lignes permettent également aux compagnies espagnoles et italiennes de s'illustrer.
L'Atlantique Sud ne connaît pas les fortes concurrences du Nord et les vitesses y sont moins élevées. On y compte de fait moins de naufrages. Des navires réussissent cependant à y devenir célèbres, notamment le Cap Arcona qui se distingue par son luxe, ou le Pasteur, dernier navire des Messageries maritimesSi la décolonisation et l'avènement de l'avion touchent fortement les paquebots de ligne de cette région, l'Atlantique Sud connaît une nouvelle vie avec les croisières. Les Antilles et le canal de Panama sont en effet devenues une destination très prisée des paquebots de croisière.
La mer Méditerranée est très fréquentée par les paquebots. De nombreuses compagnies profitent en effet de la migration venant du sud de l'Italie et des Balkans pour installer une ligne allant de la Méditerranée aux États-Unis. Le Carpathia, venu au secours du Titanic lors de son naufrage, desservait ainsi Gibraltar, Gênes et Trieste. De même, les paquebots italiens sillonnent la Méditerranée avant de s'engager dans l'Atlantique Nord. L'ouverture du canal de Suez bénéficie également à la Méditerranée qui devient souvent un passage obligé vers l'Asie.
Cependant, c'est dans le domaine des croisières que la Méditerranée s'illustre. Le tourisme à destination de l'Égypte prend en effet son essor dès le début du XIXe siècle : les gens de milieux aisés aiment naviguer sur le Nil et visiter les sites archéologiques. De même, les croisières le long des côtes italiennes séduisent particulièrement et les paquebots font escale le long de côtes dans les grandes villes d'art que sont Rome, Venise ou bien Pise. La mode persiste et, au début du XXIe siècle, la Méditerranée reste une destination privilégiée des organisateurs de croisières.
La colonisation rend l'Asie particulièrement attractive pour les compagnies maritimes. Nombreux sont en effet les fonctionnaires et militaires qui doivent s'y rendre. Dès les années 1840, la P&O organise des voyages à destination de Calcutta via l'isthme de Suez, le canal n'ayant pas encore été construit. Les parcours sur ces itinéraires à destination de l'Inde, de l'Asie du Sud-Est et du Japon sont longs (plusieurs mois), et ponctués d'escales.
La Compagnie des messageries maritimes se montre très présente sur les lignes asiatiques, notamment dans les années 1930 avec la série des « nautonaphtes » (navires à moteur) dont le plus célèbre est le Félix Roussel qui accorde une grande place aux dortoirs destinés aux permissionnaires. De même, La Marseillaise mis en service en 1949, est le fleuron de leur flotte mais souffre également des débuts de la décolonisation. Avec la baisse de la demande et l'avènement du transport aérien, ces navires deviennent inutiles.
D'autres lignes sont également desservies avec moins d'ampleur : sur l'océan Pacifique, notamment pour aider à l'immigration chinoise ; sur les mers arctiques dans le cadre de croisières, à partir des années 1950 ; les croisières vers le Groenland apparaissent également à la fin du XXe siècle.
La construction de paquebots résulte souvent d'une volonté politique. Le regain de puissance de la marine allemande provient de la volonté clairement affirmée du Kaiser Guillaume II d'Allemagne de voir son pays devenir une puissance navale. C'est ainsi que le Deutschland a l'insigne honneur de porter le nom de la mère patrie, honneur qu'il perd au bout de dix ans d'une carrière décevante. Le Lusitania et le Mauretania sont construits avec l'aide du gouvernement britannique pour que le pays reprenne une place de choix dans le trafic maritime. Le United States est également l'aboutissement de la volonté du gouvernement américain qui désirait un navire puissant et facilement transformable en transport de troupes. De même, le Rex et le Conte di Savoia sont construits à la demande de Benito Mussolini. Enfin, le France est financé par l'État et est retiré du service en 1974 après la décision du Premier ministre Jacques Chirac.
Certains navires acquièrent également une forte popularité. Le Mauretania et l’Olympic ont ainsi durant leur carrière de nombreux admirateurs et leur démantèlement provoque une certaine tristesse. Il en est de même avec l’Île-de-France dont la démolition suscite un fort émoi auprès de ses admirateurs. Le navire a en effet acquis une forte popularité en venant au secours de nombreux navires, notamment l'Andrea Doria qui lui valent le surnom de « Saint-Bernard de l'Atlantique ». De même, le Queen Mary jouit d'une très forte popularité auprès de la population britannique.
Certains paquebots sont tristement célèbres pour leurs naufrages de très grande ampleur. Ainsi, en 1873, l’Atlantic sombre au large de la Nouvelle-Écosse en faisant 600 victimes. Il devient ainsi le plus grand naufrage de son époque. Vient en 1912 le naufrage du Titanic, qui fait environ 1 500 victimes. Celui-ci met en évidence l'excès de confiance des compagnies maritimes dans leurs navires et le manque de canots de sauvetage à bord. Nombre de mesures sont prises à la suite de la catastrophe. Ceci n'empêche cependant pas l'histoire de se répéter deux ans plus tard avec le naufrage de l'Empress of Ireland : un peu plus de 1 000 personnes périssent dans les eaux gelées du Saint-Laurent. Le naufrage est cependant rapidement occulté par la Première Guerre mondiale.
Parmi les autres naufrages célèbres, celui du Lusitania en 1915 qui suscite un fort émoi ou celui mystérieux du Georges Philippar qui s'embrase lors de son voyage inaugural en 1932. La mort du journaliste Albert Londres dans la catastrophe conduit certains à suspecter un incendie criminel. En 1956, le naufrage de l'Andrea Doria fait les gros titres. Son bilan réduit d'une cinquantaine de victimes n'empêche pas un fort retentissement médiatique.
Le paquebot italien Achille Lauro est également au nombre des navires malchanceux. Il est en effet victime d'un détournement en 1985 qui se solde par la mort de l'un des otages et de son quatrième incendie en 1994 qui le ravage durant plusieurs jours et le fera sombrer.
Les paquebots ont eu un fort impact sur la culture, que ce soit durant la grande ère des liners ou après. Dès 1867, Jules Verne raconte son expérience à bord du Great Eastern dans Une ville flottante. En 1898, l'écrivain Morgan Robertson écrit le roman Le Naufrage du Titan qui raconte une catastrophe imaginaire. Quatorze ans plus tard, certains voient en ce récit un écrit prémonitoire annonçant le naufrage du Titanic, conduisant à nombre de légendes.
Dans un registre plus heureux, c'est après une traversée à bord du Carla C qu'une passagère américaine écrit un roman sur la vie à bord d'un navire de croisière. Celui-ci est par la suite adapté en série télévisée sous le titre La croisière s'amuse. De plus, un grand nombre d'ouvrages consacrés aux paquebots est publié chaque année, et des sociétés historiques et autres associations sont fondées, telles que les French Lines ou encore la Titanic Historical Society,.
Le thème du paquebot revient également beaucoup au cinéma. Ces navires sont ainsi souvent le théâtre d'histoires d'amour comme dans le film Elle et lui (Love Affair) avec Charles Boyer et Irene Dunne en 1939 et nombre d'autres. Le paquebot est également très utilisé dans les films catastrophe. Le film The Last Voyage est ainsi tourné à bord de l'Île-de-France qui est racheté et sabordé pour l'occasion. De même, L'Aventure du Poséidon est devenu un classique du genre et a fait l'objet de plusieurs remakes et parodies.
Le naufrage du Titanic attire également l'attention des cinéastes. Près de quinze films lui sont consacrés, dans différents genres. Le film Titanic de 1953 est ainsi réalisé dans la plus pure tradition hollywoodienne, tandis que celui de 1943 est une œuvre de propagande nazie. Enfin, en 1997, le blockbuster Titanic de James Cameron connaît un succès sans précédent.
Une exposition intitulée Paquebots de légende a eu lieu au Musée national de la Marine du 6 décembre 1991 au 5 mars 1992.
Cet article est partiellement ou en totalité issu de l'article intitulé « Liner (paquebot) » (voir la liste des auteurs)
Max Rémy et Laurent Le Boutilly, Les "Provinces" Transatlantiques 1882-1927, Éditions Minimonde76, 2016, 80 p. (ISBN 9782954181820)
(en) Mark Chirnside, The Olympic Class Ships : Olympic, Titanic, Britannic, Tempus, 2004, 348 p. (ISBN 9780752428680)
Corrado Ferulli, Au cœur des bateaux de légende, Hachette Collections, 2004, 240 p. (ISBN 9782846343503)
Christian Mars et Frank Jubelin, Paquebots, Sélection du Reader's Digest, 2001, 208 p. (ISBN 9782709812863)
Gérard Piouffre, L'Âge d'or des voyages en paquebot, Éditions du Chêne, 2009, 360 p. (ISBN 9782812300028)
(en) The Ship List, liste de tous les paquebots construit depuis le XIXe siècle avec les listes de passagers
Cargos - Paquebots - Autres navires de marine marchande, site consacré à tous types de navires, navigations au long cours, histoire maritime
Messageries Maritimes Site de l'histoire de la compagnie des Messageries Maritimes, des navires, des destinations.

Le paracétamol, aussi appelé acétaminophène, est un composé chimique utilisé comme antalgique (anti-douleur) et antipyrétique (anti-fièvre), qui figure parmi les médicaments les plus communs, utilisés et prescrits au monde.
Il est indiqué dans le traitement des symptômes d'intensité faible à modérée, seul ou en association avec d'autres analgésiques, notamment opioïdes, et est très populaire car il a moins de contre-indications que d'autres antalgiques et jouit d'une bonne image auprès du public.
Son mécanisme est encore mal connu ; en effet, il diminue la fièvre, mais pas par le même mécanisme que l'aspirine ou l'ibuprofène, qui agissent sur l'inflammation,.
Le paracétamol est le médicament le plus prescrit en France, et même la base des trois médicaments les plus prescrits (noms commerciaux : Doliprane, Dafalgan, Efferalgan), qui totalisent plus de 260 millions de doses[précision nécessaire].
Toutefois, au delà de 3 g par jour, et en cas d'usage au long cours, le paracétamol présente une sévère toxicité pour le foie par production d'un métabolite hépatotoxique, la N-acétyl-p-benzoquinone imine (NAPQI). En cas de surdose ou d'interactions médicamenteuses, le paracétamol est très toxique pour le foie et peut entraîner la mort par hépatite fulminante. Il est également fortement contre-indiqué chez les personnes souffrant d'insuffisance hépatique.
Le nom « paracétamol » vient de la contraction de para-acétyl-amino-phénol. Acétaminophène quant à lui provient de N-acétyl-para-aminophénol. Le nom acétaminophène est utilisé aux États-Unis, au Canada, au Japon, en Corée du Sud, en Colombie, à Hong Kong et en Iran. Dans les autres pays, on emploie le nom « paracétamol ».
L'usage d'antipyrétiques remonte à l'Antiquité : ils étaient des préparations à partir de composés naturels d'écorces de cinchona (dont dérive la quinine), soit du salicylate (dont dérive l'aspirine) contenu dans l'écorce de saule. Les substances de synthèse sont fabriquées en laboratoire, et non plus extraites directement de la nature.
Harmon Northrop Morse synthétisa dès 1878 une substance baptisée acétylaminophénol, sans toutefois lui attribuer une quelconque propriété médicale. C'est cinquante ans plus tard qu'elle fut commercialisée comme médicament sous le nom de paracétamol. À cette époque, d'autres produits sont utilisés comme remède contre la douleur et la fièvre : en 1882, Hoechst commercialise le Kairin découvert par Otto Fisher ; en 1897, l'aspirine est synthétisée par Felix Hoffmann et connaît un grand succès. BASF ne pousse guère son antipyrétique Thallin, mis au point vers 1885. L'acétanilide (1886) et la phénacétine (1887) sont également utilisées avant qu'on ne constate les graves effets secondaires de leur administration, tandis que les inconvénients de l'aspirine commencent à être connus. Le paracétamol réapparaît alors et les premières études sur les propriétés antipyrétique et antalgique du paracétamol sont conduites à la fin du XIXe siècle.
En 1886, le professeur Adolf Kussmaul, de l'université de Strasbourg, étudie l'effet antiparasitaire du naphtalène. Ses deux jeunes assistants Arnold Cahn et Paul Hepp, à court de produit pour leurs expériences, décident de se ravitailler auprès d'un pharmacien de la ville, qui leur fournit par erreur de l'acétanilide. En reprenant leur étude, ils sont intrigués par les effets antipyrétiques qu'ils obtiennent avec lui. C'est donc grâce à une erreur providentielle – autrement dit par sérendipité – que les propriétés de l'acétanilide contre la fièvre sont découvertes. En 1861, Schuchardt avait déjà découvert les propriétés antipyrétiques de l'aniline, substance servant à la production d'acétanilide (du fait d'effets toxiques prononcés, l'aniline n'eut cependant pas d'utilisation en thérapeutique) ; ses propriétés antalgiques seront découvertes un peu plus tard. L'acétanilide est l'ancêtre du paracétamol et de la phénacétine. Le docteur Hepp a un frère travaillant pour une petite compagnie (Kalle Co) qui fabrique l'acétanilide. Il lui propose d'utiliser sa découverte et de lancer sur le marché l'acétanilide afin de concurrencer l'antipyrine et l'acide salicylique. L'acétanilide devient un médicament commercialisé sous le nom d'antifébrine.
À la fin des années 1880, l'industrie des colorants avait un déchet, le paranitrophénol, avec une structure chimique assez similaire à l'acétanilide et disponible à bas prix. Carl Duisberg, responsable de la recherche et des brevets chez Bayer AG (Friedrich Bayer & Co), demanda à son équipe de trouver une exploitation intéressante pour le paranitrophénol. Oscar Hinsberg eut l'idée de le transformer en acétophénitidine. La démarche de création de cette substance fut purement commerciale et par chance, des tests montrent qu'elle semble plus puissante que l'antifébrine et provoque moins d'effets indésirables. Duisberg décide de mettre la nouvelle molécule en production et de la commercialiser à partir de 1888 sous le nom de marque « phénacétine ».
L'antifébrine et la phénacétine n'ayant pas la même rapidité ni la même durée d'action permettaient aux praticiens d'ajuster leurs prescriptions.
Cependant, l'acétanilide est très toxique et de nombreuses recherches se consacrent sur l'élaboration de dérivés mieux tolérés. Le paracétamol fut trouvé dans les urines des personnes ayant consommé de la phénacétine. En 1889, le scientifique allemand Karl Morner découvre qu'un fragment de la phénacétine, l'acétaminophène, est un produit efficace contre la douleur et la fièvre. Une étude métabolique de ce médicament montre qu'il s'agit d'un métabolite déséthylé de la phénacétine. Cette hypothèse fut formulée dès 1894, mais fut largement ignorée à l'époque. En 1893, un médecin allemand, Joseph von Mering, compare les propriétés antalgiques et antipyrétiques du paracétamol et de la phénacétine ainsi que leurs toxicités respectives. Il tire de cette étude la conclusion, erronée, que le paracétamol est plus néphrotoxique que la phénacétine : la notoriété de Von Mering fera que ce jugement ne sera pas contesté si bien que le paracétamol sera délaissé pendant un demi-siècle. La phénacétine sera largement employée dans les névralgies sous le nom de Veganine. La toxicité de la phénacétine pour le rein sera démontrée par la suite, entraînant son retrait du marché.
L'acétanilide et la phénacétine seront en concurrence avec l'aspirine jusqu'à la fin de la Seconde Guerre mondiale. En 1938, la Food and Drug Administration, dont la réglementation avait été rendue plus stricte à la suite de l'adoption – cette même année – du Federal Food, Drug, and Cosmetic Act, retira brièvement du marché la phénacétine qui avait été suspectée d'être cause d'agranulocytose.
Les travaux de David Lester et Léon Greenberg de l'université Yale et ceux de Flinn et Brodie de l'université de New York viennent finalement confirmer l'hypothèse de Karl Morner,.
En 1946, l'Institute for the Study of Analgesic and Sedative Drugs propose une bourse au New York City Department of Health afin d'étudier les problèmes associés aux agents analgésiques. Bernard Brodie et Julius Axelrod sont désignés pour étudier le lien présumé entre les agents non dérivés de l'aspirine et le développement de la méthémoglobinémie. En 1948, ils publient leur étude qui démontre que l'acétanilide est dégradé dans l'organisme en N-acétyl p-aminophénol, et que seul ce métabolite est actif contre la douleur.
Ils démontrent également que l'administration d'acétanilide est responsable de la formation de méthémoglobine, mais ils ajoutent que l'agent responsable est peut-être la phénylhydroxylamine, et non pas le paracétamol comme on le croyait auparavant. Ils suggèrent donc aux industriels de remplacer l'acétanilide, responsable de la méthémoglobinémie, par l'acétaminophène. Il y a alors un regain d'intérêt pour le paracétamol, du fait de ses propriétés antalgiques et antipyrétiques, et de sa bonne tolérance apparente.
En mai 1951, ces résultats et d'autres furent présentés à New York lors d'un symposium organisé par le Institute for the Study of Analgesic and Sedative Drugs. Peu de temps avant, au vu des études des équipes de Yale et de New York, des sociétés pharmaceutiques américaines avaient commencé à produire quelques spécialités à base de paracétamol sans chercher toutefois à en pousser la vente, car elles vendaient déjà de l'aspirine. En 1950, le Triagesic est commercialisé aux États-Unis : ce mélange de paracétamol, d'aspirine et de caféine fut cependant la cause de trois cas graves d'agranulocytose ; le Triagesic fut retiré du marché en 1951 avant que l'on ne s'aperçoive que le paracétamol n'y était pour rien. En 1953, les laboratoires Sterling-Winthrop Co sont les premiers à commercialiser le paracétamol sous l'appellation Panadol sur le marché britannique ; producteurs d'aspirine, ils ne cherchent pas à introduire le Panadol aux États-Unis. Ce sont les laboratoires MacNeil qui vont saisir l'importance des découvertes et déposer une demande d'autorisation de mise sur le marché qui leur sera accordée par la Food and Drug Administration en 1955. En juin, lesMcNeil Laboratories, une petite entreprise de Pennsylvanie, lancent donc le Tylenol Children's Elixir : c'est un sirop pour enfant contre la fièvre et la douleur, présenté dans une boîte rouge en forme de camion de pompier et qui était disponible sans ordonnance.
Le symposium de New York avait souligné un effet secondaire de l'aspirine jusqu'alors non remarqué – que ne provoque pas le paracétamol : l'irritation de l'estomac. C'est cet avantage comparatif en faveur du paracétamol qui avait décidé les laboratoires MacNeil (qui ne produisaient pas d'aspirine) à lancer ce nouveau médicament dont la promotion sera précisément axée sur cette particularité. Le produit est ensuite devenu populaire chez les adultes pour la même raison. En 1956, au Royaume-Uni, le paracétamol est vendu – seulement sous ordonnance – sous le nom de Panadol en dose de 500 mg, produites par Frederick Stearns & Co, une filiale de Sterling Drug Inc. En 1958, apparaît Panadol Elixir, une version destinée à l'usage des enfants. Le suffixe -dol à la fin du nom du médicament provient du latin dolor, qui signifie « douleur ». Le Panadol intègrera la Pharmacopée Britannique en 1963. En France, le paracétamol, associé à un antihistaminique, apparaît dans la spécialité Algotropyl, réservée à l'usage pédiatrique en 1957, commercialisé par les Laboratoires Bottu. Puis la même firme pharmaceutique met sur le marché le Doliprane dès 1964,. Le succès de ce produit, aujourd'hui propriété de Theraplix, ne sera pas immédiat dans les pharmacies françaises : il viendra après le choix d'un mode de distribution en direct (Cooper) et par le lancement de la gamme pédiatrique en 1981. De nos jours, de nombreux médicaments contenant du paracétamol ont été développés et commercialisés dans beaucoup de pays.
En 1984, une prodrogue injectable de paracétamol est mise au point, offrant un traitement analgésique en postopératoire pour les patients ne pouvant recourir à la voie orale. Ce Pro-Dafalgan doit cependant être préparé au lit du malade ; le Perfalgan, qui ne présente pas cet inconvénient, sert à faciliter l'emploi de cette molécule pour cet usage. Associé à la morphine, il permet d'en diminuer notablement la consommation (une récente étude vient toutefois relativiser l'utilité de cet emploi en association).
Dans les conditions ordinaires, le paracétamol est une poudre blanche avec un léger goût, soluble dans 70 volumes d'eau, 7 volumes d'alcool à 95 %, 13 volumes d'acétone, 40 volumes de glycérol ou 50 volumes de chloroforme. Cependant, il est insoluble dans l'éther et le benzène. Le paracétamol est stable dans l'eau, mais sa stabilité diminue en milieu acide ou basique. Les mélanges de paracétamol sont stables dans des conditions humides. Cependant, les comprimés qui contiennent de la codéine ou du stéarate de magnésium se dégradent en diacétyl-p-aminophénol dans une atmosphère humide.
La molécule est constituée d'un cycle benzénique, substitué par un groupe hydroxyle et par un groupe amide en position para. Le paracétamol ne comporte pas de carbone asymétrique et n'a pas de stéréoisomère. Un des deux doublets libres de l'atome d'oxygène du groupe hydroxyle, le cycle benzénique, le doublet libre de l'atome d'azote et l'orbitale p du carbone du carbonyle forment un système conjugué. Cette conjugaison réduit la basicité des oxygènes et de l'azote et rend le groupe hydroxyle plus acide (comme les phénols) car la délocalisation des charges s'effectue sur un ion phénolate.
La présence de deux groupes activants rend le cycle hautement réactif pour une substitution électrophile aromatique, les substituants étant ortho et para directeurs. Toutes les positions du cycle sont plus ou moins activées de la même manière et il n'y a donc pas de site privilégié dans le cas d'une substitution électrophile. Le paracétamol est le métabolite actif de l'acétanilide et de la phénacétine : le paracétamol est produit par la décomposition de ces deux produits dans l'organisme. Ces espèces chimiques sont de la même famille chimique et ont une structure chimique très proche.
Le paracétamol ne comprend pas de centre chiral et n'a aucun stéréoisomère. La synthèse n'a pas besoin d'être stéréocontrôlée et elle est plus simple que les synthèses asymétriques d'autres substances pharmaceutiques.
Le paracétamol fut synthétisé pour la première fois en 1878 par Harmon Northrop Morse. La première étape est la réduction du para-nitrophénol en para-aminophénol en présence d'étain dans de l'acide acétique glacial. Le para-aminophénol obtenu est ensuite acylé par l'acide acétique pour obtenir du paracétamol. Vignolo simplifia cette synthèse en utilisant le para-aminophénol comme produit de départ. Une seule étape d'acylation est nécessaire pour obtenir le produit désiré, ce qui raccourcit la synthèse. Plus tard, Friedlander modifia la synthèse en faisant l'acylation du para-aminophénol à partir de para-nitrophénol avec de l'anhydride acétique au lieu de l'acide acétique, ce qui donne un meilleur rendement.
L'intérêt du paracétamol a été réduit lors des premières années de commercialisation en raison d'une contamination par le para-aminophénol à cause du procédé de fabrication. Cette impureté était, comme l'acétanilide, méthémoglobinisante.
De nos jours, il existe différentes méthodes de synthèse industrielle, la plupart utilisant l'acylation du para-aminophénol avec de l'anhydride acétique.
Acétyl paraminophénol, acétyl-p-amino-phénol, hydroxy-4' acétanilide, para-acétamidophénol, para-acétamino-phénol, N-acétyl-para-aminophénol.
Le mécanisme d'action complet du paracétamol reste inconnu, un siècle après sa découverte. Cependant, il a été démontré qu'il agit principalement au niveau du système nerveux central. Selon une étude de 2006, le paracétamol agirait en inhibant au niveau central la production de prostaglandines, impliquées dans les processus de la douleur et de la fièvre, par le biais d'une action inhibitrice sur l'enzyme prostaglandine H2 synthase (PGHS), qui comporte notamment un site actif « cyclo-oxygénase » (ou COX), cible de la majorité des anti-inflammatoire non stéroïdien (AINS), et un site « peroxydase » (ou POX), sur lequel agirait le paracétamol. 
Le paracétamol n'aurait pas d'action directe sur le COX-1 et le COX-2, les deux formes de COX sur lesquelles agissent les AINS comme l'aspirine ou l'ibuprofène. On soupçonne l'existence d'une nouvelle isoenzyme, le COX-3 (enzyme produite par épissage successif de la COX-1 entraînant un décalage de lecture des bases par le corps humain), sur laquelle agirait spécifiquement le paracétamol et qui expliquerait pourquoi le paracétamol réduit la fièvre et la douleur tout en étant dénué d'activité anti-inflammatoire et antiplaquettaire. Pour l'instant, cette hypothèse n'a pas été prouvée chez l'humain. D'autres mécanismes d'action ont été évoqués pour expliquer l'activité analgésique et antipyrétique du paracétamol. Un mécanisme d'action sérotoninergique central est suspecté depuis quelque temps. Le paracétamol potentialiserait l'effet des neurones sérotoninergiques descendants de la moelle épinière exerçant un contrôle inhibiteur sur les voies de la douleur. Par ailleurs, le paracétamol pourrait agir en limitant la libération de Béta-endorphines.
Les recherches récentes montrent que l'ion peroxynitrite pourrait être la source oxydante permettant aux COX de transformer l'acide arachidonique en prostaglandine. De même que la nitrotyrosine est un marqueur spécifique de l'excès de peroxynitrites agissant comme agent nitrant sur le cycle phénolique activé de la tyrosine, le nitroparacétamol formé par nitration directe du paracétamol par les peroxynitrites les consommerait et permettrait d'annihiler la synthèse des prostaglandines.
L'absorption du paracétamol par voie orale est complète et rapide : le maximum de concentration plasmatique est atteint entre 15 minutes (comprimé effervescent) et 30–60 minutes (comprimé et poudre) après ingestion.
Le paracétamol se distribue rapidement dans tous les tissus. Les concentrations sont comparables dans le sang, la salive et le plasma. Le paracétamol est métabolisé essentiellement au niveau du foie. Les deux voies métaboliques majeures sont la glycuroconjugaison et la sulfoconjugaison. Il existe une voie métabolique moins importante catalysée par le Cytochrome p450 (plus précisément par les isoenzymes CYP2E1, CYP1A2, CYP3A4), qui aboutit à la formation d'un intermédiaire réactif toxique, la N-acétyl-p-benzoquinone imine ou NAPQI. Il est normalement rapidement éliminé par réaction avec le glutathion réduit puis évacué dans les urines après conjugaison à la cystéine et à l'acide mercaptopurique.
L'élimination du paracétamol est essentiellement urinaire : 90 % de la dose ingérée est éliminée par le rein en 24 heures, principalement sous forme glycuroconjuguée (60 à 80 %) et sulfoconjuguée (20 à 30 %) et moins de 5 % est éliminé sous forme de paracétamol. La demi-vie d'élimination est d'environ 2 heures.
En cas d'insuffisance rénale sévère, avec une clairance de la créatinine inférieure à 10 ml·min-1, l'élimination du paracétamol et de ses métabolites est retardée. La glycuroconjugaison est immature chez le nourrisson et l'enfant, le paracétamol est donc essentiellement sulfoconjugué. Le passage à une voie métabolique identique à celle de l'adulte intervient vers entre 9 et 12 ans.
Le paracétamol entre dans la composition d'une soixantaine de spécialités pharmaceutiques et peut se présenter sous différentes formes ou conditionnements. Le paracétamol seul est vendu sous de nombreuses formes galéniques comme des comprimés, en poudre, des comprimés effervescents, des comprimés orodispersibles, des gélules, du sirop, des suspensions buvables, des suppositoires pour adultes ou enfants, ou des lyophilisats. Il est aussi disponible sous forme intraveineuse.
Le paracétamol peut être associé à d'autres antalgiques au sein d'un même médicament, dans le but principal d'améliorer l'efficacité globale et d'optimiser le rapport bénéfice/risque en diminuant les posologies, mais aussi afin d'allonger la durée d'action, d'élargir le spectre d'efficacité, de diminuer l'accoutumance, d'améliorer l'observance et de minimiser le risque d'usage détourné. Le but des associations de médicaments étant de produire des interactions pharmaceutiques bénéfiques, c'est-à-dire une synergie, permettant d'augmenter l'efficacité et d'améliorer la tolérance tout en utilisant les doses les plus faibles possibles. L'association doit permettre d'élargir le spectre d'efficacité en combinant des antalgiques agissant simultanément sur des cibles différentes, mais impliqués dans des mécanismes physiopathologiques identiques.
Le paracétamol est utilisé en association avec d'autres substances actives pour profiter de ses propriétés antalgiques et antipyrétiques. L'un des problèmes des associations est l'accumulation des effets secondaires ; cependant, le paracétamol étant très bien toléré, il est particulièrement intéressant dans le cadre des associations, et c'est pourquoi les laboratoires pharmaceutiques ont développé de très nombreuses formules comprenant du paracétamol. Un dérivé lipidique, le palmitostéarate de glycérol atomisé est parfois ajouté aux mélanges pour masquer le goût du paracétamol.
Le paracétamol non associé est vendu en nom générique ou sous de nombreuses marques dont certaines très connues :
Doliprane (Sanofi, médicament le plus prescrit en France), Dafalgan (UPSA, deuxième médicament le plus prescrit en France), Efferalgan (UPSA, troisième médicament le plus prescrit en France) et de nombreuses autres marques en France. En vente libre (c'est-à-dire sans ordonnance, prescrit directement par le pharmacien), le prix d'un traitement d'une semaine est généralement réglementé à 2,08 euros (100 millilitres de sirop enfant) ou 1,94 euros (adulte, 8 fois 1000 milligrammes ou 16 fois 500 milligrammes) (France, 1,12 euro prix public TTC + 0,82 euro euro de rémunération forfaitaire du pharmacien. Le pharmacien et les employés polyvalents de pharmacie ne sont pas rémunérés à l'acte de conseil mais à la quantité de médicament prescrits par le médecin ou par le pharmacien).
Dafalgan, Mylan, Perdolan, Sandoz et de nombreuses autres marques en Belgique. En vente libre (c'est-à-dire sans ordonnance, prescrit directement par le pharmacien), le prix d'un traitement est entièrement libre, il est généralement constaté à 7,45 euros (200 millilitres de sirop enfant, soit 79 % plus cher qu'en France) ou 3,24 euros (adulte, le prix varie du simple au triple selon les marques et les conditionnements, le patient devant faire des demandes écrites pour les conditionnements les moins cher s'il souffre d'une maladie chronique), 10 fois 1 000 mg ou 20 fois 500 mg, soit 34 % plus cher qu'en France).
Tylenol ou Panadol au Canada et aux États-Unis, le prix est libre et un exemple sur internet est à 8,99 USD (7,47 euros) (adulte, 100 fois 325 mg, soit 7 % moins cher qu'en France)
Alors que les médicaments sont généralement plus chers en Allemagne qu'en France, le générique allemand Ratiopharm est à 1,57 euro, prix libre moyen constaté (adulte 16 fois 500 milligrammes, soit 19 % moins cher qu'en France). La même boîte est théoriquement vendue 1,90 euro en France (soit 2 % moins cher que le tarif réglementé), mais lorsque le patient demande un paracétamol générique en France il obtient le plus souvent de l'UPSA (1,94 euros).On le retrouve associé à d'autres substances actives dans certains remèdes contre les états grippaux (Actifed, Dolirhume, Humex Rhume, Rhinofébral), où il est efficace à la fois sur la fièvre et la douleur. Il est parfois mélangé avec de la caféine (Claradol caféiné, Exidol, Theinol), substance qui pourrait augmenter son effet analgésique, mais cette notion reste très controversée,,. Il peut également être associé à d'autres antalgiques tels que l'aspirine (Novacétol) et on le retrouve souvent associé à un opiacé comme la codéine (Dafalgan Codéine, Efferalgan codéiné, Codoliprane), le dextropropoxyphène (spécialités retirées), la poudre d'opium (Lamaline), le tramadol (Ixprim, Zaldiar), ce qui permet d'augmenter son action antalgique et de traiter les douleurs moyennes ou fortes. Depuis 2011 le dextropropoxyphène, seul ou en association, a été retiré du marché français. Il existait de nombreuses présentations de l'association paracétamol-dextropropoxyphène (Dialgirex, Di-antalvic). L'efficacité clinique antalgique (en termes de synergie de l'analgésie) de l'association paracétamol + dextropropoxyphène reste mal évaluée (à la différence de celle utilisant la codéine). Il n'est pas démontré que l'association paracétamol + dextropropoxyphène est supérieure au paracétamol seul.
Les présentations de l'association paracétamol-tramadol comportent 37,5 mg de tramadol et 325 mg de paracétamol par comprimé, ce qui permettrait d'obtenir une efficacité antalgique équivalente à 50 mg de tramadol mais avec une meilleure tolérance.
L'association du paracétamol avec un opiacé peut poser des problèmes de dépendance et de détournement d'usage.
le traitement symptomatique des douleurs aiguës ou chroniques, d'intensité légère à modérée. Il s'agit d'un antalgique de palier 1 selon la classification de l'OMS. Il peut être utilisé seul ou en association avec d'autres antalgiques (codéine, tramadol, acide acétylsalicylique, ibuprofène), il rentre alors dans la classification des antalgiques de palier 2 indiqués dans les douleurs d'intensité modérée à intense ou ne répondant pas à l'utilisation d'antalgiques périphériques seuls ;
le traitement symptomatique de la fièvre, en particulier chez l'enfant chez qui il constitue l'antipyrétique de première intention,. La revue EvidenceBasedMedecine s'est toutefois interrogée sur la solidité des données attestant de son efficacité comme antipyrétique tandis que par ailleurs l'opportunité du traitement systématique de la fièvre a pu être questionnée dans un bulletin de l'OMS de 2003. Il y était affirmé : « Aucune étude ne montre d'avantage manifeste du paracétamol à dose thérapeutique chez l'enfant fébrile atteint d'infection virale ou bactérienne ou de paludisme. D'après certaines études, la fièvre semblerait même avoir un effet bénéfique sur l'infection, bien qu'aucune étude prospective définitive n'ait été réalisée chez l'enfant pour tester cette hypothèse… Ce traitement ne devrait donc être administré qu'à l'enfant manifestement incommodé ou dont l'affection est douloureuse. Le 28 février 2011, l'American Academy of Pediatrics, a réitéré ces recommandations. »
La dose, ou posologie, maximale peut varier d'un pays à l'autre selon la recommandation des produits de santé. En France, la recommandation est de, :
adultes : 500 à 1 000 mg par prise, en espaçant les prises de 4 heures minimum. Il n'est généralement pas nécessaire de dépasser la dose de 3 g par jour mais exceptionnellement (en cas de douleurs intenses non complètement contrôlées par 3 g par jour, et sur avis médical), on peut atteindre un maximum de 4 g par jour (soit 4 × 1 000 mg ou 8 × 500 mg) ;
enfants : la dose quotidienne recommandée est de 60 mg·kg-1 par jour, à répartir en 4 ou 6 prises, soit environ 15 mg·kg-1 toutes les 6 heures ou 10 mg·kg-1 toutes les 4 heures. La dose maximale est de 80 mg·kg-1 par jour chez l'enfant de moins de 38 kg selon les recommandations officielles en France.
En cas d'oubli de la dernière prise, on peut reprendre le médicament aussitôt puis continuer selon la posologie prescrite, mais en respectant un intervalle de 4 heures entre chaque prise.
Les gélules et les comprimés sont à avaler tels quels avec une boisson comme de l'eau, du lait ou un jus de fruit.
Pour les comprimés effervescents, boire après dissolution complète du comprimé dans un verre d'eau.Depuis 2011, l'association paracétamol + dextropropoxyphène (Di-Antalvic) est retirée du marché. Ce retrait est surtout lié à des surdosages, lors d'intoxications volontaires (tentatives de suicide) et non à une toxicité à dose thérapeutique.
Les contre-indications absolues sont l'hypersensibilité au paracétamol, l'insuffisance hépato-cellulaire sévère et la porphyrie.
On peut retrouver de l'aspartame dans certaines formes commerciales ; dans ce cas le médicament est contre-indiqué en cas de phénylcétonurie.
Le paracétamol est autorisé en cas de grossesse et d'allaitement. Cependant plusieurs études ont mis en cause cette molécule. En 2016 une étude publiée dans JAMMA Pediatric a mis en évidence un lien entre prise de paracétamol pendant les second et troisième trimestres de la grossesse et l'augmentation du risque de certains troubles du comportement comme l'hyperactivité. Les auteurs restent toutefois très prudents et n'écartent pas l'hypothèse qu'un autre facteur, lié à la prise de paracétamol, pourrait expliquer l'effet neurologique observé.
Il pourrait aussi exister une relation entre la prise de paracétamol pendant la grossesse et plus spécialement au cours du premier trimestre, et le risque pour les enfants de souffrir de problèmes respiratoires ou d'asthme avant l'âge de 7 ans. L'effet serait toutefois faible, avec une augmentation de risque de 11 à 22 % et ne concerne que la prise durant la grossesse ou la petite enfance. Comme dans le cas des troubles du comportement les scientifiques ne peuvent exclure que cette augmentation du risque d'asthme ne soit pas liée aux infections motivant la prise de paracétamol. 
Perturbateur endocrinien de par un effet anti-androgène, la prise de paracétamol, d'autant plus sur de longues périodes (une à quatre semaine selon différentes études) et en association avec d'autres analgésiques, pourrait constituer durant la grossesse un facteur de risque d'anomalies du développement de l'appareil reproducteur masculin (cryptorchidisme),.
Pendant la période d'allaitement, le paracétamol passe dans le lait maternel. Toutefois, les quantités excrétées dans la lactation sont inférieures à 2 % de la quantité ingérée et le paracétamol n'est donc pas contre-indiqué pendant la période d'allaitementÀ part avec certains anticoagulants oraux et les sétrons (antiémétiques), il n'y a aucune interaction médicamenteuse particulière répertoriée pour le paracétamol.
Anticoagulant oral : le paracétamol, utilisé à des doses supérieures à 3 g par jour, pendant plus de 4 jours consécutifs, pourrait potentialiser l'activité anticoagulante des Anti-Vitamine K (AVK). Dans ce cas, une surveillance de l'INR serait recommandée,.
Sétrons : une compétition existe entre le paracétamol et l'odansétron notamment, ayant pour effet de diminuer l'efficacité antalgique du paracétamol.La prise de paracétamol peut fausser le dosage de l'acide urique sanguin par la méthode à l'acide phosphotungstique, ainsi que le dosage de la glycémie par la méthode à la glucose oxydase-peroxydase.
Pour éviter tout risque de surdosage, il faut vérifier l'absence de paracétamol dans la composition d'autres médicaments pris de façon concomitante.
Des effets indésirables ont néanmoins été rapportés sans que l'imputabilité (le fait que l'effet indésirable soit bien causé par le médicament) ait été établie la plupart du temps. Les principaux effets indésirables retrouvés dans la littérature sont :
Très rarement : éruption cutanée avec rash ou éruption urticarienne d'origine probablement allergique,, thrombopénie et asthme,.
De façon ponctuelle : hypotension,, choc anaphylactique,, purpura vasculaire, syndrome de Lyell et syndrome de Stevens-Johnson, ulcération rectale, agranulocytose, pancréatite aiguë généralement en association avec d'autres médicaments comme la codéine,, hépatite chronique active, hépatite granulomateuse et rhabdomyolyse.Une toxicité sur le foie à dose thérapeutique ne peut également être exclue chez certaines personnes à risques,.
Chez le très jeune enfant, l'administration de paracétamol pourrait augmenter le risque de survenue d'un asthme,.
In vitro, le paracétamol pourrait présenter un effet tératogène qui n'est pas présent in vivo. La molécule n'est donc pas contre-indiquée chez la femme enceinte.
Devant l'apparition d'un effet indésirable, il est nécessaire d'arrêter le médicament incriminé et de consulter son médecin.
Le paracétamol est un médicament utilisé couramment et disponible dans les pharmacies. Les cas de surdose sont courants, plus de 100 000 par an aux États-Unis, une centaine en Suisse, et ont des conséquences graves. Ils peuvent entraîner si le surdosage est au long terme (environ 10 g/jour pendant 2 semaines) une nécrose hépatique. Et dans le cas d'un surdosage rapide (25-30 g en un jour) une hépatite fulminante, qui nécessite une transplantation immédiate pour éviter le décès du patient.
La dose toxique du paracétamol est hautement variable selon les individus. En une prise unique, elle est de l'ordre de 10 g ou 150 mg·kg-1 chez l'adulte et de 150 mg·kg-1 chez l'enfant,. Certains ne parlent d'intoxication aiguë que pour des doses élevées supérieures à 200 milligrammes par kilogramme, soit plus de 14 grammes pour un humain de 70 kilogrammes.
Cependant le paracétamol peut être toxique pour le foie, même à 4 g/24 h soit des doses thérapeutiques. Administrées sur de longues périodes, ces doses se rapprochent des doses toxiques pouvant entrainer des lésions hépatiques permanentes voire mortelles, surtout chez des patients à la fonction hépatique préalablement altérée,,,,. Comme les personnes âgées ou les patients présentant des maladies hépatiques, ou dans les cas d'alcoolisme chronique qui provoque une induction enzymatique et une diminution des réserves de glutathion. Par contre seuls quelques rapports font état d'une toxicité du paracétamol lors de situations diminuant les réserves de glutathion, comme une infection par le VIH, une hépatite chronique C ou une cirrhose hépatique par exemple.
Ainsi sur de longues durées, la différence entre une dose thérapeutique et une dose toxique serait faible. Depuis que le paracétamol est présent dans de nombreux médicaments, mélangé à d'autres molécules, le risque de surdosage involontaire est majoré. Les prises de paracétamol doivent toujours être espacées de 4 heures au minimum. Pour éviter le surdosage, il est utile de discuter avec un pharmacien pour connaître les médicaments contenant du paracétamol ou bien de regarder la composition des médicaments pour détecter la présence de paracétamol.
Une des étapes de la métabolisation du paracétamol produit une molécule toxique, la N-acétyl-p-benzoquinone imine (ou NAPQI), via les cytochromes P450 (CYP2E1, CYP1A2, CYP3A4). Ce métabolite peut provoquer la mort des cellules hépatiques. 
Il est éliminé, dans le foie, par une réaction avec le glutathion (donneur de SH) qui capte les radicaux.
Aux doses thérapeutiques recommandées, la NAPQI est éliminée par l'organisme et ne représente pas un danger. Par contre, lorsque la dose de paracétamol est trop importante, la NAPQI est produite en grande quantité, les réserves de glutathion s'épuisent et le foie n'arrive plus à l'éliminer ; il subira des dommages plus ou moins importants selon la quantité de paracétamol absorbée.
Un risque accru de toxicité est provoqué par un manque de glutathion (malnutrition, anorexie, éventuellement maladies du foie) ou une formation accrue du métabolite toxique.
Le surdosage en paracétamol peut ainsi entraîner une hépatite avec de graves lésions du foie (cytolyse hépatique), conduisant à une nécrose dans les cas extrêmes. Les conséquences d'un surdosage sont graves, parfois mortelles. Les dommages causés au foie sont irréversibles, une greffe de foie devenant nécessaire lorsque les dommages sont très importants. La NAPQI entraîne la création d'adduits fixés aux protéines hépatiques, dégradation des lipides membranaires, perturbations de l'homéostasie calcique, provoquant une nécrose et une hépatite cytolytique. Le rein est touché par le même mécanisme.
La toxicité sur le foie est prédictible à l'aide de deux paramètres : la dose ingérée et le taux plasmatique du paracétamol (ou paracétamolémie). Les prises intentionnellement abusives de paracétamol peuvent être détectées rapidement et les dommages peuvent être limités par l'administration de N-acétylcystéine. Ce n'est pas le cas de surdosages non intentionnels et chroniques qui se détectent plus tardivement alors que des dommages importants ont déjà pu se produire.
De plus, il est possible de calculer la demi-vie d'élimination du paracétamol. Dans les cas d'intoxication, la nécrose hépatique empêche l'élimination et la demi-vie augmente. Une demi-vie supérieure à quatre heures témoigne d'une hépatite. Une demi-vie supérieure à douze heures indique une insuffisance hépato-cellulaire.
Les individus qui ont pris trop de paracétamol n'ont généralement pas de symptômes pendant les vingt-quatre premières heures. Bien que des nausées ou des vomissements apparaissent en premier, ces symptômes disparaissent après quelques heures. Les sujets se sentent mieux et croient que le pire est passé. Si la dose absorbée est toxique, après cette période de bien-être, le sujet a une défaillance hépatique. Dans les cas extrêmes, le sujet tombe dans le coma avant d'avoir une défaillance du foie.
Les enfants supportent mieux le paracétamol, car ils possèdent un foie et des reins plus larges par rapport à la taille de leur corps, et ils sont plus tolérants à ce produit. La demi-vie sera plus importante chez l'enfant qui possède des capacités de glucuronoconjugaison inférieures à celles de l'adulte.
Les preuves à l'heure actuelle sont insuffisantes pour conclure que l'utilisation régulière de paracétamol est associé à un risque accru d'insuffisance rénale chronique.
Une surdose massive de paracétamol, habituellement plus de 40 g, peut également entraîner une acidose lactique métabolique ; celle-ci s'installe avant la cytolyse hépatique. Le paracétamol et certain de ses métabolites inhibent la respiration cellulaire, conduisant à l'accumulation de lactates.
Toute personne ayant ingéré une dose supérieure à la dose toxique théorique ou ayant ingérée une dose inconnue supposée supérieure, doit être immédiatement transférée dans un service d'urgences hospitalier où le traitement peut être l'administration en intraveineuse ou orale de N-acétylcystéine.
L'absorption du paracétamol par voie gastro-intestinale est complète au bout de deux heures en conditions normales, donc une décontamination gastro-intestinale n'est utile que pendant ce laps de temps. L'absorption du paracétamol peut être retardée en cas d'ingestion de nourriture. L'absorption est plus rapide lorsque le paracétamol est sous forme soluble que sous la forme solide.
Le lavage gastrique n'est pas recommandé, tout comme les vomissements provoqués, par l'utilisation d'un vomitif. Le sirop d'ipéca doit notamment être considéré comme obsolète.
Le charbon activé, qui réduit l'absorption digestive du paracétamol et présente moins de risques que le lavage gastrique, est indiqué uniquement lorsque la quantité de paracétamol absorbée est potentiellement mortelle et que l'ingestion a eu lieu moins d'une heure avant. Dans ce cas, on recommande une dose unique de 1 à 2 g/kg d'une suspension aqueuse de charbon actif (Carbomix ou Toxicarb) administrée par voie orale. Auparavant, les médecins étaient réticents à administrer du charbon activé puisqu'en cas de surdosage, celui-ci peut absorber aussi l'antidote et donc diminuer son efficacité. Mais des études ont montré que l'adsorption d'une partie de la N-acétylcystéine orale par le charbon activé n'a pas de conséquences significatives, l'une d'entre elles a déterminé que seulement 39 % de la N-acétycystéine est absorbée lorsqu'elle est administrée en même temps que le charbon. Sinon, l'utilisation d'acétylcystéine par voie intraveineuse est efficace en combinaison avec du charbon activé. S'il est prévu de donner la N-acétylcystéine par voie orale, il est recommandé de différer le traitement de une à deux heures après l'administration de charbon activé.
En pratique clinique, la prise en charge est la suivante : recherche d'intoxications associées, prise des signes vitaux (pouls, pression artérielle, température, score de Glasgow, fréquence respiratoire) et prélèvement veineux (paracétamolémie, transaminases, taux de prothrombine, créatinine, ionogramme), pose d'une voie veineuse périphérique avec une solution polyionique type B26 : 2 l/24 h, charbon activé si la prise est inférieure à 2 heures.
en cas de dose ingérée connue inférieure à la dose toxique minimale, il n'y a pas de traitement nécessaire car pas d'intoxication sérieuse ;
si la dose supposée ingérée est inférieure à 8 grammes, l'administration est guidée par la paracétamolémie et le délai écoulé depuis la prise de paracétamol. Le choix du traitement est déterminé selon les abaques de Prescott (ou nomogramme de Rumack-Matthew) ;
si la dose supposée ingérée est supérieure à 8 grammes, l'administration de N-acétylcystéine est immédiate, « à l'aveugle », sans attendre les résultats du taux plasmatique de paracétamol. Quatre heures au minimum après l'ingestion, il faut déterminer la paracétamolémie et la rapporter aux abaques de Prescott. Si le taux se situe en dessous de la « ligne de traitement », on peut arrêter le traitement. Si le taux est au-dessus, il faut le continuer et l'appliquer entièrement. Chez les patients à risque (affection hépatique, alcoolisme chronique, induction du métabolisme hépatique, malnutrition), il convient d'appliquer le schéma complet même en cas de dose plus faible. Le nomogramme ne peut pas être utilisé si le moment de l'ingestion est inconnu, s'il y a eu plusieurs ingestions ou s'il y a des facteurs de risque.Le transfert en unité de réanimation est indiqué en cas de troubles hémodynamiques, neurologiques, respiratoires, de co-intoxication avec une substance exigeant une prise en charge en réanimation, d'hépatite cytolytique grave, et a fortiori, d'insuffisance hépatique. En fin de traitement il faut contrôler le taux de prothrombine, les transaminases, la créatinine et la glycémie. La sortie est possible si la paracétamolémie arrive dans les zones non toxiques, en l'absence de toxiques associés et après accord du psychiatre (en cas d'intoxication volontaire).
En cas de surdose de paracétamol, pendant quelque 8 à 10 heures qui suivent l'absorption, le glutathion ne va être consommé que progressivement et sa disparition peut être palliée par l'administration de diverses molécules comportant un groupe SH telles la méthionine ou la cystéamine ou la N-acétylcystéine. De fait, ces produits ont maintenant clairement démontré qu'ils permettent de prévenir la nécrose hépatique par le paracétamol.
Dans les surdosages, la N-acétylcystéine est utilisée pour renforcer les défenses de l'organisme vis-à-vis des métabolites oxydants toxiques et est un précurseur de la cystéine. Le métabolite responsable de la toxicité du paracétamol est la N-éthanoyl-4-hydroxyphénylhydroxylamine. Elle réagit de manière irréversible avec les thiols tels le glutathion. La fonction thiol de la N-acétylcystéine permet d'épargner le glutathion hépatique. C'est la diminution de ce glutathion hépatique qui crée cette insuffisance hépatique et la N-acétylcystéine permet alors de réduire le risque de toxicité sur le foie si elle est absorbée moins de 8 heures après l'ingestion du paracétamol. Après 8 heures, une série d'évènements toxiques dans le foie commence et le risque de nécrose hépatique et de décès augmente de façon critique. Bien que la N-acétylcystéine soit plus efficace lorsqu'elle est administrée tôt, le produit a cependant des effets bénéfiques jusqu'à 48 heures après l'ingestion. Elle n'endommage pas les cellules et peut être excrétée sans danger.
La N-acétylcystéine s'administre comme antidote soit par voie buccale (Fluimucil granulé ou Mucomyst soluté, disponibles en pharmacie), soit en perfusion intraveineuse (Fluimucil 20 % (Inpharzam), amp. à 25 ml, 1 g = 5 ml). Aux États-Unis, l'administration orale est la méthode de référence alors qu'en Europe, l'administration par voie intraveineuse est préférée. L'acétylcystéine par voie orale peut entraîner à cause de son goût et de son odeur soufrée, des vomissements et des nausées. Par voie intraveineuse, surtout en cas de perfusion trop rapide, elle peut entraîner des réactions anaphylactoïdes. Le choix de la voie d'administration, orale ou intraveineuse, dépend avant tout de l'existence ou non de vomissements.
Il existe trois schémas thérapeutiques différents, un par voie orale, deux par voie veineuse, d'efficacité équivalente tant qu'ils sont instaurés dans les 10 heures suivant l'ingestion, :
Schéma de Prescott (voie veineuse) : dose initiale de charge de 150 mg·kg-1 (dans 200 ml de glucose 5 % sur 60 minutes), puis 50 mg·kg-1 (dans 500 ml de glucose 5 % sur 4 h), puis 100 mg·kg-1 (dans 1 000 ml de glucose 5 % sur 16 h). Dose totale de 300 mg·kg-1 sur une durée totale de 21 h.
Schéma de Smilkstein, (voie veineuse) : dose initiale de charge de 140 mg·kg-1 (dans 200 ml de glucose 5 % sur 15 minutes), puis 70 mg·kg-1 (dans 100 ml de glucose 5 % sur 15 minutes) toutes les 4 h, à répéter 12 fois. Dose totale de 980 mg·kg-1 sur une durée totale de 48 h.
Schéma de Rumack (voie orale) : dose initiale de charge de 140 mg·kg-1, puis 70 mg·kg-1 toutes les 4 h, à répéter 17 fois. Dose totale de 1 330 mg·kg-1 sur une durée totale de 68 h.Si le traitement est commencé plus de 10 heures après l'ingestion, le schéma d'administration orale de Rumack et le schéma d'administration intraveineuse de Smilkstein donnent de meilleurs résultats que le schéma de Prescott.
Le paracétamol, contrairement à l'aspirine et à l'ibuprofène, est dépourvu de propriétés anti-inflammatoires. Il ne fait pas partie de la classe des anti-inflammatoires non stéroïdiens (AINS), n'étant pas un bon inhibiteur des COX et notamment de la COX-2. Les AINS eux, ont en commun la propriété de pouvoir diminuer la production des prostanoïdes en inhibant l'activité des deux isoformes de cyclo-oxygénases (COX-1 et COX-2).
En ce qui concerne le traitement de la douleur, l'activité antalgique du paracétamol est comparable à celle de l'aspirine, pour des posologies identiques de 1 à 3 g/jour et pour des douleurs de causes diverses,.
Des études renforcent la notion qu'il faut continuer à envisager le paracétamol comme traitement de première intention pour le soulagement de la douleur d'intensité légère à modérée d'après des évaluations effectuées relativement à l'innocuité, à l'efficacité et au coût. Le paracétamol a très peu d'effets secondaires. Les associations avec d'autres produits, plus puissantes ou mieux adaptées ne seront envisagées que dans un second temps, ou dans des cas spécifiques.
Dans les doses recommandées, le paracétamol n'irrite pas la paroi de l'estomac, n'affecte pas la coagulation du sang autant que les AINS, et n'affecte pas le fonctionnement du rein. L'utilisation des AINS peut être à l'origine de cas d'hémorragies gastro-intestinales ; le paracétamol, par contre, n'est pas associé à l'augmentation du risque d'épisodes gastro-intestinaux dans les doses normales. Cependant, certaines études ont montré que pour des doses élevées (plus de 2 000 mg par jour) le risque de complications intestinales augmente.
Le paracétamol ne présente pas de contre-indications pour les femmes enceintes et n'affecte pas le développement du fœtus comme le font les AINS (traitement de la persistance du canal artériel). L'utilisation des AINS par les femmes enceintes est associée, de façon importante, à l'hypertension pulmonaire persistante chez les nouveau-nés. Le paracétamol est actuellement très utilisé, notamment en pédiatrie. 
Il peut être administré aux enfants car il n'est pas associé au risque du syndrome de Reye pour les enfants possédant une déficience immunitaire.
Une étude clinique faite chez des enfants montre qu'une dose standard d'ibuprofène provoque un plus grand soulagement de la douleur qu'une dose standard de paracétamol ou de codéine.
Comme les AINS et contrairement aux opiacés, le paracétamol n'a pas été reconnu comme la cause d'euphories ou de modification d'humeur mais contrairement aux opiacés, il peut endommager le foie. Le paracétamol et les AINS présentent un faible risque d'assuétude ou d'addiction, contrairement aux opiacés.
En ce qui concerne le traitement de la fièvre, il ne semble pas exister de différence d'efficacité anti-pyrétique entre le paracétamol et les AINS,.
Concernant l'enfant, deux méta-analyses de 2004, retrouvent que l'ibuprofène aurait une rapidité d'action légèrement supérieure au paracétamol. Mais c'est le paracétamol qui permettrait le mieux d'améliorer le confort de l'enfant, notamment au niveau de l'activité et de la vigilance. Au total on peut conclure que chez l'enfant, le paracétamol, l'ibuprofène et l'aspirine ont une efficacité antipyrétique identique mais que leurs effets indésirables sont sensiblement différents, ce qui finalement justifie amplement de privilégier le paracétamol en première intention.
Le surdosage involontaire en paracétamol est la première cause de défaillance du foie en Angleterre et aux États-Unis. Les intoxications involontaires au paracétamol représentent tous les ans aux États-Unis plus de 13 000 passages aux urgences, plus de 2 000 hospitalisations et près de 100 décès selon la Food and Drug Administration. 
Ces chiffres importants sont expliqués par le fait que de nombreux produits sont disponibles aux États-Unis en vente libre sans ordonnance et contiennent du paracétamol sans que cela soit indiqué sur la boîte, et par le fait que les conditionnements des antalgiques à base de paracétamol dépassent souvent la dose potentiellement mortelle de 8 grammes par boîte. 
En France dans les années 1980, l'Agence du médicament, ancien nom de l'Afssaps, avait réduit le conditionnement des antidouleurs à base de paracétamol pour qu'ils ne dépassent pas cette dose. Depuis ce changement de conditionnement, les décès par intoxication n'ont pas augmenté alors que la consommation n'a pas cessé de croître. Ainsi en 1990, 177 420 000 boîtes de paracétamol ont été vendues en France, et 5 335 intoxications et 6 décès ont été recensés. Ces chiffres restent stables depuis cette année.
En Angleterre, à l'époque où le conditionnement n'était pas limité à un maximum de 8 grammes, les décès étaient compris entre 200 à 600 selon les sources, ce qui a mené les autorités à adopter des mesures similaires à la France à partir de 1998.
Le paracétamol est parfois utilisé lors de suicides ou de tentatives de suicides. Cependant, plus de la moitié des morts par surdosages sont des accidents. Les défaillances hépatiques aiguës consécutives à un surdosage non intentionnel donnent souvent des tableaux plus sévères et ont un pronostic moins bon que chez les patients ayant un surdosage intentionnel. En effet, les victimes de surdoses accidentelles sont souvent prises en charge plus tard, et les risques sont donc plus élevés. Cependant, comparés aux nombres de doses de paracétamol consommées chaque jour, les surdosages accidentels ne touchent qu'une minorité des utilisateurs. En France, les suicides au paracétamol sont bien moins courants mais aussi plus difficiles à évaluer car il n'existe pas de registre national des intoxications volontaires. 
Bien que le taux d'intoxication au paracétamol soit faible par rapport aux millions de tablettes utilisées chaque année, certains auteurs proposent de changer le mode de vente du paracétamol. Les conditionnements actuels limitent le risque de surdosage accidentel, la quantité de paracétamol par boîte a été diminuée et les prescriptions de médicaments combinant des narcotiques au paracétamol ont été restreintes pour réduire les accidents. Les enfants sont victimes de surdoses accidentelles en cas d'absorption massive sous la forme de sirop. Par contre, les formes effervescentes du paracétamol limitent le risque de prise accidentelle, car elles imposent de boire une grande quantité de liquide. L'association d'une substance et de son antidote dans le même médicament permet de diminuer les risques de surdose. Le Paradote est un médicament sous forme de tablettes contenant 100 mg de méthionine et 500 mg de paracétamol. La méthionine est utilisée pour remplacer le glutathion et permet de protéger le foie en cas de surdose.
Le paracétamol est l'un des médicaments les plus vendus dans le monde.[réf. nécessaire] Le nom acétaminophène est utilisé aux États-Unis, au Canada, au Japon, en Corée du Sud, à Hong Kong et en Iran. Dans les autres pays, on emploie le nom « paracétamol ».
Le paracétamol est le médicament le plus prescrit en France, et même la base des trois médicaments les plus prescrits (noms commerciaux: Doliprane, Dafalgan, Efferalgan), qui totalisent plus de 260 millions de doses.
Selon le rapport de 2005 de la Caisse nationale d'assurance maladie, la famille de médicaments la plus prescrite en France est celle des antalgiques, qui progresse encore de façon importante (+ 9,2 % par rapport à 2004) pour atteindre 340 millions de boîtes vendues,.
On retrouve en tête de liste des dix médicaments les plus prescrits en quantité en France en 2005 trois antalgiques à base de paracétamol seul : le Doliprane (1er avec 73,3 millions d'unités prescrites en 2005 soit +15,2 % depuis 2004), l'Efferalgan (2e avec 42,5 millions soit +5,8 %) et le Dafalgan (3e avec 35,5 millions soit +11,2 %). Leur classement respectif était identique en 2004. Deux antalgiques avec du paracétamol associé se placent aussi dans les 10 produits les plus prescrits : Propofan (6e avec 14,6 millions d'unités prescrites en 2005 soit -5,2 % depuis 2004) et Di-Antalvic (8e avec 12,8 millions soit -0,6 %).
Ce qui signifie que 5 des 8 produits les plus prescrits en France sont des antalgiques contenant du paracétamol, avec une très forte progression pour certains comme le Doliprane (+15 %) et Dafalgan (+11 %). Selon le rapport de l'Agence Française de sécurité sanitaire des produits de santé (AFSSAPS), les prescriptions de paracétamol en France ont été multipliées par 2 en 10 ans. Cette forte croissance signifie que l'usage de ces antalgiques s'est banalisé, ce qui pourrait être dommageable à terme.
En termes de coût, le Doliprane qui est la spécialité la plus prescrite en quantité, ne se situe qu'au 15e rang des dépenses (96,3 millions d'euros, en progression de 11,7 % depuis 2004). L'Efferalgan est 42e avec 57,5 millions et +3,0 % et le Dafalgan 52e avec 47,5 millions et +7,9 %. L'ensemble des spécialités à base de paracétamol seul représente 236 millions d'euros (+12 % depuis 2004) et le 5e rang des dépenses.
Jusqu'en 2002, l'apparition de paracétamol générique en France a été bloquée pour sauvegarder l'emploi de l'usine Doliprane de Lisieux, dont les deux députés-maires successifs de 1953 à 1989 étaient pharmaciens. Le paracétamol était dans le domaine public mais son « généricage » était bloqué au prétexte de l'absence d'un dépôt de brevet spécifique. Depuis, les ventes de paracétamol progressent plus vite que d'autres médicaments de la même classe (les antalgiques), laissant supposer que certains prescripteurs et certains patients (auto-médication) ont changé leurs habitudes, passant de l'aspirine générique (ou de l'ibuprofène) au paracétamol générique.
On peut noter que dans le rapport de 2006 de la Caisse nationale d'assurance maladie les antalgiques conservent une croissance soutenue (+4 %) et restent en tête du classement des familles de médicaments les plus prescrites, avec 358 millions de boîtes prescrites et remboursées. Ils demeurent les médicaments les plus prescrits, avec pour 2006, une croissance qui reste supérieure à +5 % pour Doliprane et Dafalgan. Les antalgiques à base de paracétamol seul ou associé représentent 4 des 10 produits les plus prescrits et l'importante baisse du Propofan (-45,8 %) n'est que fictive quand on prend en compte le marché du groupe générique correspondant.
Le paracétamol fait partie de la liste des médicaments essentiels de l'Organisation mondiale de la santé (liste mise à jour en avril 2013).
En 2015, les médicaments à base de cette molécule sont retirés des supermarchés suédois en raison de la hausse des intoxications depuis l'autorisation de leur vente en grandes surfaces en 2006.
Dans le cas d'une ingestion supposée pour les chats ou d'une surdose pour les chiens, il est important de consulter un vétérinaire immédiatement pour une désintoxication.
Le paracétamol est une substance extrêmement toxique pour les chats qui ne doivent en absorber dans aucun cas. Les chats ne possédant pas l'enzyme glucuronyl transferase, de petites quantités peuvent leur être fatales. La toxicité apparaît pour des doses journalières aussi faibles que 10 mg·kg-1. Les symptômes initiaux sont le vomissement, la salivation et la décoloration de la langue et des gencives. Au bout de deux jours, les dommages corporels sont évidents et apparaît une jaunisse. Contrairement à ce qui se passe chez l'humain, ce ne sont pas les dommages hépatiques qui causent la mort mais c'est la production de méthémoglobine et de corps de Heinz dans les globules rouges qui empêche le transport de l'oxygène dans le sang, provoquant une mort par asphyxie. Des traitements efficaces sont possibles pour les faibles doses mais ils doivent être administrés très rapidement.
Pour les chiens, le paracétamol est un antalgique utile avec un bon résultat en matière d'efficacité, qui cause moins d'ulcères gastriques que les anti-inflammatoires non stéroïdiens. Mais il ne doit être administré que sur les conseils d'un vétérinaire. En effet, le surdosage, potentiellement mortel, est rapidement atteint même avec de faibles doses. La toxicité hépatique peut survenir à partir de 100 mg·kg-1 et une méthémoglobinémie à partir de 200 mg·kg-1.
Le paracétamol est létal pour certains serpents et son utilisation dans le but de contrôler la prolifération du serpent brun arboricole (Boiga irregularis) dans l'île de Guam à l'aide de fausses souris imprégnées a été validée lors d'une étude.
D'après une étude, le paracétamol pourrait se transformer en produit toxique, lorsque les usines de traitement des eaux usées utilisent le procédé de javellisation. Le paracétamol se transformerait, sous l'action de l'ion hypochlorite ClO-, en N-acétyl-p-benzoquinone imine et en 1,4-benzoquinone. La première molécule est toxique pour le foie tandis que la seconde est suspectée d'être génotoxique et mutagène. Des études supplémentaires doivent être effectuées pour savoir quelle est la concentration de ces substances à la sortie des eaux usées et pour connaître la persistance de ces produits dans l'environnement.
Le 30 septembre 1982, la première victime d'une série macabre meurt à Chicago après avoir absorbé une capsule d'acétaminophène (commercialisé sous le nom de Extra Strength Tylenol). Au total, sept personnes furent victimes de cet empoisonnement aux États-Unis,. Ces capsules contenaient du cyanure en quantité suffisamment importante pour être létale pour un adulte. La société Johnson & Johnson proposa alors d'échanger toutes les capsules de Tylenol en circulation par des tablettes solides de Tylenol. Dans cette affaire, la société eut une perte d'un million de dollars et fut condamnée à payer de lourdes indemnités aux victimes. Le responsable n'a jamais été arrêté et cette affaire n'a jamais été élucidée.
En février 1986, une nouvelle affaire d'empoisonnement éclate aux États-Unis, à la suite de la mort d'une jeune femme de 23 ans, Diane Elsroth, à Yonkers dans la région de New York, le 8 février. Elle avait absorbé une gélule de Tylenol Extra-Fort. La gélule avait été empoisonnée au cyanure, ce qui a relancé la psychose survenue après la vague de morts de Chicago, 3 ans et demi auparavant. À ce jour, tout comme pour la vague de crimes de 1982, cette affaire n'a pas été résolue.
C. Remy, E. Marret, F. Bonnet, « Actualité du paracétamol »(Archive • Wikiwix • Archive.is • Google • Que faire ?) (consulté le 30 mars 2013), Évaluation et traitement de la douleur, 2006, p. 639-648, Département d'anesthésie-réanimation, hôpital Tenon, 9 septembre 2006, Elsevier Masson SAS.
(en) Paracetamol Information Centre : « an independent source of information, fact, and background detail, for journalists on all aspects of the clinical and home use of paracetamol ».

Le paradoxe de Fermi est le nom donné à une série de questions que s'est posées le physicien italien Enrico Fermi en 1950, alors qu'il débattait avec des amis de la possibilité d'une vie et d'une visite extraterrestre, compte tenu du modèle corallien de colonisation galactique.
Fermi, lauréat du prix Nobel en 1938, et alors qu'il est impliqué dans le projet Manhattan à Los Alamos aux États-Unis, déjeune avec plusieurs de ses amis et collègues (Emil Konopinski, Edward Teller et Herbert York). Lors du repas, il en vient à demander où sont les extraterrestres, et pose le principe du paradoxe qui porte son nom. Celui-ci est lié à la question de savoir pourquoi l'humanité n'a, jusqu'à présent, trouvé aucune trace de civilisations extraterrestres alors que le Soleil est plus jeune que beaucoup d'étoiles situées dans la galaxie. Selon Fermi, des civilisations plus avancées auraient dû apparaître parmi ces systèmes planétaires plus âgés et laisser des traces visibles depuis la Terre, telles des ondes radio. Le paradoxe de Fermi peut donc s'énoncer ainsi : 
« S’il y avait des civilisations extraterrestres, leurs représentants devraient être déjà chez nous. Où sont-ils donc ? »
 Plusieurs hypothèses ont été formulées pour expliquer ce paradoxe. La question de Fermi est redécouverte par Carl Sagan en 1966, puis elle est explicitement formulée par l'ingénieur David Viewing en 1975. La même année, Michael H. Hart énumère quatre solutions possibles au paradoxe.
Pour certains auteurs, le paradoxe n'en est pas un ; pour d'autres, il s'agit d'un dilemme ou d'un problème de logique. Pour d'autres enfin, il repose sur un anthropocentrisme qui lui retire toute possibilité de résoudre la question de la vie extraterrestre. La littérature spécialisée, mais aussi la science-fiction, la philosophie et la pensée religieuse, connaissent depuis une profusion d'essais explorant les solutions possibles au paradoxe. Il a ainsi évolué ; des outils statistiques (comme l'équation de Drake) ont tenté de le poser sous une forme scientifique. D'autres approches (comme la théorie de l'évolution, l'écologie ou la simulation informatique) en ont élargi la base. Il n'y a pas à ce jour de consensus sur la solution du problème.
La question de Fermi a en réalité été soulevée avant lui par Constantin Tsiolkovski. Développé ensuite par David Viewing et Michael Hart en 1975, le paradoxe de Fermi peut être résolu par divers scénarios, classés en quatre groupes :
ceux qui estiment qu'une civilisation technologiquement avancée a très peu de chance d'apparaître ; qu'un univers de la taille du nôtre est nécessaire pour qu'elle ait une chance de se produire une fois (mais beaucoup moins probablement deux) ;
ceux qui supposent que les extraterrestres existent mais que pour une raison ou une autre la communication et le voyage interstellaires sont impossibles ou ne sont pas jugés souhaitables ;
ceux qui supposent que la vie existe ailleurs, mais en des lieux rendant sa détection difficile — par exemple dans des océans protégés par une couche de glace, organisée autour d'évents hydrothermaux ;
C'est peu de temps avant le début de la Seconde Guerre mondiale qu'Enrico Fermi émigre aux États-Unis. Le 2 janvier 1939, avec toute sa famille, il s'installe à New York. Il enseigne à l'université Columbia avec son collègue Leó Szilárd. Les deux hommes travaillent ensuite ensemble à l’université de Chicago à l'élaboration d'une pile atomique, le premier réacteur nucléaire. Le 2 décembre 1942 est obtenue la première réaction en chaîne contrôlée de fission. Fermi travaille ensuite au laboratoire national de Los Alamos jusqu'à la fin de la Seconde Guerre mondiale, au sein du projet Manhattan. Il est ensuite fait citoyen américain en 1945, en récompense de ses travaux sur la bombe atomique,.
« Sommes-nous la seule civilisation intelligente et technologiquement avancée de l'Univers ? » C'est donc la question que se pose Fermi lors de l'été 1950, dans la cafétéria du laboratoire national de Los Alamos, au cours d'une conversation informelle avec ses trois collègues. Le déroulement de la conversation varie cependant selon les souvenirs de chaque témoin. L'échange entre Fermi et trois de ses collègues (Edward Teller, Emil Konopinski et Herbert York) n'a jamais fait l'objet d'une consignation par écrit. Selon Carl Sagan, la conversation aurait été apocryphe, mais l'enquête d'Eric M. Jones, en 1985, suggère qu'elle a bien eu lieu. Les témoignages des personnes présentes, ainsi que ceux des trois scientifiques participant au débat, constituent toutefois l'unique source d'information à ce propos. Fermi ne semble pas s'être prononcé par la suite sur cette question. Jones a recueilli le contexte de cet événement, contactant les collègues de Fermi par courrier mais aussi tous ceux qui auraient pu être présents à ce moment-là, à la Fuller Lodge, la cafétéria du personnel où se déroule la discussion. Hans Mark constitue son témoin direct le plus fiable, même s'il ne participait pas à l'échange. Herbert York atteste que l'échange a lieu lors de l'été 1950, en tous les cas après la publication du dessin d'Alan Dunn, le 20 mai 1950,.
Emil Konopinski, contacté par Jones, se souvient le plus clairement de la discussion ; il explique se rappeler que l'échange, à la Fuller Lodge, avait trait aux extraterrestres. Le physicien italien commente alors un dessin humoristique paru dans le New Yorker du 20 mai 1950 et dans lequel le dessinateur, Alan Dunn, pour expliquer de mystérieux vols d'objets ressemblant à des poubelles domestiques qui venaient de se produire à New York, représente sur leur planète des extraterrestres tirant hors d'une soucoupe volante les poubelles terriennes. S'ensuit une discussion passionnée entre les hommes présents à cette table quant à la possibilité de l'existence d'une vie extraterrestre, et de sa preuve. Konopinski ajoute que c'est à partir de ce dessin que la discussion s'oriente vers un sujet plus sérieux,,. Le paradoxe, c'est que nous n'en observons aucune trace, ni visuelle, ni radio. Fermi aurait demandé : « si les extraterrestres existent, où sont-ils donc ? » Konopinski se rappelle que sa question était plutôt : « Don't you ever wonder where everybody is ? » Selon lui, trois types de preuves existent : la présence de sondes, de vaisseaux ou de transmissions radios. Or aucune n'a été détectée par l'Homme. Selon le conférencier Michael Michaud, le développement de Fermi est une version précoce et informelle de l'équation de Drake qui vient le préciser quelques années après.
Pour Edward Teller, le souvenir de l'échange est plus flou ; il se rappelle que la discussion a eu lieu peu de temps après la fin de la guerre, lors d'une visite de Fermi au laboratoire de Los Alamos, certainement pendant l'été,. Le scientifique ne se souvient pas du point de départ de la discussion, à savoir – selon Konopinski — le dessin d'Alan Dunn. Teller explique même que la discussion n'avait rien à voir au début avec l'astronomie, mais qu'elle était largement terre-à-terre. Après quelques échanges, Fermi en est venu à lancer : « Où est tout le monde ? » (« Where is everybody ? »). Tous ceux présents ont alors éclaté de rire, puis le sujet est devenu la vie extraterrestre,. Il se rappelle en revanche avec certitude que Fermi a abordé la question de la vie extraterrestre, en proposant des chiffres et des statistiques. Teller dit que les personnes présentes étaient au nombre de huit, ce que ne confirment pas du tout Konopinski et York, qui n'en mentionnent que quatre, eux inclus.
Pour Herbert York, la discussion a commencé à partir du dessin de Dunn, puis Fermi en a mené le déroulement. Le scientifique italien était, selon York, très « expansif » (« expansive ») : il se lançait dans de nombreux calculs et probabilités concernant tous les facteurs de la vie intelligente dans l'univers. Fermi évoqua donc la probabilité de planètes semblables à la Terre, celle concernant la genèse de la vie, le temps d'existence d'une civilisation hautement avancée, etc. Fermi examine le cas d'une civilisation intéressée par la conquête de la galaxie (quels qu'en soient ses buts), et dotée de moyens techniques raisonnables, à la portée de la civilisation humaine de l'époque et maîtrisant notamment le voyage interstellaire, et ce, même à une vitesse nettement inférieure à la vitesse de la lumière. Il pose ainsi les axiomes du paradoxe qui porte son nom. Il imagine aussi qu'une telle civilisation serait capable de coloniser une nouvelle planète pour la transformer en nouvelle base de départ, chaque cycle prenant quelques centaines ou milliers d'années et permettant d'avancer, par bonds successifs, plus loin dans l'espace de quelques dizaines d'années-lumière. Sachant que la Voie lactée fait environ 50 000 années-lumière de rayon, une vitesse globale du front de colonisation de 1 % de la vitesse de la lumière suffit à la coloniser entièrement en quelques millions d'années, ce qui est très peu par rapport à l'âge de la galaxie et au temps qu'il a fallu à la vie terrestre pour évoluer jusqu'à la civilisation humaine actuelle. Si des civilisations extraterrestres existent, la logique serait donc que ce phénomène se soit déjà produit, et même éventuellement plusieurs fois. Il a ensuite conclu sur le fait qu'étant donné tous ces calculs, la Terre aurait déjà dû être visitée il y a longtemps, et même plusieurs fois depuis, posant de fait les bases de ce qui devient le paradoxe qui porte son nom,.
Cette interrogation et l'entretien informel qui s'ensuit entre Fermi et Carl Sagan, n'est mentionnée qu'en 1966 dans un court article de ce dernier. Il y défend l'idée qu'une civilisation assez ancienne peut en venir à coloniser la galaxie entière, en dépit de sa taille (100 000 années-lumière),. Le paradoxe de Fermi tombe ensuite dans l'oubli. Deux auteurs vont le redécouvrir en 1975. Dans le Quarterly Journal of the Royal Astronomical Society, l'astrophysicien Michael H. Hart énumère les arguments de Fermi. Il dénombre quatre catégories de solutions possibles : les explications physiques, les explications sociologiques, celles temporelles et enfin celles niant l'existence extraterrestre. Il soutient que le voyage interstellaire est possible pour une civilisation technologiquement avancée et que sa migration doit gagner toute la galaxie en quelques millions d'années. Ce laps de temps étant faible comparé à l'âge de la Voie lactée, Hart conclut que le fait que la Terre n'ait pas été colonisée signifie qu'il n'y a aucun extraterrestre. La même année, David Viewing formule quant à lui explicitement le paradoxe (voir infra) et répond de manière semblable à celle de Hart : les civilisations extraterrestres n'existent pas ; son article est le premier à se référer à Fermi et à sa question.
Ces deux articles trouvent des échos au sein de la communauté scientifique et c'est à cette occasion que Carl Sagan, se rappelant le questionnement du physicien rencontré quelques années plus tôt, baptise le problème du nom de « paradoxe de Fermi », en 1976
. Pour l'astronome George Abell, l'analyse de Hart traduit cependant un biais cognitif : il a usé d'arguments de manière à imposer sa vue, et sans prendre la peine d'estimer le nombre d'autres civilisations possibles dans la galaxie. Pour Giuseppe Cocconi et Philip Morrison, les arguments de Hart sont motivés par des considérations religieuses destinées à faire de l'homme le seul être pensant. L'article de Hart est cependant le fondement du paradoxe de Fermi et des solutions qui sont proposées au cours des années, à tel point que, selon Geoffrey Landis, le paradoxe devrait s'intituler plus justement le « paradoxe de Fermi-Hart », puisque le physicien italien est le premier à avoir formulé la question alors que Hart en a fourni une analyse rigoureuse dans son article.
Après la redécouverte du paradoxe de Fermi par Sagan et Hart, une abondante littérature apparaît, au croisement des domaines de l'astrophysique, de la xénobiologie, de la statistique mais aussi de la philosophie et de la science-fiction. Un panorama bibliographique a été réalisé par T. B. Kuiper et G. D. Brin en 1989.
Les astronomes Thomas Kuiper et Mark Morris expliquent en 1977 que si les voyages interstellaires sont possibles, alors une alternative existe ; ils en déduisent que : soit les civilisations technologiques à longue durée de vie sont rares, soit il y a plusieurs civilisations et, donc, la galaxie est déjà explorée ou colonisée. Trois scénarios expliquant l'absence de contact sont probables selon eux : la Terre a été préservée depuis toujours, la technologie extraterrestre a évolué jusqu'au point où elle ne nécessite plus de bases planétaires et enfin la biologie terrestre est incompatible ou même hostile à ces formes de vie. Michael D. Papagiannis dans Are We Alone, or Could They Be in the Asteroid Belt ? (1978) affirme également que les extraterrestres demeurent dissimulés dans la ceinture d'astéroïdes du Système solaire, sans interférer avec notre civilisation.
Michael Hart et l'astronome Ben Zuckerman ont organisé en 1979 une conférence à l'université du Maryland intitulée A Symposium on the Implications of Our Failure to Observe Extraterrestrials à l'issue de laquelle ils concluent que les voyages interstellaires sont possibles, bien que difficiles à effectuer, et que la colonisation n'a pu commencer que depuis quelques millions d'années seulement. Le physicien Frank Tipler avance en 1981 que même si une civilisation ne cherche pas à envahir la galaxie, il serait étonnant qu'aucune sonde de von Neumann ne soit lancée. Il en déduit que l'espèce humaine est la plus évoluée de toutes dans les environs du Système solaire (solution dénommée « Tipler's Argument »). Le sénateur William Proxmire a utilisé cet argument à la fin des années 1980 pour tenter de mettre fin au programme SETI. L'argument avancé par Tipler a toutefois été contredit par Carl Sagan et William Newman en 1983.
Dans un article de 1983, et suivant la formule « Cosmic silence » de Stanislas Lem en 1977, Glen David Brin nomme le résultat du paradoxe de Fermi « Great silence » (« Grand silence »). Il distingue deux groupes d'auteurs : ceux qui croient en l'existence de solutions expliquant la vie extraterrestre (« Contact optimists »), et ceux qui pensent que la Terre est unique dans l'univers (partisans de l'« Uniqueness Hypothesis ») ; l'ensemble forme le champ disciplinaire de la « xénologie ». Ian MacLeod dans New Light on the Drake Equation (2001) examine comment l'équation de Drake a redéfini le paradoxe de Fermi, et en quoi la littérature de l'imaginaire a produit des solutions intéressantes. Where Is Everybody ? Fifty Solutions to the Fermi Paradox and the Problem of Extraterrestrial Life (2002) de Stephen Webb examine en détail les solutions envisagées, et l'état du savoir pour chacune d'elles. Les exobiologistes Mat Coward dans The Second Question (2001), Gerrit L. Verschuur dans We Are Alone ! (1975) et Michael H. Hart dans son article Atmospheric Evolution and an Analysis of the Drake Equation (1981) examinent eux aussi plus en détail le paradoxe.
Le concept de « grand filtre », introduit en 1996 par l'économiste américain Robin Hanson, qui bloquerait éventuellement la plupart des évolutions à un niveau ou à un autre, est parfois aussi évoqué. Au cas où un tel filtre existerait, la question est de savoir s'il est aujourd'hui derrière nous ou devant nous. Une étude des chercheurs Aditya Chopra et Charles Lineweaver, publiée en janvier 2016 dans la revue Astrobiology, reprend cette hypothèse et estime que sur la plupart des planètes, dont l'habitabilité ne peut pas, selon eux, être maintenue plus de 1 ou 1,5 milliard d'années, le changement des conditions physiques « conspire à éliminer la vie naissante avant qu'elle ait une chance d'évoluer suffisamment pour réguler les cycles globaux ».
Le paradoxe formulé en 1950 par Fermi peut être résumé par la formule, souvent citée dans la littérature spécialisée : « Where is everybody? ». Toutefois, il ne s'agirait pas d'un véritable paradoxe. Selon John Mauldin, « L'une ou l'autre des prémisses du dilemme de Fermi est en jeu », sous-entendu : elle est à revoir. Le sociologue et diplomate américain Michael Michaud rappelle que le paradoxe dépend des observations réalisées : une seule détection d'une civilisation extraterrestre et il s'effondre. Selon Stephen Webb, ce dont on a besoin pour expliquer le paradoxe de Fermi est un mécanisme qui peut affecter la vie sur chaque planète de la galaxie, sans exception. S'il existe un tel mécanisme capable de stériliser toute une planète, et même si sa fréquence est invariable mais de l'ordre de quelques centaines de millions d'années, alors il est résolu.
Pour Pierre Lagrange, la véritable interrogation au cœur du paradoxe devrait être : « Pourrions-nous seulement espérer communiquer avec une civilisation qui serait « en avance » sur la nôtre de quelques millions d’années ? ». Elle serait « la seule [des hypothèses] qui prenne au sérieux le paradoxe de Fermi », conduisant ainsi à atteindre « une sorte de limite où le beau programme enclenché par Copernic tourne au cauchemar métaphysique. ». Pour Milan M. Ćirković, le paradoxe de Fermi se présente comme le paradoxe d'Olbers, dans le sens où ce dernier peut être résolu par une explication simple (la lumière n'a pas le temps d'établir un équilibre thermodynamique avec l'espace interstellaire glacé). Il suggère que le paradoxe de Fermi puisse être, lui aussi, résolu de manière simple, en principe par l'âge fini de la population stellaire (et par l'hypothétique nombre des civilisations extraterrestres induit par ce fait), et qui correspondrait à la classe des hypothèses dite de la « Terre rare ».
Il s'agirait davantage d'un problème plutôt que d'un paradoxe, d'après certains auteurs. Selon les astronomes russes I. Bezsudnov et A. Snarskii, le problème se fonde sur le principe que toutes les civilisations se développent suivant une logique et une constante identique, principe qui a été déduit du propre développement de l'humanité sur Terre. Le paradoxe s'apparente donc à un syllogisme. Ivan Almar et Jill Tarter pointent la faiblesse logique de la première prémisse : le paradoxe pose que les extraterrestres ne sont pas visibles mais l'humanité ne dispose d'aucun moyen de l'affirmer réellement. Ils pourraient être présents dans le Système solaire sans qu'aucun moyen technique ne permette de les détecter. De même, le coordinateur du projet SETI Thomas McDonough rappelle qu'avant la découverte du microscope par Antoine van Leeuwenhoek, l'humanité, qui n'en a pas déduit une formulation paradoxale pour autant, ne soupçonnait pas l'existence d'une vie invisible à l'œil nu.
Stephen Webb conclut qu'il n'existe pas de paradoxe de Fermi tant que les moyens d'investigation ne se sont pas d'abord orientés vers la recherche d'une présence extraterrestre dans le Système solaire. Les astronomes russes L. M. Gindilis et G. M. Rudnitskii concluent eux aussi que le paradoxe est caduc et qu'il est davantage un outil « astrosociologique » (« astrosociological ») destiné à stimuler des débats et des recherches. Ils rebaptisent le paradoxe de Fermi l'« Astrosociological Paradox » (ASP). Selon l'ingénieur Krafft Ehricke, le problème concerne avant tout des données scientifiques, et la question de Fermi devrait être reformulée ainsi : « Où sont les étoiles tardives de type F et où sont les étoiles précoces de type G ? »
Selon Pierre Lagrange, le problème du paradoxe de Fermi, « c’est que les solutions imaginées partent du principe que si des extraterrestres étaient proches nous devrions automatiquement les voir. Cela revient à prendre au sérieux comme modèles de contact les scénarios des séries B des années cinquante comme Le Jour où la Terre s'arrêta (The Day the Earth Stood Still) et à oublier deux très bonnes raisons de penser autrement. La première concerne la difficulté à « voir » les faits en science, le second notre capacité à comprendre une forme d’intelligence profondément étrangère à la nôtre. » Or, cet « argument de visibilité immédiate d’éventuels extraterrestres proches » est biaisé puisqu'« il revient à réduire la question de la recherche d’une manifestation dotée d’intelligence à l’observation d’un phénomène prévisible. » Selon Lagrange, les comportements intelligents sont difficilement prévisibles, comme l'a montré l'histoire de l'anthropologie ou celle des sciences. Pour Stephen Webb, une méthode inspirée de celle du crible d'Ératosthène combinée au rasoir d'Occam permettrait de résoudre le paradoxe. Il s'agit d'évaluer la probabilité de chaque solution retenue (huit selon Webb). Le résultat aboutit à annoncer que l'humanité est seule dans la galaxie, et certainement au sein de l'amas galactique local.
Robert A. Freitas, du Xenology Research Institute, considère que le paradoxe n'en est pas un et parle de l'« invalidité formelle du paradoxe de Fermi, qui ne peut pas être exprimé dans une forme syllogistique paraissant acceptable ». Si A équivaut à l'existence des extraterrestres, B à leur présence aux alentours de la Terre, et C qu'ils sont visibles, la formulation est : « Si A, alors probablement B, si B alors probablement C, or C n'est pas donc B et A ne sont pas non plus ». Cette formulation est sémantiquement et syntaxiquement invalide car « probablement » est un opérateur logique imparfait et non mesurable par le calcul. Selon Freitas, il faudrait reformuler ainsi : « Si A alors probablement B, si probablement B alors probablement C, or probablement C n'est pas, donc le probablement B n'est pas, en conséquence A n'est pas ». La formulation est alors sémantiquement valide si et seulement s'il est possible d'affirmer que « probablement C n'est pas » est vraie. Mais la valeur de « probablement C n'est pas » est indéterminée par l'expérience, donc cette seconde formulation est sémantiquement invalide. Il en découle que le paradoxe de Fermi n'a aucune valeur probante formelle. Selon Freitas, le paradoxe de Fermi n'en a que le nom ; il en caricature la logique en inventant le « paradoxe du lemming » : si la Terre était vide de toutes espèces sauf celle des lemmings, alors les lemmings devraient être partout. Cependant, la Terre est remplie d'autres espèces qui lui font concurrence et limitent leur développement. Si donc on n'observe pas de lemmings, c'est que la Terre abrite une abondance d'espèces qui luttent pour le contrôle des ressources.
L'histoire de l'humanité forme le modèle de toutes les solutions au paradoxe de Fermi. En effet, des hypothèses et scénarios envisagés font œuvre d'anthropomorphisme. Michael Michaud montre que les extraterrestres sont représentés comme des humanoïdes, possédant une anatomie et une physionomie semblable à celle de l'homme, ainsi qu'une intelligence proche. Selon lui, pourtant, la probabilité de rencontrer des extraterrestres à forme humanoïde est très faible. De même, les intentions qui leur sont attribuées (bienveillante ou malveillante) caractérisent un « effet miroir systématique » (systematic mirror image). Les principes de non-ingérence et d'isolationnisme retenus dans certains scénarios dévoilent un mécanisme par lequel l'humanité projette ses propres mythes. Les scénarios sont par conséquent conditionnés par des représentations humaines. Comme le montre le sociologue Pierre Lagrange, la genèse du programme SETI, dans les années 1960 (à la suite de l'article fondateur de Cocconi et Morrison), reproduit l'idée que « l'histoire des civilisations se place sur une échelle graduée allant des civilisations moins évoluées à la nôtre, comme si toute civilisation en marche aboutissait forcément à (ou passait par) la nôtre. C’est faire peu de cas de la notion de diversité culturelle et de celle d’innovation. »
D'autres hypothèses utilisent les données scientifiques concernant l'humanité comme axiomes. Michael D. Papagiannis pointe le fait que les sociétés humaines se répartissent en deux groupes : celles qui explorent et celles qui au contraire n'ont aucune ambition d'expansion spatiale. Selon lui, il pourrait en être de même en ce qui concerne les civilisations extraterrestres, l'hypothèse du second groupe étant peu envisagée parmi les solutions du paradoxe de Fermi. Michael Huang estime que pour chaque solution envisagée au paradoxe de Fermi, les auteurs imaginent simultanément l'évolution similaire de l'humanité. L'analogie avec l'homme (Human Analogy) est utilisée par des auteurs et elle permet, donc, de penser la logique du développement biologique et civilisationnel. À partir de cet argument, Peter Schenkel a montré que l'hypothèse d'une auto-extinction est peu plausible.
L'équation théorisée par l'astronome Frank Drake en 1961 est systématiquement, dans la littérature spécialisée, associée au paradoxe de Fermi. Stephen Webb considère que ce paradoxe, conjugué à l'équation de Drake, permet de conclure que la civilisation humaine est très probablement la seule dans la galaxie. Il note toutefois que cette conclusion ne dépend que des faibles valeurs affectées à certaines variables de l'équation seulement. Selon I. Bezsudnov et A. Snarskii, l'équation de Drake produit non des probabilités mais bien plutôt des « improbabilités » (« improbabilities »). Les auteurs considèrent que, si elle contient de nombreux facteurs, elle en oublie d'autres qui restent à définir plus finement. Le BS-model (voir infra) qu'ils proposent en 2010 est censé ajouter des facteurs ignorés initialement par Drake.
Beaucoup d'articles scientifiques ont précisé le calcul initié par Drake, et ce avant l'étude de Bezsudnov et Snarskii. Freeman Dyson, le premier (dans Interstellar Transport, 1968), a par exemple ajouté un facteur estimant le coût et le temps nécessaires pour un voyage spatial (qu'il évalue à 200 années pour franchir quatre années-lumière). David Brin précise le facteur évaluant le nombre de sites où la vie extraterrestre peut spontanément apparaître. Martyn J. Fogg considère qu'une civilisation apparue précocement dans l'histoire de l'univers aurait déjà accompli son expansion dans la galaxie avant l'émergence, sur Terre, de la vie dans les océans ; il précise donc le facteur fl. Richard K. Obousy et Gerald Cleaver (The Fermi Paradox, Galactic Mass Extinctions and the Drake Equation, 2007) ajoutent un facteur à l'équation de Drake : si l'on considère que la colonisation nécessite un développement de 106 années, alors la probabilité qu'un événement cosmique destructeur, comme un sursaut de rayon gamma (Gamma Ray Burst) ou une hypernova, est forte. Un tel événement survient en effet, statistiquement, toutes les 200 millions d'années. Selon les auteurs, toutes les civilisations sont condamnées à être anéanties par ces catastrophes cosmiques, avant qu'elles aient l'opportunité d'essaimer.
L'équation de Drake, croisée avec les hypothèses du paradoxe de Fermi, a fait l'objet de modifications statistiques afin d'en affiner la précision. Claudio Maccone a cherché à faire de l'équation de Drake un puissant outil statistique pour l'étude des solutions possibles au paradoxe de Fermi. Il note d'abord que l'équation oublie de nombreuses variables (comme la probabilité qu'une planète viable soit heurtée par un géocroiseur). Il suggère donc d'augmenter le nombre de variables afin d'affiner le calcul, opération qu'il nomme le Data Enrichment Principle. Recalculant l'équation, il aboutit à un nombre d'environ 4 590 civilisations potentielles dans la galaxie (contre 3 500 dénombrées par Drake). Cet outil lui permet également d'estimer la distance moyenne entre la Terre et une civilisation extraterrestre, qui est selon ses calculs entre 1 309 et 3 979 années-lumière.
Maccone croise ensuite son outil avec l'équation de Stephen H. Dole (1964), qui permet de calculer le nombre probable de planètes habitables par l'Homme dans la galaxie, estimée par ce dernier à environ 35 millions. Maccone aboutit plutôt à 300 millions de planètes habitables. Son outil lui permet enfin d'estimer le temps nécessaire à l'humanité pour coloniser la galaxie. S'appuyant sur le Coral Model of Galactic Colonization élaboré par Jeffrey O. Bennett et G. Seth Shostak en 2007, il estime qu'il faut deux millions d'années pour que l'humanité se répande dans la galaxie, à raison d'un voyage s'effectuant à 1 % de la vitesse de la lumière, et compte tenu d'un temps d'appropriation des planètes viables de 1 000 ans (les planètes habitables étant distancées les unes des autres d'environ 84 années-lumière). Étant donné la longévité de l'univers, Maccone conclut que le paradoxe de Fermi est résolu par son modèle statistique.
L'astrobiologiste Jacob D. Haqq-Misra et le géographe Seth D. Baum montrent que le paradoxe de Fermi est fondé sur l'observation de l'expansion humaine, or il existe des cultures non expansives (comme les Kung San du désert du Kalahari). Selon eux, la solution durable peut s'appliquer au paradoxe de Fermi, suivant l'idée que l'absence d'observations extraterrestres peut s'expliquer par la possibilité que la croissance exponentielle d'une civilisation n'est pas un modèle de développement soutenable (sustainable). L'exploration de l'espace est en effet liée à la croissance de la population, à son impact environnemental et à l'appauvrissement des ressources. La croissance non soutenable n'entraîne pas forcément la disparition d'une espèce. Jacob D. Haqq-Misra et Seth D. Baum rappellent que l'histoire de l'Île de Pâques illustre, en effet, que la destruction de l'environnement peut mettre un coup de frein au développement d'une culture, mais sans la faire disparaître (c'est le syndrome de l'île de Pâques). Ils reprennent le postulat de Sagan : étant donné la longévité de l'univers, il doit exister des civilisations extraterrestres ayant essaimé dans l'espace. Or, leur croissance insoutenable en a certainement limité l'expansion, ce qui explique qu'il n'y a aucune trace d'elles. Les mondes extraterrestres se limitent donc à des régions isolées qui demeurent dissimulées (ils nomment cette solution l'« hypothèse de la persistance », persistence hypothesis), de la même manière que sur Terre il existe encore des zones non explorées où des tribus peuvent vivre. Ils en concluent que les civilisations extraterrestres exponentiellement expansives ne peuvent exister.
La théorie de l'évolution a été utilisée pour préciser ou tester le paradoxe de Fermi et ses divers scénarios. Selon Adrian Kent, si une civilisation extraterrestre est capable de voyager sur des échelles interstellaires, et qu'elle a évolué dans de nombreux endroits, alors la sélection évolutive accélère son extinction. Kent pense de plus que de telles civilisations intelligentes refuseraient délibérément le contact, soit pour conserver la possibilité d'obtenir des ressources, soit par peur d'une guerre. Milan M. Ćirković rappelle qu'entre l'évolution de la vie sur Terre et celle apparues sur d'autres mondes dans la galaxie, il y a certainement un laps de temps important, de l'ordre de milliards d'années. S'appuyant sur le scénario de la nouvelle de science-fiction intitulée Permanence de Karl Schroeder, et considérant que la seule solution au paradoxe de Fermi est celle qui propose un juste milieu entre les effets catastrophistes et ceux gradualistes de l'évolution, menant à une sorte d'équilibre ponctué à l'échelle galactique, Ćirković suggère que l'adaptationnisme ne conduit pas forcément à l'intelligence évoluée,.
Selon Conway Morris (Life's Solution, 2004), l'évolution est convergente (hypothèse développementaliste) : aussi bien sur chaque planète que dans la galaxie entière. Au bout de plusieurs milliards d'années, des formes de vie intelligente devraient apparaître. L'évolution dans l'univers se comporte comme les gènes d'un organisme : l'évolution les pousse à s'adapter toujours davantage à l'environnement. Morris refuse cependant les théories de Darwin et de Gould et son hypothèse veut montrer qu'il existe un plan cosmique aux résonances religieuses.
Les astronomes russes I. Bezsudnov et A. Snarskii proposent de résoudre le paradoxe de Fermi en utilisant la technologie des automates cellulaires. Leur simulation, nommée Bonus Stimulated model (BS-model), conjugue la probabilité qu'une civilisation apparaisse, son temps de vie spécifique et la longévité qu'elle peut espérer obtenir en entrant en contact avec d'autres civilisations (le « bonus temporel »). Le contact avec des civilisations en développement accroît en effet la durée de vie de chacune d’un bonus temporel noté « Tb ». L’effet est cumulatif et proportionnel aux nombres de civilisations en contact les unes avec les autres, et ce à chaque itération (à chaque contact, la civilisation change sa taille avec une nouvelle couche de cellules à chacun de ses côtés). Plusieurs scénarios apparaissent selon les valeurs attribuées à chaque facteur : un état de division (split), un état civilisé (civilized) et un état dit de transition. Le modèle démontre que plus l'univers a un temps d'existence long, plus la probabilité de voir des civilisations s'agréger est forte. Pour répondre au paradoxe de Fermi, les deux auteurs expliquent qu'il est nécessaire d'attendre encore, l'univers ayant atteint son état d'équilibre (état de transition) et que, par conséquent, l'espoir de rencontrer une ou plusieurs autres civilisations est de plus en plus certain — le facteur décisif étant les distances entre les civilisations. Enfin, la durée de vie des mondes extraterrestres augmentée grâce au bonus temporel devient si importante qu'on peut envisager la possibilité d'une civilisation couvrant pratiquement tout l’univers.
Geoffrey Landis, dans un article de 1998, a recours à la théorie de la percolation pour expliquer la colonisation possible de l'espace, et apporter une solution possible au paradoxe de Fermi. Dans son modèle, la règle de percolation énonce qu'une culture peut avoir un facteur de colonisation ou pas. Une civilisation qui possède un facteur de colonisation va établir des colonies sur toutes les étoiles à portée de main. Si elle n'a pas d'étoiles dans son rayon d'émancipation, cette civilisation ne peut se disperser dans l'espace et subit alors un effondrement ou une rétrogradation. Ainsi, toute colonie donnée a une probabilité « P » de développer par la suite une civilisation colonisatrice, et une probabilité « 1–P » de développer une civilisation non colonisatrice. Pour Landis, si le modèle est pertinent, il reste cependant à préciser quantité de variables.
Selon Milan M. Ćirković, plusieurs découvertes scientifiques et avancées dans la connaissance de l'univers, ont permis de préciser certains points du paradoxe de Fermi. La détection d'exoplanètes, depuis 1995, a conduit à repenser la notion d'habitabilité et a confirmé que la formation de systèmes stellaires est un phénomène courant, voire banal. La connaissance de la composition chimique et de la dynamique de la galaxie, et en particulier de sa zone d'habitabilité, laissent à penser qu'il existe de nombreuses planètes viables plus anciennes que la Terre. La confirmation que la vie est apparue rapidement sur la Terre, la découverte d'espèces extrémophiles et l'amélioration du processus de la biogénèse, tendent à montrer que le règne du vivant est plus diversifié et plus présent dans la galaxie que prévu. L'évolution technologique (la loi de Moore surtout) humaine, constante, laisse entendre que toute civilisation suit cette démarche. Milan M. Ćirković considère cependant que ces découvertes et avancées ont compliqué le paradoxe de Fermi, qui se présente dorénavant comme un « puzzle ».
Carl Sagan dresse un panorama des auteurs ayant pensé la possibilité d'une vie extraterrestre dans Cosmic Connection (1975) : Lucien de Samosate, Cyrano de Bergerac, Fontenelle, Swedenborg, Kant ou encore l'astronome Kepler ont imaginé que les planètes étaient habitées (c'est la question de la « pluralité des mondes »). Au Ier siècle av. J.-C., Lucrèce, dans De natura rerum (v. -75/-76), mentionne la possible existence d’extraterrestres : 
« Si la même force, la même nature subsistent pour pouvoir rassembler en tous lieux ces éléments dans le même ordre qu’ils ont été rassemblés sur notre monde, il te faut avouer qu’il y a dans d’autres régions de l’espace d’autres terres que la nôtre, et des races d’hommes différentes, et d’autres espèces primitives ». En 1584, dans Le Banquet des cendres, Giordano Bruno fait également mention de la possibilité d’habitants d’autres mondes : « […] ces mondes sont autant d’animaux dotés d’intelligence ; qu’ils abritent une foule innombrable d’individus simples et composés, dotés d’une vie végétative ou d’entendement, tout comme ceux que nous voyons vivre et se développer sur le dos de notre propre monde. » Par la suite, les observations de Mars et de ses canaux, par Giovanni Schiaparelli puis Percival Lowell, ont marqué le début d'un enthousiasme populaire pour la vie extraterrestre. Or, après la fin de la Seconde Guerre mondiale, les moyens techniques étant performants, la recherche d'intelligences extraterrestres devient un problème scientifique qui donne naissance, dans les années 1970, à l'exobiologie. Sagan explique que la vie extraterrestre est alors « une notion dont le temps est venu ».
Selon Stephen Webb, le paradoxe de Fermi a été découvert quatre fois : par Tsiolkovski, Fermi, Viewing et Hart. Mais d'autres auteurs ont également posé la question : « Où sont-ils ? » Charles Fort évoque dès 1919 dans le Livre des damnés « un grand mystère » : « pourquoi ne sont-ils pas ici ? » Selon lui, l'humanité serait leur propriété. La Terre aurait été auparavant une planète sans hommes, théâtre de conflits entre civilisations extraterrestres. Un accord aurait fait de la Terre une zone neutre, actuelle possession d'une puissance galactique. Isaac Asimov, à la suite de l'article de Sagan, le mentionne dans son essai Our Lonely Planet de novembre 1958 publié dans Astounding.
Pour Michael Michaud, le Russe Constantin Tsiolkovski, père et théoricien de l'astronautique moderne, s'est posé la question de la présence des extraterrestres, et l'absence de preuves de leur existence, avant Fermi. Il suggère en 1934 dans son essai « Il y a également des planètes autour d'autres étoiles » que des civilisations extraterrestres certainement plus sages et plus anciennes que la nôtre existent certainement, mais qu'elles refusent d'interférer avec notre histoire pour ne pas nous pousser à la destruction. Une rencontre pourrait alors avoir lieu lorsque l'humanité serait plus avancée technologiquement et spirituellement. L'astronome John A. Ball dans The Zoo Hypothesis (1973), reprenant la thèse de Tsiolkovski, suggère que la Terre puisse être une sorte de réserve naturelle protégée par des puissances extraterrestres qui se refusent à y pénétrer.
Fermi a ensuite formulé son paradoxe, même si sa déclaration n'a jamais été écrite. En 1975, l'ingénieur anglais David Viewing a explicitement formulé le dilemme : « Ceci est, donc, le paradoxe : toute notre logique, tout notre anti-isocentrisme, nous assure que nous ne sommes pas uniques — qu'ils doivent être là. Et pourtant, nous ne les voyons pas ». Il est le premier à se référer au paradoxe de Fermi, selon Webb. La même année, Michael Hart publie un article dans lequel il étudie quatre catégories de solutions possibles au paradoxe de Fermi (voir supra) ; il en conclut que l'humanité est la seule civilisation intelligente de la galaxie.
Plusieurs hypothèses de résolution du paradoxe de Fermi existent. Comme le rappelle Geoffrey Landis, les solutions vont de la plus pessimiste (paraphrasable par la réponse : « il n'existe pas de civilisations extraterrestres ») à la plus optimiste, dont celle formulée par Carl Sagan dès 1962 et selon laquelle les extraterrestres sont déjà dans le Système solaire. Ces hypothèses favorables à la vie extraterrestre fournissent elles-mêmes quantité d'explications, qui vont de la disparition pour raison technologique (technological collapse) ou épuisement des ressources, au choix de ne pas coloniser l'espace ou au contraire de s'y répandre, et enfin de ne pas entrer en contact avec l'humanité. Le facteur principal demeure le temps : le paradoxe de Fermi a en effet plus de chances d'être résolu si l'espérance de vie d'une civilisation moyenne est longue. Selon la formule de Freeman Dyson, « les distances interstellaires ne sont pas une barrière à des espèces qui disposent de millions d'années d'évolution ».
Selon Seth D. Baum, Jacob D. Haqq-Misra et Shawn D. Domagal-Goldman, on peut aussi classer les solutions possibles du paradoxe en préjugeant du comportement que les civilisations extraterrestres peuvent adopter envers l'humanité. Il y aurait donc trois comportements possibles : pacifique, neutre ou belliqueux. David Brin recense 24 solutions dans son article de 1985. En 1986, il affine son classement en s'appuyant sur chaque facteur de l'équation de Drake pour répertorier les solutions possibles alors que Milan M. Ćirković distingue trois réponses : les hypothèses catastrophistes, les hypothèses de la Terre rare et les hypothèses solipsistes. Stephen Webb dresse la liste de 50 solutions possibles dans son ouvrage Where Is Everybody ? (2002), classées en trois catégories pratiques : 
« Ils n'existent pas » (They Do Not Exist).Il note que la première catégorie est la plus débattue, et la plus populaire aussi.
Stephen Webb recense 19 solutions à cette réponse possible au paradoxe de Fermi. Il avance que les recherches de signaux extraterrestres des années 1960 à 2010, aussi bien celles pointant l'espace profond que celles écoutant le Système solaire, n'ont rien détecté et ce fait seul prouve que les autres mondes n'existent pas. Cette classe de solutions, composée de cinq groupes d'hypothèses, n'imagine pas que des cultures extraterrestres aient pu exister puis disparaître mais elle postule que la vie intelligente est apparue seulement sur Terre (c'est l'« hypothèse de la Terre rare »).
La civilisation sur Terre est peut-être le résultat d'une conjonction de phénomènes uniques ou très rares à l'échelle de la galaxie. Les systèmes planétaires seraient par exemple rares car rien ne prouve (parmi les observations) que les disques protoplanétaires sont répandus et qu'ils donnent naissance à des planètes habitables. L'humanité est peut-être la première civilisation apparue dans l'histoire de l'univers car son système planétaire est le premier à avoir forgé les éléments essentiels à la vie. Si les planètes telluriques sont rares, il est donc possible de résoudre le paradoxe de Fermi. Des questions scientifiques toujours en suspens comme la formation des chondres des astéroïdes posent un problème aux théories actuelles. Selon les astronomes Brian McBreen et Lorraine Hanlon, les rayons gamma participent à l'existence des chondres, or ces radiations ne concerneraient qu'une seule étoile pour 1 000. D'autre part, la Terre est peut-être la seule planète à avoir un tel taux de métal dans son sol. Une civilisation n'y ayant pas accès ne pourrait développer la technologie nécessaire pour communiquer et essaimer dans la galaxie.
Il est également possible que les zones d'habitabilité continuelles (continuously habitable zone, CHZ) au sein d'un système planétaire soient rares et/ou de courte durée de vie. John Hart a établi un modèle montrant que la CHZ du Système solaire se situe entre 0,958 unité astronomique et 1,004 unité astronomique. Celle-ci permettrait, en moyenne, de favoriser l'émergence de la vie sur un milliard d'années selon lui. D'autres scientifiques, comme James Kastings, ont revu ces chiffres ; ce dernier établit que la durée de vie de la CHZ de notre Système solaire peut être évaluée à 4−6 milliards d'années, et que sa distance est de 0,95 à 1,15 unité astronomique,. Par ailleurs, la zone habitable galactique (galactic habitable zone, GHZ), qui ne contient que 20 % des étoiles de la Voie lactée (galaxie comprenant de 200 à 400 milliards d'étoiles,,), est une autre condition essentielle limitant l'émergence de la vie. Enfin, la Lune explique peut-être le caractère unique de la Terre. En stabilisant son obliquité (angle d'inclinaison de l'axe de rotation par rapport au Soleil, de 23,5° actuellement), elle a favorisé la vie « évoluée » sur la planète bleue.
Depuis la découverte d'exoplanètes en 1995, il apparaît que des astres massifs comme Jupiter jouent un rôle important dans la formation de planètes plus petites susceptibles d'abriter la vie. Or, il est possible que les géantes gazeuses comme Jupiter soient rares dans la galaxie, paramètre qui réduirait, voire annulerait, toute chance que la vie intelligente soit apparue ailleurs que sur Terre. Le rôle de la géante gazeuse serait double : d'une part, elle absorberait les géocroiseurs néfastes pour la Terre, servant ainsi de bouclier gravitationnel et d'autre part, elle aurait permis de stimuler la vie microbienne aux débuts de la planète bleue. Selon le physicien John G. Cramer, la présence de Jupiter au contact de la ceinture d'astéroïdes entraîne une mise en résonance de certains objets la composant, qui ont ensuite une haute probabilité d'atteindre la Terre. Si les géocroiseurs peuvent provoquer des extinctions (comme celle qui a sans doute anéanti les dinosaures), ils ont aussi un rôle stimulant dans l'évolution des espèces. Selon Cramer, ce mécanisme entraîné par Jupiter s'apparente à une « pompe de l'évolution » (pump of evolution). Sa fréquence (20 à 30 millions d'années en moyenne) expliquerait les grandes extinctions, desquelles la biosphère, à chaque fois, s'est renouvelée.
Toutefois, le paradoxe de Fermi peut être résolu par le fait que la galaxie est un endroit dangereux : les objets néfastes pour la vie y sont nombreux et leurs effets sont importants. Les trous noirs, étoiles à neutrons ou encore blazars peuvent expliquer que des civilisations extraterrestres n'ont pu apparaître ailleurs dans la Voie lactée. Les supernovas sont une solution souvent citée dans la littérature scientifique, dans la mesure où une explosion d'étoile de type I ou II dans un rayon de 30 années-lumière détruit toute vie sur la surface d'une planète habitable. Selon John Cramer, leur rôle n'est pas que néfaste : les supernovas peuvent aussi jouer le rôle de « pompe de l'évolution ». Tous les modèles actuels montrent cependant que la fréquence moyenne des supernovas (toutes les 100 millions d'années, dans un rayon de 30 années-lumière) est une explication satisfaisante au « Grand silence ». Pour James Annis, les sursauts gamma étaient plus nombreux dans le passé de l'Univers ; ils auraient pu anéantir des civilisations alors en plein développement.
Selon le biologiste de l'évolution Ernst Mayr, la vie doit suivre une dizaine d'étapes avant d'apparaître et de coloniser l'environnement. Mayr conclut que le nombre de facteurs (au nombre de huit), mais aussi le temps moyen nécessaire à l'apparition de la vie, est trop élevé pour penser que l'intelligence est un phénomène galactique répandu. Il est possible que la vie intelligente n'ait émergé que récemment, en particulier parce que son apparition est liée à la séquence principale de son étoile. Selon Mario Livio, l'étoile joue un rôle d'importance dans son émergence ; elle conditionne en effet le taux d'oxygène par la photodissociation de la vapeur d'eau ainsi que les niveaux d'oxygène et d'ozone dans l'atmosphère. Livio remarque que le temps nécessaire au développement de la couche d'ozone (qui permet à la vie de foisonner en la protégeant des rayons ultraviolets) est le même que celui nécessaire à l'apparition de la vie. Ce temps incompressible, ainsi que celui de la production cosmique de carbone, expliquent qu'il ne peut exister de civilisations plus anciennes que la nôtre. L'hypothèse de la « Terre boule de neige » (snowball Earth), qui soutient que la quasi-totalité de la surface de la Terre était recouverte de glace pendant la glaciation Varanger, développée par le géologue Paul F. Hoffman, laisse à penser que la vie a eu besoin de ces conditions extrêmes. En effet, l'explosion cambrienne et l'apparition des cellules eucaryotes suivent cette période. Il semble que la tectonique des plaques soit également un facteur facilitateur. En plus d'engendrer le champ magnétique terrestre, la tectonique promeut la biodiversité. Une planète n'ayant pas d'activité tectonique ne pourrait donc donner naissance à la vie.
Plusieurs autres hypothèses biologiques laissent à penser que la vie est un phénomène rare. Certaines hypothèses affirment que la genèse de l'ADN ne peut être un résultat du hasard. Cependant, la définition du vivant pose problème. La vie est un processus qui met en scène des cellules, qui possèdent un métabolisme, capables de reproduction et enfin qui évoluent, mais d'autres caractéristiques peuvent exister dans l'univers. Tant que la vie n'est pas mieux définie, et tant que « LUCA » (le dernier ancêtre commun universel) n'a pas été identifié, il n'est pas possible de déterminer si elle est un phénomène rare. Plus précisément, « s'il est prouvé que l'étape qui permet le passage de la chimie inorganique à l'ADN est un phénomène rare, alors nous résolvons le paradoxe de Fermi ». Le passage entre les procaryotes et les eucaryotes représente la question la plus centrale du problème. Il manque surtout un tertium comparationis, une autre forme de vie n'appartenant pas à la biosphère terrestre. C'est pourquoi l'exploration du Système solaire à la recherche de traces biologiques est si importante : découvrir d'autres formes de vie, même microbiennes, permettrait de clore le débat.
Le physicien Brandon Carter pense que l'humanité pourrait être unique dans l'univers, en s'articulant sur le « principe anthropique faible », idée qu'on peut résumer ainsi : les lois de la physique que nous observons sont celles qui permettaient l'existence d'observateurs (donc des lois d'assemblage de la matière, des durées d'existence compatibles avec l'arrivée de la vie, etc.). En revanche le principe de parcimonie ne nous autorise pas à croire que nous vivrions dans un univers suffisamment vaste pour avoir accueilli le phénomène deux fois - même s'il en existe - car nous aurions alors considérablement plus de chances d'être apparus dans un univers plus petit et en conséquence moins exigeant - donc plus probable - énergétiquement (le philosophe William Lane Craig mentionne que le principe anthropique fort, lui, est juste un argument téléologique ne prouvant rien, mais il n'est pas concerné ici).
Depuis Carter, plusieurs variantes de sa théorie ont été formulées, mais relèvent souvent d'une confusion entre les deux principes.
Le principe anthropique dit faible (Weak anthropic principle, WAP) exprime que la position de la Terre dans l'univers est nécessairement privilégiée au sens où elle doit être compatible avec l'existence d'une forme de vie évoluée, puisque nous sommes là pour l'observer. En d'autres mots, si l'univers avait été plus petit, la vie n'aurait pas eu la possibilité d'y apparaître. C'est une sorte de lapalissade, de tautologie.
Le principe anthropique dit fort (Strong anthropic principle, SAP) exprime que l'univers doit avoir reçu des lois et des paramètres fondamentaux tels que des êtres évolués puissent y apparaître à un certain moment, peut également être une explication au paradoxe de Fermi. À connotation religieuse, il postule une volonté ou une nécessité à l'origine de l'évolution de l'univers. Il suggère donc un dessein cosmique à l'origine de l'apparition de l'humanité. Enfin, il existe le Final anthropic principle (FAP) du cosmologiste John Barrow et de Frank J. Tipler qui postule que la vie intelligente doit advenir et, une fois parvenue à l'existence, elle survit pour toujours, agrégeant toujours plus de connaissances, jusqu'à remodeler l'univers lui-même,. Pour le mathématicien Martin Gardner, le FAP est une aberration logique, si bien qu'il l'a renommé ironiquement le Completely ridiculous anthropic principle (CRAP).
Si la vie peut exister, rien ne permet de dire qu'elle puisse atteindre un stade de développement suffisant pour être qualifiée d'intelligente ; c'est le scénario de la « rare mind hypothesis » (« hypothèse de l'intelligence rare »). Le seul exemple connu est l'Homo sapiens. De fait, les espèces utilisant des outils sont peut-être rares dans l'univers. L'outil dépend de l'environnement, or une planète ne possédant pas de métal (ou très peu) ne permettrait pas à des êtres de développer des techniques et des pratiques. L'évolution technologique n'est peut-être pas inévitable : rien ne prouve que le progrès scientifique est une loi sociétale. La galaxie pourrait abriter des civilisations ayant arrêté leurs développements au stade de la taille de la pierre ou du métal. Il est possible également que l'intelligence du niveau de celle de l'espèce humaine soit rare. Tout dépend de la définition de l'intelligence et de son évolution. Enfin, le langage peut aussi être une acquisition très rare, voire unique à l'humanité. Les recherches de Noam Chomsky montrent que la faculté linguistique est acquise et génétique, fruit d'une longue évolution dépendant elle-même de conditions environnementales que toutes les planètes ne pourraient avoir.
Une étude de 2018 suggère que la probabilité qu'il n'y ait pas d'autre vie intelligente dans l'univers observable est substantielle et qu'il n'est donc pas surprenant que nous n'en détections aucun signe,.
Stephen Webb recense 22 solutions à cette affirmation, qui peuvent être regroupées en quatre groupes. Comme le souligne ce dernier, cette classe de solutions repose sur le « principe de médiocrité » (« Principle of Mediocrity ») qui suppose que la Terre est une planète commune dans la galaxie, et qu'elle n'est donc pas unique. La faiblesse de ces solutions, en particulier celles à tendance sociologique, réside cependant dans le fait qu'elles présupposent que toutes les civilisations extraterrestres adoptent le même comportement. Ces scénarios appartiennent aux solutions néo-catastrophistes.
Les moyens de quitter l'orbite terrestre, bien que nombreux dans la réalité (fusée chimique, statoréacteur, voile solaire, moteur ionique) ou hypothétiques (système antigravité, moteur à tachyons ou transport dans l'hyperespace), ne pourraient pas permettre d'explorer d'autres étoiles selon Stephen Webb. Les arches spatiales (idée d'abord présentée par John Bernal en 1929 avec le vaisseau générationnel), les cylindres O'Neill et les habitats spatiaux sont limités par les grandes distances entre les planètes viables. Si les voyages interstellaires sont en pratique impossibles, alors, selon Stephen Webb, le paradoxe de Fermi est résolu. Deux solutions techniques se distinguent si l'on considère  les possibilités hypothétiques et spéculatives : le déplacement à une vitesse supérieure à celle de la lumière (Faster-than-light, FTL, métrique d'Alcubierre) et le déplacement via des singularités spatiales (tube de Krasnikov, bulle temporelle de Chris Van Den Broeck ou encore extraction de l'énergie du vide).
Il est possible également que les civilisations extraterrestres n'aient pas encore disposé du temps nécessaire pour atteindre la Terre — cette solution est nommée « explication temporelle du paradoxe de Fermi » (temporal explanation of the Fermi paradox) par John Hart. Il existe sur ce point plusieurs modèles de colonisation de la galaxie, de Sagan et Newmann à John Hart et Eric M. Jones, chacun se basant sur des variables et des facteurs spécifiques. Un modèle récent, celui de Geoffrey Landis publié en 2002, se fonde sur la théorie de la percolation. Il repose sur trois variables : la distance maximale pour établir une colonie (Landis considérant que seuls les voyages aux long cours sont possibles), le fait que chaque colonie, après un certain temps, développe sa propre culture (et donc sa propre vague de colonisation), et le fait enfin qu'une colonie ne puisse être établie sur une planète déjà colonisée. Le modèle de la percolation permet à Landis de calculer pour chaque scénario les probabilités des chemins parcourus dans la galaxie. Les civilisations se distribuent alors selon des aires d'occupation qui laissent aussi apparaître des zones de vide. Selon lui, la Terre n'a pas rencontré de civilisations extraterrestres car elle est située dans l'un de ces vides. Le modèle de Landis a cependant des faiblesses et, notamment, il n'explique pas pourquoi la Terre n'a aucune preuve radio d'autres civilisations.
Une possibilité, souvent citée dans la littérature spécialisée ou de science-fiction, est celle imaginée par l'ingénieur australien Ronald Bracewell, à partir des automates autoreproductibles du physicien John von Neumann. Il s'agit de sondes qui parcourent la galaxie et qui sont capables de s'autodupliquer, accélérant ainsi de manière exponentielle la colonisation et ce, rapidement et à un coût faible. Pour Bracewell, les sondes seraient beaucoup plus efficaces qu'un signal radio. L'astrophysicien Frank Tipler a perfectionné cette solution, dite de « Bracewell-von Neumann », et a ainsi réduit le temps de colonisation de la galaxie, l'estimant à 4 millions d'années. Il la considère comme la solution la plus plausible, à tel point qu'il y voit le seul moyen de coloniser la galaxie (selon lui, le contact entre civilisations est impossible). Il existe aussi un scénario nommé « Deadly probes scenario » (« scénario des sondes meurtrières »), ou « Berserkers » (« folles furieuses »), qui postule que ces sondes ont détruit les civilisations qu'elles ont atteintes.
Les scénarios liés aux problèmes de communication sont nombreux. La stratégie de recherche de signaux est peut-être inadaptée. Deux sortes de recherches existent actuellement : soit en ciblant une étoile (comme le projet Phoenix), soit en écoutant largement le ciel (les programmes SETI, SERENDIP et BETA). Une étude de Nathan Cohen et Robert Hohfeld montre que la meilleure des solutions est d'écouter le plus d'étoiles possibles. Cependant, le problème réside aussi dans la recherche, parmi les signaux recueillis, de messages intelligents. Le projet SETI@home de David Gedye représente la tentative la plus aboutie. Il est également possible que le signal intelligent soit déjà dans les bases de données. Le projet META a, depuis 1985, pour but de détecter des indices de messages intelligents, parmi 60 trillions de signaux. Les astronomes Benjamin Michael Zuckerman et Patrick Edward Palmer, dont le programme écoute près de 700 étoiles proches, ont détecté dix signaux qui pourraient être artificiels. Il est par ailleurs possible que les moyens de détection humains n'aient pas assez écouté le ciel, ou que chaque civilisation écoute mais que personne ne transmette (le projet spatial Darwin de l'ESA devait mettre un terme au problème en observant optiquement les mondes lointains, mais a été annulé en 2007). Pour C. Rose et G. Wright, l'envoi de messages inscrits sur certains matériaux (comme la plaque de Pioneer) est une solution plus efficace pour communiquer que par ondes électromagnétiques alors que pour Freeman Dyson, il faut concentrer les observations sur les sources infrarouges. Pour Sebastian von Hoerner (en 1961) les civilisations extraterrestres peuvent exister mais les transmissions interplanétaires ne permettent pas un échange sur des périodes raisonnables. Chacune est peut-être en train de parler et d'écouter mais la communication est irréalisable étant donné les délais d'échange.
Il est aussi possible que les civilisations extraterrestres aient développé des mathématiques différentes, leurs environnements leur ayant permis d'inventer des concepts autres. Le message peut de fait être codé dans un langage mathématique hors de notre compréhension. Le mathématicien Hans Freudenthal a tenté, en inventant le langage Lincos, de communiquer avec les extraterrestres. Les radiations des corps noirs peuvent être des tentatives de communication.
Plusieurs solutions évoquées par Stephen Webb concernent le refus ou l'impossibilité technique de communiquer avec d'autres civilisations. Un monde extraterrestre très avancé technologiquement pourrait ainsi refuser de quitter son système planétaire. Dyson pense qu'une civilisation de type II n'aurait aucun besoin de quitter son étoile d'origine. Grâce à une sphère enserrant son soleil, elle pourrait en capter toute l'énergie nécessaire, sans avoir à explorer la galaxie. Les extraterrestres pourraient tout aussi bien rester chez eux pour des raisons philosophiques, ou parce qu'ils auraient développé un puissant environnement virtuel dans lequel ils vivraient totalement. Ils peuvent en effet n'avoir aucun désir de communiquer, soit parce que la prudence et l'isolationnisme sont les traits des civilisations avancées (il existe des exemples sur Terre : le Sakoku japonais et le splendide isolement britannique), soit parce que notre intelligence ne permet pas de comprendre le signal. Les extraterrestres auraient par exemple résolu la question du besoin et auraient ainsi éliminé toute recherche intellectuelle. Les raisons sont nombreuses mais, selon Stephen Webb, aucune ne résout le paradoxe de Fermi. Des mondes extraterrestres couverts de nuages, ou baignant dans une lumière totale du fait de la présence de plusieurs soleils et ne connaissant donc pas la nuit (comme dans la nouvelle d'Isaac Asimov, Quand les ténèbres viendront, de 1941), seraient également incapables de communiquer au moyen de transmissions interstellaires. Enfin, autre hypothèse : la Terre appartient à l'horizon des particules (limite cosmologique au-delà de laquelle la lumière ne nous est pas encore parvenue) et les autres mondes demeurent inobservables.
Des difficultés techniques insurmontables pourraient également expliquer le paradoxe. Les explications sur ce point sont nombreuses. Les extraterrestres ont peut-être déjà envoyé un signal interstellaire, mais celui-ci peut prendre plusieurs formes (signal électromagnétique, gravitationnel, de particules ou de tachyons), dont certaines sont encore inconnues de l'Homme. Le problème de la fréquence d'émission est également crucial : un signal peut être émis depuis un autre monde mais l'Homme ne sait pas où le chercher. Philipp Morrison et Giuseppe Cocconi ont étudié la question à la fin des années 1950. Ils ont comparé toutes les possibilités, parmi le spectre électromagnétique, des ondes radio aux rayons gamma. Les deux concluent sur le fait que la communication interstellaire au moyen de rayons gamma est celle qui aurait le plus de chance d'être utilisée par une civilisation extraterrestre. John Ball pense également que les bouffées gamma cosmiques sont des tentatives de communiquer. Ils découvrent aussi que la bande d'émission la plus appropriée se situe entre 1 GHz et 10 GHz, voire plus précisément entre 1,42 GHz et 1,64 GHz (région nommée le « trou d'eau », waterhole en anglais), ce qui correspond au spectre de l'hydrogène. Frank Drake a tenté d'écouter cette bande d'émission ; c'est le projet Ozma. Selon Webb, rien ne prouve que les signaux hors du commun captés par les radiotélescopes (comme le signal Wow!) soient d'authentiques messages extraterrestres. Les émissions de pulsations laser seraient peut-être une solution de communication selon Stuart Kingsley (projet COSETI).
Le fait qu'aucun signal ou aucune trace extraterrestre n'aient été détectés prouverait peut-être que les civilisations ont tendance à disparaître avant d'atteindre leur maturité suffisante. Beaucoup d'hypothèses en font les victimes d'une guerre puis d'un hiver nucléaire, ou d'une guerre bactériologique/chimique. Les mondes extraterrestres ont pu aussi disparaître du fait de la surpopulation et des désastres écologiques qui l'ont suivie. À la suite de l'invention des nanotechnologies, Eric Drexler, dans son livre Engins de création, évoque l'existence possible du « grey goo » (« gelée grise ») : des nanorobots programmées pour s'autoreproduire échapperaient rapidement au contrôle de leur créateur pour, en quelques jours, recouvrir la planète entière (hypothèse de l'écophagie globale). Le physicien Robert Freitas a estimé que c'est l'un des facteurs probables d'extinction de civilisations extraterrestres. La destruction peut aussi résulter d'expériences scientifiques, comme celles portant sur l'accélération de particules. Nombre de solutions explorent également la longévité des civilisations extraterrestres. Elles expliquent le paradoxe de Fermi par le fait qu'il existe un temps de vie à chaque civilisation et que ce dernier ne leur permet pas d'essaimer dans la galaxie ou de communiquer avec d'autres mondes.
Les civilisations extraterrestres, du moins celles intelligentes, auraient tendance à n'exister que de manière éphémère, ou alors à s'autodétruire. Selon Lipunov, chaque civilisation a un temps de vie limité par des facteurs qui lui sont spécifiques. Une solution au paradoxe est celle proposée par l'astrophysicien John Richard Gott dès 1969, et nommée l'« argument de l'apocalypse » (Doomsday argument). Selon Gott, chaque civilisation, en fonction de ses caractéristiques, possède une probabilité de vie et de mort, évaluable au moyen d'un outil statistique, le « delta t ». Utilisant le principe copernicien, il estime qu'il existe 95 % de chance que l'espèce humaine perdure entre 5,1 et 7,8 millions d'années. Combiné à l'équation de Drake, le modèle de Gott établit qu'il existe moins de 121 civilisations dans la galaxie capables de radiotransmettre. Selon I. Bezsudnov et A. Snarskii, le contact, puis le rassemblement au sein d'un conglomérat des civilisations galactiques (Galactic Club), est le seul facteur qui permettrait à une civilisation de prolonger sa propre existence, et ce en raison de la stimulation intellectuelle qu'un tel échange occasionne. Un monde n'ayant pas rencontré d'autres civilisations aurait ainsi tendance à disparaître.
Selon l'écrivain de science-fiction et mathématicien Vernor Vinge, l'évolution technologique va entraîner, dans l'histoire humaine, un changement radical de civilisation, qu'il nomme la « singularité ». Reprenant l'idée de Vinge, Stanislaw Ulam et I. J. Good postulent que les extraterrestres ne communiqueraient donc pas car ils auraient atteint un niveau d'existence transcendant. Selon Stephen Webb toutefois, l'hypothèse de Vinge ne résout pas le paradoxe de Fermi car elle échoue à expliquer pourquoi aucun signal extraterrestre n'a été capté. De plus, la singularité exacerbe encore davantage le paradoxe, puisqu'elle postule que des civilisations peuvent parvenir à un haut niveau technologique.
Stephen Webb recense huit solutions à cette affirmation (dont une humoristique). Cette catégorie de solutions s'apparente à des solipsismes.
Depuis 1947 et le témoignage de Kenneth Arnold, le phénomène OVNI représente la solution la plus populaire à l'hypothèse que les civilisations extraterrestres existent et nous visitent, si bien que si les soucoupes volantes sont considérées comme réelles, alors le paradoxe de Fermi est immédiatement résolu. Selon Webb, cette solution est cependant incomplète car rien ne permet d'affirmer que les OVNI sont des machines extraterrestres. Pour Pierre Lagrange, il existe un sous-paradoxe à celui de Fermi, conséquence directe du premier : le « paradoxe des OVNI », qui pose que les OVNIs n’ont rien à voir avec des extraterrestres, pourtant, « si nous étions confrontés à des extraterrestres, tout se passerait comme dans la controverse sur les ovnis »,.
Le phénomène OVNI est peu cité dans les solutions scientifiques proposées. Robert Freitas l'exclut du paradoxe de Fermi. J. Deardorff, B. Haisch, B. Maccabee et H. E. Puthoff considèrent que quelques cas d'observations d'OVNI délivrent des indices qui laissent à penser que des entités extraterrestres visitent la Terre. Beatriz Gato-Rivera étudie quant à elle le scénario où la Terre est dans la zone d'influence d'une hypercivilisation galactique.
Webb recense une autre classe de solutions qui tendent à montrer que les extraterrestres sont présents près de la Terre, et qu'ils ont laissé des preuves de leur passage. L'explosion mystérieuse à Toungouska en 1908, les structures qui laissent penser à des ouvrages technologiques sur la Lune, ou les croyances quant au fait que sa face cachée ait pu abriter des bases extraterrestres appartiennent à cette classe, de même que la théorie des anciens astronautes popularisée par Erich von Däniken dans les années 1970. Selon ce dernier, les extraterrestres ont visité la Terre dans le passé de l'humanité, laissant des traces, comme les pyramides mayas ou encore les tracés de Nazca. Un autre scénario spécule sur le fait que des sondes extraterrestres seraient présentes en orbite, sur l'un des cinq points de Lagrange, à sa périphérie (« Artifact Hypothesis »). Les échos radio locaux (LDE) seraient ainsi émis par ces sondes, qui surveilleraient le développement de l'humanité ; c'est l'hypothèse de la sentinelle (« sentinel hypothesis »),. Or, ces rapprochements sont peu rigoureux. Carl Sagan et J.-S. Shklovsky, en 1966, examinant une tradition légendaire sumérienne qui raconte comment des êtres supérieurs auraient enseigné les grandes disciplines du savoir aux hommes, ont montré qu'il est impossible de démontrer la réalité d'un contact extraterrestre à partir de tels récits. En l'absence de preuve évidente (par exemple un artéfact extraterrestre ou une technologie avancée dessinée), établir des traces de passage extraterrestre dans l'histoire (astroarchéologie) est une entreprise dénuée de rigueur.
La planète Mars a longtemps cristallisé les hypothèses quant à l'existence de civilisations extraterrestres proches de nous. La croyance en la présence de canaux sur sa surface, depuis les observations astronomiques de Giovanni Schiaparelli en 1877, jusqu'au comportement énigmatique de son satellite Phobos, a fait de Mars le lieu privilégié des projections humaines quant à l'existence des extraterrestres. J.-S. Shklovsky a par exemple suggéré que Phobos était artificiel, alors que pour Salisbury il a été mis en orbite entre 1862 et 1877 (l'astronome Heinrich Arrest ne l'a en effet pas remarqué en 1862). D'autres lieux ont cependant cristallisé l'imagination humaine : les astéroïdes de la ceinture au-delà de Mars pourraient abriter des colonies extraterrestres selon Michael Papagiannis, alors que pour David Stephenson l'orbite excentrique de Pluton est le signe d'un projet d'astro-ingénierie. Les autres planètes naines transneptuniennes sont également autant de candidates à l'hypothèse de bases extraterrestres.
Webb aborde une classe de solutions qui font l'hypothèse que les humains sont des extraterrestres. Depuis Fred Hoyle et Chandra Wickramasinghe, qui ont pensé que les microbes auraient pu être transportés par des comètes jusque sur Terre, expliquant par-là les grandes extinctions de son histoire, la théorie de la panspermie forme le cœur de ce scénario. En 1973, l'un des découvreurs de l'ADN, Francis Crick, et Leslie Orgel vont même plus loin en posant l'idée d'une « panspermie dirigée » (directed panspermia), c'est-à-dire une intention intelligente et délibérée de semer la vie sur Terre. Les objectifs seraient multiples : préparer la planète à une future colonisation, adapter sa chimie, effectuer un test géant ou encore y sauvegarder le code génétique de toute une civilisation à l'agonie. La croyance que l'ADN des espèces vivantes sur Terre est un message ou un héritage des extraterrestres au moyen de la panspermie, théorie datant des années 1970, est fortement ancrée dans l'imaginaire.
Théorisée en 1973 par l'astronome John A. Ball, l'hypothèse du zoo pose que des extraterrestres existeraient bien et s'intéresseraient à notre espèce. Ils pourraient le faire de la même façon que nous nous intéressons aux animaux dans des réserves naturelles, par curiosité scientifique et en cherchant à interagir le moins possible avec eux, en nous observant à distance, depuis la ceinture d'astéroïdes ou des confins du Système solaire. Cette hypothèse est directement destinée, selon Ball, à résoudre le paradoxe de Fermi. Il reprend l'idée qu'étant donné la longévité de l'univers, les civilisations extraterrestres doivent être nombreuses et ont dû se répandre dans la galaxie tout entière. Or, l'absence de contact est l'argument le plus fort en faveur de l'hypothèse du zoo, dont il existe, selon les facteurs pris en compte, plusieurs variantes.
Le scénario dit du « laboratoire » pose que la Terre est le sujet d'une expérience alors que James Deardorff propose le scénario d'un embargo non étanche (leaky embargo) : certains extraterrestres ne respecteraient pas la situation intouchable de la Terre et la visiteraient. Deardorff parvient ainsi à intégrer les observations d'OVNI dans son scénario, chose que ne pouvait faire Ball. Un développement supplémentaire de cette hypothèse est celle dite de la « quarantaine galactique » : une ou plusieurs civilisations extraterrestres attendraient que l'humanité arrive à un certain niveau technologique, ou évite l'autodestruction, avant de prendre contact avec elle. Selon Webb, ce scénario et ses développements alternatifs souffrent de plusieurs défauts : ils ne sont pas testables, ils échouent à expliquer pourquoi la Terre n'a pas été colonisée longtemps avant l'apparition de la vie, et enfin ils n'expliquent pas pourquoi les télescopes n'observent aucun signe de vie intelligente dans la galaxie.
Selon Webb, l'hypothèse de l'« apartheid cosmique » est un développement de l'hypothèse du zoo qui constitue cependant à lui seul une solution autonome au paradoxe de Fermi. En 1987, Martyn J. Fogg explique que la Terre et ses espèces vivantes sont rendues intouchables par les civilisations extraterrestres en raison d'un traité galactique. Cette « hypothèse de l'interdit » (interdict hypothesis) se fonde sur l'idée que selon toute vraisemblance la galaxie serait déjà colonisée, et ce bien avant la formation du Système solaire. Depuis, la galaxie serait entrée dans une ère d'équilibre des puissances, ce qui expliquerait qu'il n'y a pas de recherches de contacts. La Terre serait ainsi située au sein de l'aire d'influence d'une de ces puissances galactiques, membre d'un Galactic Club, idée que Fogg emprunte à Sagan et Newman, qui parlent aussi d'un code de conduite commun, le Codex Galactica. La Terre étant un domaine réservé à cette puissance, aucun contact ne serait possible tant que l'humanité n'aurait pas acquis assez de technologie pour rejoindre ce club galactique. Webb voit cependant une faiblesse dans cette hypothèse : l'homogénéité culturelle est un mythe, étant donné la relativité des voyages interstellaires, ce qui constitue un obstacle à l'édification de vastes civilisations. Ce thème de la provolution est populaire dans la littérature et le cinéma, dans le Cycle de l'Élévation (1980) de David Brin notamment.
Pour Stephen Baxter, le paradoxe de Fermi peut être résolu au moyen d'une hypothèse proche de celle du zoo : l'hypothèse du « planétarium » (planetarium hypothesis). La Terre serait prise dans une puissante simulation de réalité virtuelle qui lui masquerait les signes et preuves de la présence extraterrestre. Des signaux électromagnétiques dissimuleraient la signature de leur présence en générant l'équivalent d'un planétarium, à l'échelle du Système solaire tout entier. L'idée a été reprise dans la nouvelle de Robert Heinlein, Universe, mais aussi au cinéma dans Matrix ou The Truman Show. Pour Stephen Webb, l'hypothèse, qui tend à être un solipsisme moderne, et qui va contre le rasoir d'Occam, est peu réaliste, sauf si l'on admet qu'une civilisation très puissante (de type III chez Kardachev) existe. De tels dispositifs nécessitent la maîtrise de l’astro-ingénierie. Anders Sandberg imagine quant à lui les « cerveaux de Jupiter » : des cerveaux artificiels, de la taille de Jupiter, d'une puissance de calcul phénoménale. Ces projets d'astro-ingénierie seraient capables de consommer l'énergie d'une étoile pour fonctionner.
La dernière classe de solutions à l'hypothèse que les extraterrestres existent et nous visitent, mais demeurent invisibles à nos yeux, est que ces derniers possèdent une telle avancée technologique qu'ils risqueraient d'être considérés comme des dieux. Un scénario, plus spéculatif, existe : l'Univers est une création de Dieu, qui est un extraterrestre. Smolin et Edward Harrison ont avancé l'idée que les trous noirs sont générés par des puissances démiurgiques afin de créer des univers. Cette hypothèse s'appuie notamment sur l'exo-théologie. Dans The Physics of Immortality : Modern Cosmology, God and the Resurrection of the Dead, Frank J. Tipler a utilisé le principe anthropique pour postuler l'existence d'une civilisation extraterrestre si avancée qu'elle pourrait, après le Big Crunch (si Big Crunch il y a. Voir Saul Perlmutter), générer un nombre infini de computations si bien qu'elle serait capable de reformer l'univers actuel au sein d'une simulation virtuelle. Cette civilisation pourrait notamment créer des multivers dans lesquels le principe anthropique serait effectif ; elle pourrait aussi faire en sorte que dans chaque univers l'espèce intelligente soit la seule. Les solutions spirituelles sont également envisagées : l'absence d'extraterrestres s'expliquerait par le fait que l'univers a été créé pour l'âme humaine. Il demeurerait vide pour permettre l'avènement de la parousie annoncée.
Le paradoxe de Fermi et ses solutions possibles sont mentionnés dans plusieurs œuvres de fiction, en particulier dans le genre de la science-fiction. Ce genre interroge l'imagination humaine : « Qu’il s’agisse de Solaris de Stanislaw Lem, de 2001, l'Odyssée de l'espace de Stanley Kubrick et surtout de Contact, de Carl Sagan, que Robert Zemeckis a porté à l’écran en 1997, le contact aboutit à une situation où notre propre intelligence se trouve transcendée par d’autres formes d’intelligence. » Au cinéma, l'idée que les extraterrestres aident l'humanité sans se montrer (hypothèse des « grands transparents ») a été de nombreuses fois exploitée : dans The Adjustment Bureau en 2011 notamment (d'après une nouvelle de Philip K. Dick). Dans la bande dessinée, Hergé fait un clin d'œil à cette hypothèse sous les traits de Jacques Bergier dans Vol 714 pour Sydney.
Dans Ultimate Extinction (comic book des éditions Marvel, no 1, 2006), Mr Fantastique note que Galactus fournit une solution à la contradiction apparente entre l'équation de Drake et le paradoxe de Fermi : la Terre n'aurait pas encore rencontré de civilisations extraterrestres parce qu'elles sont rendues rares par le comportement de prédation d'une d'entre elles. Dans le roman Babel Minute Zero de Guy-Philippe Goldstein édité en 2007, le Pr Ernst Alberich note qu'au moment où il discute du paradoxe, Enrico Fermi travaille à l'une de ses solutions, qu'il ne veut évoquer : la création de la première bombe H. Piégée par la logique de la guerre qui l'anime depuis toujours, l'humanité se condamne à un destin d'autodestruction, même si elle en a parfaitement conscience. Or, il en va de même pour toutes les autres espèces animales dans l'Univers explique Alberich.
Dans son roman Espace (Manifold: Space, 2001, publié en 2007) Stephen Baxter explique que toutes les espèces suffisamment évoluées n'ont ni le temps d'atteindre le type IV de l'échelle de Kardachev, ni la possibilité de se répandre dans l'Univers car elles sont : soit systématiquement éradiquées par l'explosion d'un pulsar proche, soit menées à l'extinction par l'épuisement des ressources. À l'échelle de l'Univers, de telles explosions de pulsars, et la raréfaction des ressources, sont suffisamment fréquentes pour empêcher toute civilisation de se développer sur le long terme, ou pour en conduire du moins une certaine fraction à la régression vers l'organisation tribale. D'autres romans de Baxter proposent des solutions au paradoxe de Fermi, et notamment dans The Children’s Crusade (2000), Refu-gium (2002) et Touching Centauri (2002). Enfin, dans Accelerando, Charles Stross, quant à lui, avance l'idée que, confrontée à la difficulté et au coût de l'exploration spatiale, une civilisation suffisamment avancée pourrait se replier sur elle-même plutôt que de coloniser l'univers, vivant dans des univers virtuels générés par des sphères de Dyson concentriques.
De nombreuses nouvelles ou romans de science-fiction abordent des solutions au paradoxe de Fermi : A. E. van Vogt dans Asylum (1942), David Brin dans Just a Hint (1981), The Crystal Spheres (1984) et dans Lungﬁsh (1986), Gregory Benford dans À travers la mer des soleils (1984), Charles Pellegrino dans Flying to Valhalla (1993), Joe Haldeman dans Le Message (2000), Alastair Reynolds dans L'Espace de la révélation (2000), Paul J. McAuley dans Interstitial (2000), Robert Reed dans Lying to Dogs (2002), ou encore Jack McDevitt dans Omega (2003). Dans Berserker (1967 et 1979), Fred Saberhagen reprend le scénario de sondes autoreproductibles qui détruisent les civilisations qu'elles ont atteintes. Dans Les Grands Transparents (2010), Philippe Bataille utilise l'hypothèse des « grands transparents » (l'humanité est aidée par une entité extraterrestre). Dans The Fermi Paradox Is Our Business Model (2010), Charlie Jane Anders met en scène un couple d'extraterrestres qui font du profit sur les civilisations à l'agonie ou mortes. Dans Le Paradoxe de Fermi (2002), Jean-Pierre Boudine examine la possibilité que les civilisations intelligentes s'autodétruisent, à travers un conte philosophique mêlant réflexions scientifiques et sociologiques (inspirées par l'écrivain Piero San Giorgio dans Survivre à l'effondrement économique) et solutions possibles au paradoxe. La « boutade cosmologique » de Fermi est donc pour Boudine une occasion de critiquer la tendance autodestructrice de l'humanité qui serait commune à toutes les formes de vies, celle d'une civilisation à son apogée technologique étant d'environ deux cents ans, un instant à l'échelle cosmique. Le paradoxe de Fermi ne doit plus s'entendre alors par « si les extraterrestres existent » mais par « quand ». Autrement dit, à la civilisation émettrice d'un message doit correspondre une civilisation d'une technologie équivalente pour le recevoir, adéquation rendue difficile voire impossible par leur courte durée d'existence.
(en) Stephen Webb, If the Universe Is Teeming with Aliens... Where Is Everybody ? : Fifty Solutions to Fermi's Paradox and the Problem of Extraterrestrial Life, Springer, 2002, 299 p. (ISBN 978-0387955018) 
Carl Sagan (trad. Vincent Bardet), Cosmic connection : L'Appel des étoiles [« Cosmic connection : an Extraterrestrial Perspective »], Éditions du Seuil, coll. « Points Sciences », 1975 (ISBN 978-2020049436, ASIN 2020049430), chap. 14 
(en) Michael Michaud, Contact with Alien Civilizations : Our Hopes and Fears about Encountering Extraterrestrials, New York, Copernicus Books, 2007 (ISBN 978-0-387-28598-6, lire en ligne [PDF]) 
(en) Ronald Bracewell, The galactic club : Intelligent life in outer space, San Francisco, Freeman and Co., 1975, 152 p. (ISBN 978-0716703532, présentation en ligne)
(en) Steven J. Dick, Plurality of worlds: the origins of the extraterrestrial life debate from Democritus to Kant, CUP Archive, 1984, 256 p. (ISBN 9780521319850)
(en) John Barrow et Frank Tipler, The Anthropic Cosmological Principle, Clarendon Press, 1986 (ISBN 0192821474, présentation en ligne)
(en) Brian M. Stableford, Science fact and science fiction: an encyclopedia, Routledge, 2006, 729 p. (ISBN 9780415974608) 
(en) Stephen H. Dole, Habitable Planets for Man, Blaisdell Publishing Company, 1964 (ISBN 0-444-00092-5)
(en) Ben R. Finney et Eric M. Jones (éd.), Interstellar Migration and the Human Experience, Berkeley, University of California Press, 1985 
(en) Robert T. Rood et James S. Trefil, Are we alone ? : The possibility of extraterrestrial civilizations, New York, Charles Scribner's Sons, 1983 (ISBN 978-0684178424)
Vincent Boqueho (préf. André Brack), La vie, ailleurs ?, Dunod, coll. « Quai des sciences », 2011, 256 p. (ISBN 978-2100558629)
Mathieu Agelou, Gabriel Chardin, Jean Duprat, Alexandre Delaigue et Roland Lehoucq, Où sont-ils ? Les extraterrestres et le paradoxe de Fermi, CNRS éditions, 2017, 300 p. (ISBN 978-2271116376, lire en ligne)
(en) I. Bezsudnov et A. Snarskii, « Where is everybody ? - Wait a moment… New approach to the Fermi paradox », ARXIV,‎ 2010 (lire en ligne [PDF]) 
(en) Eric M. Jones, « « Where is everybody? » : an account of Fermi's question », Los Alamos National Laboratories, Springfield, VA, NTIS,‎ mars 1985 (lire en ligne) 
(en) Geoffrey A. Landis (NASA Lewis Research Center), « The Fermi Paradox: An Approach Based on Percolation Theory », Journal of the British Interplanetary Society, London, vol. 51,‎ 1998, p. 163-166 (lire en ligne) 
(en) T. B. Kuiper et G. D. Brin, « Resource Letter ETC-1: extraterrestrial civilization », American journal of physics, Pasadena, CA, Jet Propulsion Laboratory, California Institute of Technology, no 57(1),‎ 1989, p. 12-8 (lire en ligne) 
(en) Freeman J. Dyson, « Interstellar Transport », Physics Today, vol. 21, no 10,‎ octobre 1968, p. 41-45 (lire en ligne [PDF]) 
(en) Freeman J. Dyson, « Search for Artificial Stellar Sources of Infrared Radiation », Science, vol. 131, no 10,‎ juin 1960, p. 1667-1668 (lire en ligne) 
(en) Martyn J. Fogg, « Temporal Aspects of the Interaction among First Galactic Civilizations: The 'Interdict Hypothesis' », Icarus, no 69,‎ 1987, p. 370-384 (lire en ligne [PDF]) 
(en) Gerald Cleaver et Richard K. Obousy, « The Fermi Paradox, Galactic Mass Extinctions and the Drake Equation », Université Baylor, 27 février 2007 
(en) Claudio Maccone, « The statistical Fermi paradox », Journal of the British Interplanetary Society,‎ mai-juin 2010 (lire en ligne) 
(en) Seth D. Baum, Jacob D. Haqq-Misra et Shawn D. Domagal-Goldman, « Would Contact with Extraterrestrials Benefit or Harm Humanity? A Scenario Analysis », Acta Astronautica, vol. 68, nos 11-12,‎ 2011, p. 2114-2129 (lire en ligne) 
(en) Stephen Baxter, « The planetarium hypothesis: A resolution of the Fermi paradox », Journal of the British Interplanetary Society, no 54,‎ 2001, p. 210-216 
(en) Ted Peters, « Exo-theology: speculations on extraterrestrial life », The Center for Theology and the Natural Sciences, vol. 14, no 3,‎ 1994, p. 187-206 (lire en ligne [PDF]) 
(en) Michael H. Hart et Michael Papagiannis (éd.), « N is Very Small », dans Strategies for the search for life in the universe, Boston, D. Reidel Publishing Co., 1980, 19-25 p. 
(en) Jill Tarter, The Search for Extraterrestrial Intelligence (SETI), vol. 39, 2001, 511–48 p. (lire en ligne) 
(en) Robert A. Freitas, « There is no Fermi Paradox », Icarus, vol. 62, no 3,‎ juin 1985, p. 518-520 (lire en ligne) 
(en) John M. Smart, « Answering the Fermi Paradox: Exploring the Mechanisms of Universal Transcension », Journal of Evolution and Technology (JET),‎ 2002 (lire en ligne) 
(en) F. J. Tipler, « Extraterrestrial Beings Do Not Exist », Quarterly Journal of the Royal Astronomical Society, vol. 21, no 267,‎ 1981 
Pierre Lagrange, « Les extraterrestres sont-ils seuls dans l’univers ? », Ciel et Espace,‎ novembre 2005 (lire en ligne [PDF]) 
(en) Glen David Brin, « The Great Silence : The controversy concerning extraterrestrial intelligent life », Quarterly Journal of the Royal Astronomical Society, vol. 24,‎ 1983, p. 283-309 (lire en ligne [PDF]) 
(en) Kendrick Frazier, « Carl Sagan Takes Questions: More From His ‘Wonder and Skepticism’ CSICOP 1994 Keynote », The Skeptical Inquire, vol. 29, no 4,‎ juillet-août 2005 (lire en ligne) 
(en) Giuseppe Cocconi et Philip Morrison, « Searching for Interstellar Communications », Nature, vol. 184,‎ 1959, p. 844–846 
(en) George Abell et James L. Christian (éd.), « The Search for Life Beyond Earth: A Scientiﬁc Update », dans Extraterrestrial intelligence, Prometheus,‎ 1976, 53–71 p. 
(en) Jacob D. Haqq-Misra et Seth D. Baum, « The Sustainability Solution to the Fermi Paradox », Journal of British Interplanetary Society, vol. 62,‎ 2009, p. 47-51 
(en) David Viewing, « Directly Interacting Extraterrestrial Technological Communities », Journal of the British Interplanetary Society, vol. 28, no 735,‎ 1975 
(en) W. R. Hosek, « Economics and the Fermi paradox », Journal of the British Interplanetary Society, no 60,‎ 2007, p. 137-141
(en) Milan M. Ćirković, « Fermi's paradox : the last challenge for copernicanism ? », Serbian Astronomical Journal, vol. 178,‎ 2009, p. 1-20 (DOI 10.2298/SAJ0978001C, lire en ligne) 
(en) Milan M. Ćirković, I. Dragicevic et T. Beric-Bjedov, « Adaptationism Fails to Resolve Fermi's Paradox », Serbian Astronomical Journal, vol. 180,‎ 2005, p. 89-100 (lire en ligne [PDF])
(en) J. Deardorff, B. Haisch, B. Maccabee et H.E. Puthoff, « Inflation-Theory Implications for Extraerrestrial Visitation », Journal of British Interplanetary Society, ufoskeptic.org, vol. 58,‎ 2005, p. 43-50 (lire en ligne)
(en) M.H. Hart, « An explanation for the absence of extraterrestrials on Earth », Quarterly Journal of the Royal Astronomical Society, vol. 16,‎ 1975, p. 128-135 (lire en ligne) 
(en) P.S. Wesson, « Cosmology, extraterrestrial intelligence, and a resolution of the Fermi-Hart paradox », Quarterly Journal of the Royal Astronomical Society, no 31,‎ 1990, p. 161-170
(en) J.D. Haqq-Misra et S.D. Baum, « The sustainability solution to the Fermi paradox », Journal of the British Interplanetary Society, no 62,‎ 2009, p. 47-51 
(en) C. Cotta et A. Morales, « A computational analysis of galactic exploration with space probes: Implications for the Fermi paradox », Journal of the British Interplanetary Society, no 62,‎ 2009, p. 82-88
(en) Frank Tipler, « Extraterrestrial intelligent beings do not exist », Royal Astronomical Society, Quarterly Journal, vol. 21,‎ septembre 1980, p. 267-281 (lire en ligne) 
(en) Robert A. Freitas Jr., « The search for extraterrestrial artifacts (SETA) », Journal of the British Interplanetary Society, vol. 36,‎ 1983, p. 501-506
(en) Ernst Mayr et Edward Regis (éd.), « The probability of extraterrestrial intelligent life », dans Extraterrestrials: Science and alien intelligence, Cambridge, Cambridge University Press, 1985, 23-30 p.
(en) Peter Schenkel, « The Nature of ETI, Its Longevity and Likely Interest in Mankind: The Human Analogy Re-Examined », Journal of British Interplanetary Society, vol. 52,‎ 1999, p. 13–18 (présentation en ligne) 
(en) Carl Sagan et Frank Drake, « The search for extraterrestrial intelligence », Scientific American: Exploring space (Special issue),‎ 1990, p. 150-159 (lire en ligne)
(en) Nicholas Rescher et Edward Regis (éd.), « Extraterrestrial Science », dans Extraterrestrials: Science and alien intelligence, Cambridge, Cambridge University Press, 1985 (lire en ligne), p. 83- 116
(en) T. B. Tang, « Fermi Paradox and C.E.T.I. », Journal of British Interplanetary Society, vol. 35,‎ mai 1982, p. 236-240
(en) Carl Sagan (éd.) et alii, « The number of advanced galactic civilizations », dans Extraterrestrial Intelligence: CETI, Cambridge, M.I.T. Press, 1973 (présentation en ligne), p. 164-187
(en) Guillermo A. Lemarchand, « Speculations on the First Contact : Encyclopedia Galactica or the Music of the Spheres? », dans When SETI Succeeds: The Impact of High-Information Contact, 2000, 153–163 p.  
 Portail de la science-fiction   Portail de l’astronomie   Portail de l’évolution   Portail de la vie extraterrestre et de l’ufologie   Portail du temps   Portail du paranormal