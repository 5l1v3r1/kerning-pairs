
Prince of Persia : Les Sables du temps (Prince of Persia: The Sands of Time en version originale) est un jeu vidéo développé et édité par Ubisoft. Mélange de jeu de plates-formes et d’action, il est sorti en 2003 sur PC, PlayStation 2, Xbox et GameCube, puis a été adapté sur Game Boy Advance et téléphone mobile. Il est le quatrième jeu de la franchise Prince of Persia.
Le jeu raconte l'histoire du Prince de Perse qui, trompé par le Vizir, libère les Sables du temps dans le palais du Sultan, ami de son père. Ceux-ci corrompent les êtres vivants et les transforment en Monstres des sables sous le contrôle du Vizir. Avec l'aide de Farah, fille du Maharajah d'Inde réduite en esclavage par le roi de Perse Shahraman, le joueur doit faire traverser au Prince tous les pièges du palais aidé par ses talents d'acrobate et les pouvoirs de la Dague du temps, arme magique permettant de contrôler le temps.
Le jeu rencontre un immense succès critique et commercial (plus de 2 000 000 d'exemplaires vendus), et relance la franchise Prince of Persia. Il est le premier épisode d’une trilogie, dite des Sables du temps. Ses deux suites, L'Âme du guerrier et Les Deux Royaumes, sont sorties en décembre 2004 et décembre 2006. Prince of Persia : Les Sables du temps a eu de nombreuses nominations à des prix et en a remporté plusieurs dont un E3 Game Critics Award, un Imagina Games Award, deux Game Developers Choice Awards et huit Interactive Achievement Awards. Il a également été nommé de nombreuses fois dans les classements annuels des meilleurs jeux vidéo de l'année 2003 dans la presse spécialisée.
Au Moyen Âge, traversant l'Inde en direction d'Azad, le roi Shahraman et son fils, le Prince de Perse (dont le vrai nom est inconnu), sont en route pour défaire le puissant Maharajah d'Inde pour les honneurs et la gloire. Dans l'entourage du Maharajah se trouve un espion, le Vizir, qui a pour mission de prévenir l'armée persane du moment opportun pour attaquer la forteresse indienne, en échange d'une partie des trésors qui seront saisis. Après avoir pillé la cité, dérobé un gigantesque sablier rempli de sable, volé une dague mystérieuse et réduit en esclavage Farah, la fille du Maharajah, le Vizir demande au roi Sharaman en récompense la Dague du temps trouvée par le Prince. Mais le roi refuse de prendre la Dague des mains de son fils, car il s'agit de son premier butin de guerre.
L'armée se remet en route. Le roi de Perse se rend chez le Sultan au palais d'Azad et lui apporte le Sablier du temps en cadeau, en signe d'amitié entre leurs deux peuples. Mais le Vizir pousse le Prince à insérer la Dague du temps dans le Sablier. Le Sablier s'ouvre alors et libère en un cyclone les Sables du temps, qui se mettent à ravager le palais et à corrompre les êtres humains, les transformant en Monstres des sables. Les seules personnes capables de résister à la contamination sont le Prince grâce à sa Dague, le Vizir grâce à son Sceptre et Farah grâce à son Médaillon. Il y a d'autres rares survivants qui ne résisteront pas longtemps.
Dans le but de réparer les terribles dommages qu'il vient de causer, le Prince doit retrouver le Sablier du temps et le refermer. Sa mission l'amène à rencontrer Farah, avec qui il s'empressera de s'allier et avec qui il nouera une relation amoureuse. Ils parviendront ensemble à percer les mystères des mécanismes titanesques qui régissent le fonctionnement du palais et de son système de défense automatisé. Le Prince découvre au fur et à mesure de son périple que la Dague lui permet d'influer sur le temps : le ralentir ou revenir dans le passé par exemple.
Une fois le Sablier atteint, il replace la Dague à l'intérieur pour faire un gigantesque saut dans le passé, arrivant juste avant l'assaut de la forteresse du Maharajah. Toute sa péripétie dans le Palais et sa relation avec Farah ne sont plus qu'un souvenir connu de lui seul. Toutefois, le Prince est toujours en possession de la Dague du temps. Il s'infiltre alors dans la forteresse et rejoint Farah. Il lui fait le récit de la ligne de temps qu'il a vécu. Le Vizir surgit alors et use de magie pour combattre le Prince.
Après avoir vaincu le Vizir et l'avoir empêché d'ouvrir le Sablier du temps, le Prince rend la Dague à Farah et ils se séparent tous deux conscients, finalement, que leur histoire d'amour a existé dans un autre temps.
Tout au long du jeu, le Prince semble faire le récit de son histoire à quelqu'un. La cinématique d'introduction le montre en train de s'introduire dans une chambre, tout en expliquant le principe du temps. Les remarques qu'il fait lors de sa mort ou de celle de Farah (« Non, non, non, cela ne s'est jamais produit... », « Non, elle n'est pas morte, puis-je recommencer ? ») et lorsque le jeu est sauvegardé (« Je reprendrai mon récit à partir de ce point. ») renforcent cette impression. Après l'inversion du processus des Sables par le Prince et son retour dans la chambre de Farah, il est révélé qu'il avait raconté toute son histoire à Farah, dans le but de la prévenir de la trahison du Vizir.
Ce procédé narratif a pour but de renforcer l'immersion du joueur. Celle-ci est parfois contrebalancée par des références à d'autres jeux vidéo qui ramènent le joueur à la réalité. Par exemple, lorsque le Prince et Farah se retrouvent dans le Temple de la connaissance, une grande bibliothèque, un clin d'œil est fait au jeu Myst : le Prince dit « Il nous faudrait un livre pour sortir d'ici. », ce à quoi Farah répond « Vous vous trompez de jeu, vous savez. ».
Enfant, le Prince était doté d'une grande vivacité et d'une grande curiosité. Il sait comment cacher sa sensibilité et provoque auprès de ses hommes un mélange d'admiration et de frayeur. Fougueux au début de l'aventure, le Prince gagne en maturité par la suite et comprend que l'exploit n'existe que si l'on protège ceux que l'on aime.
Le Prince a beaucoup changé depuis les précédents volets de Prince of Persia aussi bien physiquement que psychologiquement. C'est Jordan Mechner, le créateur du personnage, qui a souhaité et défini son évolution. Son aspect physique peut le rapprocher d'un personnage classique de Disney.
La relation entre Farah et le Prince va évoluer tout au long de l'aventure. Au début, elle hait le Prince, non seulement pour avoir envahi son pays, mais aussi pour avoir libéré les Sables. Néanmoins leur objectif commun les fait entretenir une relation d'amour/haine tout au long de l'aventure. Ils finissent par s'aimer, bien qu'aucun des deux n'ose l'avouer à l'autre. Il y a pourtant de forts soupçons quant à la relation sexuelle qu'auraient eu les deux protagonistes, puisqu'une cinématique les montre tous deux nus dans un bassin, avec un fondu noir lorsqu'ils s'embrassent. Le Prince va ensuite se réveiller, en se demandant si ce qu'il a vécu était un rêve ou la réalité. Il s'aperçoit finalement que Farah a disparu, en emportant son épée et sa Dague, ne lui laissant que son Médaillon, pour l'empêcher de se transformer en Créature des Sables.
Après avoir remis la Dague dans le sablier, le Prince remonte le temps jusqu'à la veille de la guerre. Farah a tout oublié de sa relation avec le Prince, seul à se souvenir des événements. Elle lui demande, après le combat contre le Vizir, pourquoi il a inventé une « histoire aussi folle ». Il l'embrasse alors, mais elle est visiblement offensée par cet acte et le repousse. Le Prince, réalisant son erreur, remonte le temps et lui remet la Dague. Avant de s'en aller, Farah lui demande son nom, et celui-ci lui répond : « Appelez-moi... Kakolookiyam ». Il la quitte en la laissant stupéfaite, car elle seule connaissait ce mot, le nom que sa mère avait donné au héros imaginaire des histoires qu'elle lui contait pour la rassurer, lorsqu'elle ne se sentait pas en sécurité. En réalité, elle lui en a parlé dans la première ligne temporelle mais ne l'a pas vécu.
Bien que Farah semble douce et délicate, elle est aussi fière est volontaire. Elle sait manier l'arc et combat aux côtés du Prince. Malheureusement, il lui arrive de rater sa cible et de toucher le Prince à la place. Il y quelques dialogues humoristiques à propos de cela. À chaque flèche manquée, elle répond « Désolée ! », et plus tard dans le jeu, alors qu'elle propose au Prince de couvrir ses arrières, celui-ci répondra sur un ton exaspéré : « Non merci. Vous pourriez me toucher. ».
Le Vizir trahit le Maharajah d'Inde pour le compte du roi de Perse Shahraman. Il cherche par ce biais à mettre la main sur la Dague du temps, une relique capable de lui conférer l'immortalité. Parmi les trésors du Maharajah se trouve le Sablier du Temps, que le Prince ouvre sous l'impulsion du Vizir. Tout au long de l'aventure, le Vizir envoie ses Monstres des sables pour vaincre le Prince, mais sans succès. Lors du combat final, le Prince apprend que le Vizir convoitait les Sables du temps pour être puissant mais aussi pour soigner la tuberculose par laquelle il est atteint. Cette révélation est un anachronisme, car la tuberculose n'a été découverte qu'au XIXe siècle.
Le roi est transformé en Monstre des sables et donc soumis à la volonté du Vizir. Son fils sera contraint de le tuer pour poursuivre sa mission. Cette action sera toutefois annulée lorsque le Prince effectuera le gigantesque saut en arrière.
Prince of Persia : Les Sables du temps allie phases de plates-formes et phases de combat à la troisième personne et chacune fait intervenir l'incroyable agilité du Prince et ses talents d'acrobate rappelant le parkour. La Dague du temps, une arme magique lui confère des pouvoirs qui l'aident dans ces deux phases. Elle contient des mesures de Sable du temps pouvant être emmagasinées au cours des combats en absorbant le Sable contenu dans les ennemis vaincus et pouvant, par exemple, être utilisées pour revenir en arrière dans le temps jusqu'à dix secondes. Le Prince peut aussi absorber des vortex de Sable disséminés dans le palais. La capacité de contenance de la Dague augmente d'une mesure à chaque fois que le Prince absorbe huit vortex. Le Prince possède une barre de vie qui peut être augmentée en buvant de l'eau de la fontaine secrète accessible par des passages cachés dans le jeu.
Tout au long du jeu, le Prince doit traverser le palais en courant sur les murs, sautant au-dessus de crevasses et escaladant des parois en sautant de murs en murs, tout en évitant les pièges, tels les fosses à piques, les scies circulaires et autres lames rétractables. Si la précision des sauts directs (d'un sol à un autre par-dessus le vide) a de l'importance comme dans tout jeu de plates-formes, le jeu utilise beaucoup l'accrochage à des corniches ou des poteaux et les passages et les sauts entre ceux-ci, ce qui est créé par un level design particulier.
Ces phases sont également émaillées d'énigmes qui doivent être résolues pour permettre au Prince d'avancer dans le palais croulant. Celles-ci se résolvent en actionnant divers mécanismes auxquels le Prince parvient grâce à ses dons d'acrobatie. Farah aide le Prince à progresser dans le palais en se faufilant sous des portes bloquées ou dans des fissures pour aller déclencher des mécanismes inaccessibles par le Prince.
Les ennemis sont tous d'anciens humains ou animaux transformés par les Sables du temps. Cet empoisonnement les a asservi aux pouvoirs du Vizir, qui leur a donné l'ordre de tuer le Prince et Farah. Le Sable contenu en chacun d'eux leur confère de nouveaux pouvoirs, dont le pouvoir de résurrection citée précédemment et la capacité de contrôler le temps : ils sont capables de le stopper lors de leurs déplacements pour toujours se trouver à proximité du Prince et de Farah (ce qui équivaut à des téléportations, du point de vue du joueur). Le Prince doit protéger la vie de Farah qui l'aide dans ses combats avec son arc.
Durant les combats, le Prince est capable de faire preuve d'une agilité surhumaine pour se défendre et frapper. Il est ainsi capable de bloquer toutes les attaques hautes venant de toutes les directions avec son épée ou de prendre des impulsions sur les murs pour renverser ses ennemis. Dans ces phases, la Dague permet au Prince de ralentir le temps et de « geler » les ennemis (utilisée comme main-gauche). Le fait de laisser un ennemi à terre plus de cinq secondes conduit à sa résurrection ; c'est pourquoi le Prince doit toujours absorber le Sable contenu en ses ennemis ou les détruire.
Chaque ennemi rencontré lors du jeu (10 différents au total hors boss) correspond a une technique de combat particulière à employer.
Les sauvegardes s'effectuent sur des points de sauvegardes répartis à intervalle régulier sur la parcours du Prince. Ils se matérialisent généralement après un combat. Ces points de sauvegardes ont pour effet de plonger le Prince dans un état de stase durant lequel il a des prémonitions sur les épreuves à venir. Ces prémonitions permettent au joueur de résoudre des énigmes à venir et lui donnent un avant-goût de ce qui l'attend en termes de combats notamment.
Quand le joueur enregistre sa partie, le jeu lui communique son pourcentage de progression dans le jeu. La progression est également marquée par l'arrachage par le Prince d'éléments de son costume : coiffe, manches, haut. Ce procédé a pour but de faire entrer le personnage dans l'action.
Le Prince acquiert au fil du jeu ses pouvoirs et trouve des épées de plus en plus puissantes, pouvant notamment détruire des murs abimés. La difficulté est progressive (nombre et force des ennemis, difficulté des enchainements lors des phases plates-formes). La mort du héros fait revenir en arrière le joueur, le faisant ainsi recommencer des séquences de jeu. Ces retours sont rares quand le Prince possède le pouvoir de remonter le temps et plus fréquents quand il ne l'a pas (début du jeu, après que Farah lui a pris la Dague, et combat final). Le renouvellement de la série en 2008 avec Prince of Persia a finalement aboli ce système de progression, repris dans les deux autres opus de la trilogie des Sables du temps, en rendant impossible la mort du Prince.
Le jeu ne possède pas à proprement parler de niveaux même si l'on peut considérer tout ce qui se situe entre deux points de sauvegardes comme un niveau. Un titre est donné à chacune de ses zones au moment de la sauvegarde. Ces titres font penser à des titres de chapitres qui agrémenterait l'histoire à la première personne que raconte le Prince. Ils décrivent également les rencontres faites, les événements, les environnements traversés ou encore le moment de la journée (dans le palais du Sultan, le jeu dure de la nuit au soleil couchant de la journée suivante).
En mars 2001, Ubi Soft achète la section divertissement de The Learning Company et acquiert ainsi les droit de la licence Prince of Persia. Un projet est rapidement mis en route puisque dès mai 2001, les premières réunions à propos d'un nouveau jeu se déroulent dans les studios d'Ubi Montréal. Il est rapidement retenu que ce qui avait fait le succès du Prince of Persia original et de sa suite était l'animation du personnage, les combats dynamiques et la qualité du level design. Le jeu se devait donc de s'axer autour de ces points forts. Ce retour aux sources permet également de s'éloigner de Prince of Persia 3D qui avait, selon les développeurs, dévié de l'esprit original de la série,,.
Le développement débute avec une équipe de sept personnes : le producteur Yannis Mallat, deux game designers, un animateur, deux ingénieurs et un concepteur artistique. Les choses s'accélèrent lorsque le créateur de la série, Jordan Mechner, prend connaissance de la maquette en juillet 2001 et se montre très enthousiaste. À partir de septembre, le projet prend plus d'importance au sein du groupe Ubisoft,.
Le matériel utilisé pour le développement inclut les kits de développement GameCube, PlayStation 2 et Xbox, des ordinateurs tournant sous Windows 2000 et le PlayStation 2 Performer Analyzer, un outil de débogage et d’analyse de performance avancés. Le jeu est programmé sous les environnements de développement Visual Studio .Net 2003 de Microsoft et CodeWarrior de Metrowerks, et est optimisé avec le moteur de grille informatique Incredibuild de Xoreax Software. Le moteur de jeu utilisé est le Jade Engine, développé par Ubisoft pour le jeu Beyond Good and Evil.
Le producteur du jeu, Yannis Mallat, résume : « Une fois la phase de conception terminée et plusieurs vidéos montrant comment le Prince pouvait bouger et interagir avec son environnement réalisées, nous avons commencé la phase d’étude technique du moteur. Après avoir sélectionné le bon moteur, nous avons formé l’équipe sur la technologie adoptée pour qu’elle se lance dans la pré-production. ». Le Prince a été conçu comme un « ninja Perse ». Pour définir des mouvements et un style de combat au Prince, l'équipe a visionné Tigre et Dragon, des films d'action avec Jet Li et des documentaires sur la capoeira. Il possède 750 animations composant l'une des palettes de mouvements et de transitions les plus conséquentes existante pour un jeu d'action en 3D à l'époque. L'animation a été travaillée selon un mode classique : esquisse sur papier, passage à la 3D, création des textures puis tests. Elle se décompose en deux types principaux d'images : des poses fortes qui montrent clairement ce que le personnage fait et des poses de fins de mouvements figées.
Le premier Prince of Persia avait défini des codes du genre action/plates-formes mais son gameplay devait, selon les développeurs, tenir compte des nouvelles normes du genre. Ainsi, l'équipe a étudié de près d'autres jeux vidéo parmi lesquels Shinobi, Onimusha: Warlords, Devil May Cry, Tomb Raider, Rygar et Ninja Gaiden. Le premier Prince of Persia a influencé le jeu Ico dont Mechner s'est inspiré pour créer l'univers des Sables du temps au niveau visuel (effets de flou, techniques de texture...) et scénaristique (collaboration entre deux personnages). Le jeu est d'ailleurs parfois considéré comme le successeur d'Ico,.
Pour créer l'univers du jeu, l'équipe de développement a commencé par lire Les Mille et Une Nuits et a visionné les films hollywoodiens qui s'en sont inspirés afin de cerner les « clichés » se rapportant à ce thème. Lors de la conception des niveaux, les proportions des tours et des salles ont été volontairement exagérées pour renforcer l'immersion du joueur et donner un côté sinistre au palais. Comme le Prince peut interagir avec le décor, les niveaux ont été aménagés dans un souci de plausibilité. Le jeu charge les niveaux au fur et à mesure pour obtenir des transitions entre les salles fluides. Des techniques du cinéma ont été utilisées pour diriger la caméra du jeu. Quatre types de suivi de la caméra ont été implémentés : caméra libre, caméra de combat, vue à la première personne et caméra fixe alternative. L'interface, quant à elle, a évolué au fil du temps pour devenir plus stylisée et créer une bordure dans le coin supérieur gauche de l'écran.
Les bruitages ont été créés en majorité par Dane Tracks, ceux restant ayant été faits en interne à Ubisoft Montréal. Le son des Monstres des sables est composé de voix murmurantes pour s'éloigner volontairement des clichés de bruits de morts-vivants.
Durant trois mois, l'équipe de développement est passée progressivement de 20 à 35 personnes puis durant six mois, elle est passée à 55 personnes. Le résultat final est un ensemble de 4 188 fichiers pour un total de 1 263 580 lignes de code.
Lors de la Game Developers Conference, Yannis Mallat a fait un court post mortem du développement du jeu dans lequel il déplore une direction artistique arrivée trop tard dans le développement, une validation des processus trop confuse et un système de gestion de versions désastreux. Il regrette aussi un gameplay entaché par des ennemis trop envahissants. Néanmoins, il se félicite de la gestion du risque, du fonctionnement du duo directeur de l'animation et directeur de l'Intelligence Artificielle et de la motivation de l'équipe à aller au bout des choses. Il juge que le jeu doit beaucoup à l'éditeur de niveau très performant et à l'organisation des tests,. Le jeu sort en novembre 2003 en Amérique du Nord et en Europe en exclusivité temporaire sur PlayStation 2 puis sur les deux autres consoles de salon leaders de la sixième génération.
La musique du jeu a été composée par Stuart Chatwood du groupe de rock canadien The Tea Party. Il assure également les parties des morceaux jouées au sarod, à la darbouka et à l'harmonium. La voix féminine dans les musiques des phases de jeu est celle de la chanteuse Maryem Tollar. La musique du générique final, Time Only Knows, est quant à elle interprétée par la chanteuse Cindy Gomez. Paul Atkins était à la batterie et Ritesh Das au tablâ.
Lors de la conception de la musique, la volonté d'Ubisoft Montréal était d'avoir une musique moderne faisant penser à de la musique persane sans en être. Pour cela, des accents rock ont été mélangés avec des rythmes et des mélodies du Moyen-Orient et des sonorités indiennes. La musique se veut à la fois lascive et inquiétante pour mettre en valeur l'aspect tragique de l'aventure.
Parmi les qualités du titre, le graphisme est loué quasi unanimement, notamment l'aspect féerique, et l'animation fluide et désymétrisée. Le gameplay est aussi encensé notamment pour ses mécaniques de jeu originales et le level design. PC Jeux le qualifie de « bijou ».  Le scénario est globalement bien considéré notamment le développement de la relation entre le Prince et Farah.
Quelques défauts sont pointés : la durée de vie jugée un peu courte,,, la gestion des caméras et des ralentissements en certaines occasions, la linéarité de certains niveaux et les combats parfois brouillons et longs. L'ambiance sonore divise même si elle est jugée globalement bonne,.
Finalement, la critique a dans l'ensemble extrêmement bien reçu Prince of Persia : Les Sables du temps, faisant du jeu l'un des mieux noté de l'histoire du jeu vidéo.
Prince of Persia : Les Sables du temps a reçu plusieurs récompenses notables et de nombreux labels de qualité (type « choix de la rédaction ») par les médias spécialisés dans le jeu vidéo.
Nommé au Prix du meilleur designer graphiquePrix de professionnels4e Game Developers Choice AwardsPrix d'excellence en game design
Nommé au Prix d'excellence en arts visuels pour la direction artistique de Mickaël Labat, Raphaël Lacoste, Céline Tellier et leur équipe
Nommé au Prix de la meilleure direction artistiquePrix de la critiqueElectronic Gaming Monthly 2003
Le jeu sort sur PlayStation 2 (PS2), PC et Game Boy Advance (GBA) pour les fêtes de fin d'année de 2003 en Europe et Amérique du Nord ; les versions Xbox et GameCube nord-américaines sortent aussi à cette période. Le jeu est très mis en avant, des publicités sont diffusés dans les magazines de jeu vidéo et à la télévision. Ubisoft propose plusieurs offres promotionnelles : un pack contenant le jeu et une PS2 est disponible pour Noël, et début 2004 le jeu Splinter Cell (une de ses nouvelles licences qui a été bien reçu en 2002-2003) est offert pour tout achat du jeu Prince of Persia, pour les versions PC et console. Le jeu connait un important succès : début février 2004, Ubisoft annonce avoir vendu 2 millions d'exemplaires dans le monde, dont 1,1 million en Europe (donc pour les versions PS2, PC et GBA). Le 20 février 2004, les versions Xbox et GameCube sortent en Europe. L'unique version japonaise sort sur PS2 le 2 septembre 2004, elle intègre le top 10 des ventes au Japon avec 14 000 unités vendues sur PlayStation 2, ce qui est très rare pour un jeu n'étant pas japonais, même si ce chiffre est très éloigné des ventes en occident. Approximativement 1 880 000 exemplaires seront vendus sur PlayStation 2. Cette version s’écoule à 700 000 exemplaires aux États-Unis, pour des recettes estimées à 24 millions de dollars américains.
Le Prince of Persia original est inclus dans les versions GameCube, PlayStation 2 et Xbox. Sa suite, Prince of Persia 2: The Shadow and the Flame, est incluse dans la version version NTSC Xbox. Le remake du premier niveau du Prince of Persia original (modélisé en 3D et contenant une photographie de l'équipe de développement) est inclus dans les versions GameCube, PlayStation 2 et PC.
Sur GameCube, si le joueur connecte une cartouche du jeu sur Game Boy Advance avec le câble Nintendo GameCube Game Boy Advance, la santé du Prince se régénère automatiquement durant le jeu.
Le jeu a fait l'objet d'un portage en 2004 sur téléphone mobile. Celui-ci a été développé et édité par Gameloft.
Prince of Persia : Les Sables du temps sur Game Boy Advance est l'adaptation du jeu sur la portable de Nintendo. Celle-ci délaisse la 3D pour une vue de côté en 2D, classique, à la manière du premier Prince of Persia. Cette version reprend la trame de la version originale. Le joueur doit donc toujours y parcourir un palais rempli de pièges. Cette version reprend certains mouvements de la version originale (appui sur les murs par exemple), des pouvoirs des sables de temps, les combats et les interactions avec Farah, qui est jouable dans cette version.
Le jeu est découpé en 16 grandes zones complétées par des boss. L'exploration du palais fait penser aux mécanismes de Castlevania. Des parchemins magiques dispersés dans le jeu permettent d'acquérir de nouveaux pouvoirs ou d'accéder à de nouvelles zones. Des potions parsèment les niveaux : potions de vie, potions de sables du temps, antidotes, élixirs augmentant la vie et le niveau de sables de temps, accessibles en reliant les versions Game Boy Advance et GameCube via le câble Nintendo GameCube Game Boy Advance. Des puits de lumière colorés peuvent également être trouvées, ils ont des effets analogues aux potions mais peuvent aussi rendre le Prince invisible, permettre au joueur de sauvegarder ou encore permettre au Prince de faire changer le comportement de certaines plates-formes. Les sauvegardes se font généralement dans des salles spéciales. D'autres salles permettent au joueur de restaurer sa jauge de vie ou d'accéder à de nouvelles zones du palais.
Cette version de Prince of Persia : Les Sables du temps a été globalement bien accueillie par la presse avec très peu de notes faibles mais tout de même peu de notes exceptionnelles. Généralement les critiques louent l'aspect graphique du jeu surtout en termes d'animation,,, mais déplorent parfois le peu de variété des décors,,. Le gameplay est honoré notamment pour sa maniabilité intuitive,, son bon équilibre des différentes phases de jeu et son level design. Les musiques ont droit à des critiques en demi-teinte mais les bruitages sont considérés comme de bonnes factures. Jeux Vidéo Magazine le trouve trop classique, 1UP.com un peu faible face à la concurrence des autres jeux de plates-formes sur Game Boy Advance et Gamekult déplore des problèmes nuisant à la portabilité du titre comme des points de sauvegardes éloignés. Jeuxvideo.com s'emballe pour le jeu notamment pour son « nombre de possibilités effarant » malgré un scénario moyen.
La version pour téléphone mobile de Prince of Persia : Les Sables du temps reprend globalement l'aspect graphique et le gameplay de la version Game Boy Advance au niveau des phases de combats et de plates-formes. Le jeu est découpé en trois univers graphiques distincts et est découpé en huit niveaux.
Développé par Gameloft et sorti le 8 janvier 2004, le jeu a été noté 9,2/10 par GameSpot et a obtenu le prix du meilleur jeu mobile aux Mobies Awards 2004, décerné par un jury de journalistes spécialisés et des professionnels et organisé par le site WGamer.
Le succès critique et commercial de Prince of Persia : Les Sables du temps a permis à Ubisoft de développer une trilogie complète avec les deux épisodes suivants : Prince of Persia : L'Âme du guerrier en 2004 et Prince of Persia : Les Deux Royaumes en 2006. La série des Sables du temps connu également un spin-off sur Nintendo DS, Battles of Prince of Persia, jeu de stratégie au tour par tour sorti en 2005. Ubisoft relança par la suite la licence  avec Prince of Persia en 2008 sur consoles de septième génération. Un remake du premier épisode sera également réalisé en reprenant l'univers des Sables du temps, Prince of Persia Classic, sur le modèle du niveau bonus présent dans certaines versions de Prince of Persia : Les Sables du temps. Un épisode intitulé Prince of Persia : Les Sables oubliés est sorti en 2010 et s'intercale chronologiquement entre Les Sables du temps et L'Âme du guerrier.
Le voyage dans le temps dans un jeu de plates-formes déjà vu dans Blinx: The Time Sweeper sur Xbox en 2002 et sa suite, Blinx 2: Masters of Time and Space, sera par la suite repris dans le jeu Braid sur Xbox 360 en 2008. Il sera également utilisé dans le jeu de course Race Driver: GRID (rembobinage). Le level design et la panoplie de mouvements (possibilités d'accrochage) ont inspiré de nombreux jeux parmi lesquels Assassin's Creed (réalisé par la même équipe) ou Mirror's Edge. La série Prince of Persia est également très liée à la série Tomb Raider. Le premier Tomb Raider était un descendant du premier Prince of Persia. Prince of Persia : Les Sables du temps s'en est inspiré et Tomb Raider: Legend s'inspire des Sables du temps,.
En 2008, Jordan Mechner scénarise avec l'écrivain iranien A.B. Sina le roman graphique Prince of Persia, dessiné par LeUyen Pham et Alex Puvilland. S'inspirant des différents jeux vidéo de la série, il devait à l'origine reprendre le personnage de Farah qui ne fut finalement pas conservé. Celui-ci se situe chronologiquement avant les autres épisodes de la série, dans un autre univers, la série de jeux vidéo se déroulant déjà dans trois univers différents dont celui des Sables du temps.
En 2004, Jerry Bruckheimer a négocié les droits d'adaptation cinématographique du jeu. Cette superproduction à 150 000 000 de dollars américains adaptée du jeu vidéo est sortie le 27 mai 2010 en France. Il est réalisé par Mike Newell et le Prince y est interprété par Jake Gyllenhaal.
Une compilation de la trilogie des Sables du temps, c'est-à-dire Les Sables du temps, L'Âme du guerrier et Les Deux Royaumes, est sortie le 18 novembre 2010 sur PlayStation 3 et s’intitule Prince of Persia Trilogy. Pour l’occasion, les graphismes sont en haute définition (720p), la 3D stéréoscopique est supportée et l’ensemble est mis sur un seul disque Blu-ray. Il est également possible de télécharger individuellement chacun de ces épisodes sur le PlayStation Network.
(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Prince of Persia: The Sands of Time » (voir la liste des auteurs).

En algorithmique, le problème du sac à dos, noté également KP (en anglais, Knapsack problem) est un problème d'optimisation combinatoire.
Il modélise une situation analogue au remplissage d'un sac à dos, ne pouvant supporter plus d'un certain poids, avec tout ou partie d'un ensemble donné d'objets ayant chacun un poids et une valeur. Les objets mis dans le sac à dos doivent maximiser la valeur totale, sans dépasser le poids maximum.
Le problème du sac à dos est l'un des 21 problèmes NP-complets de Richard Karp, exposés dans son article de 1972.
Il est intensivement étudié depuis le milieu du XXe siècle et on trouve des références dès 1897, dans un article de George Ballard Mathews (en). La formulation du problème est fort simple, mais sa résolution est plus complexe. Les algorithmes existants peuvent résoudre des instances pratiques de taille importante. Cependant, la structure singulière du problème, et le fait qu'il soit présent en tant que sous-problème d'autres problèmes plus généraux, en font un sujet de choix pour la recherche.
Ce problème est à la base du premier algorithme de chiffrement asymétrique (ou à « clé publique ») présenté par Martin Hellman, Ralph Merkle et Whitfield Diffie à l'université Stanford en 1976. Toutefois, même si l'idée est due au problème du sac à dos (dont l'algorithme porte le nom) , il est considéré comme le premier véritable algorithme de chiffrement asymétrique juste avant RSA publié l'année suivante.
La version NP-difficile de ce problème a été utilisée dans des primitives et des protocoles de cryptographie, tels que le cryptosystème de Merkle-Hellman ou le cryptosystème de Chor-Rivest. Leur avantage par rapport aux cryptosystèmes asymétriques fondés sur la difficulté de factoriser est leur rapidité de chiffrement et de déchiffrement.
Cependant, l'algorithme de Hellman, Merkle et Diffie est sujet aux « portes dérobées » algorithmiques, ce qui implique qu'il est « cassé », c'est-à-dire cryptanalysé. Le problème du sac à dos est un exemple classique de méprise en ce qui concerne les liens entre la NP-complétude et la cryptographie.
Une version revue de l'algorithme, avec une itération du problème du sac à dos, a alors été présentée, pour être sitôt cassée.
Les algorithmes de chiffrement asymétrique fondés sur le sac à dos ont tous été cassés à ce jour, le dernier en date étant celui de Chor-Rivest.
On l'utilise aussi pour modéliser les situations suivantes, quelquefois en tant que sous-problème :
dans des systèmes d'aide à la gestion de portefeuille : pour équilibrer sélectivité et diversification dans le but de trouver le meilleur rapport entre rendement et risque pour un capital placé sur plusieurs actifs financiers (actions...) ;
dans le chargement de bateau ou d'avion : tous les bagages à destination doivent être amenés, sans être en surcharge ;
dans la découpe de matériaux : pour minimiser les chutes lors de la découpe de sections de longueurs diverses dans des barres en fer, de rouleaux de papier ou de textile etc.Une autre raison de s'intéresser à ce problème est son apparition dans certaines utilisations de méthodes de génération de colonnes (ainsi pour le problème de « bin packing »).
Anecdotiquement et justifiant ainsi le nom du problème, un randonneur y est confronté au moment de préparer son périple : le sac à dos a une capacité limitée et il faut donc trancher entre prendre, par exemple, deux boîtes de conserve et une gourde de cinquante centilitres ou une seule boîte de conserve et une gourde d'un litre.
Les données du problème peuvent être exprimées en termes mathématiques. Les objets sont numérotés par l'indice i variant de 1 à n. Les nombres 
   représentent respectivement le poids et la valeur de l'objet numéro i. La capacité du sac sera notée W.
Il existe de multiples façons de remplir le sac à dos. Pour décrire l'une d'elles il faut indiquer pour chaque élément s'il est pris ou non. On peut utiliser un codage binaire : l'état du i-ème élément vaudra 
   s'il est laissé de côté. Une façon de remplir le sac est donc complètement décrite par un vecteur, appelé vecteur contenu, ou simplement contenu : 
   ; et le poids associé, ainsi que la valeur associée, à ce remplissage, peuvent alors être exprimés comme fonction du vecteur contenu.
Le problème du sac à dos peut être représenté sous une forme décisionnelle en remplaçant la maximisation par la question suivante : un nombre 
  , avec respect de la contrainte ? Sous sa forme décisionnelle, lorsque les nombres sont représentés en notation binaire, le problème est NP-complet, ce qui signifie que l'on ne connaît pas de méthode générale pour construire une solution optimale, à part l'examen systématique de toutes les solutions envisageables. Le problème d'optimisation est NP-difficile, sa résolution est au moins aussi difficile que celle du problème de décision, et il n'existe pas d'algorithme polynomial connu (polynomial en le nombre de chiffres pour décrire une instance) qui, étant donné une solution, peut dire si elle est optimale (ce qui reviendrait à dire qu'il n'existe pas de solution avec un 
Il y a un lien entre la version « décision » et la version « optimisation » du problème dans la mesure où s'il existe un algorithme polynomial qui résout la version « décision », alors on peut trouver la valeur maximale pour le problème d'optimisation de manière polynomiale en effectuant de la dichotomie pour trouver k entre 1 et 
   et en appelant l'algorithme polynomial qui résout la version « décision » (attention, si on parcourt toutes les valeurs de 
D'autre part, si un algorithme trouve la valeur optimale du problème d'optimisation en un temps polynomial, alors le problème de décision peut être résolu en temps polynomial en comparant la valeur de la solution sortie par cet algorithme avec la valeur de k. Ainsi, les deux versions du problème sont de difficulté similaire.
Cet examen systématique peut être réalisé à l'aide d'un arbre d'exploration binaire tel celui représenté ci-contre (les triangles représentent des sous-arbres).
L'arbre se décrit en descendant depuis le sommet jusqu'au bas des triangles (les feuilles de l'arbre). Chaque case correspond à un unique parcours possible. En suivant les indications portées le long des arêtes de l'arbre, à chaque parcours correspond une suite de valeurs pour 
   formant un vecteur contenu. Il est alors possible de reporter dans chaque case la valeur totale et le poids total du contenu correspondant. Il ne reste plus qu'à éliminer les cases qui ne satisfont pas la contrainte, et à choisir parmi celles qui restent celle (ou une de celles) qui donne la plus grande valeur à la fonction objectif.
À chaque fois qu'un objet est ajouté à la liste des objets disponibles, un niveau s'ajoute à l'arbre d'exploration binaire, et le nombre de cases est multiplié par 2. L'exploration de l'arbre et le remplissage des cases ont donc un coût qui croît exponentiellement avec le nombre n d'objets.
Cette preuve de NP-complétude a été présentée par Michail G. Lagoudakis reprenant un article de Richard Karp et un article de J.E. Savage.
Comme pour la plupart des problèmes NP-complets, il peut être intéressant de trouver des solutions réalisables mais non optimales. De préférence avec une garantie sur l'écart entre la valeur de la solution trouvée et la valeur de la solution optimale.
On appelle efficacité d'un objet le rapport de sa valeur sur son poids. Plus la valeur de l'objet est importante par rapport à ce qu'il consomme, plus l'objet est efficace.
L'algorithme le plus simple est un algorithme glouton. L'idée est d'ajouter en priorité les objets les plus efficaces, jusqu'à saturation du sac :
   retournée par l'algorithme glouton peut être d'aussi mauvaise qualité que possible. Considérons par exemple que nous n'ayons que deux objets à placer dans le sac. Le premier a un profit de 2 et un poids de 1, le deuxième a un profit et un poids tous deux égaux à W. Le premier objet est le plus efficace, il sera choisi en premier et empêchera la prise du second, donnant ainsi une solution de valeur 1 alors que la solution optimale vaut W. Il existe donc des valeurs du problème pour lesquelles le rapport entre la solution trouvée et la solution optimale est aussi proche de zéro que possible.
Il existe d'autres algorithmes d'approximation pour le problème de sac à dos permettant d'avoir une solution garantie à une distance 
  . La complexité en temps de ces algorithmes est, en général, fonction de l'inverse de la qualité attendue ; par exemple 
Les méthodes métaheuristiques comme les algorithmes génétiques ou les optimisations basées sur des algorithmes de colonies de fourmis permettent d'obtenir une approximation raisonnable tout en évitant de monopoliser trop de ressources.
Les algorithmes génétiques sont souvent utilisés dans les problèmes d'optimisation difficiles comme celui du sac à dos. Ils sont relativement faciles à mettre en œuvre et permettent d'obtenir rapidement une solution satisfaisante même si la taille du problème est importante.
On génère une population d'individus dont les chromosomes symbolisent une solution du problème. La représentation d'un individu est binaire puisque chaque objet sera soit retenu, soit écarté du sac. Le nombre de bits dans le génome de chaque individu correspond au nombre d'objets disponibles.
L'optimisation suit les principes habituels de l'algorithme génétique. Les individus sont évalués puis les meilleurs sont retenus pour la reproduction. Selon l'évolution retenue, les opérateurs de reproduction peuvent être plus ou moins complexes (cross-over), des mutations peuvent également intervenir (remplacement d'un 0 par 1 ou l'inverse). On peut également décider de copier le meilleur individu pour la génération suivante (élitisme). Après un certain nombre de générations, la population tend vers un optimum, voire la solution exacte.
Ce concept a été utilisé pour résoudre le problème du sac à dos multidimensionnel où plusieurs contraintes doivent être satisfaites.
Les premiers algorithmes s'appuyaient sur l'idée de l'algorithme glouton : les fourmis sélectionnaient progressivement les objets les plus intéressants. Cette sélection peut varier mais se base toujours sur des traces de phéromones déposées par les fourmis et qui conditionnent les choix ultérieurs. Parmi les solutions proposées, on peut citer le dépôt de phéromone sur les meilleurs objets, le dépôt sur des paires d'objets insérés l'un après l'autre dans la solution ou encore l'ajout de phéromones sur des paires d'objets, indépendamment de l'ordre d'insertion.
Une synthèse réalisée par des chercheurs tunisiens et français a montré que l'algorithme qui consiste à laisser des traces sur les paires d'objets successivement sélectionnés s'avère moins efficace que les variantes qui se focalisent sur un objet ou des paires quelconques. Les améliorations restent toutefois possibles puisque ces algorithmes pourraient être combinés à d'autres métaheuristiques afin de s'approcher de la solution optimale.
Le problème du sac à dos, dans sa version classique, a été étudié en profondeur. Il existe donc de nombreuses méthodes aujourd'hui pour le résoudre. La plupart de ces méthodes correspondent à une version améliorée d'une des méthodes suivantes.
Le problème du sac à dos possède la propriété de sous-structure optimale, c'est-à-dire que l'on peut construire la solution optimale du problème à i variables à partir du problème à i-1 variables. Cette propriété permet d'utiliser une méthode de résolution par programmation dynamique.
les solutions optimales du problème à i-1 variables avec la même contenance c (c.-à-d. KP(i-1,c)), auxquelles on ajoute 
On construit alors un tableau T[i,c] contenant la valeur des solutions optimales de tout problème KP(i,c) de la manière suivante :
Une fois le tableau construit, il suffit de démarrer de la case de T[n,W] et de déduire l'état des objets en remontant jusqu'à une case T[0,*].
  . Cependant, en ajoutant un algorithme de type diviser pour régner, on peut ramener la consommation de mémoire à 
gourmand en mémoire (donc pas de résolution de problèmes de grande taille).Il est à noter que cet algorithme ne s’exécute pas en temps polynomial par rapport à la taille de l'entrée. En effet la complexité étant proportionnelle à la capacité du sac W elle est exponentielle par rapport à son codage. Si les poids des objets sont décimaux, cela oblige à multiplier les poids des objets et la capacité du sac afin de les rendre entiers. Cette opération peut alors rendre l'algorithme très lent.
Comme tout problème combinatoire, le problème de sac à dos peut être résolu à l'aide d'une procédure de séparation et d'évaluation (PSE). La fonction d'évaluation d'un nœud consiste souvent à résoudre le problème en variables continues (voir plus bas). L'implémentation proposée par Martello et Toth (1990) est devenue une référence. Elle se distingue par :
la complexité considérable du code source.L'avantage de cette méthode est la faible consommation de mémoire.
L'approche hybride n'est pas réellement une nouvelle méthode de résolution. Elle consiste simplement à combiner les deux méthodes précédentes afin d'en tirer tous les avantages. Typiquement, on va appliquer une PSE jusqu'à une profondeur de recherche où le sous-problème sera jugé assez petit pour pouvoir être résolu par programmation dynamique.
Les précurseurs de cette approche sont Plateau et Elkihel (1985), suivis par Martello et Toth (1990). Il y a eu d'autres améliorations depuis.
Le problème présenté jusqu'ici est, plus précisément, le problème de sac à dos en variables binaires (01KP). Il s'agit en fait d'une variante parmi d'autres. Cette section présente ces différentes variantes. Les particularités se font sur le domaine des variables, le nombre de valeurs des objets, le nombre de dimensions du sac, etc. Ces particularités peuvent aussi être combinées.
Le problème du sac à dos en variables continues (LKP) est obtenu en enlevant la contrainte d'intégrité sur les variables. C’est-à-dire que l'on s'autorise à ne prendre qu'une fraction des objets dans le sac à dos : 
On remarquera que la valeur de la solution optimale de LKP est au plus égale au double de la valeur de la solution optimale du problème KP correspondant : 
Dans le problème de sac à dos en variables entières, on considère que l'on a plusieurs exemplaires de chaque objet. Le problème consiste donc à trouver le nombre d'exemplaires à prendre pour chacun.
Si le nombre d'exemplaires est limité, on parlera de sac à dos borné (BKP), sinon on parlera de sac à dos non borné (UKP). Le problème BKP peut être transformé en 01KP sans difficulté.
On considère ici que le sac à dos a d dimensions, avec d > 0 (d-KP). Par exemple, on peut imaginer une boîte. Chaque objet a trois dimensions, et il ne faut déborder sur aucune des dimensions. La contrainte (1) est alors remplacée par :
En pratique, la version multidimensionnelle peut servir à modéliser et résoudre le problème du remplissage d'un container dont le volume et la charge maximale sont limitées.
Un autre exemple est celui de la gestion de personnel. Dans une version simplifiée, on estime la productivité ou la compétence de chaque personne (son « poids » dans le problème), et on lui attribue d'autres variables : son coût et sa disponibilité. Chacun de ces paramètres représente une dimension du sac à dos. On définit finalement les contraintes liées à son projet eu égard les paramètres précédents : le budget disponible et le temps imparti pour réaliser le travail. La résolution permet de déterminer quelles personnes doivent être retenues pour réaliser le projet.
Une variante du problème consiste, à partir d'objets ayant plusieurs valeurs, à maximiser plusieurs fonctions objectifs, c'est le problème du sac à dos multi-objectif (MOKP). On rentre donc dans le domaine de l'optimisation multi-objectif.
Par exemple, supposons qu'on lance une société spécialisée dans les croisières. Pour se faire connaître, on décide d'inviter des gens célèbres à bord du plus beau bateau. Ce bateau ne peut supporter plus d'une tonne de passagers (ce sera la constante W). Chaque passager a une masse (wi), apporte de la publicité par sa popularité (pi1 : indice de popularité) et demande un salaire (pi2 : salaire négatif). On cherche naturellement à maximiser la publicité apportée et minimiser le salaire total à payer (maximiser le salaire négatif). De plus on veut avoir un maximum de gens sur le bateau (pi3 = 1). Il y a donc trois sommes à maximiser.
   : le bateau ne doit pas couler.D'une manière générale, on remplace la fonction objectif du problème initial par une famille de fonctions objectifs :
   supplémentaire lorsque deux objets (i et j) sont pris simultanément. Par exemple, disons qu'on souhaite maximiser la qualité du café lors d'une expédition avec un sac à dos. On peut comprendre qu'il est plus intéressant d'apporter une cuillère et un sucre plutôt qu'un seul des deux.
    {\displaystyle \max \left\{z(X)\right\}=\sum _{i=1}^{n}x_{i}p_{i}+\sum _{i=1}^{n}\sum _{j=1}^{n}x_{i}x_{j}g_{ij}}
La particularité du problème de la somme de sous-ensembles (en anglais : subset sum) est que la valeur et le poids des objets sont identiques (
  ). C'est un problème important du domaine de la cryptographie, utilisé dans plusieurs systèmes de génération de clé publique.
Dans le problème de sac à dos à choix multiple (MCKP), les objets sont regroupés en classes, et il ne faut prendre qu'un seul représentant pour chaque classe.
Par exemple, on est en train de confectionner votre boîte à outils. Si on a cinq clés à molette, on peut soit choisir la plus légère, afin de prendre un marteau performant, ou alors choisir la clé la plus performante et un marteau bas de gamme, ou alors faire un compromis. L'idée générale est qu'on ne peut pas prendre plus d'une clé, ni plus d'un marteau.
   l'ensemble des indices des objets appartenant à la classe j. On considère, bien entendu, qu'un objet n'appartient qu'à une unique classe. La formulation du problème devient :
Le problème de sac à dos multiple (MKP) consiste à répartir un ensemble d'objets dans plusieurs sacs à dos de capacités différentes. La valeur d'un objet dépend maintenant du sac dans lequel il est placé. Par exemple, on peut considérer qu'un euro a plus de valeur sur un compte d'épargne que sur un compte courant.
   (un objet n'est mis que dans un sac).Il existe une variante de ce problème dans laquelle tous les sacs ont la même capacité, on le note MKP-I.
(en) Hans Kellerer, Ulright Pferschy et David Pisinger, Knapsack Problems, Springer, 2004  (ISBN 3-540-40286-1).

Le procédé Bessemer est un procédé d'affinage de la fonte brute, aujourd’hui disparu, ayant servi à fabriquer de l'acier peu coûteux. Ce procédé porte le nom de son inventeur, Henry Bessemer, qui le brevette en 1855 et le perfectionne avec la Henry Bessemer and Company, société implantée à Sheffield, ville du Nord de l'Angleterre.
Le procédé consiste à oxyder avec de l'air les éléments chimiques indésirables contenus dans la fonte pour en obtenir du fer ou de l'acier. L'originalité du procédé consiste à exploiter la chaleur dégagée par les réactions chimiques pour maintenir la masse de métal en fusion. Après l'amélioration de Thomas (procédé Thomas ou « Bessemer basique »), le convertisseur devient un des moteurs de la révolution industrielle.
Avec la mise au point, au début du XXe siècle, de la liquéfaction des gaz, l'utilisation d'oxygène pur devient économique. Le procédé Bessemer est alors abandonné dans le milieu des années 1960 : même si l'allure générale du convertisseur est conservée, les performances et la conduite des convertisseurs modernes à l'oxygène ont peu de rapport avec celui de Bessemer.
Le haut fourneau étant un bas fourneau dont la combustion est attisée par un puissant courant d'air, les forgerons ont toujours essayé de convertir la fonte liquide produite par le haut fourneau en un métal proche de l'acier présent dans la loupe extraite du bas fourneau. Les Chinois, premiers utilisateurs du haut fourneau, affinaient la fonte en la brassant à l'air pendant plusieurs jours pour brûler le carbone dissous. Ce procédé appelé chǎo (chinois : 炒), ce qui signifie littéralement « saisir dans une poêle », a permis l'obtention de nombreux outils et armes en acier ou en fer.
Avec l'invention du haut fourneau au coke et du puddlage, la production de fonte et sa transformation en fer sont entrées dans l'ère industrielle. Ces procédés partagent la caractéristique d'opérer sur un matériau à l'état pâteux (comme le puddlage), ou liquide mais sans brassage métallurgique. Il en est de même pour la production d'acier, obtenue par recarburation du fer puddlé. Il faut non seulement réchauffer le matériau à haute température, mais aussi le maintenir chaud pendant une longue durée. Les besoins en combustibles sont élevés, alors que la productivité reste faible.
En 1847, le maître de forges américain William Kelly commence une série d'expériences pour décarburer la fonte liquide par l'emploi de l'air seul. En 1851, il expose les principes de l'affinage de la fonte à l'air à quelques maîtres de forges et métallurgistes locaux. Cependant si le procédé est théoriquement capable de convertir la fonte en fer, il n'est pas breveté et rien ne permet de prouver que les expériences menées ont abouti à l'obtention d'un métal convenablement décarburé.
Henry Bessemer est un ingénieur et inventeur anglais fécond. Dans les débuts des années 1850, il cherche à fondre ensemble du fer et de l'acier pour obtenir un acier sans devoir marteler ou forger une masse de métal. Pour ne pas polluer le métal avec le soufre contenu dans le combustible, il doit employer un four à réverbère, dans lequel il dose de la fonte et du fer pour obtenir de l'acier. Pour atteindre les températures nécessaires, il provoque un courant d'air à la surface du bain de métal liquide afin d'améliorer la combustion des fumées.
Le métal obtenu est d'excellente qualité. Bessemer parvient sans difficulté à convaincre l'empereur Napoléon III d'investir dans son procédé pour la fabrication d'acier à canon. Retournant à son laboratoire pour parfaire le procédé, il observe alors que, à la surface de la fonte liquide, se forment de fines plaques de fer décarburé. Ce fer, solide car sa température de fusion est supérieure à celle de la fonte, a été obtenu par la seule action de l'air « sans puddlage ou autre manipulation ». Il réoriente alors ses recherches pour mieux comprendre cette découverte.
Bessemer construit pour cette nouvelle étude un creuset de laboratoire avec une canne plongeant dans de la fonte grise en fusion pour y insuffler de l'air. Ce creuset est réchauffé par l'extérieur. Il obtient ainsi du fer liquide « sans aucune manipulation, mais non sans combustible ».
Il construit alors le premier convertisseur dans son laboratoire. Celui-ci consiste en un réservoir cylindrique, doté à sa base de 6 tuyères pour y insuffler l'air dans le métal en fusion. Il obtient du fer, bien que la violence de la réaction impose des précautions. En effet, au début du soufflage, la réaction est calme, le silicium s'oxydant pour former un laitier acide qui flotte sur le bain. Mais après que tout le « silicium a été calmement consumé, l'oxygène se combine alors avec le carbone, provoquant un flux croissant d'étincelles et une grande flamme blanche » : la combustion du carbone crée en effet un grand volume de monoxyde de carbone qui brasse violemment le bain.
La violence de la réaction est telle que pour Bessemer, ce premier convertisseur est « condamné en tant qu'outil commercial, à cause des projections de laitier, et uniquement pour cette raison ». Il essaye un déflecteur constitué d'une plaque de fonte : celle-ci ne résiste pas à son premier essai. Il construit alors une chambre en réfractaire au-dessus du carnau pour collecter les particules incandescentes. Il ajoute sous le trou de coulée du convertisseur un réservoir tampon, servant à la fois de réceptacle de secours et de lieu de décantation si des morceaux de matériau réfractaire étaient mélangés au métal. Une fois plein, ce réservoir permet un remplissage contrôlé du moule situé sous lui.
Le procédé inventé par Bessemer soulève beaucoup de scepticisme : il propose de souffler de l'air froid dans une masse de fonte en fusion, et estime qu'à la fin du soufflage, le métal obtenu sera plus chaud qu'au début ! Le premier essai est pourtant une réussite.
Malgré ce succès, Bessemer se méfie encore de sa découverte. Cherchant un avis impartial, il sollicite discrètement George Rennie, président de la section mécanique de la British Association. Impressionné par l'essai que Bessemer réalise devant lui, celui-ci l'encourage à communiquer immédiatement sur son invention, sans attendre que les détails techniques soient mis au point. Rennie bouleverse l'ordre du jour du congrès de cette association renommée afin que le communiqué de Bessemer ait une publicité maximale. Le 13 août 1856, à Cheltenham quelques heures avant de lire son papier, il surprend l'échange entre un sidérurgiste gallois, M. Budd, et son ami William Clay :
La réplique fut : « Je suis désolé car je me suis spécialement engagé ce matin, sinon je l'aurais fait avec plaisir. »
« Oh vous devriez venir, insista M. Budd. Saviez-vous qu'un personnage arrive de Londres pour nous présenter un papier sur la fabrication de fer doux sans combustible ? Ha, ha, ha ! »
Le discours a un grand retentissement et est intégralement reproduit le lendemain dans The Times. Mais Bessemer a anticipé les performances de son procédé : il l'annonce comme capable de recycler des ferrailles et d'ôter le soufre du métal (ce qui lui permettrait de transformer la fonte au coke en un excellent fer affiné au charbon de bois). Il montre des échantillons démontrant la qualité du fer obtenu. Mais le procédé est loin d'être mature :
En effet, les quelques maîtres de forge qui ont acheté le droit de fabriquer de l'acier Bessemer constatent vite que le fer obtenu est souvent inapte aux applications les plus exigeantes : son ami William Clay le qualifie même de « pourri à chaud et de pourri à froid ». Pour étouffer l’affaire et se donner le temps de déterminer l’origine de cette mauvaise qualité, Bessemer rachète immédiatement les droits de ses associés. Mais les constats d'échec se multiplient et l'affaire s'ébruite jusque dans la presse. Recherchant soigneusement l'origine de la mauvaise qualité du fer obtenu avec certaines fontes, il constate que « la fonte brute britannique contient de façon abondante cet ennemi mortel, le phosphore ».
Le fer, lorsqu’il contient plus de 3 ‰ de phosphore, devient en effet cassant. Alors que le puddlage permet de déphosphorer la fonte, le procédé de Bessemer n’a aucune influence sur cet élément. Bessemer se résout à limiter son procédé à l'affinage de fontes suédoises au charbon de bois et de quelques fontes hématites anglaises, élaborées à partir de minerais sans phosphore. Le procédé reste cependant rentable car ces fontes, achetées 7 £ la tonne, donnent un acier valant 50 à 60 £ la tonne.
La déception qui suit cette annonce ne sera pas oubliée : lorsque Bessemer exposera, quelques années après, les avantages du traitement au ferromanganèse, Thomas Brown, un métallurgiste dont l'enthousiasme pour le procédé Bessemer fut douché par la difficulté de mettre au point un revêtement réfractaire économique, affirma publiquement « qu'il pensait que M. Bessemer faisait la même erreur sur les coûts de son procédé que celle qu'il fit à Cheltenham ».
Le convertisseur fixe mis au point par Bessemer est efficace, mais présente d'évidents inconvénients pratiques. En effet, pour éviter le bouchage des tuyères, le soufflage doit être maintenu tant que du métal est dans le convertisseur. Or les durées de remplissage et de vidange pouvant fluctuer, il n'est pas possible de garantir la durée du soufflage, et donc la température du métal liquide car l'essentiel de la chaleur est dissipé par le vent soufflé. De plus, il n'y a pas de possibilité de stopper rapidement l'opération en cas de problème sur la soufflerie ou une tuyère.
L'idée d'un convertisseur rotatif s'impose donc rapidement. Quant à sa forme caractéristique de cornue, elle est dictée par le procédé :
les formes concaves permettent un bon maintien du revêtement réfractaire : un réacteur chimique ovoïde est un compromis entre le cylindre des convertisseurs fixes et la sphère ;
le bec pointé de manière oblique permet de diriger les projections loin de la zone de travail. Il impose un remplissage en position inclinée ;
un basculement permet l'examen des tuyères. Le fond du convertisseur est accessible, ce qui permet d'adopter un fond rapidement remplaçable en cas de problème,.
une poche permet de gérer le remplissage des lingotières. Une quenouille permet d'obstruer son trou de vidange ;
une potence hydraulique pour manutentionner les poches, et une autre, manuelle, pour la gestion des lingots. Robustes mais ne permettant pas de desservir une grande surface, ces mécanismes seront souvent abandonnés au profit de ponts roulants ;
un mécanisme puissant et commandable à distance pour le basculement du convertisseur, etc.Une usine de démonstration est édifiée à Sheffield. Après deux ans de fonctionnement, elle est rentable. Aux maîtres de forges qui achètent le droit de fabriquer de l'acier Bessemer, en échange d'une redevance de dix shillings par tonne de métal produite, il propose les plans d'un outil parfaitement au point. Thomas Turner constate, une cinquantaine d'années après cette annonce, qu'il est « remarquable que non seulement la première conception, mais aussi les détails mécaniques du procédé sont tous issus du même esprit, et que l'invention a quitté les mains de son concepteur si aboutie qu'aucune amélioration, excepté des détails mineurs, n'a été depuis introduite ».
Tous les essais menés par Bessemer pour maîtriser la violence de la réaction ayant échoué, il en conclut que, pour obtenir une bonne décarburation, l'air soufflé doit faire mousser le métal de manière que « la combustion du carbone par l'oxygène se déroule dans des myriades de petites bulles instables ». Le panache de fumées et de projection s'avère finalement essentiel pour évaluer l'avancement de l'affinage. En 1895, Ledebur décrit ainsi la réaction dans un convertisseur industriel :
Pour l'opérateur, il est donc facile de distinguer les différentes phases du soufflage : l'oxydation du silicium et du manganèse produisant le laitier, il s'agit d'une réaction calme. Puis la combustion du carbone produit un gaz, le monoxyde de carbone : le bain va donc être violemment agité. Ce gaz étant combustible et chaud, il s'enflamme spontanément au contact de l'air. Enfin, la combustion du fer génère des fumées rousses, caractéristiques de l'oxyde de fer : prolonger alors le soufflage n'amène qu'à gaspiller le fer affiné.
Pour apporter un peu plus de rigueur au suivi du soufflage, des procédures sont également développées. Beaucoup d'aciéries utilisent un spectroscope,. Une autre méthode consiste à tremper dans le métal une tige de fer et à évaluer la couleur du laitier qui s'est solidifié dessus : tant qu'une couleur brunâtre est décelable, le métal n'est pas complètement décarburé. À l'inverse, un laitier noir avec des points brillants est caractéristique d'un métal décarburé et en cours de sur-soufflage.
Les oxydations du silicium, du manganèse, du carbone et du fer sont exothermiques. Mais il apparaît rapidement aux expérimentateurs que l'obtention d'un fer complètement affiné nécessite une fonte d'une qualité bien précise :
le silicium est le principal contributeur thermique : la combustion de 1 % de silicium élève la température du bain de 300 °C. C'est la teneur visée par les sidérurgistes américains, alors que les Anglais utilisent généralement des fontes à 2 % de silicium ;
la combustion du manganèse et du fer est trop peu exothermique pour avoir une quelconque influence ;
la combustion du carbone n'est vraiment exothermique qu'aux basses températures : la combustion de 1 % de cet élément n'élève la température d'un bain à 1 500 °C que de 6 °C. La faible contribution thermique du carbone est rapidement admise par tous, alors que Bessemer l'avait estimée fondamentale.Une teneur minimum en silicium est donc nécessaire pour que le soufflage puisse s'achever : cette nécessité précipite la disparition des hauts fourneaux à vent froid. Mais produire des fontes très riches en silicium n'a pas d’intérêt économique car cela entraîne une forte consommation de combustible au haut fourneau. De plus, trop de silicium prolonge le soufflage et entraîne une forte consommation de fer par le laitier,. Cette contrainte sur la qualité de la fonte a été un frein à son affinage directement à la sortie du haut fourneau : on a donc longtemps préféré préparer au cubilot une charge de fonte liquide à partir de gueuses soigneusement choisies. Mais cet outil, outre sa consommation en combustible, est un goulot vis-à-vis du convertisseur : il est abandonné dès que l'on peut mélanger la fonte de plusieurs hauts fourneaux pour en stabiliser la composition.
Bessemer avait initialement dirigé ses recherches vers l'obtention d'un acier de haute qualité pour la fabrication de canons. Les premiers essais de son procédé ont d’ailleurs donné un fer doux de qualité satisfaisante. La recherche d’une amélioration capable d’ôter le phosphore mobilise l’énergie de Bessemer pendant plusieurs années. Mais, après la dépense de plusieurs milliers de livres sterling en expérimentations diverses, il ne trouve pas de solution pour l'éliminer. Les essais d'enrichissement de l'air avec des éléments plus réactifs (avec, entre autres, le monoxyde de carbone et des hydrocarbures) pour rendre le procédé compatible avec les fontes phosphoreuses au coke, échouent tous.
Bessemer contourne cette difficulté par l'utilisation de fonte suédoise élaborée au charbon de bois, et fait adopter son procédé partout où des minerais dépourvus de phosphore sont disponibles.
Pourtant, le puddlage s’avère capable de retirer le phosphore d’une fonte moyennement phosphoreuse,, mais les réactions qui permettent ce résultat sont mal comprises,. La solution ne sera trouvée qu'en 1877 par Thomas, avec un revêtement et un laitier à base de chaux, au lieu de silice. Bessemer avait d’ailleurs réalisé ses premières expériences avec des briques réfractaires basiques, assemblées avec de l'argile. Le revêtement siliceux adopté par la suite ne lui permet pas d’obtenir un laitier où l’oxyde de phosphore soit stable.
Il était bien connu depuis le brevet de Josiah Marshall Heath en 1839, que l'addition de manganèse améliore significativement les propriétés des aciers : quatre brevets relatifs à l'addition de manganèse pour améliorer le fer de Bessemer sont d'ailleurs déposés dans les semaines qui suivent l'annonce de Cheltenham. Parmi ces brevets, trois sont du métallurgiste anglais Robert Forester Mushet, spécialiste des alliages d'acier et de manganèse. Sa méthode consiste à mener d'abord le soufflage jusqu'à son terme, puis à ajouter dans le convertisseur une quantité soigneusement calculée de carbone et de manganèse (généralement 10 % du poids de la charge), sous la forme de ferromanganèse. En effet, le ferromanganèse utilisé alors étant une fonte qui apporte beaucoup de carbone (sa teneur est de 4 % de carbone pour seulement 8 % de manganèse), l'effet du manganèse est accompagné d'une recarburation indésirable du métal.
Or le manganèse est un élément d'alliage dont l'intérêt est multiple. Tout d'abord, il supprime la fragilité à chaud due au soufre contenu dans le métal, en formant du sulfure de manganèse (MnS). Cet effet est bien connu des métallurgistes de l'époque, même s'il est encore mal expliqué. Il n'est donc pas brevetable et ne concerne pas Bessemer, qui se limite d'ailleurs à l'affinage de fontes de qualité exemptes de cet élément.
Le manganèse réagit aussi avec l'oxygène apporté par le soufflage, qui reste dissous dans le métal. Cet élément est soit combiné avec le carbone, sous la forme de monoxyde de carbone, soit combiné avec le fer, sous la forme d'oxyde de fer. Le monoxyde de carbone, très soluble dans le fer liquide, est rejeté par le métal au moment de la solidification, et forme alors des poches de gaz, les soufflures, qui affaiblissent considérablement les pièces moulées. L'oxyde de fer, typique des fortes teneurs en oxygène (cas de sursoufflage), forme des inclusions qui fragilisent également le fer.
Or le métal liquide issu du convertisseur contient beaucoup d'oxygène dissous. Même en évitant soigneusement tout sursoufflage, les soufflures se forment. Ces cavités sont typiques du procédé pneumatique de Bessemer et sont incompatibles avec l'obtention de lingots ou de pièces moulées de qualité. Mushet, dont le brevet a été inspiré par l'observation d'un acier Bessemer suroxydé, n'a pas remarqué qu'un dégagement de gaz au moment de la solidification est indissociable du nouveau procédé.
Pour Bessemer, qui cherche un réactif susceptible de s'associer avec l'oxygène sous une forme non gazeuse, les brevets de Mushet sont « un obstacle qui pourrait m'empêcher tout usage du manganèse dans mon procédé, dans n'importe quelle forme que ce métal puisse être ». Il cherche alors à limiter le soufflage en le stoppant dès que la bonne teneur en carbone est atteinte. Cette méthode ne donne cependant pas de résultats satisfaisants : ce dégagement de gaz persiste, l'acier bouillonne et est qualifié de « sauvage ». Il cherche alors, sans plus de succès, un autre élément capable de fixer l'oxygène dissous (de « calmer » l'acier), économique et qui ne dégrade pas la qualité du métal.
Finalement, avec la mise au point de ferromanganèses plus riches, il devient possible de combiner les avantages d'un soufflage partiel avec l'utilisation du manganèse. Bessemer, qui estime que le brevet de Mushet n'a pas de rapport avec le « calmage », utilise alors publiquement le ferromanganèse. L'acier obtenu est enfin d'excellente qualité, malléable à chaud et tenace à froid,, sans dégrader pour autant la compétitivité du procédé. Le silicium, meilleur désoxydant que le manganèse, peut être aussi utilisé, pour empêcher la formation de soufflures dans les aciers moulés. Mais une addition importante de silicium peut s'avérer néfaste s'il n'y a pas assez d'oxygène pour l'oxyder complètement, le silicium libre étant un élément d'alliage incompatible avec le forgeage,.
Bessemer s'emploie à démontrer qu'à sa découverte correspond une invention économiquement viable. Les déconvenues des premiers maîtres de forges ayant adopté son procédé lui ont clairement montré les limites de son convertisseur, à réfractaire siliceux. Il consacre deux années à identifier les limites de son procédé et à en améliorer chaque détail.
Il va recenser les minerais anglais sans phosphore, et faire en sorte que la fonte et le fer élaborés selon ses recommandations soient distingués des autres productions,. Il rachète les licences de ceux qui avaient investi dans son procédé et qui avaient échoué, faute de minerai convenable. Quant à lui, il se cantonne à l'affinage de fonte suédoise importée, laissant à ses licenciés la possibilité de transformer le minerai local en fer. Ceux-ci participent activement à la mise au point du procédé, comme le suédois Göransson qui, grâce à une subvention de 50 000 couronnes suédoises allouée par l'Académie royale des sciences de Suède pour mettre au point le procédé, devient le premier entrepreneur à utiliser industriellement le procédé Bessemer.
Jusqu'alors, l'acier, obtenu par la cémentation de fer, restait un matériau complexe à fabriquer. On l'obtenait en chauffant du fer produit par puddlage en contact avec du charbon de bois, pendant 6 semaines. Si la lenteur du puddlage comme de la cémentation garantissent la qualité du métal, le coût des combustibles pénalise sa diffusion. L'acier naturel comme l'acier au creuset partagent ce handicap : l'amélioration du procédé par Benjamin Huntsman ne concerne en effet que la qualité du métal produit. Quel que soit le procédé, la production reste aussi tributaire de la disponibilité du charbon de bois. Un tel matériau est donc coûteux : 50 à 60 £ la long ton (soit environ 3 390 à 4 070 £ en 2008).
Avec le convertisseur de Bessemer, l'utilisation nécessaire de ferromanganèse à la fin du soufflage recarbure le fer. L'obtention d'un fer malléable est donc difficile mais, avec un peu de rigueur, cette recarburation devient une opportunité pour transformer le fer en acier. En effet, si « le procédé n'est pas adapté à la production de fer doux, il devient, avec quelques modifications, capable de produire de l'acier de bonne qualité, avec une grande étendue de composition et de solidité ».
En outre, avec l'aide du colonel Eardley Wilmot, Bessemer insiste sur la qualité du fer coulé qui, contrairement au fer puddlé, ne dépend pas du cinglage puisqu'il est exempt de laitier. Désireux de prouver que son matériau est adapté aux exigences militaires les plus contraignantes, Bessemer propose même que les royalties versées par l'arsenal de Woolwich ne soient exigibles que si le procédé est nettement plus compétitif que les fours à réverbère employés jusqu'alors. Il explique au War Office que son procédé est 10 fois moins cher et produit des lingots homogènes de 5 tonnes au lieu de 25 kilos. Pourtant, échaudé par les échecs de quelques maîtres de forges, le successeur de Wilmot n'adoptera pas son procédé. Bessemer insiste en forgeant les premiers canons en acier, aux caractéristiques très supérieures aux canons en fer,.
Ces essais ne sont pas isolés et les avantages de l'acier Bessemer sont largement admis par l'ensemble des métallurgistes. L'industrialisation de ce métal est en route. Dès le début des années 1860, les métallurgistes français Gruner et Lan affirment sans détour qu'« il est bien évident pour nous que l'acier puddlé, ainsi que le fer puddlé aciéreux (fer à grains), ont fait leur temps. Ils vont céder le pas à l'acier Bessemer, comme ils ont remplacé eux-mêmes l'ancien acier de forge et l'acier cémenté plus ou moins corroyé. »
La supériorité de l'acier sur le fer a été démontrée à de nombreuses reprises : en jouant à la fois sur la teneur en carbone et sur les traitements thermiques, le métallurgiste peut obtenir un matériau plus dur, plus résistant tout en restant suffisamment ductile. En 1856, Mushet pose un rail en acier qui, après six ans, apparaît comme neuf là où des rails en fer ne dépassaient pas 3 mois. Quelques années après, Bessemer démontre que la durée de vie d'un rail en acier atteint 20 fois celle d'un rail en fer.
Mais Bessemer constate que les traitements thermiques de l'acier sont inconnus des industriels. La maîtrise de la teneur en carbone exige également un évident savoir-faire, même s'il est « bien connu que la soudure de grandes masses de fer puddlé engendre plus de risques et d'aléas dans son résultat que n'importe quelle étape du procédé Bessemer ».
Toutes ces incertitudes font que Gustave Eiffel construit sa tour en fer puddlé en 1889 alors que la supériorité de l'acier avait été généralement admise. Si elle avait été achevée, la tour de Watkin, d'architecture identique mais construite en acier, aurait dominé de 46 mètres le monument parisien.
À cause de son incapacité à déphosphorer, le convertisseur de Bessemer reste une invention marginale dans la plupart des pays. Bessemer va rechercher activement, mais sans succès, une méthode pour adapter le convertisseur à la déphosphoration.
Le revêtement mis au point par Thomas, consistant en de la dolomie cuite avec du goudron, ne semble pas révolutionner le procédé. Pourtant, la conduite du soufflage est différente : l'oxydation du phosphore étant très exothermique, il n'est plus nécessaire d'élaborer des fontes riches en silicium. La basicité du laitier favorise aussi la production de silice mais gêne celle de l'oxyde de manganèse. Mais il n'est plus possible d'interrompre l'élaboration au moment où la décarburation du métal devient suffisante car l'oxydation du phosphore ne se produit que tout à la fin du soufflage.
Le laitier de déphosphoration étant en outre un excellent engrais, la conduite du convertisseur est optimisée pour en garantir la qualité.
C'est surtout la rareté des minerais non phosphoreux, presque inexistants en France et en Allemagne, et minoritaires en Grande-Bretagne, qui marginalise le procédé Bessemer en Europe. Dans quatre pays les plus industrialisés (Allemagne, États-Unis, France et Grande-Bretagne), la proportion d'acier Thomas, proche de zéro en 1880, correspond à 62 % de la production Bessemer et Thomas en 1913, sachant que la production américaine connaît une croissance spectaculaire (3 380 kt en 1880 et 33 500 kt en 1913) sans utiliser le procédé Thomas.
Le procédé Bessemer ne va donc survivre que là où ne sont produites que des fontes sans phosphore. Il se maintient en Suède, où l'utilisation de convertisseurs fixes positionnés en sortie de haut fourneau s'est brièvement développée. Les États-Unis n'adopteront jamais le procédé Thomas, la fonte phosphoreuse y étant affinée par le procédé Martin-Siemens.
Si la miniaturisation ou l'amélioration de la tenue du fond du convertisseur inspirent beaucoup de brevets, ces améliorations de l'outil restent marginales, autant par leur aspect novateur que par leur généralisation. En revanche, la recherche sur le procédé lui-même va amener des idées plus radicales. Une idée consiste à utiliser un air de soufflage préchauffé. Le principe, simple en apparence et généralisé aux hauts fourneaux, n'est pourtant breveté qu'en 1935 et ne débouche sur aucune application pratique. Ce procédé, qui ne se justifie que pour l'affinage de fontes à la fois pauvres en phosphore et en silicium, est d'un intérêt marginal. Mais surtout, il est proposé au moment où l’intérêt économique du recyclage des ferrailles et les contraintes de qualité métallurgiques marginalisent le convertisseur Bessemer et ses dérivés.
L'intérêt d'enrichir de l'air de soufflage avec de l'oxygène est évident et n'échappe pas à Bessemer. L'affinage devient à la fois plus rapide et plus exothermique, ce qui permet de recycler des ferrailles avec le convertisseur. L'addition d'oxygène est donc essayée dès que celui-ci devient disponible à faible coût, grâce aux procédés de liquéfaction de l'air développés par Carl von Linde et Georges Claude. En 1925, à Oberhausen, un convertisseur Thomas fonctionne avec de l'air enrichi. En 1930, la première grande installation de liquéfaction de l'air permet à la Maximilianshütte à Sulzbach-Rosenberg de généraliser l'addition d'oxygène,. Après la guerre, le procédé est essayé plus largement en Europe. Celui-ci permet de doper la production des aciéries à un moment où il faut alimenter les usines construites grâce au plan Marshall.
L'azote dissous reste cependant un élément fragilisant pour le fer : l'oxygène pur est donc essayé dès les années 1920. Mais souffler avec un air contenant plus de 40 % d'oxygène présente des difficultés considérables. La réaction de l'oxygène pur avec la fonte mène à des températures de 2 500 °C à 2 700 °C : dans ces conditions, sans utilisation de tuyères refroidies, le fond d'un convertisseur est détruit en quelques heures. L'oxygène est alors dilué dans des gaz dont le craquage à haute température est endothermique : le dioxyde de carbone est essayé à Domnarvet en Suède en 1947-1949, la vapeur d'eau est essayée vers 1950 en France et en Allemagne, puis adoptée dans le Pays de Galles en 1958. Finalement, la mise au point de tuyères en cuivre, refroidies par une injection périphérique d'hydrocarbures dont le craquage est suffisamment endothermique pour éviter la fusion du cuivre, va achever l'évolution du convertisseur. Mais les procédés développés (LWS, OBM, Q-BOP…) ont alors atteint un niveau de productivité et de complexité sans aucun rapport avec le convertisseur développé par Bessemer.
(en) Henry Bessemer, Sir Henry Bessemer, F.R.S. An autobiography, 1905 (voir dans la bibliographie)
Adolf Ledebur, Manuel théorique et pratique de la métallurgie du fer, 1895 (voir dans la bibliographie)
(en) Hilary Bauerman, F.G.S, A treatise of the Metallurgy of Iron, 1890 (voir dans la bibliographie)
John Percy (trad. traduction supervisée par l'auteur), Traité complet de métallurgie, t. 4, Paris, Librairie polytechnique de Noblet et Baudry éditeur, 1865 (lire en ligne)
Adolf Ledebur (trad. Barbary de Langlade revu et annoté par F. Valton), Manuel théorique et pratique de la métallurgie du fer, Tome I et Tome II, Librairie polytechnique Baudry et Cie éditeur, 1895 (Tome 1 et Tome 2 à)
(en) Hilary Bauerman, F.G.S (ill. J. B. Jordan), A treatise of the Metallurgy of Iron, Londres, Crosby Lockwood and Son, coll. « Weale's scientific & technical series / Mining and metallurgy », 1890, 6e éd. (1re éd. 1868) (lire en ligne)
(en) Philip W. Bishop, The Beginnings of Cheap Steel, Project Gutenberg's, coll. « Contributions from The Museum of History and Technology », 1959 (lire en ligne), p. 27-47
(en) Thomas Turner (dir.), The metallurgy of iron: By Thomas Turner... : Being one of a series of treatises on metallurgy written by associates of the Royal school of mines, C. Griffin & company, limited, coll. « Griffin's metallurgical series », 1908, 3e éd., 463 p. (ISBN 1177692872 et 978-1177692878, lire en ligne)
(en) Robert Forester Mushet, The Bessemer-Mushet process or Manufacture of cheap steel, 31 mars 1883 (lire en ligne)
(en) Bradley Stoughton, Ph.B., B.S., The metallurgy of iron and steel, New York, McGraw-Hill Book Company, 1908, 509 p. (lire en ligne), p. 95, 101, 112
Procédé Thomas Portail de la production industrielle   Portail de la chimie   Portail de la métallurgie
Le procédé Manhès-David est un procédé d'affinage des mattes de cuivre inventé en 1880 par l'industriel français Pierre Manhès et son ingénieur Paul David. Inspiré du procédé Bessemer, il consiste en l'utilisation d'un convertisseur pour oxyder avec de l'air les éléments chimiques indésirables (essentiellement le fer et le soufre) contenus dans la matte afin de la transformer en cuivre. 
La quantité des éléments à oxyder, ainsi que la faible chaleur produite par les réactions chimiques, ont imposé des modifications du convertisseur. Manhès et David lui donnent la forme d'un cylindre, avec des tuyères alignées d'un bout à l'autre. Quelques années plus tard, les Américains William H. Peirce et Elias A. C. Smith le maçonnent avec un revêtement réfractaire basique, beaucoup plus durable que celui utilisé par les inventeurs français. Si cette amélioration ne modifie pas les principes du procédé, elle facilite son utilisation à grande échelle, accélérant le basculement de la production de cuivre du Royaume-Uni vers les États-Unis.
Au début du XXIe siècle, le convertisseur Peirce-Smith assure l'affinage de 90 % des mattes cuivreuses et intervient dans 60 % du nickel extrait. Ce convertisseur, comme l'addition d'oxygène pur, l'automatisation de la conduite, le traitement des fumées et la taille croissante des outils ont assuré la pérennité du procédé Manhès-David, même si les outils modernes n'ont plus guère de rapport avec leurs ancêtres.
Tout comme le fer produit par un haut fourneau en sort allié à d'autres éléments chimiques pour former la fonte, le cuivre extrait du minerai se présente sous la forme d'un alliage de cuivre, de soufre, de fer, etc. appelé matte. Appliquer les mêmes procédés à ces deux métaux est donc une démarche logique. Transposer le procédé Bessemer à la métallurgie du cuivre a été ainsi proposé, et le principe validé en 1866, soit dix ans après l'invention de Henry Bessemer, par l'ingénieur russe Semenikow. Pourtant, l'application pratique est plus complexe :
En effet, les mesures montrent que l'affinage de la fonte au convertisseur est possible car la combustion des éléments indésirables est fortement exothermique : l'oxydation du silicium et du carbone y produit respectivement 32,8  et   10,3 kilojoules par kilogramme. À l'inverse, si une matte de cuivre contient en abondance du fer et du soufre, il faut d'abord dissocier ces éléments, ce qui consomme 6,8 kilojoules par kilogramme de FeS, avant que leur oxydation, qui ne produit respectivement que 5,9  et   9,1 kJ/kg, puisse commencer,.
Les premiers affinages de matte cuivreuse par un convertisseur sont réalisés à Ducktown, dans le Tennessee, où A. Raht réalise de 1866 à 1875 un affinage partiel de la matte. En 1867, les Russes Jossa et Latelin essaient de valider expérimentalement les études de Semenikow. En 1870, ils stoppent leurs expériences après n’avoir parvenu qu'à élever la teneur en cuivre de leur matte de 31 % à 72-80 %.
En Angleterre, John Hollway approfondit ces essais jusqu'en 1878. Comme ses prédécesseurs, il constate que si le soufflage commence de manière satisfaisante, celui-ci devient de plus en plus intermittent au fur et mesure de l'avancement de l'affinage. Les obstacles relevés sont nombreux :
le poids des scories produites est égal à celui du cuivre, leur volume beaucoup plus important que celui du métal enfourné. Il faut donc vidanger régulièrement la cornue ;
la densité du métal en fusion évolue fortement (le cuivre ayant une densité triple des pyrites dont il est issu) ;
le revêtement réfractaire siliceux est absorbé par les scories, dans lequel il sert de fondant.Toutes les difficultés rencontrées ne peuvent être facilement résolues : le bilan thermique de la réaction d'affinage à l'air du cuivre n'est pas aussi favorable que pour le fer, la matte se solidifie au nez des tuyères avant d'être affinée. Même modifié, un convertisseur Bessemer n'est capable, au mieux, que d'ôter le fer et une partie du soufre. Hollway échoue, mais en publiant tous les détails de ses expériences, il identifie les problèmes essentiels.
Dans les années 1870, l'industriel français Pierre Manhès commence des essais avec un petit convertisseur Bessemer ordinaire de 50 kg, dans son usine de Vedène, puis aux usines d'Éguilles, près d'Avignon. Il cherche à affiner une matte à 25 à 30 % de cuivre préalablement fondue dans un creuset. Mais comme Hollway, il ne parvient pas à affiner complètement la matte. L'oxydation des éléments indésirables se produit comme prévu, mais l'opération est vite perturbée par l'apparition du cuivre métallique. En effet, la matte, qui est un composé ionique, est immiscible avec les scories, mais aussi avec le métal en fusion. Ce dernier, plus dense (ρcuivre ≈ 9), descend au fond du convertisseur et colmate les tuyères :
Pierre Manhès brevète alors l'utilisation d'additifs dont l'oxydation dégagerait suffisamment de chaleur pour éviter tout figeage. Finalement c'est le Français Paul David, alors ingénieur dans cette usine, qui, en 1880, suggère la solution. Il propose des tuyères horizontales, placées à une distance suffisante du fond du convertisseur, de telle sorte que le cuivre puisse se réunir au-dessous d'elles et que le vent souffle constamment dans la matte. En 1881, leur convertisseur est à la fois techniquement opérationnel et rentable.
Pendant l'automne 1884, le procédé est adopté aux États-Unis, par la Parrot Silver and Copper Company, à Butte. En 1885, A. J. Schumacher parvient à y réaliser un affinage complet dans une même cornue. Sa méthode est ensuite perfectionnée par C. O. Parsons et F. Klepetco à Great Falls dans le Montana. Enfin, Franklin Farrel, toujours aux Parrot Works, systématise le débouchage des tuyères par ringardage. La forme de la cornue y est perfectionnée et devient le standard jusqu'à ce que le convertisseur cylindrique la supplante.
Vers 1883-1884, au moment de l'adoption du procédé aux États-Unis, les deux inventeurs français étudient une évolution de la forme du convertisseur, qui devient un cylindre d'axe horizontal. L'alignement des tuyères sur une génératrice du cylindre fait qu'elles se situent toutes à la même immersion. La forme permet aussi de limiter la hauteur du bain, ce qui simplifie la conduite du procédé car la pression nécessaire pour souffler l'air est plus faible, et la couche de cuivre qui stagne au fond est moins épaisse. De plus, il est facile de corriger la profondeur d'immersion des tuyères en pivotant plus ou moins le convertisseur. Il est aussi possible de couler la matte par une ouverture située à une extrémité, près de l'axe de rotation, et de vidanger les scories par une autre ouverture latérale, diamétralement opposée aux tuyères,.
Le convertisseur cylindrique se généralise progressivement, sans toutefois faire disparaître rapidement les convertisseurs verticaux en forme de cornue,. Les deux types de convertisseurs deviennent de plus en plus gros, passant d'une capacité de 1 tonne à 8 tonnes en 1912, voire 15 tonnes pour les convertisseurs cylindriques en 1920.
Les scories s'enrichissant en oxyde de fer pendant le soufflage, elles deviennent basiques et se combinent alors avec le revêtement réfractaire siliceux, qui est très acide. Un revêtement basique ne serait pas attaqué et diminuerait donc le coût de production. L'adoption d'un revêtement inspiré de celui mis au point par Sidney Thomas et Percy Gilchrist en 1877 est suggérée par Hollway lors de ses derniers essais au début des années 1800. Mais l'idée n'est pas testée, des problèmes fondamentaux liés au soufflage étant alors plus prioritaires que l'optimisation du réfractaire. 
En 1890, un revêtement réfractaire basique est essayé sur un des convertisseurs Manhès-David de Parrot Smelter, à Butte, sous la direction de Herman A. Keller. Les essais ne débouchent pas sur un revêtement compatible avec une marche industrielle. Puis Ralph Baggaley, toujours dans le Montana, parvient, après de nombreux essais, à industrialiser en 1906 un revêtement basique à la Pittsmont Smelter… qui est abandonné en 1908 lorsqu'il quitte l'usine. Pour autant, le Norvégien Knudsen réussit dès 1903 à utiliser un revêtement basique à la Sulitjelma gruber : il y réalise deux soufflages successifs, d'abord dans un petit convertisseur au revêtement basique, puis dans un deuxième convertisseur classique, au revêtement acide.
Enfin, en 1909,, à la Baltimore Copper Company's Smelter, les Américains William H. Peirce et Elias A. C. Smith parviennent à traiter les principaux inconvénients des réfractaires basiques. Ceux-ci sont en effet plus fragiles, et surtout se dilatent et dissipent plus la chaleur que les réfractaires acides. En mettant au point un maçonnage adapté au convertisseur cylindrique et en augmentant la quantité de métal enfournée, ils résolvent les derniers problèmes,.
Le convertisseur de Peirce et Smith s'avère beaucoup plus avantageux que celui de Manhès et David. Le réfractaire basique, qui ne réagit pas avec les scories, dure beaucoup plus longtemps. Cette amélioration permet de s'affranchir des manutentions dues au remplacement des convertisseurs, d'éviter la construction d'installations de maçonnage et de convertisseurs de rechange (deux convertisseurs en maçonnage pour un en service en 1897 à l'Anaconda Copper), et de limiter les risques de percées dues à un mauvais contrôle de l'usure du réfractaire. La couche de réfractaire peut alors être plus fine, ce qui augmente la capacité du convertisseur. Celle-ci ne dépend pas de l'usure du réfractaire, ce qui simplifie la gestion des flux de métal en fusion dans l'usine.
Si la matière utilisée pour préparer le réfractaire acide contient du cuivre, voire de l'argent ou de l'or (fréquemment associés au cuivre dans les quartz aurifères), ces métaux rejoignent la matte au fur et à mesure de l'ablation du revêtement. Au vu de sa destruction rapide, l'avantage économique d'un réfractaire acide ne se conçoit donc que si sa consommation apporte de la valeur au procédé. Cette situation est cependant assez rare et, même dans ce contexte, une silice riche en métaux précieux peut être économiquement valorisée par d'autres moyens. Ainsi, en 1921, le réfractaire basique est considéré comme « le principal élément de la réduction des coûts dans la pyrométallurgie des minerais de cuivre ». Dans certains cas, une baisse du coût de la conversion, de 15-20 dollars à 4-5 dollars, a été rapportée.
Le procédé ne justifie son intérêt économique qu'avec des qualités précises de matte, appelées parfois « matte bronze ». En effet, si l'oxydation des éléments indésirables (essentiellement le fer) est exothermique, il est nécessaire d'apporter en proportion de la silice afin de limiter la viscosité des scories. Or la silice est enfournée froide et les scories n'ont pas de valeur, ces scories contenant de plus toujours un peu (0,5 % au début du XXIe siècle) de cuivre.
Pour un revêtement acide, la matte contient couramment 40 à 50 % de cuivre (soit respectivement 32 à 22 % de fer), mais il est possible, dans certains cas, de descendre jusqu'à 32 % de cuivre tout en restant rentable. Mais dans ce cas, la teneur en fer ne doit pas être trop élevée, pour éviter une consommation trop rapide du réfractaire.
Si, au début XXe siècle aux États-Unis, une matte est dite pauvre quand elle contient 45 à 50 % de cuivre, en Europe, « des mattes contenant de 15 à 35 % de cuivre y sont rentablement affinées au convertisseur », même si cela complique les opérations. Le soufflage est alors bien plus long et l'opération se déroule en plusieurs étapes. Un revêtement acide, dans ces conditions, ne dure que de 7 à 10 soufflages. Un revêtement basique s'avère beaucoup plus adapté : comme il n'est pas attaqué par les scories, il peut affiner des mattes très pauvres et le soufflage peut être optimisé pour diminuer la teneur en cuivre des scories. Depuis la fin du XXe siècle, la conversion d'une matte de 50 à 60 % de cuivre avec un convertisseur basique Peirce-Smith est courante.
Il est aussi possible d'affiner un « cuivre noir », contenant plus de 70 % de cuivre, du fer et un peu de soufre. Il est issu du recyclage de déchets cuivreux ou d'un traitement direct du minerai. Un tel composé métallique peut être utilisé pur ou mélangé avec la matte à divers stade du soufflage.
Quelques différences apparaissent selon que le revêtement réfractaire est acide (silice liée par de l'argile) ou basique (à base de magnésite).
Au début du XXe siècle, un convertisseur d'une capacité de 7 tonnes utilise 16 tonnes de réfractaire acide qui durent de 6 à 9 soufflages. La conception du convertisseur prend en compte les interventions fréquentes : il peut s'ouvrir en deux (partie inférieure et supérieure) pour que la régénération de la couche d'usure, en sable damé, ne dure qu'une heure et demie.
À la même époque, le revêtement basique, beaucoup plus performant, permet de réaliser des convertisseurs d'une capacité d'une quarantaine de tonnes, dont le revêtement en briques magnésiennes tient jusqu'à 75 coulées. Ce réfractaire, qui ne réagit pas avec les scories, ne génère pas de fondant acide : s'il ne se combine pas à la silice, les scories évoluent vers une masse infusible d'oxydes de fer. Manhès, Baggaley et d'autres, qui avaient essayé d'introduire du sable par les tuyères, avaient échoué car celui-ci n'a pas le temps de réagir avec les scories,. La difficulté est contournée en enfournant, avant la matte en fusion, quelques tonnes de sable.
Inadapté à la production en masse, le revêtement acide disparait progressivement au début du XXe siècle. En 1912, aux États-Unis, 80 % de l'affinage du cuivre est réalisé dans des convertisseurs revêtus de réfractaire basique, dont beaucoup ont été conçus pour un réfractaire acide. En 1925, le revêtement acide a disparu : « chaque livre de cuivre affinée dans le monde l'est dans un convertisseur revêtu de réfractaire basique ».
Au début du XXIe siècle, une usine moderne dispose de 2 à 5 convertisseurs, d'une capacité unitaire de 100  à   250 tonnes, transformant chacun 600  à   1 000 tonnes de matte en 500  à   900 tonnes de cuivre par jour. La durée de vie d'un convertisseur Peirce-Smith moderne se situe entre 50 000  et   90 000 tonnes (soit, en 1984, 100 à 200 jours). Le maçonnage dure une semaine. Les réfractaires les plus performants sont les briques en magnésie liée au chrome. En 2010, avec 250 convertisseurs opérationnels dans le monde, les convertisseurs cylindriques Peirce-Smith assurent 90 % de l'affinage des mattes cuivreuses.
Le principe fondamental du convertisseur Manhès-David est calqué sur celui de Bessemer. Il consiste à se débarrasser de tous les éléments indésirables plus oxydables que le cuivre. Le soufflage oxyde successivement le fer et le soufre, et doit être stoppé quand tous les éléments indésirables ont été oxydés.
Les réactions d'oxydation doivent être suffisamment exothermiques pour que la température reste supérieure à celle du métal en fusion. Mais il faut également que les scories soient suffisamment fluides. Pour cela, il faut qu'elles se combinent à un fondant, la silice. Celle-ci est apportée soit par le revêtement réfractaire du convertisseur (réfractaire acide), soit par un enfournement préalable d'une charge de sable dont le poids atteint, approximativement, 10 % du poids de la matte (réfractaire basique).
La matte à affiner est versée entre 900 °C et 1 200 °C. Suit alors une brève étape, caractérisée par une flamme rouge avec un épais panache de fumée blanche, où sont oxydés les constituants secondaires (carbone, arsenic, etc.).
Lorsque la flamme devient verte, l'oxydation ne concerne plus que le sulfure de fer(II) (FeS), qui se prolonge jusqu'à l'épuisement du fer. Alors que le fer s'oxyde en oxyde de fer(II) (FeO) et passe dans les scories, le soufre s'oxyde en dioxyde de soufre (SO2), qui sort du réacteur sous forme gazeuse :
L'oxydation du fer génère l'essentiel de la chaleur et des scories produites pendant l'affinage. Pendant tout ce temps, la température doit rester supérieure à 1 208 °C pour que la silice puisse se combiner au FeO :
Lorsque tout le fer a été oxydé, la flamme devient blanche avec une teinte bleue : elle correspond à l'oxydation du cuivre en présence de scories. En effet, à haute température, l'oxydation produit essentiellement des silicates de cuivre (Cu4SiO4) au lieu de réagir avec le soufre. On vidange donc les scories et on ajoute une charge de déchets cuivreux à recycler afin de refroidir la charge. À ce stade, le métal a atteint une teneur en cuivre d'environ 80 %. Il ne contient plus de fer, mais reste riche en soufre et en éléments difficilement oxydables : on appelle cet alliage intermédiaire « métal blanc » ou « matte blanche ».
Pendant cette dernière phase, la flamme devient plus petite, claire et peu éclairante, avec une teinte rougeâtre. La fin du soufflage ne se traduisant pas par une modification de la flamme, il est décidé à partir de prises d'échantillons. Pour autant, l'oxydation du cuivre à la fin du soufflage est importante : alors que les scories vidangées à l'issue du premier soufflage ne contiennent que 2 % de cuivre, les scories finales peuvent en contenir plus de 20 %. Même si elles sont alors avantageusement recyclé en amont de la filière, il vaut mieux limiter leur quantité en multipliant, si nécessaire, les vidanges, ou en visant une quantité maximale de cuivre en fin de soufflage par des appoints réguliers de matte.
Pendant tout le soufflage, le convertisseur est de plus en plus incliné pour éloigner les tuyères du fond. Celles-ci ayant quand même tendance à se colmater, il est nécessaire de les déboucher régulièrement au moyen d'une barre en acier. Le soufflage est donc une opération exigeante et, si on est en réfractaire acide, la gestion du revêtement s'ajoute à celle du métal en fusion. « Peut-être, qu'aucune opération métallurgique ne requiert un personnel plus expérimenté, et une surveillance plus attentive et globale, que le procédé de conversion. »
Le métal issu du convertisseur contient plus de 99 % de cuivre. Il est appelé « blister » (c'est-à-dire « cloque ») à cause des bulles de SO2 dissous qui s'y forment quand il se solidifie,.
En 1895, Carl von Linde réussit la liquéfaction de l'air. Indépendamment de cette démarche, Georges Claude met en service en 1905 un procédé industriel de liquéfaction de l'air. Leurs brevets autorisent la production industrielle, et surtout économique, de grandes quantités d'oxygène et d'azote liquide. Des entreprises naissent rapidement (Claude fonde Air liquide, alors que von Linde crée Linde AG et Praxair). Dans les années 1920, les sidérurgistes expériment l'enrichissement en oxygène de l'air soufflé dans les convertisseurs. Après la seconde Guerre mondiale, la méthode se généralise, jusqu'à la mise au point du convertisseur à l'oxygène, qui supplante rapidement les convertisseurs Thomas et Martin-Siemens.
L'enrichissement à l'oxygène avec un convertisseur Pierce-Smith est essayé plus tardivement, la première fois en 1966 à la Saganoseki Smelter, au Japon. L'addition d'oxygène facilite le bouclage énergétique du soufflage et rend le procédé plus flexible. La pratique se répand au début des années 1970. Pour autant, contrairement à la sidérurgie, aucun convertisseur à l'oxygène pur n'est mis au point même si la teneur en oxygène peut exceptionnellement atteindre 60 %. En effet, de très hauts niveaux d'enrichissement sont généralement improductifs : au-delà de 25 %, la durée de vie des réfractaires peut baisser. Dans ce cas, une injection périphérique d'azote aux tuyères permet de les ménager. Un fort enrichissement ne se justifie en fait qu'en début de soufflage, quand l'utilisation de l'oxygène est maximale (les tuyères sont alors profondes et l'abondance d'éléments oxydables protège le cuivre). En 2010, la teneur en O2 atteint couramment 30 % et la moitié des convertisseurs est opérée à plus de 29 % de O2.
L'injection d'oxygène accélère significativement les réactions chimiques. En début de soufflage, utiliser un air enrichi à 25-30 % d'O2 permet un gain de temps de 15 à 30 %. Vers le milieu du soufflage, l'enrichissement est progressivement arrêté. Sur l'ensemble de l'opération, la chaleur se dissipe moins et il est possible de fondre des additions cuivreuses (scories des fours à anodes, de scories de conversion, déchets cuivreux, etc.). Selon leur qualité, on compte 3  à   8 tonnes d'additions froides par tonne d'oxygène ajoutée. Pour éviter les surchauffes au nez des tuyères, on injecte aussi par les tuyères des minerais de cuivre concentrés en poudre,. En 2010, 10 à 15 % du blister est issu d'additions froides. 
Enfin, l'enrichissement du vent se traduit par des fumées à la fois moins abondantes et plus riches en SO2, ce qui facilite leur traitement. Avec 25,1 % d'oxygène, la teneur en SO2 des fumées d'un convertisseur avec une captation de fumées de type Hoboken passe de 8 à 10 %.
Les pyrites cuivreuses contiennent souvent du nickel. Les procédés d'extraction sont semblables à ceux du cuivre, et permettent d'obtenir une matte contenant environ 20 % de cuivre et 25 % de nickel. Ainsi « M. Manhès a aussitôt cherché à appliquer son procédé au traitement des minerais sulfurés et arséniés de nickel, et réussit à obtenir de bons résultats, au moyen de quelques modifications. »
En effet, au début du soufflage, une matte contenant du nickel s'oxyde comme une matte cuivreuse. Mais lorsqu'il reste dans la matte moins de 1 à 0,5 % de fer, le nickel commence à s’oxyder énergiquement, aussi vite que le soufre. Par ailleurs, l'oxydation du nickel étant peu exothermique, il n'est pas possible de finir le soufflage. On l'interrompt donc au stade de la matte blanche, qui ne contient alors presque plus de fer, mais reste riche en soufre. Si l'affinage d'une matte de nickel ne peut pas être aussi complet que celui d'une matte cuivreuse, le convertisseur reste pertinent : en 1896, Henri Moissan constate qu'il permet « en une seule opération, [de] passer d’une matte première à 16 % de nickel à une matte riche contenant 70 %, et cela en quelques heures, en évitant toute une série de grillages suivis de fusions, si longue et si dispendieuse. » La matte est facile à traiter, et les scories, qui contiennent alors 5 % de nickel, sont avantageusement recyclées.
Au début du XXIe siècle, l'efficacité du convertisseur du Peirce-Smith fait qu'il est universellement adopté dans l'affinage de la matte issue des minerais sulfureux de nickel (pentlandite). La teneur en nickel de la matte brute varie suivant les procédés : environ 30 % pour les mattes issues du four électrique, 40 % pour celles produites par fusion flash. On bascule le convertisseur, ou on injecte de l'azote, dès que le fer est suffisamment oxydé. Généralement, la matte affinée contient 40 à 70 % de nickel, et 20 % de soufre. La teneur finale en fer dépend des procédés avals : 1 à 4 % si on traite par lixiviation, 0,5 % si on traite par solidification lente ou électrolyse. Les scories issues du convertisseur sont presque systématiquement envoyées dans un four électrique qui les maintient en fusion, afin de décanter toutes les gouttes de métal en fusion qu'elles contiennent car l'agitation importante du bain dans le convertisseur ne permet pas une séparation parfaite entre scorie et matte. On ne recycle les scories que si elles contiennent plus de 0,6 % de nickel.
La température de travail, 1 400 °C, est plus haute que pour le cuivre. Le nickel se combine avec la silice pour former une scorie très pâteuse, qui colmate les tuyères. Le ringardage ne doit donc jamais s'interrompre, sinon tout est bouché en moins d'une minute. En 2011, l'injection d'oxygène se généralise pour les mêmes raisons qu'avec la production de cuivre.
En 2011, le procédé Manhès-David, en participant au traitement des minerais sulfureux de nickel, intervient dans 60 % de l'extraction du nickel. Quelques procédés consistent même à ajouter du soufre dans le ferronickel brut issu des latérites, afin d'en faire une matte compatible avec le procédé Manhès-David. Mais, comme pour le cuivre, le convertisseur est pénalisé par ses émissions de fumées polluantes. Ainsi, des procédés alternatifs d'affinage du nickel ont été développés. Par exemple, une fusion flash suffisamment oxydante peut donner directement une matte blanche. L'affinage en continu est aussi moins polluant que le convertisseur.
Avant même l'invention du procédé Manhès-David, le voisinage supportait mal les pluies acides dues au traitement des minerais de cuivre riches en soufre : les industriels américains qualifient alors de smoke farming (« agriculture des fumées »), l'activité des agriculteurs consistant à leur intenter des procès. Le traitement des fumées n'est alors guère pratiqué : « Plutôt que de subir un tel racket, les fondeurs trouvaient plus économique de, soit acheter les fermes voisines, soit les cultiver eux-mêmes, ou de les louer en se couvrant par une clause relative aux fumées. »
Au début du XXe siècle, l'apparition de grands complexes métallurgiques rend le traitement des fumées à la fois nécessaire et économiquement possible. Le procédé Manhès-David s'avère aussi polluant que le grillage et la fusion du minerai de cuivre, qui génèrent les mêmes fumées sulfureuses et chargées d'oxydes et de sulfate de zinc, de plomb, d'arsenic, etc. Toutes les fumées sont alors collectées et cheminent dans des carneaux, parfois longs de plusieurs kilomètres, dans lesquels les particules se déposent. La cheminée des usines atteint quelquefois des records de hauteur. Cela s'avérant encore insuffisant, les premiers filtres à manches et électrofiltres sont alors industrialisés.
La contrainte environnementale se durcit ensuite tout au long du xxe siècle. Des investissements importants s'imposent pour, à la fois, satisfaire aux normes et rester compétitif. Aux États-Unis, entre 1975 et 1987, la capacité de fusion a diminué de 36 %. En Europe, en 2001, 99 % du soufre est retraité. L'obtention d'un tel résultat représente un quart des coûts de transformation du minerai en cuivre. Parmi les outils utilisés, « le convertisseur reste le principal problème environnemental dans les fonderies de cuivre ». Il est en effet difficile à étancher, et son fonctionnement intermittent perturbe les installations de traitement des fumées.
Le traitement des fumées de convertisseur consiste généralement à un dépoussiérage, puis d'une transformation du dioxyde de soufre en acide sulfurique. Les étapes du traitement consistent alors en, successivement :
un lavage par aspersion d'un brouillard d'eau, qui absorbe les poussières fines ainsi que les gaz solubles (fluorures, chlorures, etc.) ;
la production d'acide sulfurique H2SO4, par hydratation du SO3.Une usine produit, selon la teneur en soufre de la matte, de 2,5  à   4 tonnes d'acide sulfurique par tonne de cuivre produite. Pour un traitement économiqueModèle:Rnote, la fumée captée doit être la plus riche possible en SO2,. En général, le procédé est géré pour que la concentration en SO2 de la fumée entrant dans l'unité de production d'acide ne soit jamais inférieure à 3 % (la concentration en SO2 se situe entre 8 et 12 % à l'intérieur d'un convertisseur Peirce-Smith moderne). L'aspiration des fumées est un point délicat : trop puissante, elle dilue la fumée et la refroidit, ce qui génère des condensats corrosifs vis-à-vis des installations de traitement.
La contrainte environnementale a motivé beaucoup d'améliorations du convertisseur. Par exemple, le convertisseur Hoboken, développé dans les années 1930, dispose d'une aspiration fixe, donc étanche. La teneur SO2 des fumées dépasse alors 12 %. Avec le captage des fumées, la conduite du procédé par l'observation des flammes devient une pratique complètement obsolète : la composition chimique de la fumée est analysée en permanence par des capteurs dédiés. Au-delà de ces améliorations, certains procédés alternatifs combinent la fusion et l'affinage dans un même réacteur, qui est directement inspiré du convertisseur, comme dans les procédés Noranda et Mitsubishi, développés à partir des années 1980. L'objectif est à la fois d'avoir un procédé continu et d'éviter à la matte en fusion de voyager hors du réacteur. Mais le remplacement est lent à cause de l'excellente efficacité chimique et opératoire du convertisseur Peirce-Smith.
À la fin du XVIe siècle, la Grande-Bretagne produit 75 % du cuivre mondial, essentiellement issu des mines corniques. Vers 1870, selon Paul Louis Weiss, elle en produit encore plus de la moitié, avec des minerais importés de toutes les parties du globe, principalement du Chili, d'Espagne et d'Allemagne. En effet, les procédés mis au point, comme le four à réverbère ou la flottation par moussage, sont tenus secrets et consolident le monopole britannique. Mais à partir de cette date, l'expansion rapide de l'extraction du cuivre en Amérique du Nord enlève aux métallurgistes britanniques leur prépondérance. En 1873, les Américains du Nord deviennent les premiers producteurs de cuivre du monde. En 1881, l'arrivée du chemin de fer à l'énorme gisement de Butte, donne aux compagnies américaines la domination mondiale de la production de cuivre.
Le procédé Manhès-David ne fait pas partie des améliorations britanniques sur le grillage et la fusion. Il complète une série d'innovations qui échappent à la Grande-Bretagne, comme le grillage sur sole tournante ou l'électrolyse, accélérant le basculement de la production vers le Nouveau Monde :
Ce basculement sur les volumes s'accompagne d'une évolution similaire sur les prix qui enterre définitivement les vieux procédés. Le XIXe siècle est en effet marqué par une érosion continue du prix du cuivre : 160 £ la tonne sur la première décennie, puis 130 £, 101 £, 94 £ et 88 £ sur la période 1841-1850. Bien que les secrets de la méthode galloise viennent d'être éventés par la chimie moderne, les fondeurs gallois parviennent à s'entendre pour maintenir les prix à ce niveau jusqu'au début des années 1870. De 1885 à 1887, le prix du cuivre atteint un bas historique, sous les 50 £ la tonne, malgré une forte demande attisée par les progrès de l'électricité et du téléphone. Les progrès techniques adoptés aux États-Unis (filière haut fourneau - convertisseur - raffinage par électrolyse) permettent aux nouveaux mineurs américains, canadiens et australiens de maintenir leur profits, même à ces cours.
La domination américaine va aussi se développer au détriment du Chili, pourtant doté de formidables gisements. En effet, la production de cuivre y baisse de moitié pendant la guerre du Pacifique (1879-1884), avant de s'interrompre presque complètement pendant la guerre civile de 1891 : les mineurs chiliens ne peuvent poursuivre leurs investissements. En Europe, la créativité des industriels, dont fait partie Pierre Manhès, ne peut compenser la rareté du minerai. Enfin, après l'échec du corner sur le cuivre de 1887, les industriels américains vont constituer plusieurs cartels qui, en générant une série de mouvements spéculatifs, vont déconnecter les cours du cuivre des coûts de production,.
L'appellation Manhès-David disparait dans le milieu des années 1920, en même temps que le convertisseur à revêtement acide. Le convertisseur Peirce-Smith qui le supplante ne présente pourtant pas de différence notable sur le procédé, mais il représente un tel progrès pour les fondeurs américains engagés dans une croissance capacitaire, que ceux-ci ne se réfèrent plus à Manhès et David. En outre, bien qu'ils reconnaissent volontiers la paternité de l'invention aux deux Français, ils ne voient aucun intérêt dans les améliorations postérieures proposées par David, comme le sélecteur. L'adoption du terme Peirce-Smith illustre le développement d'une filière typiquement américaine, fondée sur de nouveaux procédés et adaptée à la production de masse à partir de minerais plus pauvres,.
Mais, contrairement à la sidérurgie qui a abandonné les procédés Bessemer et Thomas pendant la seconde moitié du XXe siècle, le convertisseur Peirce-Smith n'a pas été remplacé par un procédé plus performant : en 2010, il représente encore 90 % de l'affinage des mattes cuivreuses. En effet, la sidérurgie a eu accès à des gisements de plus en plus riches au cours du XXe siècle, avec des teneurs en fer supérieure à 50 %. À l'inverse, l'extraction du cuivre, s'est faite à partir de gisements de plus en plus pauvres, la teneur moyenne en cuivre du minerai passant de 3 % au début du XXe siècle à 0,6 % au début du XXIe siècle,. Avec ces minerais, la pyrométallurgie ne représente maintenant plus que 10 % de l'énergie nécessaire à toute l'extraction du cuivre. L'effort de recherche s'est alors concentré sur les étapes amont de prétraitement du minerai.
Pour autant, le convertisseur Peirce-Smith a évolué. Dans la seconde moitié du XXe siècle des recherches ont été menées pour épauler l'opérateur dans la conduite du soufflage. Depuis, il n'est plus question de se fonder uniquement sur la couleur de la flamme pour déterminer l'avancement du soufflage. La composition de la fumée est analysée en permanence, la température est contrôlée par un pyromètre optique, installé dans une tuyère, sinon au bec du convertisseur. Des machines automatiques réalisent le débouchage des tuyères. 
Bien que les convertisseurs sont maintenant des outils géants, affinant souvent plus de 200 tonnes de matte à chaque soufflage, la modernisation de leur conduite les a rendu beaucoup plus flexibles. Par exemple, la silice enfournée peut être remplacée par du minerai concentré de cuivre, selon des quantités variant de quelques dizaines à plusieurs centaines de kilogrammes par tonne de matte. En modulant finement la teneur en oxygène du vent et en contrôlant le rapport FeO/SiO2 des scories, il est possible d'améliorer encore le rendement de l'opération, et de limiter les coûts de traitement des fumées et des scories. Ainsi, au début du XXIe siècle, à l'instar de beaucoup de procédés pyrométallurgiques, les convertisseurs Peirce-Smith évoluent essentiellement pour parfaire le procédé inventé par Pierre Manhès et Paul David.
(en) Donald M. Levy, Modern Copper Smelting, C. Griffin & company, limited, 1912 (lire en ligne) (voir dans la bibliographie)
(en) Marc E. Schlesinger, Matthew J. King, Kathryn C. Sole et William G. I. Davenport, Extractive Metallurgy of Copper, 2011 (voir dans la bibliographie)
(en) Edward Dyer Peters, Modern Copper Smelting, The Engineering and Mining Journal, 1905, 7e éd. (lire en ligne)
(en) Frank K. Krundwell, Michael S. Moats, Venkoba Ramachandran, Timothy G. Robinson et William G. Davenport, Extractive Metallurgy of Nickel, Cobalt and Platinum Group Metals, Elsevier, 2011, 610 p. (ISBN 978-0-08-096809-4, lire en ligne)
(en) Marc E. Schlesinger, Matthew J. King, Kathryn C. Sole et William G. I. Davenport, Extractive Metallurgy of Copper, Elsevier, 2011, 5e éd. (ISBN 978-0-08-096789-9, lire en ligne)
Paul Louis Weiss, Le Cuivre : Origine - Gisements - Propriétés - Métallurgie - Applications - Alliages, J.-B. Baillière et fils, 1894, 344 p. (ASIN B0019TU3SK, lire en ligne)
(prénom inconnu) Douglas, « L'aluminium et les produits nouveaux obtenus par le moyen de l'électricité », Bulletin de la Société de l'Industrie Minérale, 3e série, t. XV,‎ 1920, p. 558-560 (lire en ligne)
Paul Jannettaz, (prénom inconnu) Douglas et Paul David, « La bessemérisation des mattes cuivreuses », Bulletin de la Société de l'Industrie Minérale, 3e série, t. XV,‎ 1920, p. 560-573 (lire en ligne)
(en) Larry M. Southwick, « William Peirce and E.A. Cappelen Smith and Their Amazing Copper Converting Machine », JOM, The Mineral, Metals & Materials Society (TMS), vol. 60, no 10,‎ 2008 (lire en ligne)
Pierre Blazy et El-Aid Jdid, « Pyrométallurgie et électroraffinage du cuivre - Pyrométallurgie », dans Techniques de l'ingénieur, Éditions techniques de l'ingénieur, 10 décembre 2001 (lire en ligne)
(en) E. P. Mathewson, « Relative Elimination of Iron, Sulphur and Arsenic in Bessemerizing Copper-Mattes », Transactions of the American Institute of Mining Engineers,‎ 1907, p. 154-161 (lire en ligne)
Henri Moissan et Léon Victor René Ouvrard, Le Nickel, Gauthier-Villars et fils, Masson et Cie, 1896 (lire en ligne)
Matte (métallurgie) Portail de la production industrielle   Portail de la chimie   Portail de la métallurgie
Le procédé Thomas ou procédé Thomas-Gilchrist est un procédé historique d'affinage de la fonte brute, dérivé du convertisseur Bessemer. Il porte le nom de ses inventeurs qui le brevettent en 1877 : Sidney Gilchrist Thomas et son cousin Percy Carlyle Gilchrist. En permettant l'exploitation du minerai de fer phosphoreux, le plus abondant, ce procédé permet l'expansion rapide de la sidérurgie hors du Royaume-Uni et des États-Unis.
Le procédé diffère essentiellement de celui de Bessemer par le revêtement réfractaire du convertisseur. Celui-ci, en étant constitué de dolomie cuite avec du goudron, est basique, alors que celui de Bessemer, constitué de sable damé, est acide. Le phosphore, en migrant du fer vers les scories, permet à la fois l'obtention d'un métal de qualité satisfaisante, et de phosphates recherchés comme engrais.
Après avoir notamment favorisé la croissance spectaculaire de la sidérurgie lorraine, le procédé s'efface progressivement devant le convertisseur Siemens-Martin avant de disparaître vers le milieu des années 1960 : avec la mise au point de la liquéfaction des gaz, l'utilisation d'oxygène pur devient économique. Même si les convertisseurs modernes à l'oxygène pur opèrent tous avec un milieu basique, leurs performances comme leur conduite n'ont plus que peu de rapport avec leur ancêtre.
Lorsqu'il annonce la découverte de son procédé à Cheltenham, le 13 août 1856, Henry Bessemer affirme que le brassage simultané du laitier et du métal rend son procédé capable d'ôter « le soufre et toutes les matières volatiles qui s'attachent si intimement au fer ». Cependant, les quelques maîtres de forges qui ont acheté le droit de fabriquer de l'acier Bessemer constatent vite que le fer obtenu est souvent inapte aux applications les plus exigeantes, le qualifiant même de « pourri à chaud et de pourri à froid ». Pour étouffer l’affaire et se donner le temps de déterminer l’origine de cette mauvaise qualité, Bessemer rachète immédiatement les droits de ses associés. Mais les constats d'échec se multiplient et l'affaire s'ébruite jusque dans la presse. Recherchant soigneusement l'origine de la mauvaise qualité du fer obtenu avec certaines fontes, il constate que « la fonte brute britannique contient de façon abondante cet ennemi mortel, le phosphore ».
Le fer, lorsqu’il contient plus de 3 ‰ de phosphore, devient en effet cassant. Alors que le puddlage permet de déphosphorer la fonte, le procédé de Bessemer n’a aucune influence sur cet élément. La recherche d’une amélioration capable d’ôter le phosphore mobilise l’énergie de Bessemer pendant plusieurs années. Mais, après la dépense de plusieurs milliers de livres sterling en expérimentations diverses, il ne trouve pas de solution pour l'éliminer. Les essais d'enrichissement de l'air avec des éléments plus réactifs (avec, entre autres, le monoxyde de carbone et des hydrocarbures) pour rendre le procédé compatible avec les fontes phosphoreuses au coke, échouent tous.
Faute de solution, Bessemer se résout à limiter son procédé à l'affinage de fontes suédoises au charbon de bois et de quelques fontes hématites anglaises, élaborées à partir de minerais sans phosphore. Le procédé reste rentable car son efficacité compense le coût plus élevé des fontes de qualité. Il n'est guère prisé qu'aux États-Unis, en Grande-Bretagne et, dans une moindre mesure, en Suède, les seuls pays dont la production sidérurgique décolle.
À température ambiante, le phosphore s'oxyde pour former, en solution aqueuse, l'acide phosphorique H3PO4. L'anion orthophosphate PO43- se forme au cours de la réaction suivante :
une activité de l'anion PO43− faible : le composé formé doit donc être rapidement fixé par un cation comme le Ca2+, le Mn2+, le Mg+, le Na+…À haute température, la forme anhydre de cet oxyde acide est le pentoxyde de phosphore P2O5, qui se forme au cours de la réaction suivante :
Toutefois, comme le montre le diagramme d'Ellingham, cet oxyde de phosphore est décomposé par le fer suivant la réaction :
Si la déphosphoration au puddlage, qui opère sur du fer à relativement basse température, n'est que peu pénalisée par cette réaction, il n'en est pas de même dans un convertisseur. En effet, la température y est beaucoup plus élevée car, en fin de soufflage, elle correspond au moins à celle du fer en fusion (1 535 °C). Pour s'affranchir de cette réaction, il faut qu'un oxyde basique se combine avec le P2O5 dans le laitier dès sa création. On parvient ainsi à la fois à stabiliser le phosphore sous une forme oxydée, et à diminuer l'activité du P2O5 de façon à favoriser sa création.
2 P + 5 FeO + n CaO → n CaO.P2O5 + 5 Fe            avec n = 3 ou 4 en présence d'un excès de chaux,.
L'oxyde de phosphore est donc un acide qui ne peut subsister que s'il est fixé par une scorie basique. Un certain John Heaton réussit à partir de 1866 à déphosphorer de la fonte liquide en employant du nitrate de sodium mais « l'opération était si difficile à contrôler, et les coûts si importants que la méthode dut être abandonnée alors que de grands espoirs s'étaient fondés sur sa réussite ». Le métallurgiste français Gruner, qui recense les procédés de déphosphoration, rejette l'emploi d'alcalins qui, s'ils fluidifient le laitier et augmentent sa réactivité, dégradent les réfractaires. Les oxydes de calcium et de magnésium restent alors les seuls corps susceptibles de fixer économiquement le pentoxyde de phosphore.
Si on admet qu'un milieu oxydant et basique est nécessaire pour former un phosphate, il faut s'assurer que celui-ci le reste. La fonte est fortement oxydée par le procédé Bessemer, mais il faut attendre la disparition complète du silicium, du carbone et du manganèse pour disposer d'une activité en oxygène libre satisfaisante. Quant à l'obtention d'un milieu basique, elle est incompatible avec le garnissage du convertisseur Bessemer : la silice qu'il contient est immédiatement absorbée par le laitier basique, détruisant à la fois le réfractaire et la basicité du laitier.
En 1872, le métallurgiste Snelus brevette l'usage de la chaux pour constituer un laitier basique. Il expérimente un laitier synthétique à base de chaux et d'oxydes de fer, dans un convertisseur maçonné avec des briques en chaux magnésienne cuites à très haute température, et parvient à déphosphorer de manière satisfaisante quelques quintaux de fonte. Mais les briques utilisées sont coûteuses et Snelus communique peu sur ses essais,,. De plus, le caractère hygroscopique de ce produit le rend inutilisable dans les convertisseurs de l'époque.
Sidney Gilchrist Thomas est un jeune commis dans un tribunal qui a dû interrompre ses études à la mort de son père. Il suit cependant des cours du soir de chimie inorganique pendant 7 ans et, en 1875, commence à synthétiser les recherches relatives au problème de la déphosphoration de la fonte. En 1876, convaincu qu'un revêtement réfractaire basique est la clé du problème, il envisage de construire un creuset doté de tuyères et dont le revêtement réfractaire serait facilement modifiable. Les essais avec ce convertisseur primitif demandent cependant quelques moyens et du temps. Il s'associe alors avec son cousin Percy Carlyle Gilchrist, chimiste à la Blaenavon Ironworks, qui réalise et analyse les essais.
Ces essais sont suffisamment prometteurs pour justifier, le 2 avril 1877, le dépôt d'un premier brevet,. Ce premier convertisseur, d'une capacité de 3 kg, ne leur permet cependant pas de valider avec certitude un réfractaire efficace. Un nouveau convertisseur d'une capacité de 200 kg de métal est construit au début de l'année 1878 avec le soutien de la Blaenavon Ironworks, dont le directeur, Edward Martin, est venu s'associer aux deux cousins. Ce convertisseur permet de confirmer qu'un réfractaire constitué de chaux liée avec du silicate de sodium permet de mener un affinage déphosphorant la fonte. Il permet aussi de valider l'efficacité du réfractaire, qui tient une cinquantaine de soufflages, et de constater que le phosphore n'est éliminé qu'à la fin de l'affinage.
Mais le laitier, qui absorbe le phosphore, n'est obtenu que grâce à l'usure du revêtement réfractaire. Thomas et Gilchrist ont alors l'idée d'enfourner dans le convertisseur le laitier nécessaire en même temps que la fonte. L'idée est pertinente car « pour que le procédé ait un intérêt technique, la consommation de réfractaire doit être évitée par un gros ajout d'additions basiques, de manière à garantir un laitier basique dès les premiers instants du soufflage ». Mais ces additions, infusibles, ne peuvent être enfournées que froides : seul un convertisseur de taille industrielle est capable de réchauffer ce laitier synthétique. En outre, ces additions, en remplissant la cornue au détriment du métal, réduisent de plus de 20 % la productivité du convertisseur.
Des essais d'enfournement de chaux, réalisés sur un convertisseur de 6 tonnes à la Dowlais Iron and Co, confirment l'efficacité de la déphosphoration mais butent encore sur la réalisation d'un revêtement réfractaire robuste,. À l'usine de Bolckow Vaughan, une production industrielle démarre le 4 avril 1879, sur un convertisseur d'une capacité de 15 tonnes. Bien que ce convertisseur soit limité à 10 tonnes d'acier Thomas à cause du volume occupé par la chaux, il est le premier équipé d'un revêtement à base de dolomie, dont le coût comme la tenue donnent entière satisfaction, s'avérant même plus économique que le revêtement acide de Bessemer.
Les succès enregistrés à la Dowlais Iron and Co décident les deux cousins à publier leur invention. Ceux-ci préparent une intervention pour le congrès de l'Iron and Steel Institute de l'automne 1878, qui se déroule à Paris. Mais leur exposé est jugé sans intérêt et sa lecture est repoussée à la session suivante. Pourtant, le problème de la déphosphoration intéresse au plus haut point la communauté des sidérurgistes. L'introduction du renommé métallurgiste Isaac Lowthian Bell, qui commence le congrès en présentant ses recherches sur la déphosphoration, suscite un débat dans lequel Thomas intervient discrètement :
En attendant le congrès suivant, le procédé est encore perfectionné car le revêtement de chaux magnésienne essayé à la Dowlais Iron and Co a été abandonné au profit de briques de dolomie cuites avec du goudron. La rumeur se répand alors et « Middlesbrough est rapidement assiégée par les forces combinées de Belgique, France, Prusse, Autriche et d'Amérique ». Le congrès de l'Iron and Steel Institute du printemps 1879 consacre le succès des deux cousins et des maîtres de forges qui s'étaient associés à eux. Dès le mois d'août, des aciéries d'Europe continentale font part de leurs succès.
La dolomie est un carbonate de calcium et de magnésium de formule chimique CaMg(CO3)2. C'est donc une roche proche du calcaire où une partie du calcium est remplacée par du magnésium. Pour obtenir un matériau réfractaire stable, il est mélangé avec du goudron.
À la fin du XIXe siècle, sa mise en œuvre pour la sidérurgie obéit à des critères bien déterminés : 
La magnésie présente dans la dolomie favorise la déphosphoration car elle est plus basique que la chaux. L'oxyde de fer, bien que légèrement basique, est proscrit car il se comporte comme un fondant. Pratiquement, la dolomie n'est employée que si elle referme moins de 3 % d'oxyde de fer et moins de 2 % de silice.
Le convertisseur Thomas est très semblable à celui de Bessemer. Cependant, il doit être plus gros pour assimiler la charge de chaux. Le soufre est aussi partiellement retiré par le procédé, mais il est plus économique de désulfurer avec du manganèse que par un long sursoufflage.
Les opérations de désilication et de décarburation sont semblables à celles du procédé « acide » Bessemer ; seule la cinétique des réactions change, la basicité du laitier favorisant la production de silice mais gênant celle d'oxyde de manganèse,. L'oxydation du phosphore impose quelques opérations complémentaires : on verse préalablement dans la cornue une charge de chaux vive correspondant à 12 à 15 % du poids de la fonte, puis on enfourne la fonte. Enfin le soufflage est prolongé au-delà de la décarburation pour permettre la déphosphoration.
Pendant la déphosphoration, le phosphore oxydé migre dans le laitier. La forte élévation de la température s'explique par l'aspect exothermique de l'oxydation de cet élément. En effet, alors que la contribution thermique de la combustion du carbone, du fer et du manganèse n'est que de quelques degrés, la combustion de 1 % de silicium élève la température du bain de 300 °C et celle de 1 % de phosphore l'augmente de 183 °C. Mais au bout du compte, si le soufflage du procédé Thomas génère plus de chaleur que le procédé Bessemer, il faut tenir compte du fait qu'il faut y réchauffer une grande quantité de chaux.
Pour les sidérurgistes, il est donc essentiel de produire une fonte qui contienne un minimum de silicium, pour ne pas acidifier le laitier par la production de silice, et un maximum de phosphore, dont la combustion va garantir la réussite thermique de l'opération et la qualité du laitier. Les « fontes Thomas » contiennent donc idéalement moins de 1 % de silicium, alors que celles destinées au procédé Bessemer ont une teneur supérieure. La teneur en phosphore doit être supérieure à 2 %, ce qui exclut les fontes issues de minerais insuffisamment phosphoreux : les minerais américains, trop phosphoreux pour le procédé Bessemer acide et pas assez pour le procédé basique, entrent dans cette catégorie et ne peuvent être affinés qu'au four Martin-Siemens basique,.
La dernière étape, appelée « sursoufflage », doit être arrêtée dès que le phosphore est éliminé car elle correspond aussi au début de la combustion du fer, qui se manifeste par l'émission de fumées rousses. Or la fin de combustion du phosphore ne peut pas être identifiée par une modification du panache des fumées : seule la prise d'échantillons de métal, dont on analyse le faciès de leur fracture à froid, permet de confirmer la fin de la déphosphoration,.
Le revêtement réfractaire est performant mais l'opération le sollicite plus qu'avec le procédé Bessemer : au soufflage plus long et très exothermique au moment de la déphosphoration, il faut ajouter les basculements plus fréquents du convertisseur pour prendre un échantillon. En 1890, le fond d'un convertisseur dure en moyenne 14 soufflages, et le reste du convertisseur entre 40 et 70 soufflages.
Le phosphore ne s'éliminant qu'à la fin du soufflage, la combustion complète du carbone est donc un préalable à l'élimination du phosphore. Si l'on veut augmenter le taux de carbone de l'acier, il faut recarburer ensuite de manière contrôlée le métal en fusion. Cette recarburation doit se dérouler après la vidange du laitier pour éviter que le carbone, en réduisant l'oxyde de phosphore, ne libère cet élément.
Avant l'invention de Thomas, l'avènement du procédé Bessemer rendait les minerais sans phosphore (hématite) incontournables. Même en Grande-Bretagne, où ceux-ci restent assez abondants, leur prix double rapidement par rapport aux minerais phosphoreux. Outre cette considération, Bell insiste, dès 1870, sur la valeur que pourraient avoir les résidus de déphosphoration :
« Rompre l'association qui lie ensemble le [phosphore au fer] peut défier les compétences de nos meilleurs scientifiques. Mais il serait bon de rappeler que la production annuelle de fonte à partir du minerai de Cleveland contient à elle seule 30 000 tonnes d'un phosphore utile à l'agriculture. S'il était utilisé comme engrais sous la forme d'acide phosphorique, il vaudrait un quart de million, sachant que la différence de prix entre la fonte de Cleveland et une fonte hématite approche 4 millions, et cela uniquement à cause de ces 250 000 £ de phosphore »
L'acier obtenu par le procédé Thomas est séparé, par décantation, du laitier contenant les oxydes de phosphore. Les quantités produites sont importantes : le poids de laitier représente 25 % du métal (soit 70 % du volume étant donné sa plus faible densité). Cette scorie est utilisable comme engrais. L'acide phosphorique qui s'y trouve se présente essentiellement sous forme de phosphate tétracalcique (4CaO.P2O5) dont la majeure partie est assimilable par la végétation.
Humaniste et très préoccupé par les questions sociales, Thomas se dit « tourmenté par la question du laitier ». En 1883, le laitier issu des aciéries est encore inutilisé en Grande-Bretagne. En Allemagne, cependant, des essais d'épandage de scories, préalablement broyées, sont menés dès l'hiver 1882. Les sols allemands, sableux et acides, sont chaulés par la scorie basique, et la récolte suivante, dopée par le phosphore, s'avère excellente.
Ces résultats sont encore confidentiels. Thomas, malade, exhorte son cousin Gilchrist à se concentrer sur la valorisation des laitiers de convertisseur, allant jusqu'à pronostiquer que « quelle que puisse être risible l'idée, je suis convaincu qu'à la fin, en prenant en compte les coûts de production, l'acier sera le coproduit, et le phosphore le produit principal ».
En 1889, 700 000 tonnes de laitier riche en phosphore sont produites. Pour que ce laitier ait une valeur commerciale, sa teneur en phosphore doit être supérieure à 16 %. Beaucoup d'aciéries maximisent la teneur en phosphore en enfournant d'abord les deux tiers de la chaux nécessaire, le reste étant ajouté à la fin du soufflage, après avoir enlevé la scorie saturée en phosphore qui s'est formée jusqu'à ce moment. Une fois refroidie, un broyage fin est suffisant pour assurer l'assimilation par la végétation,.
Dans quatre pays les plus industrialisés à l'époque (Allemagne, États-Unis, France et Grande-Bretagne), la proportion d'acier Thomas, proche de zéro en 1880, correspond à 62 % de la production Bessemer et Thomas en 1913, sachant que la production américaine connaît une croissance spectaculaire (3 380 kt en 1880 et 33 500 kt en 1913) sans utiliser le procédé Thomas.
En 1880, le Royaume-Uni produit à lui seul 1,31 Mt d'acier liquide alors que quatre autres pays européens, Allemagne, Belgique, France et Luxembourg, n'en produisent que 1,22 Mt. En 1908, à la veille de la Première Guerre mondiale, ce pays a perdu sa position dominante. En effet, le procédé permet l'utilisation intensive de la minette lorraine, riche en phosphore (d'environ 0,5 à 1 % du minerai). En particulier, la production allemande d'acier, qui profite des gisements de la Lorraine annexée et de la cartellisation des industries, représente à cette date 11,2 Mt, contre 5,3 Mt pour le Royaume-Uni, la production au four Martin atteignant alors dans chacun des deux pays environ 4 Mt.
Le nouveau procédé a donc complètement modifié le poids industriel des nations, notamment en Europe. Compte tenu également du niveau des investissements, c'est aussi l'essor des grandes entreprises qui sont capables d'intégration verticale et/ou de spécialisation.
À l'intérieur des pays, la production métallurgique se concentre dans certaines régions. En France, de nombreux petits centres métallurgiques ruraux qui avaient survécu à l'arrivée et aux perfectionnements des hauts fourneaux au début du XIXe siècle disparaissent au profit notamment des industries de la Lorraine non annexée. En 1869, avant la guerre franco-allemande de 1870, les deux départements de Meurthe et de Moselle produisent 1,4 % de l'acier français ; en 1913 l'acier Thomas des usines de la seule Meurthe-et-Moselle correspond à 69 % de la production nationale. Cette évolution est aussi accentuée par la très forte amélioration des moyens de transports qui permettent la livraison des produits manufacturés loin des centres de production.
Suivant le minerai utilisé, chaque maître de forges adapte le procédé : on met au point des désilications préalables, des procédures pour évaluer l'avancement de la déphosphoration pendant le sursoufflage, des réfractaires basiques alternatifs, etc.. L'inconvénient majeur de l'acier Thomas est sa haute teneur en azote, qui fragilise le métal. De nombreuses études sont menées pour limiter l'entrée de cet élément, qui entre dans le métal pendant le sursoufflage. L'enrichissement en oxygène du vent soufflé est notamment vite essayé pour limiter les entrées d'azote.
Outre la diminution de la teneur en azote, l'intérêt thermique d'enrichir de l'air de soufflage avec de l'oxygène est évident. L'affinage devient à la fois plus rapide et plus exothermique, ce qui permet de recycler des ferrailles avec le convertisseur. L'addition d'oxygène se généralise donc dès que celui-ci devient disponible à faible coût, grâce aux procédés de liquéfaction de l'air développés par Carl von Linde et Georges Claude. En 1925, à Oberhausen, un convertisseur Thomas fonctionne avec de l'air enrichi. En 1930, la première grande installation de liquéfaction de l'air permet à la Maximilianshütte à Sulzbach-Rosenberg de généraliser l'addition d'oxygène,. Après la Seconde Guerre mondiale, le procédé est adopté plus largement en Europe de l'Ouest. Un air enrichi à 30 % d'oxygène permet de doper rapidement la production des aciéries à un moment où il faut alimenter les usines construites grâce au plan Marshall.
L'idée a cependant des limites, car souffler avec un air contenant plus de 40 % d'oxygène présente des difficultés considérables. La réaction de l'oxygène pur avec la fonte mène à des températures qui atteignent, au moment de la déphosphoration, 2 500 à 2 700 °C ; dans ces conditions, le fond d'un convertisseur est détruit en quelques heures. L'oxygène est alors dilué dans des gaz dont le craquage à haute température est endothermique. La vapeur d'eau, brévetée dès 1943 en Suède,, essayée vers 1950 en France et en Allemagne, puis adoptée dans le Pays de Galles en 1958 s'avère économique. Utilisée en fin de soufflage, elle permet de baisser la teneur finale en azote dans l'acier jusqu'à 0,001 %, tout en admettant une charge composée de 15 % de ferrailles. Ultime évolution du convertisseur Thomas, ce procédé, appelé VLN (pour Very Low Nitrogen), apparaît temporairement comme une alternative économique au procédé Martin-Siemens, qui produit alors 80 % de l'acier mondial.
Mais la vapeur d'eau amène de l'hydrogène dans le métal, ce qui le fragilise. Le dioxyde de carbone, essayé à Domnarvet en Suède en 1947-1949, est un refroidissant moins nocif mais cher. Au même moment, le convertisseur à l'oxygène pur commence à être enfin au point, avec l'apparition du procédé LD. Finalement, c'est la mise au point de tuyères en cuivre, refroidies par une injection périphérique d'hydrocarbures dont le craquage est suffisamment endothermique pour éviter la fusion du cuivre, qui va achever l'évolution du convertisseur Thomas.
Ces nouveaux convertisseurs à l'oxygène (LWS, OBM, Q-BOP…) ont alors atteint un niveau de productivité et de complexité sans aucun rapport avec le convertisseur développé par Bessemer. Mais les contraintes métallurgiques modernes imposent une déphosphoration poussée : l'usage combiné de réfractaires basiques et de chaux est systématique dans les convertisseurs à l'oxygène, quelle que soit la fonte employée.
(en) Henry Bessemer, Sir Henry Bessemer, F.R.S. An autobiography, 1905 [détail des éditions] (lire en ligne)
Adolf Ledebur, Manuel théorique et pratique de la métallurgie du fer, Tome I et Tome II (voir dans la bibliographie)
(en) Walter MacFarlane, The principles and practice of iron and steel manufacture (voir dans la bibliographie)
(en) R.W. Burnie, Memoir and letters of Sidney Gilchrist Thomas, Inventor (voir dans la bibliographie)
Adolf Ledebur (trad. Barbary de Langlade revu et annoté par F. Valton), Manuel théorique et pratique de la métallurgie du fer, Tome I et Tome II, Librairie polytechnique Baudry et Cie éditeur, 1895 [détail des éditions]
(en) G. Reginald Bashforth, The manufature of iron and steel, vol. 2 : Steel production, Londres, Chapman & Hall Ltd, 1951, 461 p. (lire en ligne)
(en) Thomas Turner (dir.), The metallurgy of iron: By Thomas Turner... : Being one of a series of treatises on metallurgy written by associates of the Royal school of mines, C. Griffin & company, limited, coll. « Griffin's metallurgical series », 1908, 3e éd., 463 p. (ISBN 1177692872 et 978-1177692878, lire en ligne)
(en) Walter MacFarlane, The principles and practice of iron and steel manufacture, Longmans, Green, and Co, 1917, 5e éd. (lire en ligne)
(en) R.W. Burnie, Memoir and letters of Sidney Gilchrist Thomas, Inventor, John Murray, 1891 (lire en ligne)
(en) William Tulloch Jeans, The Creators of the Age of Steel, 1884, 356 p. (ISBN 1417953810 et 978-1417953813, lire en ligne)
(en) Hermann Wedding (trad. from the german by: William B. Phillips, PH. D. & Ernst Prochaska), Wedding's basic Bessemer process [« Basische Bessemer - oder Thomas-Process »], New York Scientific Publishing Company, 1891, 224 p. (lire en ligne)
Convertisseur (métallurgie) Portail de la production industrielle   Portail de la chimie   Portail de la métallurgie
Le programme Apollo est le programme spatial de la NASA mené durant la période 1961 – 1975 qui a permis aux États-Unis d'envoyer pour la première fois des hommes sur la Lune. Il est lancé par le président  John F. Kennedy le 25 mai 1961, essentiellement pour reconquérir le prestige américain mis à mal par les succès de l'astronautique soviétique, à une époque où la guerre froide entre les deux superpuissances battait son plein.
Le programme avait pour objectif de poser un homme sur la Lune avant la fin de la décennie. Le 21 juillet 1969, cet objectif était atteint par deux des trois membres d'équipage de la mission Apollo 11, Neil Armstrong et Buzz Aldrin. Cinq autres missions se sont posées par la suite sur d'autres sites lunaires et y ont séjourné jusqu'à trois jours. Ces expéditions ont permis de rapporter 382 kilogrammes de roche lunaire et de mettre en place plusieurs batteries d'instruments scientifiques. Les astronautes ont effectué des observations in situ au cours d'excursions sur le sol lunaire d'une durée pouvant atteindre huit heures, assistés à partir d’Apollo 15 par un véhicule tout-terrain, le rover lunaire.
Aucun vol orbital américain n'avait encore été réalisé en mai 1961. Pour remplir l'objectif fixé par le président, la NASA lança plusieurs programmes destinés à préparer les futures expéditions lunaires : le programme Gemini pour mettre au point les techniques de vol spatial et des programmes de reconnaissance (programme Surveyor, Ranger…) pour, entre autres, cartographier les zones d'atterrissage et déterminer la consistance du sol lunaire. Pour atteindre la Lune, les responsables finirent par se rallier à la méthode audacieuse du rendez-vous en orbite lunaire, qui nécessitait de disposer de deux vaisseaux spatiaux dont le module lunaire destiné à l'atterrissage sur la Lune. La fusée géante de 3 000 tonnes Saturn V, capable de placer en orbite basse 140 tonnes, fut développée pour lancer les véhicules de l'expédition lunaire. Le programme drainera un budget considérable (169 milliards de dollars US actuels) et mobilisera jusqu'à 400 000 personnes. Deux accidents graves sont survenus au cours du projet : l'incendie au sol du vaisseau spatial Apollo 1 dont l'équipage périt brûlé et qui entraîna un report de près de deux ans du calendrier et l'explosion d'un réservoir à oxygène du vaisseau spatial Apollo 13 dont l'équipage survécut en utilisant le module lunaire comme vaisseau de secours.
Les missions lunaires ont permis d'avoir une meilleure connaissance de notre satellite naturel. Le programme Apollo a favorisé la diffusion d'innovations dans le domaine des sciences des matériaux et a contribué à l'essor de l'informatique ainsi que des méthodes de gestion de projet et de test. Les photos de la Terre, monde multicolore isolé dans un espace hostile, ainsi que celles de la Lune, monde gris et mort, ont favorisé une prise de conscience mondiale sur le caractère exceptionnel et fragile de notre planète. Le programme est à l'origine d'une scission dans la communauté scientifique et parmi les décideurs entre partisans d'une exploration robotique jugée plus efficace et ceux pour qui l'exploration humaine a une forte valeur symbolique, qui justifie son surcoût.
Durant les années 1950, la guerre froide bat son plein entre les États-Unis et l'Union soviétique, les deux superpuissances de l'époque. Celle-ci se traduit par des affrontements militaires indirects (guerre de Corée), et une course aux armements qui porte notamment sur le développement de missiles intercontinentaux porteurs de têtes militaires nucléaires capables d'atteindre le territoire national de l'adversaire. Les deux pays développent ces fusées en s'appuyant largement sur les travaux et l'expertise de savants et techniciens allemands qui ont mis au point le premier engin de ce type lors de la Seconde Guerre mondiale, la fusée V2. L'Union soviétique prend une certaine avance en réussissant en 1956 le premier tir d'un missile intercontinental, la R-7 Semiorka, ancêtre direct de la fusée Soyouz. Cette fusée de 267 tonnes est particulièrement puissante car elle doit emporter une bombe A pesant 5 tonnes. Les missiles américains à longue portée, développés plus tardivement, car conçus pour lancer des bombes H techniquement plus avancées et beaucoup plus légères (1,5 tonne), sont de taille plus réduite et sont encore en phase de mise au point à la fin des années 1950.
En juillet 1955, les États-Unis et l'URSS annoncent, chacun de leur côté, qu'ils lanceront un satellite artificiel dans le cadre des travaux scientifiques prévus pour l'Année géophysique internationale (juillet 1957—décembre 1958).
Début 1956, le concepteur de la Semiorka, Sergueï Korolev, réussit à convaincre les dirigeants soviétiques d'utiliser son missile comme lanceur spatial.
À la surprise générale, le 4 octobre 1957, l'Union soviétique est la première à placer en orbite le satellite Spoutnik 1. L'opinion internationale est fascinée par cet événement qui semble présager le début d'une nouvelle ère technique et scientifique. C'est un choc pour les responsables et l'opinion publique américains, jusqu'alors persuadés de leur supériorité technique. Les dirigeants soviétiques, d'abord surpris par l'impact de ce lancement, ne tardent pas à comprendre le prestige international que le régime peut retirer des succès de sa politique spatiale ; ils décident de se lancer dans un programme ambitieux.
À la même époque, le programme Vanguard, pendant américain du programme spatial russe lancé tardivement et trop ambitieux, enchaîne les échecs. L'équipe de Wernher von Braun parvient finalement à lancer le premier satellite américain, Explorer 1, le 1er février 1958 grâce au lanceur Juno I improvisé à partir d'un missile balistique Redstone. Mais la petite taille de la charge utile comparée à celle de Spoutnik semble confirmer l'avance soviétique. Bien que réticent à investir massivement dans le spatial civil, le président américain Dwight D. Eisenhower décide le 29 juillet 1958 de la création d'une agence spatiale civile, la NASA, qui doit permettre de fédérer les efforts américains pour mieux contrer les réussites soviétiques : la course à l'espace est lancée. La même année voit le début du programme Mercury qui doit permettre la mise en orbite des premières missions habitées américaines.
Mais les Soviétiques, qui disposent d'une avance importante et d'une fusée fiable pouvant emporter une grosse charge utile, continuent au cours des années suivantes de multiplier les premières : premier être vivant placé en orbite avec la chienne Laïka (Spoutnik 2), premier satellite à échapper à l'attraction terrestre (Luna 1), premier satellite à s'écraser sur la Lune (Luna 2), première photo de la face cachée de la Lune (Luna 3), premier être vivant à revenir vivant après un séjour dans l'espace (les chiens Belka et Strelka de Spoutnik 5), premier survol de Vénus (Venera 1).
Lorsqu'il arrive au pouvoir en janvier 1961, le président américain John F. Kennedy est, comme son prédécesseur, peu enclin à donner des moyens importants au programme spatial civil. Mais le lancement du premier homme dans l'espace par les Soviétiques (Youri Gagarine, 12 avril 1961) le convainc de la nécessité de disposer d'un programme spatial ambitieux pour récupérer le prestige international perdu. L'échec du débarquement de la baie des Cochons (avril 1961) destiné à renverser le régime de Fidel Castro installé à Cuba, qui écorne un peu plus l'image des États-Unis auprès des autres nations, contribue également sans doute à son changement de position.
John Kennedy demande à son vice-président Lyndon B. Johnson de lui désigner un objectif qui permettrait aux États-Unis de reprendre le leadership à l'Union soviétique. Parmi les pistes évoquées figurent la création d'un laboratoire spatial dans l'espace et un simple survol lunaire. Le vice-président, qui est un ardent partisan du programme spatial, lui répond que la recherche et l'industrie américaine ont la capacité d'envoyer une mission habitée sur la Lune et lui recommande de retenir cet objectif. Le 25 mai 1961, le président annonce devant le Congrès des États-Unis, lors du Special Message to the Congress on Urgent National Needs, le lancement d'un programme qui doit amener des astronautes américains sur le sol lunaire « avant la fin de la décennie »,. Il confirme sa décision dans un autre discours resté célèbre, « we choose to go to the Moon », le 12 septembre 1962.
La proposition du président reçoit un soutien enthousiaste des élus de tous les horizons politiques ainsi que de l'opinion publique, traumatisés par les succès de l'astronautique soviétique. Le premier budget du nouveau programme baptisé Apollo — nom choisi par Abe Silverstein à l'époque directeur des vols spatiaux habités, — est voté à l'unanimité par le Sénat américain. Les fonds alloués à la NASA vont passer de 400 millions de dollars en 1960 à 5,9 milliards de dollars en 1966, année de son budget le plus conséquent (environ 45 milliards valeur 2015). La NASA, grâce aux qualités manœuvrières de son administrateur James E. Webb, un vieux routier de la politique, put obtenir chaque année les fonds qu'elle souhaitait jusqu'au débarquement sur la Lune, même lorsque le soutien des élus commença à faiblir après 1963. James Webb sut en particulier s'assurer un appui solide auprès du président Lyndon B. Johnson qui avait succédé au président Kennedy assassiné en 1963.
Dès 1959 des études sont lancées au sein de l'agence spatiale américaine dans une perspective à long terme, sur la manière de poser un engin habité sur la Lune. Trois scénarios principaux se dégagent :
l'envoi direct d'un vaisseau sur la Lune (« Direct Ascent ») : une fusée de forte puissance, de type Nova, envoie le vaisseau complet ; celui-ci atterrit sur la Lune puis en décolle avant de retourner sur la Terre ;
le rendez-vous orbital autour de la Terre (EOR pour « Earth-Orbit Rendez-vous ») : pour limiter les risques et le coût de développement de la fusée Nova, les composants du vaisseau sont envoyés en orbite terrestre par deux ou plusieurs fusées moins puissantes. Ces différents éléments sont assemblés en orbite en utilisant éventuellement une station spatiale comme base arrière. Le déroulement du vol du vaisseau, par la suite, est similaire à celui du premier scénario ;
le rendez-vous en orbite lunaire (LOR pour « Lunar Orbital Rendez-vous ») : une seule fusée est requise mais le vaisseau spatial comporte deux sous-ensembles qui se séparent une fois que l'orbite lunaire est atteinte. Un module dit « lunaire » se pose sur la Lune avec deux des trois astronautes et en décolle pour ramener les astronautes jusqu'au module dit « de commande », resté en orbite autour de la Lune, qui prend en charge le retour des astronautes vers la Terre. Cette solution permet d'économiser du poids par rapport aux deux autres scénarios (beaucoup moins de combustible est nécessaire pour faire alunir puis décoller les hommes sur la Lune) et permet de concevoir un vaisseau destiné à sa mission proprement lunaire. En outre, la fusée à développer est moins puissante que celle requise par le premier scénario.
Lorsque le président américain John Kennedy donne à la NASA, en 1961, l'objectif de faire atterrir des hommes sur la Lune avant la fin de la décennie, l'évaluation de ces trois méthodes est encore peu avancée. L'agence spatiale manque d'éléments : elle n'a pas encore réalisé un seul véritable vol spatial habité (le premier vol orbital de la capsule Mercury n'a lieu qu'en septembre 1961). L'agence spatiale ne peut évaluer l'ampleur des difficultés soulevées par les rendez-vous entre engins spatiaux et elle ne maîtrise pas l'aptitude des astronautes à supporter de longs séjours dans l'espace et à y travailler ; ses lanceurs ont essuyé par ailleurs une série d'échecs qui l'incite à la prudence dans ses choix techniques.
Aussi, bien que le choix de la méthode conditionne les caractéristiques des véhicules spatiaux et des lanceurs à développer et que tout retard pris dans cette décision pèse sur l'échéance, la NASA va mettre plus d'un an, passé en études et en débats, avant que le scénario du LOR soit finalement retenu.
Au début de cette phase d'étude, la technique du rendez-vous en orbite lunaire (LOR) est la solution qui a le moins d'appui malgré les démonstrations détaillées de John C. Houbolt du Centre de Recherche de Langley, son plus ardent défenseur. Aux yeux de beaucoup de spécialistes et responsables de la NASA, le rendez-vous entre module lunaire et module de commande autour de la lune paraît instinctivement trop risqué : si les modules n'arrivent pas à se rejoindre en orbite lunaire, les astronautes occupant le module lunaire n'ont pas le recours de retourner vers la Terre contrairement aux autres scénarios ; ils sont alors condamnés à tourner indéfiniment autour de la Lune. Les avantages du LOR, en particulier le gain sur la masse à placer en orbite, ne sont pas appréciés à leur juste mesure. Toutefois, au fur et à mesure que les autres scénarios sont approfondis, le LOR gagne en crédibilité. Les partisans du vol direct — Max Faget et ses hommes du Centre des Vols Habités — se rendent compte de la difficulté de faire atterrir un vaisseau complet sur le sol lunaire accidenté et aux caractéristiques incertaines. Wernher von Braun, qui dirige l'équipe du Centre de vol spatial Marshall qui doit développer le lanceur et est partisan d'un rendez-vous orbital terrestre, finit lui-même par être convaincu que le LOR est le seul scénario qui permettra de respecter l'échéance fixée par le président Kennedy.
Au début de l'été 1962, alors que les principaux responsables de la NASA se sont tous convertis au LOR, ce scénario se heurte au veto de Jerome B. Wiesner, conseiller scientifique du président Kennedy. Le choix du LOR est finalement entériné le 7 novembre 1962. Dès juillet, onze sociétés aérospatiales américaines sont sollicitées pour la construction du module lunaire sur la base d'un cahier des charges sommaire.
Le 5 mai 1961, soit vingt jours avant le lancement du programme Apollo, l'astronaute Alan Shepard effectue le premier vol spatial américain (mission Mercury 3). En fait, il s'agit d'un simple vol suborbital car la fusée Mercury-Redstone utilisée (il n'y a pas d'autre lanceur disponible) n'a pas une puissance suffisante pour placer en orbite la petite capsule spatiale Mercury d'une masse un peu supérieure à une tonne. Le programme lunaire nécessite de pouvoir placer en orbite basse une charge utile de 120 tonnes. Le changement d'échelle qui en résulte est particulièrement important : la NASA va passer de la fusée de 30 tonnes qui a lancé Alan Shepard aux 3 000 tonnes de Saturn V qui nécessitera de développer des moteurs d'une puissance aujourd'hui inégalée ainsi que des technologies nouvelles comme l'utilisation de l'hydrogène liquide.
Les effectifs affectés au programme spatial civil vont croître en proportion. Entre 1960 et 1963, le nombre d'employés de la NASA passe de 10 000 à 36 000. Pour accueillir ses nouveaux effectifs et disposer d'installations adaptées au programme lunaire, la NASA crée trois nouveaux centres entièrement affectés au programme Apollo aux périmètres précisément délimités :
Le Manned Spacecraft Center (MSC), édifié en 1962 près de Houston au Texas, est destiné à la conception et la qualification des vaisseaux spatiaux (module lunaire et CSM), l'entraînement des astronautes et le suivi des missions à partir de leur décollage. Parmi les installations présentes sur le site, on trouve le centre de contrôle des missions, les simulateurs de vol et des équipements destinés à simuler les conditions spatiales et utilisés pour tester les livraisons des industriels. Le centre est dirigé par Robert Gilruth, ancien ingénieur de la NACA, qui joue un rôle de premier plan pour l'activité des vols habités américains depuis 1958. Contrairement aux deux autres établissements créés pour le programme Apollo, le MSC est activé dès le programme Gemini. Il emploie en 1964 15 000 personnes dont 10 000 employés de sociétés aérospatiales,.
Le Centre de vol spatial Marshall (George C. Marshall Space Flight Center ou MSFC) est une ancienne installation de l'Armée de Terre (Redstone Arsenal) située près de Huntsville dans l'Alabama transférée en 1960 à la NASA avec les spécialistes en majorité allemands de missiles balistiques dirigés par Wernher von Braun qui y travaillaient. Von Braun en restera le responsable jusqu'en 1970. Le centre est spécialisé dans la conception et la qualification des lanceurs de la famille Saturn. On y trouve des bancs d'essais, des bureaux d'étude et des installations d'assemblage. Les premiers exemplaires de la fusée Saturn I y sont construits avant que le reste de la production soit confié à l'industrie. Il emploiera jusqu'à 20 000 personnes,.
Le Centre spatial Kennedy (KSC), situé sur l'île Meritt en Floride, est le site d'où sont lancées les fusées géantes du programme Apollo. La NASA qui a besoin d'installations à l'échelle de la fusée Saturn V met en construction en 1963 cette nouvelle base de lancement qui jouxte celle de Cape Canaveral appartenant à l'Armée de l'Air américaine et d'où sont parties, jusqu'alors, toutes les missions habitées et les sondes spatiales de l'agence spatiale. Le centre effectue la qualification de la fusée assemblée (« all up ») et contrôle les opérations sur le lanceur jusqu'à son décollage. Il emploie en 1965 environ 20 000 personnes. Au cœur du centre spatial, le complexe de lancement 39 comporte deux aires de lancement et un immense bâtiment d'assemblage, le VAB (hauteur 140 mètres), dans lequel plusieurs fusées Saturn V peuvent être préparées en parallèle. Plusieurs plates-formes de lancement mobiles permettent de transporter la fusée Saturn assemblée jusqu'au site de lancement. Le premier lancement depuis le nouveau terrain est celui d’Apollo 4 en 1967. Jusqu'en 2011, le complexe était utilisé pour lancer la navette spatiale américaine,.
D'autres établissements de la NASA, jouent un rôle moins direct ou ne consacrent qu'une partie de leur activité au programme Apollo. En 1961, le Centre spatial John C. Stennis est édifié dans l'État du Mississippi. Le nouveau centre dispose de bancs d'essais utilisés pour tester les moteurs-fusées développés pour le programme. L'Ames Research Center est un centre de recherche ancien (1939) situé en Californie dont les souffleries sont utilisées pour mettre au point la forme de la capsule Apollo en vue de sa rentrée dans l'atmosphère terrestre. Le Langley Research Center (1914), situé à Hampton (Virginie) abrite également de nombreuses souffleries. Il a servi jusqu'en 1963 de siège au MSC et continue, par la suite, à abriter certains simulateurs du programme. Le Jet Propulsion Laboratory (1936), près de Los Angeles (Californie), est spécialisé dans le développement des sondes spatiales. C'est dans ce centre que sont conçues les familles de sondes spatiales qui vont permettre de reconnaître l'environnement lunaire (programme Surveyor, etc.).
Les principales entreprises de l'astronautique sont fortement impliquées dans le programme qui se traduit par un accroissement considérable des effectifs — le personnel affecté aux projets de la NASA passe durant cette période de 36 500 à 376 500 — et la construction d'établissements de grande taille. La société californienne North American, avionneur célèbre pour avoir construit les B-25 et le chasseur Mustang durant la Seconde Guerre mondiale, va jouer un rôle central dans le programme. L'arrêt et l'échec de plusieurs projets aéronautiques ont conduit son président à miser sur le développement de l'astronautique. La société s'est déjà distinguée dans le domaine en produisant l'avion fusée X-15. Pour le programme Apollo, la société fournit pratiquement tous les composants sensibles hormis le module lunaire qui est confié à la société Grumman implantée à Bethpage, Long Island (État de New York). La division moteur Rocketdyne de North American fabrique les deux principaux moteurs-fusées les J-2 et F-1 dans l'usine de Canoga Park, tandis que sa division Espace construit le deuxième étage de la Saturn V à Seal Beach et le module de commande et de service Apollo à Downey. L'incendie du vaisseau Apollo 1 et de nombreux problèmes rencontrés dans le développement du programme entraîneront la fusion de North American avec la société Rockwell Standard Corporation en 1967 ; le nouveau groupe développera dans les années 1970-1980 la navette spatiale américaine avant d'être absorbé en 1996 par Boeing. La société McDonnell Douglas construit le troisième étage de la Saturn V à Huntington Beach en Californie tandis que le premier étage est construit dans l'établissement de Michoud (Louisiane) de la NASA par la société Chrysler. Parmi les fournisseurs de premier plan figure le laboratoire des instruments du Massachusetts Institute of Technology (MIT) qui conçoit le système de pilotage et de navigation des deux vaisseaux habités Apollo.
Le projet Apollo a constitué un défi sans précédent sur le plan de la technique et de l'organisation : il fallait mettre au point un lanceur spatial dont le gigantisme générait des problèmes jamais rencontrés jusque-là, deux nouveaux moteurs innovants par leur puissance (F-1) ou leur technologie (J-2), des vaisseaux spatiaux d'une grande complexité avec une exigence de fiabilité élevée (probabilité de perte de l'équipage inférieure à 0,1 %) et un calendrier très tendu (huit ans entre le démarrage du programme Apollo et la date butoir fixée par le président Kennedy pour le premier atterrissage sur la Lune d'une mission habitée). Le programme a connu de nombreux déboires durant la phase de développement qui ont tous été résolus grâce à la mise à disposition de ressources financières exceptionnelles avec un point culminant en 1966 (5,5 % du budget fédéral alloué à la NASA), mais également une mobilisation des acteurs à tous les niveaux et la mise au point de méthodes organisationnelles (planification, gestion de crises, gestion de projet) qui ont fait école par la suite dans le monde de l'entreprise.
La mise au point du moteur F-1, d'architecture conventionnelle mais d'une puissance exceptionnelle (2,5 tonnes d'ergols brûlés par seconde) fut très longue à cause de problèmes d'instabilité au niveau de la chambre de combustion qui ne furent résolus qu'en combinant études empiriques (comme l'utilisation de petites charges explosives dans la chambre de combustion) et travaux de recherche fondamentale. Le deuxième étage de la fusée Saturn V, qui constituait déjà un tour de force technique du fait de la taille de son réservoir d'hydrogène, eut beaucoup de mal à faire face à la cure d'amaigrissement imposée par l'augmentation de la charge utile au fur et à mesure de son développement.
Mais les difficultés les plus importantes touchèrent les deux modules habités du programme : le CSM et le module lunaire Apollo. Le lancement du développement du module lunaire avait pris un an de retard à cause des atermoiements sur le scénario du débarquement lunaire. Il s'agissait d'un engin entièrement nouveau pour lequel aucune expérience antérieure ne pouvait être utilisée, par ailleurs très complexe du fait de son rôle. Les problèmes multiples — masse nettement supérieure aux prévisions initiales, difficulté de mise au point des logiciels indispensables à la mission, qualité déficiente, motorisation — entraînèrent des retards tellement importants qu'ils mirent à un moment en danger la tenue de l'échéance du programme tout entier,,,.
Les tests prennent une importance considérable dans le cadre du programme puisqu'ils représentent près de 50 % de la charge de travail totale. L'avancée de l'informatique permet pour la première fois dans un programme astronautique, de dérouler automatiquement la séquence des tests et l'enregistrement des mesures de centaines de paramètres (jusqu'à 1 000 pour un étage de la fusée Saturn V) ce qui permet aux ingénieurs de se concentrer sur l'interprétation des résultats et réduit la durée des phases de qualification. Chaque étage de la fusée Saturn V subit ainsi quatre séquences de test : un test sur le site du constructeur, deux sur le site du MSFC, avec et sans mise à feu avec des séquences de test par sous-système puis répétition du compte à rebours et un test d'intégration enfin au centre spatial Kennedy une fois la fusée assemblée.
Le premier groupe de sept astronautes sélectionnés pour le programme Mercury avait été recruté parmi les pilotes d'essais militaires ayant un diplôme de niveau minimum licence dans des domaines touchant à l'ingénierie, âgés de moins de 40 ans et satisfaisant une batterie de critères physiques et psychologiques. Les vagues de recrutement effectuées en 1962 (9 astronautes du groupe 2), 1963 (14 astronautes du groupe 3) et 1966 (15 astronautes du groupe 5) utilisent les mêmes critères de sélection en abaissant l'âge à 35 puis 34 ans, diminuant l'exigence en nombre d'heures de vol et élargissant la gamme des diplômes acceptés. En parallèle, deux groupes d'astronautes scientifiques détenteurs d'un doctorat sont recrutés en 1965 (groupe 4) et 1967 (groupe 6) dont un seul volera.
Les astronautes passent beaucoup de temps dans les simulateurs du CSM et du module lunaire mais reçoivent également, entre autres, des cours d'astronomie pour la navigation astronomique, de géologie pour les préparer à l'identification des roches lunaires et de photographie. Ils passent de nombreuses heures de vol sur des avions d'entraînement à réaction T-38 pour maintenir leur compétence de pilote (trois astronautes du groupe 3 se tueront en s'entraînant sur T-38). Ils sont impliqués très en amont dans le processus de conception et de mise au point des vaisseaux habités. Enfin, on leur demande de consacrer une partie de leur temps à des tâches de relations publiques qui se traduisent par des tournées dans les entreprises qui participent au projet. Deke Slayton joue un rôle officieux mais effectif de chef des astronautes en sélectionnant les équipages de chaque mission et défendant le point de vue des astronautes durant l'élaboration du projet et des missions.
Les véhicules spatiaux Apollo sont initialement conçus pour donner une autonomie complète à l'équipage en cas de coupure des communications avec le centre de contrôle à Terre. Cette autonomie procurée par les programmes du système de navigation et de pilotage sera dans les faits fortement réduite lorsque les procédures suivies par les missions Apollo seront figées : c'est le contrôle au sol à Houston qui fournira les principaux paramètres tels que la position du vaisseau spatial ainsi que le vecteur de la poussée avant chaque allumage des moteurs. Houston dispose au moment des premiers vols vers la Lune de moyens de calcul plus puissants et, grâce à la télémesure, connaît parfaitement la position des vaisseaux et leur trajectoire. Une fois une phase de vol engagée, c'est toutefois à l'ordinateur de bord d'appliquer les corrections nécessaires en se basant sur ses capteurs et ses capacités de calcul. Par ailleurs, l'ordinateur joue un rôle essentiel pour le contrôle des moteurs (fonction autopilote) et gère de nombreux sous-systèmes, ce qui lui vaut le surnom de quatrième homme de l'équipage. Sans l'ordinateur, les astronautes n'auraient pu poser le module lunaire sur la Lune car lui seul pouvait optimiser suffisamment la consommation de carburant pour se contenter des faibles marges disponibles.
La NASA est, dès le lancement du projet, très sensible aux problèmes de fiabilité. L'envoi d'astronautes sur le sol lunaire est une entreprise beaucoup plus risquée que les vols spatiaux autour de la Terre. Pour les missions en orbite terrestre, en cas d'incident grave, le retour est assuré relativement facilement par une brève poussée des rétrofusées. Par contre, une fois que le vaisseau a quitté l'orbite terrestre, un retour des astronautes sur Terre nécessite que les principaux sous-systèmes ne connaissent aucune défaillance. De manière assez empirique, la NASA avait déterminé que les composants du vaisseau devaient permettre d'atteindre une probabilité de succès de mission de 99 % tandis que la probabilité de perte de l'équipage devait être inférieure à 0,1 % en ne tenant pas compte des micro-météorites et des rayons cosmiques dont les effets étaient mal connus à l'époque,. L'architecture des sous-systèmes et la qualité des composants élémentaires des véhicules et du lanceur devaient donc respecter ces objectifs.
Des choix techniques garantissant une grande fiabilité sont retenus sur le module lunaire comme sur le module de commande et de service. Les ergols liquides utilisés par les moteurs sont hypergoliques, c'est-à-dire qu'ils s'enflamment spontanément quand ils sont mis en contact et ne sont pas à la merci d'un système d'allumage défaillant. Leur mise sous pression est effectuée classiquement grâce à de l'hélium supprimant le recours à une fragile turbopompe. Pour parvenir au taux de fiabilité visé sur les autres sous-systèmes, la NASA envisage d'abord de donner aux astronautes la possibilité de réparer les composants défaillants. Mais ce choix suppose de former les astronautes à des systèmes nombreux et complexes, d'emporter des outils et des pièces de rechange et de rendre accessibles les composants à réparer, ce qui les rend vulnérables à l'humidité et à la contamination. La NASA renonce à cette solution en 1964 et décide d'intégrer dans la conception du vaisseau des solutions de contournement permettant de pallier toute anomalie affectant un sous-système critique.
En cas de panne, des systèmes de secours prennent le relais dans un mode plus ou moins dégradé. Ainsi, le système de navigation du module lunaire (ordinateur et système inertiel) est doublé par un système de secours développé par un autre constructeur pour éviter qu'une même faille logicielle mette en panne les deux systèmes. Les quatre groupes de moteurs de contrôle d'attitude sont regroupés par paires indépendantes, chacune d'entre elles pouvant couvrir le besoin en mode dégradé. Le système de régulation thermique est doublé. Les circuits d'alimentation électrique sont également doublés. L'antenne de télécommunications en bande S peut être remplacée par deux antennes plus petites en cas de défaillance. Il n'y a néanmoins pas de parade à une panne de moteur : seuls des tests poussés avec un maximum de réalisme peuvent permettre d'atteindre le taux de fiabilité attendu. Des solutions techniques conservatrices mais éprouvées sont dans certains cas retenues. C'est le cas de l'énergie électrique sur le module lunaire (choix des batteries), des systèmes pyrotechniques (choix de systèmes existants standardisés et éprouvés) ainsi que l'électronique de bord (les circuits intégrés, bien qu'acceptés dans les ordinateurs, ne sont pas retenus pour le reste de l'électronique).
Selon Neil Armstrong, les responsables du projet avaient calculé qu'il y aurait environ 1 000 anomalies à chaque mission Apollo (fusée, CSM et LEM), chiffre extrapolé du nombre de composants et du taux de fiabilité exigé des constructeurs. Il y en aura en fait en moyenne 150, ce qu'Armstrong attribue à l'implication exceptionnellement forte des personnes ayant travaillé sur le projet.
Depuis Spoutnik 1, les dirigeants de l'Union soviétique et les responsables du programme spatial soviétique avaient toujours fait en sorte de maintenir leur avance sur le programme américain. Il ne faisait aucun doute dans l'esprit des dirigeants américains comme dans celui de l'opinion publique que l'URSS allait lancer son propre programme de vol habité vers la Lune et tenter de réussir avant les États-Unis pour conserver le prestige associé à leur domination durant la première phase de la course à l'espace. Néanmoins, après une déclaration publique en 1961 d'un dirigeant soviétique semblant relever le défi, aucune information officielle ne filtrera plus sur l'existence d'un programme lunaire habité soviétique au point de susciter le doute sur son existence chez certains représentants du congrès américain qui commencèrent, pour cette raison, à contester le budget alloué au programme Apollo à compter de 1963. Cependant, pour les dirigeants de la NASA, la menace d'une réussite soviétique exerça une pression constante sur le calendrier du programme Apollo : la décision de lancer la mission circumlunaire Apollo 8, alors que le vaisseau spatial Apollo n'était pas complètement qualifié, constituait une certaine prise de risque, qui avait été largement motivée par la crainte de se faire devancer par les Soviétiques. Certains indices contribuèrent par la suite à diminuer la pression sur les décideurs de la NASA dans la dernière ligne droite qui précéda le lancement d’Apollo 11. Au cours des années 1970, aucune information ne filtra sur la réalité du programme soviétique et dans l'atmosphère de désenchantement qui suivit la fin du programme Apollo, le célèbre journaliste américain Walter Cronkite annonça gravement à son public que l'argent dépensé pour celui-ci avait été gaspillé, car « les Russes n'avaient jamais été dans la course ». Ce n'est qu'avec la glasnost à la fin des années 1980 que commenceront à paraître quelques informations sur le sujet et il fallut attendre la chute de l'URSS pour que la réalité du programme lunaire soviétique soit reconnue par les dirigeants russes.
À compter du début des années 1960, le programme spatial habité soviétique, si performant jusque-là, tourne à la confusion. Sergueï Korolev, à l'origine des succès les plus éclatants de l'astronautique soviétique, commence à concevoir à cette époque la fusée géante N-1 pour laquelle il réclame le développement de moteurs cryogéniques performants (c'est-à-dire utilisant de l'hydrogène comme ceux en cours de développement chez les Américains) mais se heurte au refus de Valentin Glouchko qui possède un monopole sur la fabrication des moteurs-fusées. Aucun programme lunaire n'est lancé en 1961 car les responsables soviétiques sont persuadés que la NASA court à l'échec. Le premier secrétaire du PCUS Nikita Khrouchtchev demande en juin 1961 à son protégé Vladimir Tchelomeï, rival de Korolev, de développer un lanceur, le Proton et un vaisseau LK-1 (LK pour Lounnyï korabl' - Лунный корабль - vaisseau lunaire) en vue d'un vol habité circumlunaire. Korolev riposte en proposant une mission de débarquement lunaire basée sur un vaisseau concurrent, le Soyouz (Союз), apte à des rendez-vous en orbite et un module d'atterrissage L3. Constatant les progrès américains, Khrouchtchev décide finalement le 3 août 1964, avec trois ans de retard, de lancer les équipes soviétiques dans la course à la Lune : les programmes Proton (Прото́н) / Zond (Зонд, « sonde ») de survol de la Lune par une sonde inhabitée et N1-L3 de débarquement d’un cosmonaute sur la Lune de Korolev reçoivent alors le feu vert du Politburo. Toutefois, le limogeage de Khrouchtchev, remplacé par Léonid Brejnev à la tête du Parti communiste de l'URSS en octobre de la même année, se traduit par de nouveaux atermoiements et des problèmes dans la répartition des ressources budgétaires entre les deux programmes.
Gravement handicapé par la mort de Korolev en 1966 et par l'insuffisance des moyens financiers, le développement de la fusée N-1 rencontre des problèmes majeurs (4 vols, 4 échecs en 1969-1971) qui conduisent à son abandon le 2 mai 1974. C'est la fin des ambitions lunaires de l'URSS. Le lanceur Proton comme le vaisseau Soyouz après des débuts laborieux jouent aujourd'hui un rôle central dans le programme spatial russe.
Les principaux composants du programme Apollo sont la famille de lanceurs Saturn ainsi que les deux vaisseaux habités : le CSM et le module lunaire. Pour le séjour sur la Lune, un véhicule est développé ainsi qu'un ensemble d'instruments scientifiques, l'ALSEP.
Trois types de lanceurs sont développés dans le cadre du programme Apollo : Saturn I qui va permettre de confirmer la maîtrise du mélange LOX/LH2, Saturn IB utilisé pour les premiers tests du vaisseau Apollo en orbite terrestre et enfin, le lanceur lourd Saturn V dont les performances exceptionnelles et jamais dépassées depuis, permettront les missions lunaires.
Les débuts de la famille de lanceurs Saturn sont antérieurs au programme Apollo et à la création de la NASA. Début 1957, le Département de la Défense (DOD) américain identifie un besoin pour un lanceur lourd permettant de placer en orbite des satellites de reconnaissance et de télécommunications pesant jusqu'à 18 tonnes. À cette époque, les lanceurs américains les plus puissants en cours de développement peuvent tout au plus lancer 1,5 tonne en orbite basse car ils dérivent de missiles balistiques beaucoup plus légers que leurs homologues soviétiques. En 1957, Wernher von Braun et son équipe d'ingénieurs, venus comme lui d'Allemagne, travaillent à la mise au point des missiles intercontinentaux Redstone et Jupiter au sein de l'Army Ballistic Missile Agency (ABMA), un service de l'Armée de Terre situé à Huntsville (Alabama). Cette dernière lui demande de concevoir un lanceur permettant de répondre à la demande du DOD. Von Braun propose un engin, qu'il baptise Super-Jupiter, dont le premier étage, constitué de huit étages Redstone regroupés en fagot autour d'un étage Jupiter, fournit les 680 tonnes de poussée nécessaires pour lancer les satellites lourds. La course à l'espace, qui débute à la fin de 1957, décide le DOD, après examen de projets concurrents, à financer en août 1958 le développement de ce nouveau premier étage rebaptisé Juno V puis finalement Saturn (la planète située au-delà de Jupiter). Le lanceur utilise, à la demande du DOD, huit moteurs-fusées H-1 simple évolution du propulseur utilisé sur la fusée Jupiter, ce qui doit permettre une mise en service rapide.
Durant l'été 1958, la NASA, qui vient tout juste d'être créée, identifie le lanceur comme un composant clé de son programme spatial. Mais au début de 1959, le Département de la Défense décide d'arrêter ce programme coûteux dont les objectifs sont désormais couverts par d'autres lanceurs en développement. La NASA obtient le transfert en son sein du projet et des équipes de von Braun fin 1959 ; celui-ci est effectif au printemps 1960 et la nouvelle entité de la NASA prend le nom de Centre de vol spatial Marshall (George C. Marshall Space Flight Center MSFC).
La question des étages supérieurs du lanceur était jusque-là restée en suspens : l'utilisation d'étages de fusée existants, trop peu puissants et d'un diamètre trop faible, n'était pas satisfaisante. Fin 1959, un comité de la NASA travaille sur l'architecture des futurs lanceurs de la NASA. Son animateur, Abe Silverstein, responsable du centre de recherche Lewis et partisan de la propulsion par des moteurs utilisant le couple hydrogène/oxygène en cours d'expérimentation sur la fusée Atlas-Centaur, réussit à convaincre un von Braun réticent d'en doter les étages supérieurs de la fusée Saturn. Le comité identifie dans son rapport final six configurations de lanceur de puissance croissante (codés A1 à C3) permettant de répondre aux objectifs de la NASA tout en procédant à une mise au point progressive du modèle le plus puissant. Le centre Marshall étudie en parallèle à l'époque un lanceur hors normes capable d'envoyer une mission vers la Lune : cette fusée baptisée Nova, est dotée d'un premier étage fournissant 5 300 tonnes de poussée et est capable de lancer 81,6 tonnes sur une trajectoire interplanétaire.
Lorsque le président Kennedy accède au pouvoir au début de 1961, les configurations du lanceur Saturn sont toujours en cours de discussion, reflétant l'incertitude sur les missions futures du lanceur. Toutefois, dès juillet 1960, Rocketdyne, sélectionné par la NASA, avait démarré les études sur le moteur J-2 consommant hydrogène et oxygène et d'une poussée de 89 tonnes retenu pour propulser les étages supérieurs. Le même motoriste travaillait depuis 1956, initialement à la demande de l'armée de l'Air, sur l'énorme moteur F-1 (677 tonnes de poussée) retenu pour le premier étage. Fin 1961, la configuration du lanceur lourd (C-5 futurs Saturn V) est figée : le premier étage est propulsé par cinq F-1, le deuxième étage par cinq J-2 et le troisième par un J-2. L'énorme lanceur peut placer jusque 140 tonnes en orbite basse et envoyer 41 tonnes vers la Lune. Deux modèles moins puissants doivent être utilisés durant la première phase du projet :
la C-1 (ou Saturn I), utilisée pour tester des maquettes des vaisseaux Apollo, est constituée d'un premier étage propulsé par huit moteurs H-1 couronné d'un second étage propulsé par six RL-10 ;
la C-1B (ou Saturn IB), chargée de qualifier les vaisseaux Apollo sur l'orbite terrestre, est constituée du 1er étage de la S-1 couronné du troisième étage de la C-5.Fin 1962, le choix du scénario du rendez-vous en orbite lunaire (LOR) confirme le rôle du lanceurs Saturn V et entraîne l'arrêt des études sur le lanceur Nova.
Le véhicule spatial Apollo (ou module de commande et de service abrégé en CSM) transporte les astronautes à l'aller et au retour. Pesant plus de 30 tonnes, il est pratiquement dix fois plus lourd que le vaisseau Gemini. La masse supplémentaire (21,5 tonnes) est en grande partie représentée par le moteur et les ergols qui fournissent un delta-v de 2 800 m/s permettant au vaisseau de s'insérer en orbite lunaire puis de quitter cette orbite. Le vaisseau Apollo reprend une disposition inaugurée avec le vaisseau Gemini : un module de commande (CM) abrite l'équipage et un module de service (SM) contient le moteur de propulsion principal, l'essentiel des sources d'énergie ainsi que l’équipement nécessaire à la survie des astronautes. Le module de service est largué juste avant l'atterrissage.
Le module de commande Apollo est la partie dans laquelle les trois astronautes séjournent durant la mission, sauf lorsque deux d'entre eux descendent sur la Lune au moyen du module lunaire. Pesant 6,5 tonnes et de forme conique, sa structure externe comporte une double paroi : une enceinte constituée de tôles et nid d'abeilles à base d'aluminium qui renferme la zone pressurisée et un bouclier thermique qui recouvre la première paroi et dont l'épaisseur varie en fonction de l'exposition durant la rentrée atmosphérique. Le bouclier thermique est réalisé avec un matériau composite constitué de fibres de silice et microbilles de résine, dans une matrice de résine époxy. Ce matériau est inséré dans un nid d'abeille en acier.
L'espace pressurisé représente un volume de 6,5 m3. Les astronautes sont installés sur trois couchettes côte à côte parallèles au fond du cône et suspendues à des poutrelles partant du plancher et du plafond (la pointe du cône). En position allongée, les astronautes ont en face d'eux, suspendu au plafond, un panneau de commandes large de deux mètres et haut de un mètre présentant les principaux interrupteurs et voyants de contrôles. Les cadrans sont répartis en fonction du rôle de chaque membre d'équipage. Sur les parois latérales se trouvent des baies réservées à la navigation, d'autres panneaux de commande ainsi que des zones de stockage de nourriture et de déchets. Pour la navigation et le pilotage, les astronautes utilisent un télescope et un ordinateur qui exploite les données fournies par une centrale inertielle.
Le vaisseau dispose de deux écoutilles : l'une située à la pointe du cône comporte un tunnel et est utilisée pour passer dans le module lunaire lorsque celui-ci est amarré au vaisseau Apollo. L'autre placée sur la paroi latérale est utilisée à Terre pour pénétrer dans le vaisseau et dans l'espace pour les sorties extra véhiculaires (le vide est alors effectué dans la cabine car il n'y a pas de sas). Les astronautes disposent par ailleurs de cinq hublots pour effectuer des observations et réaliser les manœuvres de rendez-vous avec le module lunaire. Le module de commande dépend pour les principales manœuvres comme pour l'énergie et le support-vie du module de service. Il dispose de quatre grappes de petits moteurs d'orientation permettant les manœuvres lors de la rentrée. Celles-ci s'effectuent en orientant le module en roulis, la capsule ayant une incidence voisine de 25 à 30 degrés par rapport à son axe de symétrie. Cette incidence est obtenue par balourd statique de construction.
Le module de service (SM ou « Service Module » en anglais) est un cylindre d'aluminium non pressurisé de 5 mètres de long et 3,9 mètres de diamètre pesant 24 tonnes. Il est accouplé à la base du module de commande et la longue tuyère du moteur-fusée principal de 9 tonnes de poussée en dépasse de 2,5 mètres. Le module est organisé autour d'un cylindre central qui contient les réservoirs d'hélium servant à pressuriser les réservoirs d'ergols principaux ainsi que la partie haute du moteur principal. Autour de cette partie centrale, l'espace est découpé en six secteurs en forme de parts de gâteau. Quatre de ces secteurs abritent les réservoirs d'ergols (18,5 tonnes). Un secteur contient trois piles à combustible qui fournissent la puissance électrique et en sous-produit l'eau ainsi que les réservoirs d'hydrogène et d'oxygène qui les alimentent. L'oxygène est également utilisé pour renouveler l'atmosphère de la cabine. Un secteur reçoit des équipements qui ont varié en fonction des missions : appareils scientifiques, petit satellite, caméras, réservoir d'oxygène supplémentaire. Le module de service contient également les radiateurs qui dissipent l'excédent de chaleur du système électrique et qui régulent la température de la cabine. Quatre grappes de petits moteurs de contrôles d'attitude sont disposées sur le pourtour du cylindre. Une antenne comportant cinq petites paraboles, assurant les communications à grande distance, est déployée une fois le vaisseau lancé.
La tour de sauvetage est un dispositif destiné à éloigner le vaisseau spatial du lanceur Saturn V si celui-ci subit une défaillance durant les premières phases du vol. Le recours à des sièges éjectables, utilisé sur le vaisseau spatial Gemini, est exclu compte tenu du diamètre de la boule de feu que créerait l'explosion de la fusée Saturn V. La tour de sauvetage est constituée d'un propulseur à poudre situé au bout d'un treillis métallique lui-même perché au sommet du vaisseau Apollo. En cas d'incident, le moteur-fusée de la tour arrache le vaisseau de la fusée tandis qu'un petit propulseur l'écarte de la trajectoire de la fusée. La tour est alors larguée et le vaisseau entame sa descente en suivant une séquence similaire à celle d'un retour sur Terre. Si le lancement se déroule sans problème, la tour est éjectée lorsque le deuxième étage de la fusée Saturn est mis à feu,.
Le module lunaire comporte deux étages : un étage de descente permet d'atterrir sur la Lune et sert par ailleurs de plate-forme de lancement au deuxième étage, l'étage de remontée, qui ramène les astronautes au vaisseau Apollo en orbite à la fin de leur séjour sur la Lune. La structure du module lunaire est, pour l'essentiel, réalisée avec un alliage d'aluminium choisi pour sa légèreté. Les pièces sont généralement soudées entre elles mais parfois également rivetées.
Le corps de l'étage de descente, qui pèse plus de 10 tonnes, a la forme d'une boîte octogonale d'un diamètre de 4,12 mètres et d'une hauteur de 1,65 mètre. Sa structure, constituée de deux paires de panneaux parallèles assemblés en croix, délimite cinq compartiments carrés (dont un central) et quatre compartiments triangulaires. La fonction principale de l'étage de descente est d'amener le LEM sur la Lune. À cet effet, l'étage dispose d'un moteur fusée à la fois orientable et à poussée variable. La modulation de la poussée permet d'optimiser la trajectoire de descente mais surtout de poser en douceur le LEM qui s'est fortement allégé en consommant ses ergols. Le comburant, du peroxyde d'azote (5 tonnes), et le carburant, de l'aérozine 50 (3 tonnes), sont stockés dans quatre réservoirs placés dans les compartiments carrés situés aux quatre coins de la structure. Le moteur se trouve dans le compartiment carré central. Le deuxième rôle de l'étage de descente est de transporter tous les équipements et consommables qui peuvent être abandonnés sur la Lune à la fin du séjour, ce qui permet de limiter le poids de l'étage de remontée.
L'étage de remontée pèse environ 4,5 tonnes. Sa forme complexe, qui résulte d'une optimisation de l'espace occupé, lui donne l'allure d'une tête d'insecte. Il est essentiellement composé de la cabine pressurisée qui héberge deux astronautes dans un volume de 4,5 m3 et du moteur de remontée avec ses réservoirs d'ergols. La partie avant de la cabine pressurisée occupe la plus grande partie d'un cylindre de 2,34 mètres de diamètre et de 1,07 mètre de profondeur. C'est là que se tient l'équipage lorsqu'il n'est pas en excursion sur la Lune. Le pilote (à gauche face à l'avant) et le commandant de bord sont debout, tenus par des harnais qui les maintiennent en place en impesanteur et durant les phases d'accélération. Sur la cloison avant, chaque astronaute a devant lui un petit hublot triangulaire (0,18 m2) incliné vers le bas, qui lui permet d'observer le sol lunaire avec un bon angle de vision, ainsi que les principales commandes de vol et cadrans de contrôle regroupés par panneaux généralement dédiés à un sous-système. Les commandes et contrôles communs sont placés entre les deux astronautes (par exemple la console d'accès à l'ordinateur de navigation), certaines commandes sont doublées (commandes pilotant l'orientation et la poussée des moteurs), les autres commandes sont réparties en fonction des tâches assignées à chaque astronaute. Les panneaux de commandes et coupe-circuit se prolongent sur les parois latérales situées de part et d'autre des astronautes.
Le pilote a au-dessus de sa tête un petit hublot (0,07 m2) qui lui permet de contrôler la manœuvre de rendez-vous avec le module de commande. L'arrière de la cabine pressurisée est beaucoup plus exigu (1,37 × 1,42 m pour 1,52 m de haut) : son plancher est plus haut de 48 cm et, de plus, encombré par un capot recouvrant le sommet du moteur de remontée. Les parois latérales sont occupées par les rangements et à gauche, par une partie du système de contrôle environnemental. Au plafond se trouve l'écoutille utilisée pour passer dans le Module de Commande derrière laquelle se trouve un tunnel court (80 cm de diamètre pour 46 cm de long) comportant un système de verrouillage utilisé pour solidariser les deux vaisseaux. Les forces en jeu au moment de l'accostage qui pourraient déformer le tunnel sont amorties par des poutres qui les répercutent sur toute la structure.
Le LEM ne dispose pas de sas, qui aurait ajouté trop de poids. Pour descendre sur le sol lunaire, les astronautes font le vide dans la cabine et, à leur retour, ils pressurisent la cabine avec les réserves d'oxygène. Pour descendre, ils se glissent dans l'écoutille : celle-ci donne sur une petite plate-forme horizontale qui débouche sur l'échelle dont les barreaux sont situés de part et d'autre d'une des jambes de l'étage de descente.
Pour remplir la mission lunaire, la NASA dut concevoir plusieurs instruments scientifiques, équipements et véhicules destinés à être mis en œuvre sur le sol lunaire. Les principaux développements sont :
le rover lunaire, utilisé à partir de la mission Apollo 15, est un véhicule rustique tous-terrains à propulsion électrique, alimenté par des batteries. Pouvant atteindre la modeste vitesse de 14 km/h, il permet de porter le rayon d'action des astronautes de quelques centaines de mètres à une dizaine de kilomètres et dispose d'une capacité d'emport de 490 kg ;
l'ALSEP est un ensemble d'instruments scientifiques installé par les astronautes près de chaque site d'atterrissage à partir d’Apollo 12. Alimenté en énergie électrique par un générateur thermoélectrique à radioisotope (RTG) il comporte de quatre à sept instruments scientifiques dont la composition a varié selon les missions : sismomètre actif ou passif, spectromètre de masse, réflecteur laser, gravimètre, détecteur de poussière, etc. Ces instruments ont fourni en continu, jusqu'à leur arrêt en 1977, des informations sur l'atmosphère, le sol et le sous-sol lunaire : sismicité, vent solaire, température, composition de l'atmosphère, champ magnétique, etc ;
les combinaisons spatiales (modèle Apollo A7L) portées par les astronautes, d'une masse de 111 kg avec le système de survie, furent spécialement conçues pour les longues excursions sur le sol lunaire (plus de sept heures pour certaines sorties des équipages d’Apollo 15, 16 et 17) au cours desquelles les astronautes devaient se déplacer dans un environnement particulièrement hostile — températures extrêmes, micro-météorites, poussière lunaire — tout en effectuant de nombreux travaux nécessitant une certaine flexibilité.
Les six missions lunaires Apollo ont été programmées pour que le module lunaire atterrisse au tout début du jour lunaire (qui dure 28 jours terrestres). Les astronautes bénéficient ainsi d'une lumière rasante pour le repérage du terrain à l'atterrissage (entre 10 et 15° d'élévation au-dessus de l'horizon selon les missions) et de températures relativement modérées : la température au sol passe progressivement de 0 à 130 °C entre le lever du Soleil et le moment où le Soleil culmine au bout de 177 heures terrestres. Compte tenu de ces conditions, pour chaque lieu d'atterrissage, la fenêtre de lancement de la fusée Saturn était réduite à un jour par mois pour un site donné.
Le site retenu est toujours situé sur la face visible de la Terre pour que les communications entre le vaisseau et la Terre ne soient pas interrompues ; il n'est pas trop éloigné de la bande équatoriale de la Lune pour limiter la consommation de carburant que nécessiterait un déport du vaisseau vers des latitudes plus élevées.
La fusée décolle systématiquement depuis le Pad 39 du centre spatial Kennedy. Le lancement des 3 000 tonnes de la fusée est particulièrement spectaculaire : les cinq moteurs du premier étage sont allumés simultanément consommant 15 tonnes de carburant chaque seconde puis la fusée, qui est retenue par des pinces, est lâchée dès que les ordinateurs ont vérifié que la poussée des moteurs a atteint sa puissance nominale. La fusée s'élève d'abord très lentement, mettant près de dix secondes à se dégager de la tour de lancement. La séparation du premier étage S1-C intervient deux minutes et demie après le lancement à une altitude de 56 km alors que la fusée a atteint une vitesse de Mach 8 (10 000 km/h soit environ 2 800 m.s−1). Peu après, les moteurs-fusées du deuxième étage S-II s'allument : la jupe inter-étages se détache et la tour de sauvetage est éjectée car le vaisseau spatial est suffisamment haut pour pouvoir retomber sans son aide en cas d'interruption de la mission. Le deuxième étage est à son tour largué alors que la fusée atteint une vitesse de 24 680 km/h (soit environ 6 856 m/s) et une altitude de 185 km. Le troisième étage S-IVB est alors mis à contribution durant 140 secondes pour placer l'ensemble de la fusée restante sur une orbite circulaire de 180 km onze minutes et demie après le décollage.
Une fois placés en orbite basse, les vaisseaux Apollo (LEM et Module de Commande et de Service) ainsi que le troisième étage de la fusée effectuent une orbite et demi autour de la Terre puis le moteur du troisième étage est rallumé pour injecter l'ensemble sur une orbite de transfert vers la Lune (TransLunar Injection - TLI). L'injection se traduit par une augmentation de la vitesse de 3 040 m/s (10 944 km/h). Environ une demi-heure après la fin de la poussée, le Module de Commande et de Service (CSM) se détache du reste du train spatial puis pivote de 180° pour venir repêcher le LEM dans son carénage. Après avoir vérifié l'arrimage des deux vaisseaux et pressurisé le LEM, les astronautes déclenchent par pyrotechnie la détente de ressorts situés dans le carénage du LEM : ceux-ci écartent le LEM et le CSM du troisième étage de la fusée Saturn à une vitesse d'environ 30 cm/s. Le troisième étage va alors entamer une trajectoire divergente qui, selon les missions le place en orbite autour du Soleil ou l'envoie s'écraser sur la Lune.
Durant le trajet de 70 heures vers la Lune, des corrections peuvent être apportées à la trajectoire du CSM et du LEM pour optimiser la consommation finale de propergols. Initialement, le déroulement d’une mission Apollo prévoyait une quantité relativement importante de carburant pour ces manœuvres. À l'usage, à peine 5 % de cette quantité sera consommée grâce à la précision de la navigation. Le train spatial est mis en rotation lente pour limiter l'échauffement des vaisseaux en réduisant la durée de l'exposition continue au Soleil.
Une fois arrivé à proximité de la Lune, le moteur du CSM est allumé pour placer les vaisseaux en orbite en les freinant. Si ce freinage n'est pas réalisé, la trajectoire permet aux vaisseaux de revenir se placer en orbite terrestre après avoir fait le tour de la Lune sans utiliser leurs moteurs. Cette disposition sauvera d'ailleurs la mission Apollo 13. Un peu plus tard, le moteur du CSM est utilisé une deuxième fois pour placer les deux vaisseaux sur une orbite circulaire de 110 km d'altitude.
La descente sur la Lune repose en grande partie sur le système de guidage, navigation et contrôle (PGNCS : Primary Guidance and Control System) piloté par l'ordinateur embarqué (LGC). Celui-ci va d'une part, déterminer périodiquement la position et la trajectoire réelle du vaisseau en utilisant d'abord la centrale inertielle puis le radar d'atterrissage (fonction de navigation), et d'autre part, calculer la trajectoire à suivre en utilisant ses programmes et piloter, en fonction de tous ces éléments, la poussée et l'orientation des moteurs (fonction de guidage). Le pilote du LEM peut toutefois corriger l’altitude en cours à tout moment et, dans la dernière phase, reprendre complètement la main sur les commandes des moteurs. Mais seul le système de navigation et de pilotage permet, en optimisant trajectoire et consommation des ressources, de poser le LEM avant d'avoir épuisé tout le carburant.
Cette phase est désignée par l'acronyme DOI (Descent Orbit Insertion) dans la terminologie de la NASA.
L'objectif de cette phase est d'abaisser l'altitude du LEM de 110 km à 15 km au-dessus du sol lunaire. À cet effet, son orbite circulaire est transformée en une orbite elliptique de 15 km par 110 km. Cette phase permet de réduire la distance à parcourir jusqu’au sol lunaire à un faible coût en propergols (elle ne nécessite qu'une brève impulsion du moteur). La limite des 15 km a été retenue pour éviter que la trajectoire finale ne s'approche trop du relief.
Deux des trois astronautes de l'équipage prennent place dans le Module Lunaire pour descendre sur la Lune. Ils initialisent le système de navigation avant d'entamer la descente vers la Lune. Le LEM et le CSM se séparent avant que le moteur ne soit mis en marche (jusqu’à Apollo 12). Le changement d'orbite est initié lorsque le vaisseau spatial se situe aux antipodes (à une demi-orbite) du point où démarrera la phase suivante. Une fois que la distance entre le LEM et le module de commande est suffisante (une centaine de mètres), une petite accélération est d’abord imprimée par les moteurs contrôlant l'attitude pour plaquer le carburant du moteur de descente contre les vannes de distribution puis le moteur de descente est allumé brièvement pour freiner le LEM d'environ 25 m/s (90 km/h).
À partir d’Apollo 14, pour économiser les propergols de l'étage de descente, c'est le moteur du Module de Commande et de Service qui est sollicité pour abaisser l'orbite. Le CSM accompagne donc le LEM dans son orbite elliptique et s'en sépare avant que la descente propulsée ne démarre.
Cette phase est caractérisée par une action continue du moteur de descente. Elle démarre lorsque le LEM a atteint le point le plus bas de son orbite elliptique. Elle se décompose elle-même en trois phases : la phase de freinage, la phase d'approche et la phase d'atterrissage.
La phase de freinage vise à réduire la vitesse du vaisseau de la manière la plus efficace possible : celle-ci va passer de 1 695 m/s (6 000 km/h) à 150 m/s (550 km/h). Le moteur est allumé à 10 % de sa puissance durant 26 secondes, le temps que le moteur s'aligne grâce à son cardan sur le centre de gravité du vaisseau, puis il est poussé au maximum de sa puissance. Le module lunaire qui au début de la trajectoire est pratiquement parallèle au sol va progressivement s'incliner tandis que sa vitesse de descente nulle au départ augmente jusqu'à 45 m/s en fin de phase. Lorsque le LEM se trouve à une altitude inférieure à 12-13 km, le radar d'atterrissage accroche le sol et se met à fournir des informations (altitude, vitesse de déplacement) qui vont permettre de vérifier que la trajectoire est correcte : jusqu'alors celle-ci était extrapolée uniquement à partir de l'accélération mesurée par la centrale à inertie. Une différence trop importante entre les données fournies par le radar et la trajectoire visée ou le non fonctionnement du radar sont des motifs d'interruption de la mission.
La phase d'approche démarre à 7 km du site visé alors que LEM est à une altitude de 700 mètres. Elle doit permettre au pilote de repérer la zone d'atterrissage et de choisir le lieu précis (dégagé) où il souhaite atterrir. Son point de départ est désigné sous le terme de « porte haute » (« high gate »), expression empruntée à l'aéronautique.
Le module lunaire est progressivement redressé en position verticale fournissant au pilote une meilleure vision du terrain. Celui-ci peut ainsi localiser le point d'atterrissage auquel conduit la trajectoire grâce à une échelle gravée sur son hublot graduée en degrés (Landing Point Designator, LPD) : l'ordinateur fournit à la demande l'angle sous lequel l'astronaute peut voir le lieu d'atterrissage sur cette échelle. Si celui-ci juge que le terrain n'est pas propice à un atterrissage ou qu'il ne correspond pas au lieu prévu, il peut alors corriger l'angle d'approche en agissant sur les commandes de vol par incrément de 0,5° dans le sens vertical ou 2° en latéral.
Lorsque le module lunaire est descendu à une altitude de 150 mètres ce qui le place théoriquement à une distance de 700 mètres du lieu visé (point désigné sous le terme de low gate), démarre la phase d'atterrissage. Si la trajectoire a été convenablement suivie, les vitesses horizontale et verticale sont respectivement alors de 66 km/h et 18 km/h. La procédure prévoit que le pilote prenne la main pour amener le module lunaire au sol mais il peut, s'il le souhaite, laisser faire l'ordinateur de bord qui dispose d'un programme de pilotage pour cette dernière partie du vol. En prenant en compte les différents aléas (phase de repérage allongée de deux minutes, modification de la cible de dernière minute de 500 mètres pour éviter un relief, mauvaise combustion finale, jauge de propergol pessimiste), le pilote dispose d'une marge de 32 secondes pour poser le LEM avant l'épuisement des ergols. La dernière partie de la phase est un vol stationnaire à la manière d'un hélicoptère qui permet à la fois d'annuler toutes les composantes de vitesse mais également de mieux repérer les lieux. Des sondes situées sous les semelles du train d'atterrissage prennent contact avec le sol lunaire lorsque l'altitude est inférieure à 1,3 mètre et transmettent l'information au pilote. Celui-ci doit alors couper le moteur de descente pour éviter que le LEM ne rebondisse ou ne se renverse (la tuyère touche presque le sol).
Le séjour sur la Lune est rythmé par les sorties extra-véhiculaires : une unique sortie pour Apollo 11 mais jusqu’à trois sorties pour les dernières missions. Avant chaque sortie, les astronautes doivent faire le plein en eau et oxygène de leur système de survie portable puis enfiler leur tenue. Ils font ensuite le vide avant d’ouvrir l’écoutille qui donne accès à l’échelle.
Les outils et les instruments scientifiques sont sortis des baies de stockage de l’étage de descente puis sont déployés non loin du LEM ou à plus grande distance. À partir d’Apollo 14, les astronautes disposent d’une brouette puis dans le cadre des vols suivants du rover lunaire qui leur permet de s’éloigner d’une dizaine de kilomètres du LEM en transportant de lourdes charges. Le rover occupe une baie entière du module lunaire ; il est stocké en position repliée sur une palette que les astronautes abaissent pour libérer le véhicule. Le rover est déployé par un système de ressorts et de câbles agissant via des poulies et actionnés par les astronautes.
Avant de quitter la Lune, les échantillons géologiques placés dans des conteneurs sont hissés jusqu’à l’étage de remontée grâce à un palan. Le matériel qui n’est plus nécessaire (survie portable, appareils photos, etc.) est abandonné pour alléger au maximum l’étage de remontée,.
La phase de remontée doit permettre au LEM de rejoindre le module de commande resté en orbite. Cet objectif est atteint en deux temps : l'étage du LEM décolle du sol lunaire pour se mettre en orbite basse puis à l'aide de poussées ponctuelles du moteur-fusée, il rejoint le module de commande.
Avant le décollage, la position précise du LEM au sol est entrée dans l'ordinateur afin de déterminer la meilleure trajectoire. L'instant du départ est calculé de manière à optimiser la trajectoire de rendez-vous avec le module de Commande. L'étage de descente reste au sol et sert de plate-forme de lancement. La séparation des deux étages est déclenchée avant le décollage par de petites charges pyrotechniques qui sectionnent les quatre points solidarisant les deux étages ainsi que les câbles et tuyauteries.
Le Module Lunaire suit d'abord une trajectoire verticale jusqu'à une altitude d'environ 75 mètres pour se dégager du relief lunaire puis s'incline progressivement pour rejoindre finalement à l'horizontale le périlune (point bas) d'une orbite elliptique de 15 km sur 67 km.
Un rendez-vous en orbite lunaire est alors effectué entre le CSM (piloté par le troisième membre d'équipage, le seul de la mission à ne pas aller sur la Lune) et le LEM en orbite lunaire. Après que les pierres lunaires ont été transférées, le LEM est libéré et lancé sur une trajectoire qui l'amènera à s'écraser sur la Lune. Le vaisseau spatial peut alors entamer son retour vers la Terre. Apollo 16 et Apollo 17 resteront en orbite une journée de plus pour réaliser des expériences scientifiques et larguer un petit satellite scientifique de 36 kg.
Pour quitter l'orbite lunaire et placer le vaisseau spatial sur la trajectoire de retour vers la Terre, le moteur du module de commande et de service est sollicité durant deux minutes et demie après avoir soigneusement orienté le vaisseau ; il fournit un delta-v d'environ 1 000 m/s qui doit permettre au vaisseau de rejoindre l'orbite terrestre. C'est l'un des moments critiques de la mission car une défaillance du moteur ou une mauvaise précision dans l'orientation condamnerait les astronautes. Le moteur est allumé alors que le vaisseau se situe sur la face située à l'opposé de la Terre de manière que la nouvelle trajectoire, une orbite de transfert fortement elliptique, frôle la surface de la Terre à 40 km d'altitude dans la position qu'elle occupera à l'arrivée du vaisseau. Le trajet de retour dure environ trois jours mais peut être un peu raccourci en optant pour une trajectoire plus tendue. Peu après l'injection sur le trajet de retour (trans-Earth Injection, TEI), une sortie extravéhiculaire est effectuée pour récupérer les films photographiques des caméras placés dans le module de service qui doit être largué avant l'entrée dans l'atmosphère terrestre.
De petites corrections sont effectuées au cours du trajet pour optimiser l'angle d'entrée dans l'atmosphère et le point de chute. Au fur et à mesure que le vaisseau se rapproche de la Terre, la vitesse du vaisseau, qui était tombée à 850 m/s à la limite de l'influence des champs de gravité de la Terre et de la Lune, s'accroît jusqu'à atteindre 11 km/s lorsque le vaisseau pénètre dans les couches denses de l'atmosphère ; celles-ci font sentir leur influence à compter de 120 km d'altitude. Peu avant de pénétrer dans l'atmosphère, le module de service du vaisseau est largué au moyen de systèmes pyrotechniques, emportant avec lui le moteur principal et la majorité des réserves d'oxygène et d'électricité. La rentrée dans l'atmosphère se fait sous un angle très précis fixé à 6,5° avec une tolérance de 1°. Si l'angle de pénétration est trop important, le bouclier thermique qui est porté normalement à une température de 3 000 °C durant la rentrée dans l'atmosphère, subit une température supérieure à celle pour laquelle il est conçu et la décélération est plus importante ; ces deux phénomènes pouvant entraîner la mort de l'équipage. Avec un angle inférieur, le vaisseau spatial peut rebondir sur la couche atmosphérique et repartir sur une longue trajectoire elliptique condamnant son équipage incapable de manœuvrer et ne disposant que de très peu de réserves d'air.
Après une phase de décélération qui atteint 4 g, le vaisseau a perdu sa vitesse horizontale et descend pratiquement à la verticale. À 7 000 mètres d'altitude, la protection située à l'extrémité conique du vaisseau est éjectée et deux petits parachutes se déploient pour stabiliser la cabine et faire chuter sa vitesse de 480 km/h à 280 km/h. À 3 000 mètres, trois petits parachutes pilotes sont déployés latéralement par des mortiers pour extraire les trois parachutes principaux en évitant qu'ils s'emmêlent. Le vaisseau percute la surface de l'océan à une vitesse de 35 km/h (soit environ 10 m/s). Les parachutes sont immédiatement largués et trois ballonnets se gonflent de manière à éviter que le vaisseau reste la pointe sous l'eau. Une flottille comprenant un porte-avions ou un porte-hélicoptères est positionnée à l'avance sur la zone où doit amerrir le module de commande. Des avions sont chargés de localiser le point de chute tandis que des hélicoptères amènent sur place des plongeurs qui, montés sur des embarcations légères, récupèrent les astronautes et placent des élingues sur le vaisseau pour qu'il puisse être hissé sur le pont du porte-aéronefs,.
Aucun vol orbital américain n'avait encore eu lieu au lancement du programme Apollo. Le seul vol du programme Mercury — ce programme avait débuté en 1959 — avait eu lieu trois semaines avant le discours du président Kennedy et fut un simple vol balistique faute de disposer d'une fusée suffisamment puissante. Il fallut attendre la mission Mercury-Atlas 6 du 20 février 1962 pour que John Glenn devienne le premier astronaute américain à boucler une orbite autour de la Terre. Trois autres vols habités eurent lieu en 1962 et en 1963.
À l'issue du programme Mercury, des aspects importants du vol spatial, qui devaient être mis en application pour les vols lunaires, n'étaient toujours pas maîtrisés alors qu'il n'était pas possible de les tester au sol. Les dirigeants de la NASA lancèrent un programme destiné à acquérir ces techniques sans attendre la mise au point du vaisseau très sophistiqué de la mission lunaire : le programme Gemini devait remplir trois objectifs :
mettre au point les techniques permettant de travailler dans l'espace au cours de sorties extra-véhiculaires ;
étudier les conséquences de l'impesanteur sur la physiologie humaine au cours de vols de longue durée.Le vaisseau spatial Gemini, qui devait initialement être une simple version améliorée de la capsule Mercury, se transforma au fur et à mesure de sa conception en un vaisseau complètement différent de 3,5 tonnes (contre environ une tonne pour le vaisseau Mercury), capable de voler avec deux astronautes durant deux semaines. Le vaisseau était lancé par une fusée Titan II, missile de l'armée de l'air américaine reconverti en lanceur. Le programme rencontra des problèmes de mise au point. Le lanceur souffrait d'effet pogo, les piles à combustible utilisées pour la première fois fuyaient et la tentative de mise au point d'une aile volante pour faire atterrir la capsule sur le sol ferme échoua. Tous ces déboires gonflèrent le coût du programme de 350 millions de dollars à un milliard de dollars. Toutefois, fin 1963, tout était rentré dans l'ordre et deux vols sans équipage purent avoir lieu en 1964 et au début de 1965. Le premier vol habité Gemini 3 emporta les astronautes Virgil Grissom et John Young le 23 mars 1965. Au cours de la mission suivante, l'astronaute Edward White réalisa la première sortie dans l'espace américaine. Huit autres missions, émaillées d'incidents sans conséquence, s'échelonnèrent jusqu'en novembre 1966 : elles permirent de mettre au point les techniques de rendez-vous spatial et d'amarrage, de réaliser des vols de longue durée (Gemini 7 resta près de 14 jours en orbite) et d'effectuer de nombreuses autres expériences.
Parallèlement au programme Apollo, la NASA lance plusieurs programmes pour affiner sa connaissance du milieu spatial et du terrain lunaire. Ces informations sont nécessaires pour la conception des engins spatiaux et préparer les atterrissages. En 1965, trois satellites Pegasus sont placés en orbite par une fusée Saturn I pour évaluer le danger représenté par les micrométéorites ; les résultats seront utilisés pour dimensionner la protection des vaisseaux Apollo. Les sondes Ranger (1961–1965), après une longue série d'échecs, ramènent à compter de fin 1964, une série de photos de bonne qualité de la surface lunaire qui permettent d'identifier des sites propices à l'atterrissage.
Le programme Lunar Orbiter, composé de cinq sondes qui sont placées en orbite autour de la Lune en 1966–1967, complète ce travail : une couverture photographique de 99 % du sol lunaire est réalisée, la fréquence des micrométéorites dans la banlieue lunaire est déterminée et l'intensité du rayonnement cosmique est mesurée. Le programme permet également de valider le fonctionnement du réseau de télémesure. Les mesures effectuées indiquent que le champ gravitationnel lunaire est beaucoup moins homogène que celui de la Terre rendant dangereuses les orbites à basse altitude. Le phénomène, sous-estimé par la suite, réduira à 10 km l'altitude de l'orbite du LEM d’Apollo 15 dont l'équipage était endormi, alors que la limite de sécurité avait été fixée à 15 km pour disposer d'une marge suffisante par rapport aux reliefs. Le 2 juin 1966, la sonde Surveyor 1 effectue le premier atterrissage en douceur sur la Lune fournissant des informations précieuses et rassurantes sur la consistance du sol lunaire (le sol est relativement ferme) ce qui permet de dimensionner le train d'atterrissage du module lunaire.
La fusée Saturn I (ou Saturn C-1) avait été conçue alors que le cahier des charges du programme lunaire n'était pas encore figé. Sa capacité d'emport s'avéra finalement trop faible même pour remplir les objectifs des premières phases du programme. Néanmoins, dix des douze fusées commandées furent construites et lancées entre le 27 octobre 1961 et le 30 juillet 1965, dont six avec l'ensemble des étages. Aucun des composants de cette fusée ne fut réutilisé dans la suite du programme. Après cinq vols consacrés à la mise au point de la fusée (missions SA-1, SA-2, SA-3, SA-4, SA-5), Saturn I fut utilisée pour lancer deux maquettes du vaisseau Apollo (missions A-101, A-102) et placer trois satellites Pegasus en orbite (missions A-103, A-104, A-105).
Les vols de la fusée Saturn IB permirent la mise au point du troisième étage de la fusée Saturn V (l'étage IVB dont le moteur consommait du dihydrogène liquide) et d'effectuer les premiers tests du vaisseau spatial Apollo :
AS-201 (rétrospectivement et officieusement Apollo 1a) (26 février 1966), mission non habitée, premier essai du lanceur Saturn IB. C'est un vol purement balistique culminant à 450 km (sans mise en orbite) qui emporte un véritable vaisseau Apollo et non une maquette. Il permet de tester avec succès l'étage IVB qui sera réutilisé sur la fusée Saturn V, le moteur principal du vaisseau Apollo qui est mis à feu pour porter la vitesse à 8 km/s, ainsi que le bouclier thermique de la capsule Apollo durant la phase de rentrée atmosphérique ;
AS-203 (rétrospectivement et officieusement Apollo 3) (5 juillet 1966), est une mission non habitée dont l'objectif est d'étudier le comportement de l'hydrogène et de l'oxygène liquide dans les réservoirs une fois la fusée placée en orbite. La mission est un succès.
AS-202 (rétrospectivement et officieusement Apollo 2) (25 août 1966) est une mission non habitée. La fusée Saturn 1-B, comme dans le premier vol AS-201, lance sa charge utile sur une longue trajectoire balistique qui lui fait parcourir les trois-quarts du tour de la Terre. La mission doit permettre de tester le comportement du vaisseau Apollo et de la tour de sauvetage fournis dans des versions complètement opérationnelles. Le vaisseau Apollo dispose pour la première fois de ses programmes de pilotage et de navigation et de ses piles à combustible. Le moteur du vaisseau Apollo est allumé à quatre reprises. La rentrée dans l'atmosphère à 8 500 m/s permet de tester le comportement du bouclier thermique soumis à un échauffement prolongé.
Le 27 janvier 1967, alors que l'équipage du premier vol habité Apollo 1 (initialement AS-204) qui doit décoller un mois plus tard effectue une répétition au sol en conditions réelles, un incendie se déclare dans le vaisseau Apollo (CSM) dans lequel les trois astronautes se trouvent sanglés sur leurs couchettes. Les flammes font rage dans l'atmosphère confinée et composée uniquement d'oxygène. Virgil Grissom, Edward White et Roger Chaffee décèdent asphyxiés sans être parvenus à ouvrir l'écoutille dont le mécanisme complexe ne permettait pas une ouverture rapide. Le vaisseau avait rencontré de nombreux problèmes de mise au point avant l'accident. Le déclenchement de l'incendie sera attribué, sans être clairement identifié, à un court-circuit dû à un fil électrique dénudé. L'enquête révèle l'utilisation de nombreux matériaux inflammables dans la cabine et beaucoup de négligences dans le câblage électrique et la plomberie. Le déclenchement et l'extension de l'incendie avait été favorisé par l'atmosphère d'oxygène pur (dépourvue d'azote) donc extrêmement inflammable, une solution qui était déjà celle des vaisseaux Mercury et Gemini,.
De nombreuses modifications furent apportées pour que la cabine du vaisseau offre une meilleure résistance au feu. L'écoutille fut modifiée pour pouvoir être ouverte en moins de dix secondes. Une atmosphère d'azote et d'oxygène était utilisée durant la première phase du vol. L'ensemble du programme Apollo subit une revue qui entraîna la modification de nombreux composants. Les exigences de qualité et les procédures de test furent renforcées. Tout le programme subit un décalage de 21 mois accroissant la pression sur les équipes : la fin de la décennie approchait. Par ailleurs, tout le monde s'inquiétait de l'avancement du programme soviétique, même si aucune information officielle ne filtrait de l'Union soviétique.
Les déboires du vaisseau spatial Apollo permirent au programme de développement de la fusée géante Saturn V de rattraper son retard. Celle-ci avait en effet rencontré de nombreux problèmes touchant en particulier le deuxième étage (le S-II qui est encore aujourd'hui le plus gros étage à hydrogène jamais conçu) : excès de poids, phénomènes de vibration (effet pogo), etc.
Apollo 4 (9 novembre 1967), mission non habitée, premier essai du lanceur Saturn V.La mission Apollo 4 est le premier vol du lanceur géant Saturn V. À cette occasion, un vaisseau Apollo effectue pour la première fois une rentrée atmosphérique qui restera la rentrée terrestre la plus rapide jusqu'à Stardust. Afin de recueillir un maximum d'informations sur le comportement de la fusée, 4 098 capteurs sont installés. Le premier lancement des Saturn V est un succès complet.Apollo 5 (22 janvier 1968 – 23 janvier 1968), mission non habitée, essai du lanceur Saturn IB et du module lunaire.La mission Apollo 5 doit permettre de tester le module lunaire dans des conditions de vol réelles, c'est-à-dire dans le vide spatial. Il s'agit en particulier de vérifier le fonctionnement de ses moteurs d'ascension et de descente, ainsi que sa capacité à effectuer les manœuvres de séparation prévues. La mission est également destinée à tester une manœuvre d'urgence consistant à mettre à feu les moteurs d'ascension sans avoir largué l'étage de descente (manœuvre d'interruption de la phase d'atterrissage). Malgré quelques caprices de l'électronique du module lunaire, le fonctionnement de celui-ci peut être validé par ce vol.Apollo 6 (4 avril 1968) mission non habitée, deuxième vol des Saturn V.La mission Apollo 6 est une répétition plus complète d’Apollo 4. Le test est peu satisfaisant : deux des moteurs J-2 du 2e étage cessent prématurément de fonctionner ce qui ne peut être compensé que par une durée de fonctionnement prolongée des autres moteurs de l'étage. Alors que la fusée est sur son orbite de parking, l'unique moteur J-2 du 3e étage refuse de se rallumer pour simuler l'injection sur une trajectoire lunaire. En sollicitant le moteur du vaisseau Apollo, les équipes de la NASA parviennent malgré tout à effectuer les tests attendus. Malgré ces péripéties, la NASA estima que désormais la fusée Saturn V et les véhicules Apollo pouvaient embarquer des équipages en toute sécurité.
Le premier vol habité n'a lieu qu'en octobre 1968 mais les missions destinées à valider le fonctionnement des différents composants du programme et à effectuer une répétition presque complète d'une mission lunaire, se succèdent rapidement. Quatre missions préparatoires se déroulent sans anomalie majeure sur une période de sept mois.
Apollo 7 (11 octobre 1968 – 22 octobre 1968).Apollo 7 est la première mission habitée du programme Apollo. Son but est de valider les modifications effectuées sur le vaisseau spatial à la suite de l'incendie d’Apollo 1 (CMS version 2). Une fusée Saturn IB est utilisée car le module lunaire ne fait pas partie de l'expédition. Au cours de son séjour en orbite, l’équipage répète les manœuvres qui seront effectuées lors des missions lunaires. Après avoir quitté l’orbite terrestre et effectué leur rentrée dans l’atmosphère, la capsule et son équipage sont récupérés sans incident dans l’Atlantique. C’était la première mission américaine à envoyer une équipe de trois hommes dans l'espace et à diffuser des images pour la télévision. La fusée Saturn IB ne sera plus utilisée par la suite dans le cadre du programme d'exploration lunaire.Apollo 8 (21 décembre 1968 – 27 décembre 1968)La mission Apollo 8 est le premier vol habité à quitter l’orbite terrestre. À ce stade d'avancement du programme, il s'agit d'une mission risquée car une défaillance du moteur du vaisseau Apollo au moment de sa mise en orbite lunaire ou de son injection sur la trajectoire de retour aurait pu être fatale à l'équipage d'autant que le module lunaire a été remplacé par une maquette. Mais les dirigeants de la NASA redoutent un coup d'éclat des Soviétiques pour la fin de l'année et décident de courir le risque. Les astronautes font au total dix révolutions autour de la Lune. Durant ce vol, ils réalisent de nombreux clichés de la Lune dont le premier lever de Terre. Apollo 8 permet pour la première fois à un homme d'observer directement la « face cachée » de la Lune. L'une des tâches assignées à l'équipage consistait à effectuer une reconnaissance photographique de la surface lunaire, notamment de la mer de la Tranquillité où devait se poser Apollo 11.Apollo 9 (3 mars 1969 – 13 mars 1969)Apollo 9 constitue le premier essai en vol de l’ensemble des équipements prévus pour une mission lunaire : fusées Saturn V, module lunaire et vaisseau Apollo. Pour la première fois, on baptise le vaisseau Apollo (Gumdrop) et le Lem (Spider), une décision destinée à faciliter les communications avec le sol lorsque les deux vaisseaux ont un équipage. Les astronautes effectuent toutes les manœuvres de la mission lunaire tout en restant en orbite terrestre. Le module lunaire simule un atterrissage puis réalise le premier rendez-vous réel avec le vaisseau Apollo. Les astronautes effectuent également une sortie extravéhiculaire de 56 minutes pour simuler le transfert d'équipage du module lunaire au vaisseau Apollo en passant par l'extérieur (manœuvre de secours mise en œuvre en cas d'amarrage infructueux entre les deux vaisseaux). En outre, ils testent l'utilisation du module lunaire comme « canot de sauvetage » dans la perspective d'une défaillance du vaisseau Apollo ; c’est cette procédure qui sera utilisée avec succès par l’équipage d’Apollo 13.Apollo 10 (18 mai 1969 – 26 mai 1969)Les dirigeants de la NASA envisagèrent que cette mission soit celle du premier atterrissage sur le sol lunaire, car l'ensemble des véhicules et des manœuvres avait été testé sans qu'aucun problème majeur n'ait été détecté. Mais, dans la mesure où les Soviétiques ne semblaient pas préparer de mission d'éclat, ils préférèrent opter pour une dernière répétition au réalisme encore plus poussé. Peu après avoir quitté son orbite terrestre basse, le vaisseau Apollo, surnommé « Charlie Brown », exécuta la manœuvre d'amarrage au LEM. Après s'être séparé du troisième étage de Saturn V, il effectua une rotation à 180° puis arrima son nez au sommet du module lunaire avant de l'extraire de son carénage. Une fois le train spatial placé en orbite autour de la Lune, le module lunaire, surnommé « Snoopy », entama la descente vers le sol lunaire qui fut interrompue à 15,6 km de la surface. Après avoir largué l'étage de descente non sans quelques difficultés dues à une erreur de procédure, le LEM réalisa un rendez-vous avec le vaisseau Apollo. La mission reproduisit les principales étapes du vol final, à la fois dans l'espace et au sol. Young était aux commandes du vaisseau Apollo alors que Stafford et Cernan occupaient le module lunaire.
Les sept missions suivantes lancées entre 1969 et 1972 ont toutes pour objectifs de poser un équipage en différents points de la Lune, présentant un intérêt géologique. Apollo 11 est la première mission à remplir l'objectif fixé par le président Kennedy. Apollo 12 est une mission sans histoire, contrairement à Apollo 13 qui, à la suite d'une explosion dans le module de service, frôle la catastrophe et doit renoncer à se poser sur la Lune. La NASA a modifié le modèle de module lunaire emporté par les missions à partir d’Apollo 15 pour répondre aux attentes des scientifiques : le séjour sur la Lune est prolongé grâce à des réserves de consommables plus importantes. Le module lunaire plus lourd transporte le rover lunaire qui accroît le rayon d'action des astronautes durant leurs sorties.
Apollo 11 (16 juillet 1969 – 24 juillet 1969)Le 21 juillet 1969, les astronautes Neil Armstrong et Buzz Aldrin, après un atterrissage mouvementé dans la mer de la Tranquillité, font leurs premiers pas sur la Lune. Armstrong, qui est le premier à sortir du module lunaire, prononce sa phrase devenue depuis célèbre « C'est un petit pas pour [un] homme, [mais] un bond de géant pour l'Humanité » - « That's one small step for [a] man; one giant leap for mankind ». L'objectif principal de la mission était de réussir l'atterrissage. L'équipage installe une version simplifiée de la station scientifique ALSEP et la sortie extravéhiculaire, au cours de laquelle 21,7 kilogrammes de roche et de sol lunaires sont collectées, ne dure que 2 heures 30. Après un séjour de 21 heures 38 sur le sol lunaire, le module lunaire décolle sans encombre. À leur arrivée sur Terre, l'équipage et les échantillons lunaires sont placés en quarantaine durant 21 jours pour éviter une éventuelle contamination terrestre par des virus extraterrestres, une procédure exigée par les scientifiques qui sera abandonnée à partir d’Apollo 15.Apollo 12 (14 novembre 1969 – 24 novembre 1969)32 secondes après son décollage, la fusée Saturn V est frappée par la foudre, entraînant une perte temporaire de la puissance électrique et des instruments du module de commande, mais l'équipage réussit à redémarrer ce dernier et poursuivre la mission. Le module lunaire fait un atterrissage de précision dans l'Océan des Tempêtes à 180 m de la sonde spatiale Surveyor 3 dont certains éléments seront ramenés à Terre pour évaluer l'incidence de leur séjour prolongé sur le sol lunaire et dans le vide. Charles Conrad et Alan Bean installent une station scientifique automatisée ALSEP, mènent à bien des observations géologiques et prennent de nouvelles photographies de la Lune et de sa surface. Ils recueillent également 34,1 kg d'échantillons du sol lunaire. Durant ce séjour sur le sol lunaire de 31 heures 31 minutes, les deux astronautes réalisent deux excursions d'un total de 7 heures 45 minutes parcourant ainsi 2 km à pied et s'éloignent jusqu'à 470 m du module lunaire. De nombreuses améliorations ont été réalisées en particulier dans la précision de l'atterrissage par rapport à la mission Apollo 11. Les résultats sont si positifs qu'on projette d'envoyer Apollo 13 dans une zone plus accidentée.
Apollo 13 (11 avril 1970 – 17 avril 1970)La mission est interrompue à la suite de l'explosion d'un réservoir d'oxygène liquide situé dans le module de service d'Odyssey durant le transit de la Terre à la Lune, 55 heures 54 minutes après son envol. Le CSM est pratiquement hors service sans oxygène ni puissance électrique. Les astronautes n'osent pas se servir de son moteur pour manœuvrer. Ils se réfugient dans le module lunaire Aquarius dont ils utilisent les ressources et le moteur pour les manœuvres de correction de trajectoire qui permettent d'optimiser la trajectoire de retour vers la Terre. Heureusement, la trajectoire de transit Terre-Lune a été calculée pour que, en l'absence de manœuvre, le train spatial puisse revenir vers la Terre après avoir fait le tour de la Lune. Les astronautes réintègrent le vaisseau Odyssey immédiatement avant l'arrivée à Terre, larguent le module lunaire qui a servi de radeau de sauvetage avant d'effectuer une rentrée dans l'atmosphère sans encombre. L'explication de l'accident est déterminée sans ambiguïté : durant une vidange du réservoir d'oxygène, quinze jours avant le décollage, la gaine des fils électriques qui le traversent a fondu et ceux-ci se sont retrouvés entièrement dénudés. Lorsque Jack Swigert a actionné le brassage prévu du réservoir, des étincelles ont jailli et déclenché son explosion.Apollo 14 (31 janvier 1971 – 9 février 1971).Le début du transit vers la Lune est marqué par un incident qui manque d'interrompre la mission : l'équipage doit s'y reprendre à cinq reprises pour parvenir à amarrer le module CSM au module lunaire. Apollo 14 atterrit dans la région accidentée de Fra Mauro qui était l'objectif initial d’Apollo 13. Un des moments marquants de la mission se produit lorsque Alan Shepard, qui est le premier (et le seul) des astronautes du programme Mercury à marcher sur la Lune, tire deux balles de golf à l'aide d'un club emmené clandestinement. Shepard et Edgar Mitchell passèrent plus de neuf heures au cours de deux sorties à explorer une zone où la NASA pensait trouver des roches figurant parmi les plus anciennes. Ils ramènent 42,9 kg d'échantillons rocheux.Apollo 15 (26 juillet 1971 – 7 août 1971)Apollo 15 est la première mission à emporter un module lunaire alourdi grâce, entre autres, à l'optimisation du lanceur Saturn V. Le poids supplémentaire est principalement constitué par le rover lunaire et des consommables (oxygène et puissance électrique) embarqués à bord du module lunaire Apollo qui permettent d'allonger le séjour sur la Lune de 35 heures à 67 heures. David Scott et James Irwin passent 2 jours et 18 heures sur le sol lunaire. Au cours de leurs trois sorties extravéhiculaires, qui durent en tout 18 heures 36 minutes, ils parcourent plus de 28,2 km à proximité du mont Hadley grâce au rover lunaire. Parmi les 76 kg de roches prélevées, les astronautes trouvent ce qu'on pense être un cristallin de la croûte lunaire originelle vieille d'environ 4,6 milliards d'années. Un petit satellite emportant trois expériences scientifiques est largué alors que le CSM est en orbite autour de la Lune. Worden fait une sortie spatiale de seize minutes dans l'espace alors que le vaisseau Apollo se trouve encore à 315 000 km de la Terre. Au retour, durant la descente vers le sol terrestre, un des trois parachutes se met en torche sans dommage pour l'équipage.Apollo 16 (16 avril 1972 – 27 avril 1972)Apollo 16 est la première mission à se poser sur les hauts-plateaux lunaires. John Watts Young et Charles Duke passent 20 heures 14 minutes sur la Lune, installant plusieurs expériences, parcourant 26,7 km à l'aide du rover lunaire et recueillant 95,4 kg d'échantillons rocheux. L’équipage largue un mini-satellite destiné à étudier les particules et le champ magnétique solaire.Apollo 17 (7 décembre 1972 – 19 décembre 1972)Apollo 17 est la dernière mission sur la Lune. L'astronaute Eugene Cernan et son compagnon Harrison Schmitt, un géologue civil américain, le seul astronaute scientifique du programme Apollo à avoir volé, sont les derniers hommes à marcher sur la Lune : ils y passent 22 h 05 min, parcourant grâce à la Jeep lunaire 36 km dans la région des monts Taurus, près du cratère de Littrow. C'est l'équipage qui ramène le plus de roches lunaires (111 kg) et effectue la plus longue sortie extra-véhiculaire.
La NASA se préoccupe dès 1963 de la suite à donner au programme Apollo. En 1965, l'agence crée une structure affectée aux missions postérieures à celles déjà planifiées regroupées sous l'appellation Apollo Applications Program (AAP). La NASA propose plusieurs types de mission dont le lancement en orbite d'une station spatiale, des séjours prolongés sur la Lune mettant en œuvre plusieurs nouveaux modules dérivés du LEM, une mission habitée vers Mars, le survol de Vénus par une mission habitée, etc. Mais les objectifs scientifiques trop vagues ne réussissent pas à convaincre le Congrès américain beaucoup moins motivé par les programmes spatiaux « post-Apollo ». Par ailleurs, les priorités des États-Unis ont changé : les dispositifs sociaux mis en place par le président Lyndon Johnson dans le cadre de sa guerre contre la pauvreté (Medicare et Medicaid) et surtout un conflit vietnamien qui s'envenime prélèvent une part croissante du budget. Ce dernier ne consacre aucun fonds à l'AAP pour les années 1966 et 1967. Les budgets votés par la suite ne permettront de financer que le lancement de la station spatiale Skylab réalisée en utilisant un troisième étage de la fusée Saturn V.
En 1970, le programme Apollo lui-même est touché par les réductions budgétaires : la dernière mission planifiée (Apollo 20) est annulée tandis que les vols restants sont étalés jusqu'en 1974. La NASA doit se préparer à se séparer de 50 000 de ses employés et sous-traitants (sur 190 000) tandis que l'on annonce l'arrêt définitif de la fabrication de la fusée Saturn V qui ne survivra donc pas au programme. Un projet de mission habitée vers Mars (pour un coût compris entre trois et cinq fois celui du programme Apollo) proposé par un comité d'experts sollicité par le nouveau président républicain Richard Nixon ne reçoit aucun appui ni dans la communauté des scientifiques ni dans l'opinion publique et est rejeté par le Congrès sans débat,. Le 20 septembre 1970, le responsable de la NASA, démissionnaire, annonce que les contraintes budgétaires nécessitent de supprimer deux nouvelles missions Apollo 18 et Apollo 19,.
L'annulation des missions laisse trois fusées Saturn V inutilisées dont l'une permettra néanmoins de lancer la station spatiale Skylab. Les deux restantes sont aujourd'hui exposées au Johnson Space Center et au centre spatial Kennedy. La station spatiale Skylab est occupée successivement par trois équipages lancés par des fusées Saturn IB et utilisant des vaisseaux Apollo (1973). Une fusée Saturn IB fut utilisée pour le lancement de la mission Apollo-Soyouz qui emportait un vaisseau spatial Apollo (1975). Ce sera la dernière mission à utiliser du matériel développé dans le cadre du programme Apollo. Le coût du programme est évalué à 25,4 milliards de dollars en 1969 (équivalent à 135 milliards de dollars, en 2006).
L'objectif fixé au programme Apollo par le président Kennedy en 1961 est rempli au-delà de toute espérance. L'astronautique américaine a su développer dans un temps record un lanceur d'une puissance inimaginable dix ans auparavant, maîtriser complètement le recours à l'hydrogène pour sa propulsion et réaliser ce qui paraissait, peu de temps auparavant, relever de la science-fiction : amener l'homme sur un autre astre. Malgré le saut technologique, le taux de réussite des lancements des fusées Saturn a été de 100 % et tous les équipages ont pu être ramenés à Terre. Aux yeux du monde entier le programme Apollo est une démonstration magistrale du savoir-faire américain et de sa supériorité sur l'astronautique soviétique qui au même moment accumule les échecs. Pour beaucoup d'Américains cette victoire démontre la supériorité de la société américaine même si cette foi dans leur système est fortement ébranlée à la même époque par l'ampleur de la contestation étudiante liée à la guerre du Viêt Nam et l'agitation sociale qui touche en particulier la minorité noire dans les grandes villes liée avec le mouvement des droits civiques.
Le programme Apollo, lorsqu'il est lancé, répond à des considérations de politique extérieure : l'architecture des missions et la conception des véhicules sont définies sans se soucier de leur pertinence et de leur pérennité du point de vue de la recherche scientifique. Celle-ci est intégrée dans le projet tardivement et avec beaucoup de difficultés. Absorbés par les défis techniques à relever, la NASA et le MSC — ce dernier était particulièrement concerné puisque chargé de la conception des vaisseaux habités et de l'entraînement des astronautes — ont du mal à consacrer des forces à la prise en compte des besoins scientifiques. Enfin, membres de la NASA et scientifiques (ceux-ci étant représentés notamment par le National Academy of Sciences et le Space Science Board) tâtonnèrent longtemps pour mettre au point un mode de travail constructif, chacun voulant assumer la conduite des projets. Après avoir lancé les premières études en 1962, le Space Science Board définit au cours de l'été 1965 les points clés à traiter pour les quinze prochaines années dans le domaine de la recherche lunaire. Ce document servira de cahier des charges pour la conception des expériences scientifiques à mettre en œuvre au cours des missions Apollo.
Pour mener des recherches scientifiques sur le terrain, il valait mieux disposer de scientifiques entraînés comme astronautes que de pilotes — le vivier dans lequel avait puisé jusque-là la NASA — formés à la géologie. En 1965, malgré les réticences d'une partie du management, la NASA recrute six scientifiques. Seuls deux d'entre eux étaient des pilotes vétérans et les autres durent suivre une formation de pilote de chasseur à réaction. Début 1966, le MSC, après avoir été plusieurs fois relancé par la direction de la NASA, mit en place une structure destinée aux expériences scientifiques permettant d'amorcer le processus de développement des instruments embarqués. Seul le géologue Schmitt aura l'occasion d'aller sur la Lune.
Les missions Apollo ont permis de collecter en tout 382 kg de roches lunaires dans six régions différentes de notre satellite (à comparer aux 336 grammes ramenés sur Terre par les missions soviétiques robotisées du programme Luna à la même époque). Ces roches sont conservées d'abord dans un bâtiment construit à cet effet au Centre spatial de Houston, le Lunar Receiving Laboratory remplacé depuis 1979 par le Lunar Sample Laboratory Facility. Une organisation est mise en place pour la fourniture de petits échantillons de roches aux scientifiques du monde entier qui en font la demande. Un institut consacré aux sciences planétaires, le Lunar and Planetary Institute, est créé à la même époque à Houston pour faciliter la coopération internationale et centraliser les résultats des études menées. Par ailleurs de nombreuses données scientifiques ont été collectées au cours des missions : mesures effectuées par les astronautes durant leur séjour sur le sol lunaire, photographies prises depuis l'orbite lunaire, relevés effectués par les instruments logés dans une des baies du module de service à partir de la mission Apollo 15. Enfin, les stations scientifiques ALSEP, comportant de trois à huit instruments et déposées sur le sol lunaire durant les sorties extravéhiculaires, ont transmis leurs mesures aux stations terrestres jusqu'à l'épuisement de leur source d'énergie radioactive en septembre 1977. Les réflecteurs laser qui faisaient partie des ALSEP mais n'ont pas besoin d'une source d'énergie, car complètement passifs, sont encore utilisés de nos jours pour mesurer les variations de distance entre la Terre et la Lune.
Contre toute attente les roches lunaires ramenées comme les observations et les mesures effectuées n'ont pas permis de trancher entre les différents scénarios de formation de la Lune : produit de la collision entre un astre vagabond et la Terre (thèse aujourd'hui privilégiée), capture d'un astre par la Terre, formation en parallèle, etc. En effet, l'interprétation de données issues d'un milieu extraterrestre s'est avérée beaucoup plus difficile que ce que les scientifiques avaient imaginé, car nécessitant entre autres, un gros effort de recherche interdisciplinaire. Les échantillons de roche collectées indiquent une géologie  complexe aussi les scientifiques estiment que la Lune est, dans ce domaine, en grande partie inexplorée  malgré les six expéditions Apollo. Les données collectées par les quatre sismomètres ont permis d'esquisser une modélisation de la structure interne de la Lune : une croûte de 60 km d'épaisseur surmontant une couche homogène et de nature différente de 1 000 km d'épaisseur avec en profondeur un cœur à moitié fondu (1 500 °C) constitué sans doute de silicates. Les altimètres laser d’Apollo 15 et 16 ont confirmé que le centre de gravité de la Lune ne coïncidait pas avec son centre géométrique. Les données géologiques et géochimiques recueillies ont été par contre beaucoup plus difficiles à interpréter et n'ont permis de tirer que des conclusions générales : les échantillons reflètent une composition chimique différente de celle de la Terre avec une proportion plus faible des éléments les plus volatils et plus d'éléments radioactifs que la moyenne cosmique. Trois types de roche semblent prédominer : des basaltes riches en fer dans les mers, des plagioclases ou anorthosites riches en aluminium dans les zones situées en altitude et des basaltes riches en uranium et en thorium avec des concentrations importantes de potassium, terres rares et phosphore (basaltes « KREEP »). Mais pour certains scientifiques de cette époque, ces roches ne reflètent pas la composition du sol de la Lune primordiale sans doute enseveli par le bombardement constant subi par celle-ci depuis plusieurs milliards d'années.
L'impact du programme Apollo et des programmes spatiaux américains contemporains sur l'évolution technologique est indirect et porte sur des domaines bien précis. Il est difficile de distinguer la contribution du programme de celle des projets militaires (missile balistique) qui le précèdent ou l'accompagnent. Si les technologies concernées peuvent être clairement identifiées, il est beaucoup moins facile de mesurer précisément l'incidence du programme spatial sur les progrès constatés.
L'industrie métallurgique, qui doit répondre à des exigences particulièrement sévères (allègement, absence de défaut) et aux contraintes de l'environnement spatial (vide entraînant la sublimation des métaux, vibration, chaleur), crée de nouvelles techniques de soudure, dont le soudage par explosion, pour obtenir des pièces sans défaut. Le recours à l'usinage chimique, qui deviendra plus tard un procédé essentiel pour la fabrication des composants électroniques, est fréquent. Il a fallu mettre au point de nouveaux alliages et recourir à des matériaux composites. Les instruments de mesure installés dans les engins spatiaux ont dû satisfaire des exigences de précision, fiabilité et rapidité beaucoup plus élevées que la norme. L'instrumentation biomédicale est née de la nécessité de contrôler l'état de santé des astronautes en vol. Enfin, les projets de la NASA des années 1960 ont permis d'affiner les techniques de calcul de la fiabilité et de mettre au point un grand nombre de techniques de gestion de projet : PERT, WBS, gestion de la valeur acquise, revue technique, contrôle qualité.
Le programme Apollo a contribué à l'essor de l'informatique : le développement des programmes de navigation et de pilotage des vaisseaux Apollo voit apparaître la scission entre matériel et logiciel. Les méthodes de programmation et de test sont également en partie nées des exigences de fiabilité et de la complexité des logiciels développés pour le programme. Enfin, le projet lance l'utilisation des circuits intégrés qui ont fait leur apparition en 1961. La NASA achète au début du programme 60 % de la production mondiale pour les besoins des ordinateurs des vaisseaux Apollo.
L'ère spatiale débute en plein âge d'or d'une science-fiction américaine inspirée par les réalisations techniques nées de la Seconde Guerre mondiale et incarnée par des écrivains comme Isaac Asimov, Robert Heinlein, Arthur C. Clarke. Leurs œuvres dressent en images saisissantes et crédibles, le portrait d'une civilisation terrestre et plus particulièrement américaine qui s'est étendue aux planètes voisines ou aux étoiles. Des ingénieurs comme le futur concepteur de la fusée Saturn V Wernher von Braun (ce dernier à travers ses contacts avec Walt Disney) contribuent également à populariser l'idée de l'exploration de l'espace par l'homme. Lorsque le programme Apollo est lancé, la rhétorique sous-jacente de la littérature de fiction spatiale (nouvelle frontière, conquête de l'espace) est reprise dans le discours des responsables politiques et de l'agence spatiale. Aiguillés par la NASA, des magazines comme Life, la télévision américaine en pleine expansion, transforment la course à l'espace et le programme Apollo en particulier, en un feuilleton haletant, suivi avec passion par les Américains et dont les astronautes sont les héros. Le film 2001, l'Odyssée de l'espace, réalisé en collaboration étroite avec les spécialistes de l'industrie spatiale et qui sort en 1968, reflète l'idée que se font beaucoup d'un futur spatial qui semble désormais à portée de main.
Lorsque les astronautes d’Apollo 8 effectuent le voyage initial vers la Lune, donnant à des millions de téléspectateurs pour la première fois la possibilité d'apercevoir leur planète plongée dans l'espace, ils sont sans doute nombreux à partager le sentiment qui inspire au poète Archibald MacLeish ce texte intitulé « Riders on earth together, Brothers in eternal cold » (« Passagers solidaires de la Terre, frères dans le froid éternel ») qui fut imprimé le jour de Noël à la Une du New York Times :
cold-brothers who know now they are truly brothers »« Contempler la Terre telle qu'elle est réellement, petit joyau bleu flottant dans un silence éternel,
frères qui réalisent maintenant qu'ils sont vraiment frères. ».Les photos de la Terre prises depuis l'espace lointain par les équipages du programme Apollo frapperont les esprits à l'époque. La plus célèbre de ces photos est La Bille bleue prise par les astronautes d’Apollo 17. D'autres photos, comme celles montrant un lever de Terre au-dessus d'un sol lunaire dépourvu de couleurs ou celles mettant en évidence la minceur de la couche atmosphérique ont fait prendre conscience du caractère unique et fragile de notre planète, le vaisseau Terre. Ces images ont sans doute contribué à l'expansion des mouvements écologiques au cours des décennies suivantes.
Le 20 juillet 1969, 600 millions de téléspectateurs, soit un cinquième de la population mondiale de l'époque, assistent en direct à la télévision aux premiers pas de Neil Armstrong et Buzz Aldrin. Si presque tout le monde s'accorde sur le fait qu'il s'agit d'un évènement marquant, il y a toutefois des voix pour s'élever contre le gaspillage d'argent comme certains représentants de la communauté noire américaine, à l'époque en pleine ébullition. L'écrivain de science-fiction Ray Bradbury, qui participe à un débat à la télévision à Londres, durant lequel il se heurte aux critiques émanant, entre autres, de l'activiste politique irlandaise Bernadette Devlin, s'insurge « Au bout de six milliards d'années d'évolution, cette nuit, nous avons fait mentir la gravité. Nous avons atteint les étoiles... et vous refusez de fêter cet évènement ? Allez au diable ! ».
Les mots de Neil Armstrong, « C'est un petit pas... », furent immédiatement repris et adaptés tandis que l'expression « Si on a pu envoyer des hommes sur la Lune, alors on devrait pouvoir… » devint une phrase passe-partout. Mais l'intérêt pour le programme spatial faiblit rapidement. Le déroulement de la mission Apollo 12, pourtant filmé en couleurs contrairement à Apollo 11, fut beaucoup moins suivi. Les commentaires très techniques, hors de portée de l'Américain moyen, l'absence de péripéties banalisaient l'évènement. Il fallut l'accident d’Apollo 13, qui replaçait l'homme au cœur de la mission, pour raviver l'intérêt du public.
Plusieurs films et de nombreux documentaires ont pris pour sujet le programme Apollo. On peut citer notamment :
Moonwalk One : reportage sorti en 1970 et réalisé par Theo Kamecke, qui - contemporain des évènements - situe l'événement dans son contexte historique.
From the Earth to the Moon : mini-série américaine en 12 épisodes de 50 minutes, créée d'après le livre d'Andrew Chaikin (en) (en), A Man on the Moon (en), et diffusée sur le réseau HBO.
In the Shadow of the Moon : documentaire de 2008 constitué à partir de films d'actualités diffusés à l'époque, de documents internes de la NASA et d'interviews de plusieurs astronautes encore en vie.
The Dish : semi-fiction réalisée en 2000 par Rob Sitch, retraçant l'histoire de la construction d'une station de réception terrestre en Australie qui doit recevoir la première émission télévisuelle émise depuis la Lune par Apollo 11.
First Man : film de Damien Chazelle sorti en 2018, basé sur la vie de Neil Armstrong et la mission Apollo 11.
Apollo 11, un documentaire réalisé par Todd Douglas Miller est sorti en 2019. Utilisant des images retrouvées dans les archives de la Nasa, pour la plupart inédites et toutes restaurées, ce documentaire, sans narration, montre les dessous de la mission.
Au début des années 1970, alors que le programme Apollo touche à sa fin, certains décideurs politiques envisagent l'arrêt des vols habités trop coûteux et aux retombées limitées. La fin de la guerre froide et l'effondrement du programme spatial soviétique a privé le projet habité américain d'une grande partie de ses justifications. Mais Richard Nixon ne veut pas être celui qui a arrêté les missions habitées auxquelles se rattache encore malgré tout une part de prestige. Par ailleurs, si l'opinion publique et la communauté scientifique s'accordent sur la nécessité de réduire le budget spatial en particulier consacré aux vols habités, le président n'est pas insensible au lobbying de l'industrie et aux considérations électorales : la Californie qui concentre une grande partie des emplois de l'astronautique — les effectifs employés par l'industrie aérospatiale en Californie passent de 455 000 à 370 000 personnes entre 1967 et 1970 — est un enjeu important pour les élections à venir. En partie pour répondre aux critiques sur le coût du programme Apollo, la NASA a élaboré à cette époque son projet de navette spatiale qui doit permettre d'abaisser de manière significative le prix du kilogramme placé en orbite par rapport aux lanceurs non réutilisables. Le président Nixon donne son feu vert au programme de la navette spatiale mais celle-ci devra s'inscrire par la suite dans un cadre budgétaire spatial civil en décroissance constante : les sommes allouées à la NASA passent progressivement de 1,7 % du budget total de l'État fédéral en 1970 à 0,7 % en 1986, son point le plus bas. Les espoirs suscités par la navette spatiale seront déçus :  on estime en 2008, alors que le programme de la navette est en voie d'achèvement, que chaque vol de la navette spatiale américaine revient à 1,5 milliard de dollars en intégrant les coûts de développement : un coût non concurrentiel par rapport à celui d'un lanceur classique. La souplesse opérationnelle n'est pas non plus  au rendez-vous : la cadence de lancement atteint 5 % de celle prévue initialement.
La communauté scientifique américaine tire un bilan négatif du programme Apollo. Les retombées scientifiques du programme sont limitées au regard des sommes investies et la part du programme spatial consacrée à la science (satellites scientifiques, sondes spatiales) a diminué durant les années Apollo. Le phénomène se répètera d'ailleurs au cours des décennies suivantes, les programmes scientifiques de la NASA étant régulièrement victimes soit des dépassements budgétaires des programmes spatiaux habités soit d'arbitrages en leur défaveur. Aussi, l'Académie des Sciences américaine demande à l'époque que l'activité spatiale soit recentrée sur des thèmes scientifiques et ses applications dans le domaine de la météorologie, l'agriculture, l'hydrologie, l'océanographie, etc. Elle s'oppose également au développement de la navette spatiale. La communauté scientifique est aujourd'hui dans son ensemble toujours peu favorable aux missions habitées au-delà de l'orbite basse : en 2004, à la suite de la relance des missions habitées vers la Lune et Mars, le comité chargé du financement de l'astrophysique au sein de l'American Physical Society, s'inquiétait de l'importance des fonds monopolisés par ce type de mission aux objectifs mal cernés au détriment de projets, comme les télescopes spatiaux, qui avaient largement prouvé leur intérêt scientifique.
Après les progrès fulgurants des années 1960 dont le débarquement lunaire constitue l'acmé, le vol spatial habité, contrairement à toutes les prédictions de l'époque, s'est replié durant ces cinquante dernières années sur l'orbite terrestre basse. L'astronaute Gene Cernan, dans son autobiographie publiée en 1999, écrit « Tout se passe comme si le programme Apollo avait vu le jour avant son heure, comme si le président Kennedy avait été chercher une décennie au cœur du XXIe siècle et qu'il avait réussi à l'insérer au début des années 1960 ». Pour l'historien américain J.R. McNeill, l'aventure du programme Apollo et de l'exploration spatiale en général pourrait être une impasse condamnée à devenir dans le futur une simple note de bas de page de l'histoire de la civilisation, à moins que des découvertes ne relancent son intérêt ou que renaisse une course au prestige entre des nations disposant de moyens financiers suffisants,.
À l'époque du débarquement sur la Lune, il existait déjà une petite minorité d'incrédules qui se recrutait aux États-Unis dans les classes sociales les plus défavorisées, coupées de toute connaissance scientifique, et les minorités. L'audience de la thèse du moon hoax (canular lunaire) s'élargit dans les années 1970 lorsqu'un climat de défiance vis-à-vis des institutions s'installe chez beaucoup d'Américains dans le sillage du scandale du Watergate et de la guerre du Viêt Nam : c'est à cette époque, symbolisée dans les médias par le film Les Trois Jours du Condor, qu'est tourné Capricorn One (1978) qui raconte l'histoire d'un faux débarquement sur Mars mis en scène par la NASA. En 2001, l'émission « Théorie du complot : avons-nous atterri sur la Lune ? », basée sur des pseudo témoignages scientifiques et diffusée sur la chaine de télévision Fox, rencontre un succès d'audience qui témoigne surtout de l'absence de culture scientifique de ses auditeurs. Malgré ses incohérences évidentes, la théorie du faux débarquement sur la Lune continue à trouver des partisans pour les raisons déjà citées mais sans doute également parce que l'événement est si éloigné de toute expérience personnelle, qu'il dégage pour beaucoup un sentiment d'irréalité.
La stagnation du programme spatial habité américain après les succès du programme Apollo suscite un intense sentiment de frustration chez beaucoup de passionnés d'astronautique. Au moment même où le programme Apollo subit un coup d'arrêt à la fin des années 1960, naissent des associations militant pour un programme spatial habité ambitieux prolongeant l'effort spatial engagé. Selon T.E. Dark, l'apparition de ces mouvements est à mettre en relation avec la crise que subit à la fin des années 1960 l'idée de progrès, une croyance au cœur de la société américaine. L'apparition du mouvement écologique, un scepticisme naissant vis-à-vis des bienfaits de la croissance économique et la crainte d'un déclin culturel américain expliquent principalement cette crise. Promouvoir le programme spatial était un moyen de faire revivre l'idée de progrès sous une autre forme.
L'association la plus connue à l'époque, la L5 Society, préconise la colonisation de l'espace par la création de gigantesques habitats spatiaux au point de Lagrange L5. Elle reçoit l'attention du Congrès américain ainsi que de la NASA. Mais le concept d'habitats spatiaux géants ne dépassera jamais le stade de l'étude théorique, car il nécessite de lancer un million de tonnes en orbite autour de la Terre en six ou dix ans, un objectif qui ne pouvait être atteint que si le coût de la mise en orbite était abaissé à 55 dollars le kg comme envisagé par l'étude de Gerard K. O'Neill et la NASA en 1975-1977. La L5 Society disparait en 1987, victime des désillusions nées de la crise de l'énergie et des déboires de la navette spatiale américaine. En 1998, est fondée la Mars Society qui milite pour la colonisation de Mars. Son créateur, Robert Zubrin, rédige plusieurs ouvrages très documentés sur les moyens de mener une mission habitée sur Mars. The Planetary Society est une association plus ancienne, née en 1980, dont le fondateur le plus connu est Carl Sagan, qui a un ancrage international et compte plus de 100 000 membres. Plus réaliste, elle milite surtout pour l'exploration du système solaire mais a tout de même apporté son soutien au programme de mission habitée vers la « planète rouge » de la Mars Society.
Depuis la mission habitée Apollo 17 de 1972, plus aucun astronaute ne s'est éloigné de plus de quelques centaines de kilomètres de la Terre. Le 20 juillet 1989, pour le 20e anniversaire de l'atterrissage d’Apollo 11, le président des États-Unis George H. W. Bush lance un programme spatial ambitieux sur 30 ans, le Space Exploration Initiative (SEI), qui doit permettre l'installation d'une base permanente sur la Lune. Mais son coût, l'absence de soutien dans l'opinion publique et les fortes réticences du Congrès font capoter le projet. En 2004, son fils, le président George W. Bush, rend publics les objectifs à long terme qu'il souhaite assigner au programme spatial américain alors que l'accident de la navette spatiale Columbia vient de clouer au sol une flotte de navettes spatiales vieillissantes et que le sort de la station spatiale internationale, dont l'achèvement approche, est en suspens. Le projet présidentiel Vision for Space Exploration veut replacer l'Homme au cœur de l'exploration spatiale : le retour d'astronautes sur la Lune est programmé avant 2020 pour une série de missions destinées à préparer une éventuelle présence permanente de l'homme sur le sol lunaire et mettre au point le matériel nécessaire à de futures missions habitées sur Mars fixées à une échéance beaucoup plus lointaine,. Cette fois ci, l'opinion comme le Congrès sont favorables au projet : le programme Constellation est alors mis sur pied par la NASA pour répondre aux attentes présidentielles. Il prévoit la construction de deux types de lanceur Ares I et Ares V ainsi que, de manière similaire au programme Apollo, deux vaisseaux habités Altair et Orion. La NASA utilise, en les adaptant, des moteurs-fusées développés pour la fusée Saturn V, les propulseurs à poudre de la navette spatiale ainsi que de nombreuses installations au sol remontant à l'époque du programme Apollo. Mais le programme prend du retard et se heurte à un problème de financement qui selon les plans initiaux, doit s'effectuer sans augmentation substantielle du budget global de la NASA. À la suite de son investiture, le président américain Barack Obama fait expertiser le programme Constellation par  la commission Augustine, créée à cet effet le 7 mai 2009. Celle-ci conclut qu'il manque trois milliards de dollars par an pour atteindre les objectifs fixés mais confirme l'intérêt d'une seconde exploration humaine de la Lune comme étape intermédiaire avant une mission habitée vers Mars. Début février 2010, le président Obama annonce l'annulation du programme Constellation qui est confirmée par la suite,.
(en) G. Brooks, James M. Grimwood, Loyd S. Swenson, Chariots for Apollo : A History of Manned Lunar Spacecraft, 1979 (lire en ligne).
(en) W. David Compton, Where No Man Has Gone Before : A History of Apollo Lunar Exploration Missions, 1989 (lire en ligne).
(en) Roger E. Bilstein, Stages to Saturn : A Technological History of the Apollo/Saturn Launch Vehicles, 1996 (lire en ligne).
(en) Hansen, James R, Enchanted Rendezvous : John C. Houbolt and the Genesis of the Lunar-Orbit Rendezvous Concept, 1995 (lire en ligne).
(en) Richard W. Orloff (NASA), Apollo by the numbers : A Statistical Reference, Washington, National Aeronautics and Space Administration, 2000-2004 (ISBN 978-0-16-050631-4, OCLC 44775012, LCCN 00061677, lire en ligne).
(en) Sunny Tsiao, Read You Loud and Clear! : The Story of NASA’s Spaceflight Tracking and Data Network, 2008 (lire en ligne).
(en) Divers auteurs sous la direction de R. Johnston, L. Dietlein, et C. Berry, Biomedical Results of Apollo (SP-368), 1975 (lire en ligne).
(en) Conférence NASA Remembering Space Age, Remembering Space Age : remembrance and cultural representation of the space age (SP-2008-4703), 2008 (lire en ligne).
(en) Conférence NASA Societal impact of spaceflight, Societal impact of spaceflight (SP-2007-4801), 2007 (lire en ligne).
Charles Frankel, L'aventure Apollo. Comment ils ont décroché la Lune, Dunod, 2018, 288 p. (ISBN 978-2100772407, lire en ligne)
Philippe Henarejos, Ils ont marché sur la Lune. Le récit inédit des explorations Apollo, Belin, 2018 (ISBN 978-2410000726)
Xavier Pasco, La politique spatiale des États-Unis 1958-1985 : Technologie, intérêt national et débat public, L'Harmattan, 1997 (ISBN 978-2-7384-5270-2).
Alain Duret, Conquête spatiale : du rêve au marché, Paris, Éditions Gallimard, 2002 (ISBN 978-2-07-042344-6).
Jacques Villain, À la conquête de l'espace : de Spoutnik à l'homme sur Mars, Paris, Vuibert Ciel & Espace, 2007, 2e éd. (ISBN 978-2-7117-2084-2).
(en) W. David Woods, How Apollo flew to the moon, New York, Springer, 2008 (ISBN 978-0-387-71675-6, LCCN 2007932412).
(en) David A. Mindell, Digital Apollo Human and Machine in Spaceflight, Cambridge, The MIT Press, 2008, relié (ISBN 978-0-262-13497-2, LCCN 2007032255).
(en) Thomas J. Kelly, Moon lander : how we developed the Apollo Lunar Module, Smithsonian Books 2001, 2001 (ISBN 978-1-58834-273-7).
Giles Sparrow, La Conquête de l'espace, Saint-Laurent, ERPI, 2007 (ISBN 978-2-7613-2726-8, OCLC 298592629).
Notices dans des dictionnaires ou encyclopédies généralistes : Encyclopædia Britannica • Encyclopædia Universalis • Gran Enciclopèdia Catalana • Swedish Nationalencyklopedin
(en) Archives des documents de la NASA sur les missions Apollo (fiches techniques, rapports de mission, journaux de bord, procédures, etc.)
 Portail de l’astronautique   Portail de l’exploration   Portail de la Lune   Portail des États-Unis   Portail des années 1960   Portail des années 1970