
Strip Me è il terzo album della cantante inglese Natasha Bedingfield, pubblicato il 7 dicembre 2010. L'uscita dell'album è stata anticipata dal singolo promozionale Touch e da Strip Me, traccia che dà il nome all'album.
In Europa, Strip Me è stato in seguito ripubblicato nel maggio 2011, con il titolo di Strip Me Away, contenente ulteriori tracce.
All I Need feat. Kevin Rudolf (Natasha Bedingfield, Danielle Brisebois, John Shank, Kevin Rudolf) – 3:45
Weightless (Less Is More Version) (Natasha Bedingfield, Steve Kipner, Wayne Wilkins, Andre Merritt) – 4:31
All I Need feat. Kevin Rudolf (Natasha Bedingfield, Danielle Brisebois, John Shank, Kevin Rudolf) – 3:45
Weightless (Less Is More Version) (Natasha Bedingfield, Steve Kipner, Wayne Wilkins, Andre Merritt) – 4:31

In finanza aziendale ed economia finanziaria, la struttura del capitale definisce il modo in cui un'impresa finanzia i propri investimenti tramite una qualche combinazione di debito, capitale di rischio (o equity, con voce inglese) o titoli finanziari di natura mista. La struttura del capitale è dunque la composizione o, appunto, "struttura," del capitale finanziario dello stato patrimoniale di un'impresa. Ad esempio, un'impresa il cui capitale è costituito per 20 milioni di euro da capitale di rischio e 80 milioni di euro di debito sarà per il 20% equity financed (finanziata tramite capitale di rischio) e per l'80% debt financed (finanziata tramite debito); il rapporto tra il debito di un'impresa e il suo capitale, in questo caso l'80%, è detto leva finanziaria, o con voce inglese leverage.
La teoria economica ha studiato formalmente le scelte di finanziamento delle imprese a partire da un noto lavoro di Modigliani e Miller del 1958. La scelta di una data fonte di finanziamento – risorse generate internamente all'azienda, debito, capitale di rischio – determinerà la struttura del capitale. L'attenzione della teoria economica è incentrata non tanto su quale o quali modalità di finanziamento risultino più convenienti, quanto sul perché del comportamento delle imprese concretamente osservato – in altre parole, l'approccio della teoria economica è in questo contesto positivo, e non normativo.
Lo studio della struttura del capitale ha inizio con il lavoro di Modigliani e Miller (1958), che giunge a una conclusione di irrilevanza della struttura del capitale (ossia la proposizione in base alla quale cambiamenti nella struttura del capitale non creerebbero valore per le imprese), sotto ipotesi ideali circa l'assenza di frizioni nei mercati finanziari. La rimozione di tali ipotesi semplificatrici ha dato luogo a due filoni classici di teorie della struttura del capitale:
Teorie del trade-off, che concludono che esiste, per ciascuna impresa, una struttura ottimale del capitale;
Teorie del pecking order, in generale basate sulla rimozione dell'ipotesi di perfetta informazione alla base del risultato di Modigliani e Miller, che concludono che, in presenza di asimmetrie informative tra i manager delle imprese e il mercato, le imprese troveranno più conveniente ricorrere a forme di finanziamento il cui valore è meno sensibile rispetto all'informazione oggetto dell'asimmetria informativa.Una linea di ricerca più recente, fondata sui risultati della finanza comportamentale, è relativa all'ipotesi del market timing, in base alla quale le imprese ricorrerebbero alla forma di finanziamento più conveniente in un dato momento, sulla base di preferenze "irrazionali" degli investitori; ad esempio, emetterebbero azioni allorché il prezzo di mercato delle azioni è superiore a una loro valutazione "razionale", e viceversa ricorrerebbero al debito quando le azioni sono sottovalutate dal mercato.
Le diverse teorie della struttura del capitale non sono in generale considerate mutuamente esclusive. Poiché ciascuna teoria si fonda sulla rimozione di una particolare ipotesi del risultato originale di irrilevanza della struttura del capitale di Modigliani e Miller, e nella pratica è verosimile che diverse ipotesi siano al contempo violate, è lecito attendersi che i fatti osservati empiricamente risultino da una combinazione dei diversi effetti evidenziati dalle varie teorie. Obiettivo della ricerca sulla struttura del capitale delle imprese è determinare di quanto riscontro goda ciascuna teoria dal punto di vista empirico, nonché verificare quale o quali teorie producano gli effetti economicamente più rilevanti, e sotto quali condizioni.
La finalità delle diverse teorie di seguito illustrate non è suggerire alle imprese modi per finanziare i loro investimenti in modo più economico — più formalmente, le teorie della struttura del capitale non hanno una funzione normativa. Le teorie della struttura del capitale hanno, per contro, una funzione positiva, ossia cercano di descrivere accuratamente il comportamento delle imprese, e di far luce sui meccanismi che ne guidano le scelte.
La ricerca in ambito economico sulla struttura del capitale nasce con un noto lavoro di Franco Modigliani e Merton Miller del 1958, in cui si formula la più semplice versione del teorema di Modigliani-Miller. Questo risultato è di norma considerato puramente teorico, in quanto deriva da ipotesi difficilmente verificate in realtà sulle condizioni alla base delle decisioni di un'impresa riguardo alla struttura del capitale. Il risultato di Modigliani-Miller afferma che, in un mercato privo di frizioni, il valore di un'impresa è indipendente dalla sua struttura del capitale. Una conclusione di questo tipo è utile per studiare le condizioni sotto le quali la struttura del capitale è in effetti rilevante, ossia quali delle ipotesi di Modigliani-Miller devono essere violate allorché la struttura del capitale influisce sul valore di un'impresa.
Al fine di illustrare il teorema di Modigliani-Miller, si supponga un mercato finanziario caratterizzato da assenza di qualsiasi tipo di frizioni (informazione perfetta e simmetrica tra tutti gli operatori del mercato, assenza di costi di transazione e di default sul debito, assenza d'imposte). Si ipotizzi inoltre che le imprese e gli investitori individuali siano in grado di prendere somme di denaro a prestito allo stesso tasso d'interesse.
Il valore di un'impresa è indipendente dalla struttura del suo capitale: intuitivamente, non si può modificare la dimensione della torta tagliandola in pezzi di misure differenti;
Il costo del capitale di rischio (equity) di un'impresa con debito è uguale a quello di un'impresa priva di debito, più un premio per il rischio finanziario. In altri termini, aumentando la leva finanziaria, da un lato si modifica la distribuzione del rischio finanziario tra le diverse classi di investitori (azionisti e creditori dell'impresa); d'altro canto tuttavia il rischio associato all'impresa nel complesso risulta invariato, così che non si crea alcun valore aggiuntivo.Nel loro lavoro del 1963, Modigliani e Miller estendono il loro teorema, includendo il caso in cui siano presenti imposte sul reddito delle imprese. In questo caso, dal momento che i pagamenti degli interessi sul debito sono deducibili, la presenza di debito di finanziamento creerebbe valore per l'impresa. Portando questo ragionamento alle sue estreme conseguenze, se ne dovrebbe dedurre che le imprese dovrebbero finanziare interamente i propri investimenti tramite debito, senza alcun capitale di rischio. Modigliani e Miller osservano tuttavia come la presenza di debito incrementi il rischio d'insolvenza; il trade-off tra i benefici fiscali del debito e i costi d'insolvenza anticipati darebbe luogo a una struttura del capitale ottimale. Questa conclusione apre la strada a una serie di teorie della struttura del capitale note come teorie del trade-off.
In un noto lavoro del 1977, Merton Miller cerca di riscattare l'intuizione originale del risultato da lui ottenuto con Modigliani nel 1958, ossia l'irrilevanza della struttura del capitale. Basandosi su un'argomentazione di equilibrio economico generale, mostra come, dato l'equilibrio del mercato del capitale di rischio o equity, il mercato del debito raggiunge un livello di equilibrio del tasso d'interesse tale che il guadagno in termini di valore derivante a un'impresa da un qualsiasi livello di debito è nullo. Sarebbe lecito a questo punto domandarsi perché, dal momento che non vi sarebbe, sulla base dell'argomentazione di Miller, nessun vantaggio dal debito, nella pratica le imprese abbiano strutture del capitale anche molto diverse. La spiegazione proposta da Miller  è nota come argomentazione della mutazione neutrale: l'adozione di una data struttura del capitale, non danneggiando né avvantaggiando un'impresa, può essere paragonata a quel che sarebbe una mutazione avente analoghe proprietà nel contesto della teoria dell'evoluzione nella biologia. In altre parole, non essendoci una struttura del capitale "preferita," o in qualche senso ottimale, nella pratica è lecito aspettarsi una gran varietà di strutture del capitale.Miller ottiene il suo risultato come segue. In primo luogo, si osservi che, se la presenza di debito apporta un qualche beneficio al valore dell'impresa, il valore così creato deve essere, in base a un'argomentazione di non arbitraggio, tale che:
    {\displaystyle V_{\mathrm {L} }=V_{\mathrm {U} }+rB\left[1-{\frac {(1-\tau _{\mathrm {C} })(1-\tau _{\mathrm {PS} })}{1-\tau _{\mathrm {PB} }}}\right]}
   denotano le aliquote d'imposizione fiscale sul reddito d'impresa e sui redditi personali (dell'investitore marginale) derivanti da guadagno in conto capitale e sul reddito derivante dalla ricezione di pagamenti d'interessi sul debito d'impresa, rispettivamente. L'espressione sopra può essere ottenuta tramite una classica argomentazione di non arbitraggio; il flusso di cassa che l'azionista dell'impresa con debito riceve sarà, dato un livello di profitto 
  Per replicare questo flusso di cassa, è sufficiente acquistare una stessa quota dell'impresa priva di debito (ottenendo così 
    {\displaystyle B{\frac {(1-\tau _{\mathrm {PB} })-(1-\tau _{\mathrm {C} })(1-\tau _{\mathrm {PS} })}{1-\tau _{\mathrm {PB} }}}}
  , dal momento che i pagamenti di interessi sono deducibili). Combinando i due valori, si ottiene l'espressione di sopra.
   sia trascurabile, dal momento che il pagamento di imposte sui guadagni in conto capitale può essere differito indefinitamente, fino alla loro realizzazione - così che, ad ogni buon conto, l'imposizione sui guadagni in conto capitale non influisce sulla domanda e offerta di debito. Si consideri dunque il mercato del debito d'impresa nel complesso, ossia dei titoli di debito emessi da tutte le imprese dell'economia, supponendo che il mercato delle azioni emesse da tutte le imprese nell'economia sia in equilibrio (la legge di Walras fa sì che anche il mercato del debito sarà in equilibrio). La funzione di domanda di titoli di debito è inizialmente orizzontale, ossia costante per un livello di tasso di rendimento pari a quello domandato dagli investitori esenti da tassazione (
  , che sarà pari al rendimento di equilibrio sul mercato delle azioni, onde escludere un arbitraggio tra i due mercati, come deve essere in condizioni di equilibrio). Una volta esauriti gli investitori esenti da tassazione, il tasso di rendimento domandato sarà più elevato; al fine di compensare gli investitori per la tassazione, sarà infatti pari a:
   è l'aliquota dell'investitore marginale. Il significato dell'espressione sopra è evidente: l'investitore marginale riceverà, al netto delle tasse, esattamente un rendimento pari a 
    {\displaystyle [r_{0}/(1-\tau _{\mathrm {PB} }^{m})]\times (1-\tau _{\mathrm {PB} }^{m})=r_{0}}
  ), così che non troverà conveniente non comprare debito e rivolgersi invece al mercato delle azioni. Questa condizione di indifferenza tra i due mercati da parte dell'investitore marginale è alla base dell'equilibrio nel modello di Miller.
Analogamente si deriva l'offerta di titoli di debito da parte delle imprese. Essa sarà costante per un tasso di rendimento pari a:
  La giustificazione dell'espressione sopra è che, ancora una volta, al netto dell'imposizione fiscale l'impresa offre un rendimento pari a quello che predomina in equilibrio nel mercato delle azioni, cioè 
La condizione di equilibrio del mercato del debito è che domanda e offerta siano uguali; sulla base delle espressioni derivate sopra, ciò significa:
    {\displaystyle {\frac {r_{0}}{1-\tau _{\mathrm {C} }}}={\frac {r_{0}}{1-\tau _{\mathrm {PB} }^{m}}}\quad \iff \quad \tau _{\mathrm {C} }=\tau _{\mathrm {PB} }^{m}}
  In altre parole, la quantità aggregata di debito presente sul mercato sarà tale che l'aliquota d'imposizione fiscale sui redditi dell'investitore marginale 
  . Investitori con aliquote più elevate non deterranno alcun titolo di debito nei loro portafogli.
  . Sostituendo questi risultati nell'espressione per il valore aggiuntivo creato dal debito, si ottiene:
  ossia, ancora una volta la struttura del capitale è irrilevante, nel senso che, in condizioni di equilibrio del mercato del debito aggregato, essa non crea alcun valore per le imprese.Il risultato di Miller non gode di ampio sostegno tra in ambito accademico (così come tra i practitioners). Le critiche all'argomentazione di Miller si fondano essenzialmente sul carattere restrittivo delle sue ipotesi.
Un primo aspetto negativo del modello di Miller, ad esempio, è che il risultato di equilibrio viene meno allorché si ammette la possibilità di vendite allo scoperto. Si consideri in particolare il caso di un investitore i, la cui aliquota d'imposizione fiscale sui redditi percepiti dai titoli di debito è minore di quella dell'investitore marginale:
  Si supponga per un istante che il risultato di Miller sia ancora valido, e che l'investitore i venda allo scoperto un euro di azioni, finanziando così l'acquisto di un euro di titoli di debito. i dovrà, a scadenza dell'investimento, pagare un rendimento 
   sulle azioni che ha venduto allo scoperto; dai titoli di debito otterrà un rendimento, al netto della tassazione, pari a:
    {\displaystyle {\frac {r_{0}}{1-\tau _{\mathrm {PB} }^{m}}}\times (1-\tau _{\mathrm {PB} }^{i})>r_{0}}
  Dunque l'investitore i conseguirebbe un profitto strettamente positivo, con un investimento iniziale nullo: si tratta di un perfetto esempio arbitraggio. Da un lato, se una tale possibilità di arbitraggio realmente esistesse, un investitore nelle condizioni di i vorrebbe naturalmente prendere posizioni "infinite" su azioni e titoli di debito, al fine di ottenere un profitto "infinito." Questo però renderebbe impossibile un equilibrio del mercato dei titoli di debito, così che il risultato di Miller, in definitiva, non potrebbe valere. D'altra parte, un modello realistico dovrebbe escludere la situazione in cui si può ottenere qualcosa dal nulla, come nel caso di i qui esposto. In conclusione, se si ammette la possibilità di vendite allo scoperto, il modello di Miller porta a conclusioni paradossali.
Una linea di critica alternativa parte da ipotesi più generali di quelle di Miller. Un esempio è il caso di una struttura dell'imposizione fiscale più sofisticata, in cui è infatti possibile recuperare un risultato di ottimalità della struttura del capitale (DeAngelo e Masulis, 1980); inoltre, rimuovendo l'ipotesi di informazione perfetta, è possibile ottenere una preferenza delle imprese per particolari forme di finanziamento (è la conclusione generale delle teorie del pecking order, si veda sotto, nonché Myers (1984) e Myers e Majluf (1984)).
Le teorie del trade-off della struttura del capitale partono dall'ipotesi che, in presenza di una frizione di qualche forma nei mercati finanziari (cosicché non si ricade nelle ipotesi del teorema di Modigliani-Miller), il debito presenta dei benefici e dei costi per un'impresa. Il trade-off tra costi e benefici determina una struttura ottimale del capitale, corrispondente al livello di indebitamento che eguaglia i benefici marginali ai costi marginali del debito.
Un esempio, fornito dagli stessi Modigliani e Miller nel loro lavoro del 1963, è dato dal caso del debito in presenza di tassazione: se da un lato la presenza di indebitamento crea dei benefici fiscali per le imprese, poiché i pagamenti sugli interessi sono deducibili, d'altra parte essa aumenta i costi attesi derivanti dall'eventuale insolvenza. Il livello ottimale del debito uguaglierà i benefici marginali ai costi marginali, determinando una struttura del capitale ottimale.Le teorie del trade-off non si limitano al caso dei benefici fiscali e dei costi d'insolvenza; diversi lavori hanno evidenziati benefici e costi alternativi. Uno dei primi esempi è dato dal lavoro di Myers (1977), che introduce il problema del debt overhang: in presenza di debito, un'impresa che agisce nell'interesse dei propri azionisti potrebbe non sfruttare delle opportunità di investimento aventi un valore attuale netto positivo, poiché parte dei benefici da esse derivanti andrebbero a vantaggio dei creditori dell'impresa, senza alcuna ricaduta positiva per gli azionisti. Ciò costituirebbe un ulteriore costo del debito, che andrebbe ad aggiungersi ai costi attesi d'insolvenza proposti da Miller e Modigliani.Un esempio stilizzato può rendere più chiara questa argomentazione. Si supponga che un'impresa abbia un progetto d'investimento il cui valore attuale netto è pari a 100 000 euro. Poiché il valore attuale netto è positivo, è efficiente per l'impresa intraprendere l'investimento. Se tuttavia l'impresa ha un deficit patrimoniale pari anch'esso a 100 000 euro (il valore di mercato dell'attivo, ad esempio, è pari a 500 000 euro, mentre il debito è pari a 600 000 euro), tutto il profitto derivante dall'investimento andrà ai suoi creditori, a ripagare il debito. Se il management agisce nell'interesse degli azionisti, non avrà dunque incentivo a intraprendere il progetto d'investimento — che pure sarebbe efficiente — dal momento che questo non apporterà alcun beneficio agli azionisti stessi (ciò accade, in particolare, se il nuovo progetto di investimento deve essere finanziato almeno in parte dagli azionisti).
I casi evidenziati da Miller e Modigliani e Myers non sono gli unici esempi di teorie del trade-off. L'elemento che caratterizza questo insieme di teorie è costituito dalla conclusione che, tramite un trade-off tra benefici e costi attesi del debito, si possa determinare una struttura ottimale del capitale, tale cioè che massimizzi il valore di un'impresa. Questa proposizione distingue le teorie del trade-off dalle teorie del pecking order e del market timing.
All'interno delle teorie del trade-off è possibile distinguere una sottoclasse di teorie che individuano una struttura ottimale del capitale, basate su modelli d'agenzia. Tali teorie rimuovono l'ipotesi di informazione simmetrica di Modigliani e Miller (1958), così come le teorie del pecking order (si veda sotto); a differenza di queste ultime, fondate principalmente sullo studio di casi di selezione avversa (con voce inglese, adverse selection), le teorie basate sui costi d'agenzia si concentrano su problemi legati all'azzardo morale (o azione nascosta; con voce inglese, moral hazard).
L'esempio più classico, e largamente citato, è dovuto a Jensen e Meckling (1976). Essi argomentano come da un lato, tanto maggiore è la percentuale del capitale di rischio (equity) detenuta dal management di un'impresa, tanto più gli incentivi del management saranno allineati con quelli della proprietà, così che si eviteranno comportamenti del management che nuocciono al valore dell'impresa (ad es. costituzione di "imperi" tramite l'acquisizione di altre imprese, uso del jet aziendale, etc.: tali comportamenti negativi sono noti come costi d'agenzia dell'equity). Portando quest'argomentazione all'estremo, se ne potrebbe concludere che le imprese dovrebbero essere possedute al 100% dai propri manager, e dovrebbero finanziare i propri investimenti interamente con debito.
Le imprese avranno in genere, tuttavia, un rapporto di leva finanziaria inferiore al 100%, a causa costi d'agenzia associati al debito. In particolare, Jensen e Meckling evidenziano il problema del risk shifting, o asset substitution. Dal momento che, per la responsabilità limitata, in caso di fallimento la proprietà di un'impresa non sopporta costi oltre la misura del proprio investimento iniziale, il rischio di fallimento associato a un dato progetto d'investimento è interamente a carico dei creditori.
Tanto maggiore sarà il debito, tanto più la proprietà, e il management, avranno incentivo a intraprendere progetti d'investimento più rischiosi: questo perché i creditori sopporteranno il rischio, laddove la proprietà otterrà il guadagno nel caso in cui il progetto vada a buon fine. Creditori "razionali" (nel senso dato al termine nell'ambito dell'economia) incorporeranno questa considerazione nel processo tramite il quale determinano la loro domanda per i titoli di debito di un'impresa, richiedendo uno sconto tanto maggiore quanto maggiore sarà l'incentivo dell'impresa al risk shifting. Poiché il debito sarà più costoso, l'impresa sostiene dunque un costo, associato al problema d'agenzia del risk shifting.
La soluzione del trade-off tra costi d'agenzia dell'equity e del debito determinerà la struttura ottimale del capitale. Questa corrisponderà al livello della leva finanziaria che rende minimi i costi totali d'agenzia, dati dalla somma dei costi d'agenzia dell'equity e del debito.
Un'argomentazione differente, anch'essa basata sull'ipotesi della sussistenza di costi d'agenzia, è quella proposta da Michael Jensen in un noto lavoro del 1986. Jensen parte dall'ipotesi che il management di un'impresa, se ha il controllo su un elevato free cash flow (ossia, quanto rimane del flusso di cassa allorché tutti i progetti d'investimento caratterizzati da un valore attuale netto positivo sono stati intrapresi) ha la tendenza a utilizzare tale risorsa finanziaria per progetti dannosi per l'impresa. Esempi di tali progetti potrebbero essere acquisizioni improduttive o sprechi di vario tipo.
Il debito presenterebbe dunque un beneficio che va al di là dei benefici fiscali evidenziati da Miller e Modigliani (1963) e DeAngelo e Masulis (1980). Nello specifico, costringerebbe il management a una serie di pagamenti fissi di interessi sul debito; in tal modo, andrebbe a ridurre il cash flow liberamente disponibile al management, mitigando i costi d'agenzia rappresentati dai progetti improduttivi che il management potrebbe altrimenti intraprendere.
Le teorie del pecking order partono dalla rimozione dell'ipotesi di Modigliani e Miller (1958, 1963) di informazione perfetta. Nello specifico, ipotizzano che il management delle imprese disponga di informazioni più precise riguardo a un qualche aspetto delle prospettive d'investimento delle imprese stesse. La conclusione che unifica le diverse teorie del pecking order è che le imprese preferiranno ricorrere alla forma di finanziamento il cui valore è meno sensibile rispetto alla particolare informazione oggetto dell'asimmetria informativa.L'esempio più semplice, e che ha dato il via a questo ramo della ricerca sulla struttura del capitale, è fornito da Myers e Majluf (1984): ipotizzando che il management conosca il vero valore delle attività dello stato patrimoniale delle imprese, che sono invece note solo in maniera imprecisa agli investitori e al mercato, le imprese preferiranno finanziare i propri investimenti in primo luogo tramite cash flow generati internamente, quindi tramite debito, e ricorreranno all'emissione di nuove azioni solo come estrema soluzione, laddove nessuna delle alternative precedenti fosse disponibile. L'intuizione alla base di questo risultato può essere espressa come segue.
Si consideri il confronto tra l'emissione di titoli di debito e nuove azioni (equity). Il finanziamento tramite debito consente di procedere con l'investimento; in caso l'investimento stesso vada a buon fine, i creditori otterranno un pagamento fisso, così che tanto maggiore è il rendimento dell'investimento, tanto maggiore sarà il profitto degli azionisti. Se invece si emettono nuove azioni, i nuovi azionisti parteciperanno, in proporzione al loro investimento, in qualunque guadagno derivante dall'investimento stesso: in altre parole, il profitto degli azionisti originari sarà minore. Dunque solo le imprese che prevedono per i propri investimenti un rendimento non troppo elevato emetteranno nuove azioni — se il rendimento atteso fosse assai elevato, preferirebbero ricorrere al debito, per non doversi trovare a dividere profitti notevoli con i nuovi azionisti.
Il mercato, replicando questo stesso ragionamento, penalizzerà un'emissione di nuove azioni, considerandola un segnale che le prospettive degli investimenti dell'azienda emittente non sono buone. Questo fatto risulta ampiamente verificato empiricamente: il prezzo delle azioni ordinarie di norma perde un 2% in seguito all'emissione di nuovo capitale di rischio, nei mercati USA. Di conseguenza, al fine di evitare una penalizzazione del proprio valore di mercato, le imprese eviteranno di emettere nuove azioni, preferendo il ricorso al debito (questa conclusione, per contro, non è universalmente riconosciuta in letteratura, ad esempio da chi propende per teorie della struttura del capitale diverse dal pecking order).
Un esempio numerico può rendere più chiara l'argomentazione di Myers e Majluf alla base del risultato del pecking order. Si supponga che sul mercato operino imprese "buone," il cui valore è 
  . Tutte le imprese possono investire 20 in un progetto d'investimento che renderà, con certezza, un profitto pari a 25 (ipotizzando un tasso di sconto nullo, il progetto ha dunque un valore attuale netto positivo, e pari a 
Ricorrendo al linguaggio della teoria dei giochi, si può affermare che esiste un equilibrio di Nash con informazione imperfetta caratterizzato da probabilità agli occhi degli investitori:
    {\displaystyle V=V_{B}=100\quad \Rightarrow \quad {\mbox{Non emettere azioni}}}
  L'insieme di strategie e probabilità sopra esposto è un equilibrio di Nash se nessun tipo di impresa ha un incentivo a deviare dalla strategia prescritta dall'equilibrio stesso, date le probabilità. Questa condizione può essere verificata come segue. In primo luogo, le imprese "cattive" dovranno emettere una quota di nuove azioni 
  così che l'investimento sia interamente finanziato. Ciò implica α = 20/75 = 0,27. Un'impresa "cattiva" potrebbe deviare dalla propria strategia di equilibrio e non investire. Ciò implicherebbe tuttavia un profitto per gli azionisti esistenti pari a 50, minore del profitto che otterrebbero investendo, dato da:
  D'altra parte, un'impresa "buona" potrebbe deviare dalla propria strategia di equilibrio e investire, emettendo nuove azioni. In tal caso, date le probabilità di equilibrio, gli investitori la riterrebbero un'impresa "cattiva," così che dovrebbe emettere 
   anch'essa. Questo implica che investire emettendo nuove azioni è meno conveniente che non investire affatto: infatti, emettendo nuove azioni gli azionisti otterrebbero un profitto pari a:
  L'equilibrio così ottenuto descrive un ordine di priorità per le fonti di finanziamento (con termine inglese, un pecking order appunto). In primo luogo, sotto le nostre ipotesi le imprese "buone" non emetteranno nuove azioni; se investono, finanzieranno il proprio investimento tramite una qualche forma di finanziamento alternativa, ad esempio il debito. Secondariamente, l'emissione di nuove azioni rivela agli occhi degli investitori che un'impresa è del tipo "cattivo;" in seguito all'annuncio di un'emissione di nuove azioni dunque, gli investitori rivedranno al ribasso la propria valutazione dell'impresa stessa (questo risultato è coerente con l'evidenza empirica sulle emissioni di nuove azioni nei mercati USA).
Un importante caveat è che non necessariamente l'ordinamento nelle preferenze sulle forme di finanziamento stabilito da Myers e Majluf deve essere rispettato. Nella formulazione più generale della teoria del pecking order, un'impresa finanzierà i propri investimenti ricorrendo alla forma di finanziamento il cui valore è meno sensibile rispetto alla particolare informazione oggetto dell'asimmetria informativa; per particolari tipi di informazione, l'emissione di nuovo capitale di rischio potrebbe risultare preferibile rispetto all'indebitamento.
Le teorie del market timing hanno un più recente sviluppo (sono riconducibili ai lavori di Stein (1996) e Baker e Wurgler (2002)), e partono da posizioni almeno in parte distanti da quelle delle più ortodosse teorie del trade-off e del pecking order. In particolare, l'idea del market timing si fonda sull'ipotesi, presa in prestito dalla letteratura sulla finanza comportamentale, che il mercato possa dare una valutazione non efficiente (nel gergo della disciplina, "irrazionale") delle azioni di un'impresa o del suo debito. Il senso in cui l'"irrazionalità" della valutazione di mercato è da intendersi è che il valore di mercato delle azioni (o del debito) di un'impresa possa non corrispondere a un valore di riferimento, "razionale" appunto, dato dal valore atteso scontato dei flussi di cassa associati alle azioni stesse (o al debito). Non vi è, in generale, un accordo tra gli studiosi riguardo a quest'ultima ipotesi; lo scontro tra i fautori dell'ipotesi di non razionalità e i sostenitori dell'efficienza informativa del mercato assume talvolta toni accesi e quasi ideologici.La teoria del market timing non si preoccupa di valutare la bontà dell'ipotesi di una valutazione non razionale da parte del mercato, ma semplicemente la prende per buona, e procede con la seguente argomentazione. Si ipotizzi che, in un dato momento, il mercato dia una valutazione esageratamente positiva delle azioni di un'impresa; si supponga inoltre che l'impresa abbia un progetto d'investimento per il quale necessita di raccogliere un finanziamento, in forma di sottoscrizione di nuovo capitale di rischio (che comporterebbe l'emissione sul mercato di nuove azioni) o di debito. Dal momento che il mercato assegna un valore elevato alle azioni dell'impresa, questa troverà conveniente emettere nuove azioni piuttosto che ricorrere al debito. Poiché le azioni di nuova emissione avranno una elevata valutazione ("irrazionale," o inefficiente dal punto di vista informativo, e che dunque non risente degli effetti messi in luce dalla teoria del pecking order), sarà sufficiente emetterne una quantità relativamente ridotta al fine di raccogliere il finanziamento necessario ad effettuare l'investimento, così che gli attuali azionisti dell'impresa vedranno la propria partecipazione diluita in maniera trascurabile. Un ragionamento analogo può essere effettuato, naturalmente, per il caso in cui la valutazione di mercato del debito di un'impresa sia elevata in maniera "irrazionale."La conclusione dunque è che le decisioni di un'impresa riguardo alla propria struttura del capitale non sarebbero motivate da considerazioni ottimalità (come nel caso della teoria del trade-off), né da preoccupazioni circa l'informazione che una data scelta di finanziamento rivelerebbe al mercato (come nella teoria del pecking order). Esse sarebbero semplicemente guidate da ciò che il mercato, in un dato istante di tempo, preferisce, per motivi non necessariamente "razionali."
Le teorie del trade-off e del pecking order, più datate, sono state sottoposte a un gran numero di verifiche empiriche; sulla più recente teoria del market timing si ha una serie di risultati di carattere preliminare. Qualunque verifica empirica di una teoria della struttura del capitale ha (almeno) due scopi: in primo luogo, valutare se la teoria in questione regge il confronto con il dato empirico, ossia se non è immediatamente falsificata dai dati.
Il secondo scopo di una verifica empirica è più sottile; si è osservato sopra come ciascuna teoria parta dalla rimozione di una qualche ipotesi del teorema di Modigliani-Miller del 1958: la teoria del trade-off ipotizza che vi siano imposizione fiscale e costi d'insolvenza o d'agenzia; la teoria del pecking order si fonda sull'esistenza di asimmetrie informative tra imprese e investitori; la teoria del market timing sull'ipotesi che la valutazione di mercato dei titoli di un'impresa possa discostarsi dal loro valore "razionale". Dal momento che nella pratica è verosimile che più ipotesi di Modigliani-Miller siano violate al contempo, un buon test di una teoria della struttura del capitale dovrebbe essere in grado di appurare l'effettivo potere esplicativo della teoria in questione, paragonata alle altre teorie: in altre parole, il test dovrebbe accertare se la teoria presa in esame contribuisce a spiegare un qualche comportamento delle imprese che non è giustificabile nei termini delle teorie alternative.
Una verifica empirica di una teoria della struttura del capitale è, in genere, un'analisi statistica di dati relativi alle scelte di finanziamento di un campione d'imprese. Le verifiche empiriche studiano, tramite gli strumenti dell'econometria, dati di bilancio delle imprese, e in particolare la loro leva finanziaria, e cercano di metterli in relazione con variabili che corrispondono ai comportamenti predetti dalle diverse teorie. Laddove i risultati su un campione d'imprese siano vicini alle previsioni di una data teoria in maniera statisticamente significativa (intuitivamente, ciò corrisponde al caso in cui la probabilità di affermare una conclusione errata sulla base dei risultati sia ragionevolmente bassa), si ha evidenza favorevole alla teoria in questione; in caso contrario, si ha evidenza incompatibile con teoria in questione, o a essa sfavorevole.
Allo stato attuale, la ricerca empirica sulla struttura del capitale non ha raggiunto un consenso riguardo a quale delle teorie sopra illustrate sia da considerarsi "corretta." Diverse teorie sono in grado di rendere conto di diversi aspetti del comportamento delle imprese, così che la performance empirica di una data teoria dipende in maniera cruciale dal grado in cui le condizioni del test e del set di dati utilizzati per una sua verifica corrispondono alle ipotesi alla base della teoria stessa.
Graham (2000) considera la tesi di Miller e Modigliani (1963), secondo la quale un'impresa deriverebbe un vantaggio fiscale dall'indebitamento, dal momento che i pagamenti di interessi sono deducibili. Secondo quest'argomentazione, le imprese dovrebbero incrementare il loro indebitamento fino al punto in cui il beneficio marginale del debito è nullo — ossia, fino al punto in cui il beneficio apportato da un dollaro extra di debito è completamente compensato dall'incremento dei costi attesi d'insolvenza, così che non è più conveniente per l'impresa indebitarsi ulteriormente.
L'aspetto ingegnoso del lavoro di Graham è il calcolo del beneficio marginale derivante dall'indebitamento; Graham stima questo valore con un metodo di simulazione basato sull'espressione di Miller (1977, si veda sopra) per il vantaggio fiscale del debito, ottenendo per una qualsiasi impresa una curva del beneficio marginale del debito. La curva dell'impresa tipica è inizialmente orizzontale: per livelli di indebitamento modesti, il vantaggio di un aumento del debito è costante. Raggiunto un dato livello di indebitamento, il beneficio marginale del debito decresce, poiché i costi attesi d'insolvenza si fanno più rilevanti.
Sulla base della tesi di Miller e Modigliani, le imprese dovrebbero indebitarsi almeno fino al punto in cui la loro curva del beneficio marginale del debito inizia a decrescere: fino a quel punto, infatti, incrementare il debito apporta valore. Graham riscontra tuttavia come la gran parte delle imprese del suo campione (imprese quotate sui mercati USA, i cui dati sono ottenuti dal dataset Compustat) ha livelli di debito significativamente inferiori.Una possibile conclusione è che la tesi di Miller e Modigliani sia errata, e che sussistano ulteriori costi del debito, non presi in considerazione, che rendono ottimale un livello di indebitamento inferiore a quello predetto sulla base delle curve derivate tramite l'espressione di Miller. Una soluzione alternativa è che Graham non abbia considerato voci di bilancio che, pur non essendo debito, hanno una funzione equivalente al debito. È questo il caso dei piani di stock option a favore dei dipendenti, che costituiscono una passività per le imprese; Graham et al. (2004) mostrano come, una volta tenuto conto di tale voce, il livello di indebitamento (in senso generalizzato, includendo i piani di stock option) sia in media più vicino a quello predetto da Miller e Modigliani.
La principale conclusione delle teorie del pecking order è che un'impresa ricorrerà a diverse fonti di finanziamento – liquidità generata internamente, debito, ed equity – seguendo un ordine preciso (detto appunto pecking order), legato al grado di asimmetria informativa tra l'impresa e il mercato, nonché all'oggetto dell'asimmetria informativa stessa.
In termini più concreti, il modello di Myers e Majluf (1984) predice che un'impresa finanzierà i propri investimenti in primo luogo tramite risorse finanziarie interne, quindi ricorrendo al debito, e solo una volta che ogni altra fonte di finanziamento è esaurita l'impresa emetterà nuove azioni. La ragione di questa conclusione è che emettere nuove azioni andrebbe a danneggiare gli azionisti esistenti, diluendo la loro quota del profitto derivante dall'investimento; se l'investimento è particolarmente buono, l'impresa rinuncerà a investire piuttosto che diluire la quota degli azionisti esistenti.Una conseguenza del modello di Myers e Majluf è dunque che l'annuncio di un'emissione di nuove azioni rivela al mercato informazioni circa la qualità degli investimenti che l'impresa intende finanziare tramite il nuovo capitale; nello specifico, tali investimenti saranno di cattiva qualità – caratterizzati, ad esempio, da un valore attuale netto negativo, o comunque troppo modesto. Di conseguenza, in seguito all'annuncio di un'emissione di nuove azioni (detta con termine inglese Seasoned Equity Offering o SEO) il prezzo di mercato delle azioni dell'impresa dovrebbe subire una caduta. Questo risultato gode di un certo riscontro nella pratica, ed è osservato tramite la tecnica econometrica dell'event study.Un test più diretto della teoria del pecking order nella versione di Myers e Majluf è quello proposto da Myers e Shyam-Sunder (1999), in seguito riproposto e rielaborato, ad es. da parte di Frank e Goyal (2003). Piuttosto che concentrarsi sugli effetti di un annuncio di emissione di nuove azioni sul prezzo di mercato, Myers e Shyam-Sunder osservano che la teoria conclude essenzialmente che, una volta esaurite le fonti di finanziamento interne, le imprese ricorreranno (principalmente) al debito per finanziare i propri investimenti.
Myers e Shyam-Sunder definiscono quindi il deficit finanziario di un'impresa come la differenza tra gli investimenti in un dato anno e le fonti interne di finanziamento disponibili. Se la teoria di Myers e Majluf è corretta, tale ammontare dovrebbe corrispondere al debito di nuova emissione per lo stesso periodo di tempo – in altre parole: la teoria del pecking order spiegherebbe non tanto il livello del debito (questo sarebbe piuttosto il compito della teoria del trade-off), bensì le variazioni del debito. Al fine di testare statisticamente questa proposizione, Myers e Shyam-Sunder stimano il seguente modello di regressione lineare:
   il deficit finanziario dell'impresa. Se la teoria di Myers e Majluf è corretta, la statistica 
   associata a questa regressione sarà prossima al 100%, ossia le emissioni di debito saranno interamente spiegate dal deficit finanziario; inoltre il coefficiente di regressione 
   prossime a 1, ma inferiori a 1 in maniera statisticamente significativa. Questo conferebbe almeno in parte la teoria del pecking order; resta tuttavia da determinare quale variabile o quali variabili spieghino la restante parte della variabilità delle emissioni di debito.
Un test assai citato è condotto da Fama e French (2002). Fama e French confrontano la teoria del trade-off e la teoria del pecking order, sulla base di un campione di imprese quotate sui mercati statunitensi (imprese ricomprese nel dataset Compustat, largamente utilizzato negli studi empirici di finanza). I loro risultati sono parzialmente in accordo con entrambe le teorie, ma offrono al contempo alcuni risultati incompatibili con entrambe.
In primo luogo, Fama e French osservano come la leva finanziaria delle imprese tenda a convergere nel lungo periodo a un valore fisso, dal quale si discosta solo temporaneamente. Questo sarebbe compatibile con la teoria del trade-off: il valore fisso sarebbe il valore ottimale della leva finanziaria, a cui le imprese tenderebbero a convergere. D'altra parte, la velocità con cui avviene la convergenza è considerata da Fama e French troppo modesta per considerare questo effetto come economicamente rilevante.Per contro, Fama e French riscontrano come le imprese con maggiori profitti abbiano una leva finanziaria minore; questo risultato sarebbe compatibile con l'ipotesi del pecking order: imprese con maggiori profitti preferirebbero ricorrere alla cassa per finanziare i propri investimenti, piuttosto che a finanziamenti esterni (il debito, in questo caso), per evitare l'effetto negativo sul loro valore di mercato che consegue a un'emissione di debito (previsione compatibile con l'argomentazione di Myers e Majluf (1984), per quanto gli effetti di un'emissione di debito sarebbero comunque inferiori a quelli di un'emissione di equity). Il risultato sarebbe inoltre incompatibile con la teoria del trade-off, in base alla quale imprese più profittevoli, avendo più da guadagnare dai benefici fiscali del debito, dovrebbero avere una leva finanziaria più elevata.Infine, Fama e French osservano dei fatti incompatibili con entrambe le teorie. Da un lato, l'evidenza a favore della teoria del pecking order citata sopra sembrerebbe contraddire la teoria del trade-off; d'altro canto, Fama e French riscontrano inoltre come la teoria del pecking order non sia in grado di giustificare il fatto che, negli anni più recenti del campione, numerose imprese sono finanziate interamente tramite equity, senza alcun ricorso al debito.
Rajan e Zingales (1995) conducono un test su dati internazionali, relativi ai paesi del G7. Questi studiosi riscontrano come alcuni risultati ottenuti su imprese quotate nei mercati statunitensi si ritrovino nel campione internazionale. Risulta tuttavia poco chiaro se le spiegazioni date a tali fatti stilizzati sulla base di dati relativi a imprese quotate sui mercati USA siano adeguate al caso dei mercati internazionali.Ad esempio, imprese con maggiore leva finanziaria sono caratterizzate da un più elevato rapporto valore di libro-valore di mercato delle azioni. La spiegazione di questo risultato proposta per il mercato americano si basa su un'argomentazione di Fama e French (1992), che ipotizzano che il valore di mercato delle azioni sconti un rischio d'insolvenza, associato a una leva finanziaria più elevata.
Rajan e Zingales, tuttavia, osservano come, se questo fosse vero, l'effetto riscontrato dovrebbe essere ancora più rilevante tra le imprese con una leva finanziaria più elevata. Dal momento che, per contro, il risultato sembrerebbe essere pertinente soltanto alla proporzione del campione composta da imprese aventi leva finanziaria meno elevata, la spiegazione di Fama e French non sarebbe valida.Una spiegazione alternativa è che le imprese emettano equity, ossia nuove azioni, andando a ridurre la propria leva finanziaria, allorché il valore di mercato delle azioni è più elevato (e di conseguenza il rapporto valore di libro-valore di mercato più basso); questa spiegazione giustificherebbe i risultati di Rajan e Zingales, e appare compatibile con la teoria del market timing (che tuttavia non era ancora stata sviluppata all'epoca del lavoro di Rajan e Zingales, nel 1995).
I primi lavori sulla(e) teoria(e) del market timing hanno un carattere spiccatamente empirico, e riportano evidenza favorevole alla teoria stessa. Baker e Wurgler (2002) riscontrano come la struttura del capitale delle imprese sia largamente spiegata da una variabile che battezzano "media ponderata del finanziamento esterno" (in inglese, external finance weighted-average, EFWA); quest'ultima misura è definita come:
    {\displaystyle \left({\frac {M}{B}}\right)_{\mathrm {EFWA} ,t}=\sum _{s=0}^{t}{\frac {e_{s}+d_{s}}{\sum _{r=0}^{t}e_{r}+d_{r}}}\left({\frac {M}{B}}\right)_{s}}
   è il rapporto valore di mercato-valore di libro delle azioni di un'impresa, o Q di Tobin. L'intuizione sottostante a tale misura è che essa (i) riassume l'evoluzione storica del rapporto Q di Tobin per un'impresa, e (ii) attribuisce un peso maggiore ai valori Q di Tobin corrispondenti agli anni in cui l'impresa ha fatto più intensamente ricorso a fonti esterne di finanziamento (equity o debito).
I risultati di Baker e Wurgler indicano come la EFWA spieghi in maniera statisticamente ed economicamente significativa il rapporto equity-to-assets tra valore di libro delle azioni e totale attivo dello Stato Patrimoniale delle imprese, assunto quale misura con cui un'impresa ricorre al finanziamento tramite equity, ossia emissioni azionarie. Il significato di questo risultato è che l'evoluzione storica del rapporto Q di Tobin spiega la struttura del capitale corrente di un'impresa; in altri termini, il capitale di un'impresa sarà costituito per una percentuale più elevata da equity se storicamente le azioni dell'impresa hanno avuto un elevato valore di mercato: è, ovviamente, la tesi del market timing.Welch (2002) giunge a una conclusione opposta rispetto a Baker e Wurgler. In particolare, Welch mostra come il management delle imprese non riesca ad adattare rapidamente la struttura del capitale in risposta a shock esogeni dei rendimenti di mercato delle azioni. La struttura del capitale delle imprese appare il risultato dell'evoluzione dei rendimenti delle azioni, che varierebbe, in maniera fuori dal controllo del management, la componente di equity del capitale; il management non sarebbe in grado di portare avanti una strategia attiva di market timing.
Un approccio alternativo ai test empirici presentati sopra è quello seguito da Graham e Harvey (2001, 2002). Graham e Harvey conducono un sondaggio tra i direttori finanziari (utilizzando la sigla inglese, CFO – Chief Financial Officer) di un campione di imprese quotate sui mercati USA, nel tentativo di valutare fino a che punto i criteri decisionali utilizzati dai CFO nella pratica corrispondono a quanto previsto dalla teoria economica.
I risultati di Graham e Harvey evidenziano alcune differenze tra la teoria e la pratica della finanza aziendale. In particolare, le risposte al sondaggio date dai CFOs indicano come questi basino le proprie decisioni su criteri euristici o informali, che non sempre rispecchiano le indicazioni normative della teoria economica.I principali fattori alla base della politica sull'indebitamento risultano essere, ad esempio, l'obiettivo di mantenere una buona flessibilità finanziaria, nonché quello di conservare un buon rating, o merito di credito. Alcuni risultati sembrerebbero poi puntare nella direzione indicata dalla teoria del market timing: quando emettono nuove azioni, le imprese sembrano preoccupate soprattutto dell'andamento recente del loro valore di mercato.Sebbene Graham e Harvey riscontrino evidenza parzialmente favorevole all'ipotesi del pecking order, dai risultati del sondaggio si evince come i CFO non si preoccupino di effetti quali l'asset substitution (Jensen e Meckling, 1976), i costi d'agenzia del free cash flow (Jensen, 1986), problemi di tassazione degli azionisti o dei creditori, asimmetrie informative o altre simili questioni di carattere teorico. Gli effetti di tali problemi possono essere incorporati nei prezzi delle azioni e dei titoli di debito delle imprese, così che, ancorché indirettamente, rientrino comunque nelle decisioni del management; Graham e Harvey tuttavia non testano questa ipotesi.
Franco Modigliani, Merton Miller, The Cost of Capital, Corporate Finance, and the Theory of Investment, in American Economic Review, vol. 48, nº 3, 1958,  pp. 261-297.
 Merton Miller, Franco Modigliani, Corporate Income Taxes and the Cost of Capital: A Correction, in American Economic Review, vol. 53, nº 3, 1963,  pp. 433-443.
 Merton Miller, Debt and Taxes, in Journal of Finance, vol. 32, nº 2, 1977,  pp. 261-275.
Harry DeAngelo, Ronald W. Masulis, Leverage and Dividend Irrelevancy Under Corporate and Personal Taxation, in Journal of Finance, vol. 35, nº 2, 1980,  pp. 253-264.
 Michael Jensen, William H. Meckling, Theory of the Firm: Managerial Behavior, Agency Costs and Ownership Structure, in Journal of Financial Economics, vol. 3, nº 4, 1976,  pp. 305-360.
 Michael Jensen, Agency Costs of Free Cash Flow, Corporate Finance, and Takeovers, in American Economic Review, vol. 76, nº 2, 1986,  pp. 323-329.
 Stewart C. Myers, Determinants of Corporate Borrowing, in Journal of Financial Economics, vol. 5, nº 2, 1977,  pp. 147-175.
George A. Akerlof, The Market for 'Lemons': Quality Uncertainty and the Market Mechanism, in Quarterly Journal of Economics, vol. 84, nº 3, 1970,  pp. 488-500.
 Stewart C. Myers, The Capital Structure Puzzle, in Journal of Finance, vol. 39, nº 3, 1984,  pp. 575-592.
 Stewart C. Myers, Nicholas J. Majluf, Corporate Financing and Investment Decision When Firms Have Information That Investors Do Not Have, in Journal of Financial Economics, vol. 13, nº 2, 1984,  pp. 187-221.
Malcolm Baker, Jeffrey Wurgler, Market Timing and Capital Structure, in Journal of Finance, vol. 57, nº 1, 2002,  pp. 1-32.
 Jeremy Stein, Rational Capital Budgeting in an Irrational World, in Journal of Business, vol. 69, nº 4, 1996,  pp. 429-455.
Eugene Fama, Efficient Capital Markets: A Review of the Theory and Empirical Work, in Journal of Finance, vol. 25, nº 2, 1970,  pp. 383-417.
 Eugene Fama, Market Efficiency, Long-Term Returns, and Behavioral Finance, in Journal of Financial Economics, vol. 49, nº 3, 1998,  pp. 283-306.
 Eugene Fama, Kenneth R. French, The Cross-Section of Expected Stock Returns, in Journal of Finance, vol. 47, nº 2, 1992,  pp. 427-465.
 John R. Graham, Campbell R. Harvey, The Theory and Practice of Corporate Finance: Evidence from the Field, in Journal of Financial Economics, vol. 60, nº 2, 2001,  pp. 187-243.
 John R. Graham, Campbell R. Harvey, How Do CFOs Make Capital Budgeting and Capital Structure Decisions?, in Journal of Applied Corporate Finance, vol. 15, nº 1, 2002.
 Raghuram Rajan, Luigi Zingales, What Do We Know About Capital Structure? Some Evidence From International Data, in Journal of Finance, vol. 50, nº 5, 1995,  pp. 1421-1460.
 Sheridan Titman, The Modigliani and Miller Theorem and the Integration of Financial Markets, in Financial Management, vol. 31, nº 1, 2002,  pp. 101-115.
 Ivo Welch, Common Flaws in Empirical Capital Structure Research, in Working paper, Brown University e NBER, 2008.
Eugene Fama, Kenneth French, Taxes, Financing Decisions, and Firm Value, in Journal of Finance, vol. 53, nº 3, 1998,  pp. 819-843.
 Eugene Fama, Kenneth French, Testing Trade-Off and Pecking Order Predictions About Dividends and Debt, in Review of Financial Studies, vol. 15, nº 1, 2002,  pp. 1-33.
 John R. Graham, How Big Are the Tax Benefits of Debt?, in Journal of Finance, vol. 55, nº 5, 2000,  pp. 1901-1941.
 John R. Graham, Mark H. Lang e Douglas A. Shackelford, Employee Stock Options, Corporate Taxes, and Debt Policy, in Journal of Finance, vol. 59, nº 4, 2004,  pp. 1585-1618.
 Ronald W. Masulis, The Effects of Capital Structure Change on Security Prices: A Study of Exchange Offers, in Journal of Financial Economics, vol. 8, nº 2, 1980,  pp. 139-178.
Paul Asquith, David W. Mullins, Jr., Equity Issues and Offering Dilution, in Journal of Financial Economics, vol. 15, nº 1, 1986,  pp. 61-89.
 Robert S. Chirinko, Anuja R. Singha, Testing Static Tradeoff Against Pecking Order Models of Capital Structure: A Critical Comment, in Journal of Financial Economics, vol. 58, nº 4, 2000,  pp. 417-425.
 Murray F. Frank, Vidhan K. Goyal, Testing the Pecking Order Theory of Capital Structure, in Journal of Financial Economics, vol. 67, nº 2, 2003,  pp. 217-248.
 Eugene F. Fama, Kenneth R. French, Financing Decisions: Who Issues Stock?, in Journal of Financial Economics, vol. 76, nº 5, 2005,  pp. 549-582.
 Mark T. Leary, Michael R. Roberts, The Pecking Order, Debt Capacity and Information, in Wharton School Working Paper 33-05, 2005.
 Stewart C. Myers, Lakshmi Shyam-Sunder, Testing Static Tradeoff Against Pecking Order Models of Capital Structure, in Journal of Financial Economics, vol. 51, nº 2, 1999,  pp. 219-244.
Malcolm Baker, Jeffrey Wurgler, The Equity Share of New Issues and Aggregate Stock Returns, in Journal of Finance, vol. 55, nº 5, 2000,  pp. 2219-2257.
 Malcolm Baker, Robin Greenwood e Jeremy Wurgler, The Maturity of Debt Issues and Predictable Variation in Bond Returns, in Journal of Financial Economics, vol. 70, nº 2, 2003,  pp. 261-291.
 Dirk Jenter, Market Timing and Managerial Portfolio Decisions, in Journal of Finance, vol. 60, nº 4, 2004,  pp. 1903-1949.
 Ivo Welch, Columbus' Egg: The Real Determinant of Capital Structure, in NBER Working Paper 8782, 2002.
Baker, Malcolm, Richard S. Ruback e Jeffrey Wurgler. Behavioral Corporate Finance, (in inglese). in Handbook of Corporate Finance: Empirical Corporate Finance (a cura di B. Espen Ebcko). Elsevier/North Holland, 2006.
(EN) Patrick Bolton, Mathias Dewatripont, Contract Theory, Cambridge, MA, MIT Press, 2005. ISBN 0-262-02576-0.
(EN) Richard A. Brealy, Stewart C. Myers; Franklin Allen, Principles of Corporate Finance, New York, McGraw-Hill Irwin, 2006, ISBN 0-07-295723-9.
Frank, Murray Z. e Vidhan K. Goyal. Tradeoff and Pecking Order Theories of Debt, (in inglese). in Handbook of Corporate Finance: Empirical Corporate Finance (a cura di B. Espen Ebcko). Elsevier/North Holland, 2006.
 Milton Harris, Artur Raviv, The Theory of Capital Structure, in Journal of Finance, vol. 46, nº 1, 1991,  pp. 297-355.
(EN) Jean Tirole, The Theory of Corporate Finance, Princeton, Princeton University Press, 2006. ISBN 0-691-12556-2.
(EN)  Corporate Debt, articolo sulla struttura del capitale della studiosa Annette Poulsen, sulla Concise Enciclopedia of Economics.
Lo Studio Ovale è lo studio ufficiale del Presidente degli Stati Uniti d'America. Posizionato nella West Wing della Casa Bianca, l'ufficio è di forma ellittica e ha tre grandi finestre rivolte a sud dietro la Resolute desk del presidente e un camino nella parte a nord della sala.
Lo Studio Ovale ha quattro porte: la porta a est dà sul giardino di rose della Casa Bianca, la porta a ovest porta a uno studio privato più piccolo, la porta a nord-est dà sul corridoio principale dell'Ala Ovest e la porta a nord-est porta all'ufficio del segretario del presidente.

Subterranean Homesick Blues è un brano musicale di Bob Dylan, contenuto nell'album Bringing It All Back Home pubblicato nel marzo 1965. Il mese seguente venne fatto uscire anche come singolo, divenendo il primo brano di Dylan ad entrare nella Top 40 di Billboard raggiungendo la 39ma posizione negli Stati Uniti ed entrando nella Top 10 in Gran Bretagna alla posizione numero 9. Successivamente, la canzone, ritenuta uno dei classici di Dylan, è stata inclusa in numerose raccolte come Bob Dylan's Greatest Hits (1967), Biograph (1985), e Dylan (2007).
Il brano è importante non solo perché è il primo pezzo propriamente "elettrico" di Dylan, ma anche per l'innovativo video clip approntato per la canzone che appare nel film documentario Dont Look Back diretto da D. A. Pennebaker.
Primo esempio del nuovo corso musicale del Dylan anni sessanta, il brano segna un momento fondamentale nella carriera dell'artista. Stilisticamente la canzone è un incrocio tra Chuck Berry, Jack Kerouac e una canzone di Woody Guthrie e Pete Seeger intitolata Taking It Easy, il cui testo recita frasi simili a quelle del brano di Dylan (mom was in the kitchen preparing to eat / sis was in the pantry looking for some yeast) e che proprio Dylan mette in parodia sia qui (Johny's in the basement / Mixing up the medicine / I'm on the pavement / Thinking about the government) che nell'album successivo con Tombstone Blues (Mama's in the fact'ry / She ain't got no shoes / Daddy's in the alley / He's lookin' for the food). Rielaborando il tutto e iniettandolo di energia R&B, il mitragliamento di parole che si susseguono vertiginose ha come suo più prossimo discendente la musica Rap, quasi vent'anni prima la nascita del genere. La rima interna ai versi, sul filo di un ritmo galoppante, ignorando simmetrie e regole grammaticali, con un uso tutto tonico del linguaggio permette ricche e inaspettate associazioni e metafore.
Nonostante Dylan non fosse stato un membro vero e proprio dell'originale Beat Generation degli anni cinquanta, sono chiaramente riscontrabili nella genesi del pezzo sia l'influenza del romanzo di Kerouac I sotterranei, pubblicato nel 1958, sia quella del romanzo Memorie dal sottosuolo di Fyodor Dostoevsky, i cui lavori erano molto apprezzati e popolari presso gli scrittori Beat come Kerouac e Allen Ginsberg.
Scritta nell'appartamento di John Court, socio di Albert Grossman, la canzone fu la prima manifestazione della metamorfosi di Bob Dylan da cantante folk di protesta a surreale musicista rock elettrico. Il testo dal fraseggio frammentato, che usa uno slang stradaiolo, offre una critica nichilista al Sogno Americano cha aveva pochi precedenti nel mondo della musica rock. Alcune espressioni nel testo, divenute celebri,
furono utilizzate come slogan da vari movimenti di protesta, e un gruppo di militanti radicali di sinistra prese il proprio nome "Weathermen" direttamente dal testo della canzone.Non c'era una vittima specifica bersaglio della canzone, e nessun messaggio in essa se non il ricorrente «Look out kid» ("Stai attento ragazzo") per segnalare il pericolo delle numerose trappole insite nella società contemporanea. A differenza delle precedenti canzoni di protesta sociale di Dylan, qui non c'è nessuna proposta sul cosa fare per cambiare le cose, soltanto frivoli e assurdi consigli su come comportarsi in un mondo che sta andando a rotoli sempre più velocemente. Per Joan Baez, questo cambiamento di intenti fu un vero tradimento degli ideali che lei e Bob avevano portato avanti negli anni passati e criticò aspramente il nuovo linguaggio "nichilista" di Dylan, arrivando a dichiarare al giornalista Robert Sheldon: «Lui finisce per dire che non c'è una dannata cosa che si può fare per cambiare la società, io dico esattamente l'opposto. Ho paura che il messaggio che esce da Dylan nel 1965 sia solo: Andiamo tutti a casa a farci delle gran canne, perché tanto non c'è nient'altro da fare».
La rivista Rolling Stone ha classificato il brano alla posizione numero 332 nella lista delle "500 Migliori canzoni di tutti i tempi", e Subterranean Homesick Blues ha avuto una larga influenza, venendo spesso citata da artisti e non. John Lennon disse di trovare la canzone talmente "accattivante" da ritenere quasi impossibile scrivere un'altra canzone che potesse anche solo "competere" con essa.Il titolo del brano è stato omaggiato e parodiato in diverse canzoni di vari artisti, ad esempio, i Radiohead con Subterranean Homesick Alien contenuta nell'album OK Computer del 1997, la ska punk band Mustard Plug con Suburban Homesick Blues sul disco Evildoers Beware, 300 M.P.H. Torrential Outpour Blues dei White Stripes e i The Grifters con la loro Subterranean Death Ride Blues, B-side di un singolo del 1996. Dal nome del brano è stato anche tratto il titolo del secondo episodio della prima serie del telefilm Law & Order, Subterranean Homeboy Blues.
In aggiunta all'influenza in campo musicale, la canzone ha avuto anche l'onore di avere il primo "moderno" video promozionale, progenitore di quelli che diventeranno i video musicali nell'era di MTV. In realtà, non si tratta di un video vero e proprio, ma di un segmento tratto dal film Dont Look Back di D. A. Pennebaker, un documentario sul tour di Bob Dylan in terra inglese nel 1965. Nel film, Dylan, fautore dell'idea, tiene in mano dei cartelli rivolto alla telecamera, sui quali sono scritte le parole del testo del brano, che lui fa cadere uno ad uno cercando di mantenere il ritmo man a mano che la canzone prosegue e le strofe si susseguono. I cartelli erano stati scritti la sera prima da Donovan, Allen Ginsberg, Bob Neuwirth e Dylan stesso. La scena venne girata l'8 maggio 1965 in un vicolo dietro il Savoy Hotel di Londra dove Ginsberg e Neuwirth fanno spesso capolino sullo sfondo ai margini dell'inquadratura.In aggiunta a quello girato vicino al Savoy Hotel, furono girati anche altri due filmati promozionali: uno in un parco, e l'altro sul tetto di un edificio non precisato (forse il Savoy Hotel). Un montaggio tratto dai tre video è visibile nel documentario No Direction Home diretto da Martin Scorsese.

Le Suite per violoncello solo di Johann Sebastian Bach sono conosciute per essere fra le più note e le più virtuosistiche opere mai scritte per violoncello, e si ritiene generalmente che sia stato Pau Casals a dare loro fama. Furono scritte fra il 1717 e il 1723 presumibilmente per uno dei violoncellisti che all'epoca lavoravano alla corte di Köthen, ma vi sono anche ragioni per supporre che le ultime suites siano state concepite indipendentemente, forse per strumenti diversi dal violoncello.
Furono probabilmente composte nel periodo 1717–1723, quando Bach fu kapellmeister a Köthen; l'uniformità e la coerenza di queste opere suggerisce che possano essere state concepite insieme o consequenzialmente, presumibilmente per uno dei violoncellisti di Köthen, come Christian Bernhard Linigke o Christian Ferdinand Abel, assai più noto come gambista.
È impossibile stabilire una esatta e precisa cronologia delle suites, non vi sono dati certi riguardo all'ordine con cui furono concepite e/o se fossero state scritte prima o dopo le Sonate e partite per violino solo. In ogni caso, gli studiosi – basandosi su un'analisi comparata degli stili di queste diverse opere – ritengono che le suites per violoncello furono scritte per prime, datandole prima del 1720, l'anno indicato sulla copertina della copia autografa di Johann Sebastian Bach delle Sonate e partite per violino solo.Queste opere sono particolarmente significative nella storia degli strumenti ad arco: mentre fino al tempo di Bach era consuetudine che il violoncello suonasse parti di accompagnamento e le parti più melodiche nello stesso registro venivano affidate a strumenti della famiglia della viola da gamba, in queste suites, come in parti dei concerti brandeburghesi, al violoncello è affidata una parte da solo. Si può ritenere Bach un innovatore che favorisce il soppianto della viola da gamba, ma alcuni suppongono anche che sia probabile che Bach avrebbe fatto questo perché si trovò in difficoltà nel dare parti virtuosistiche alla viola da gamba. Infatti il principe Leopoldo di Anhalt-Köthen, presso cui lavorava in quel momento, era un gambista e suonava le opere di Bach, ma non era un particolare virtuoso, sicché potrebbe essere risultato difficile dare alla viola da gamba parti complicate, quindi Bach, non avendo la possibilità di scrivere parti complesse per la viola da gamba, avrebbe scritto opere più ambiziose per il violoncello.
Già mentre Bach è in vita vengono copiati numerosi manoscritti delle suites, tuttavia esse non divengono mai un pezzo largamente conosciuto; la prima edizione stampata delle suites, curata da Louis Pierre Norblin, è edita dalla casa editrice Janet et Cotelle a Parigi nel 1824. In questa edizione dell'opera si nota che la destinazione e la considerazione delle suites cambia e per molto tempo le si ritiene non danze od opere virtuosistiche ma studi, tanto che sul frontespizio della prima edizione è scritto Sonates ou Études.
Nonostante Robert Schumann avesse scritto un accompagnamento pianistico per le suites, esse non erano largamente conosciute prima del XX secolo. Nel Novecento, divennero conosciute grazie all'opera di Pau Casals che, all'età di 13 anni, trovò l'edizione delle suites a cura di Grützmacher durante una raccolta di beneficenza a Barcellona e iniziò a studiarle. Tuttavia, non volle eseguirle in pubblico fino al 1925, all'età di 48 anni, e successivamente accettò di inciderne la prima registrazione assoluta. La loro popolarità crebbe costantemente da allora e la registrazione di Casals è ancora disponibile tutt'oggi.
Oggi le suites costituiscono una delle più grandi opere per violoncello e, dopo il recupero da parte di Casals, quasi ogni violoncellista aspira a suonarle nel miglior modo possibile, notissimi violoncellisti come Mstislav Rostropovich, Emanuel Feuermann, Pierre Fournier, Jacqueline du Pré, Paul Tortelier, André Navarra, Yo-Yo Ma, Gregor Piatigorsky, Mischa Maisky, János Starker, Anner Bijlsma, Heinrich Schiff, Pieter Wispelwey e Mario Brunello ne hanno registrato esecuzioni. Yo-Yo Ma vinse il Best Instrumental Soloist Grammy Award nel 1985 per il suo album "Six Unaccompanied Cello Suites" mentre Mischa Maisky ha venduto più di 300.000 copie della sua registrazione delle suites, molto al di sopra delle vendite medie della musica classica. Celeberrima fu l'esecuzione improvvisata durante la caduta del Muro di Berlino di Mstislav Rostropovich che fece il giro del mondo di tutte le televisioni. Non mancano nemmeno esecuzioni in luoghi suggestivi come l'esecuzione delle suites fatta da Mario Brunello sul Monte Fuji nel 2007, per - secondo Brunello - avvicinarsi di più all'assoluto e alla perfezione. Parti delle suites inoltre furono suonate da Yo-Yo Ma per il funerale del senatore statunitense Edward Kennedy e nel settembre 2002 durante l'anniversario degli attentati dell'11 settembre 2001.
Differentemente dalle Sonate e partite per violino solo di Bach, per queste suites non rimangono manoscritti autografi, in tal modo è difficile sviluppare una edizione critica; esistono però dei manoscritti non autografi:
una copia manoscritta fatta da Anna Magdalena Bach probabilmente per un allievo di Bach, Georg Heinrich Ludwig Schwanenberger, conservata presso la Staatsbibliothek di Berlino. Questo manoscritto presenta numerose discrepanze con lo stile delle altre opere per archi di Bach, in particolar modo per il fraseggio, le arcate e le articolazioni; Anner Bijlsma in Bach, The Fencing Master spiega come, secondo lui, tali discrepanze possano invece essere frutto dell'ingegno di Bach e sostiene che questo manoscritto possa essere una copia perfetta di un eventuale originale, tuttavia l'ambiguità (a volte anche in termini di grafia) del manoscritto e l'assenza di copie autografe non permette risposte certe. L'edizione delle suites curata da August Wenzinger si basa interamente su questo manoscritto. Il frontespizio di questo documento riportauna copia manoscritta fatta da Johann Peter Kellner, amico e allievo di Johann Sebastian Bach, e maestro di cappella a Grafenrode in Turingia, conservata nella Staatsbibliothek di Berlino. Si tratta della copia più antica delle suites di cui siamo venuti in possesso, tuttavia, data l'incompiutezza - con spesso numerose omissioni - del manoscritto in molte sue parti, si ritiene che tale copia sia stata fatta da una bozza delle suites e assolvesse esigenze di studio del suo proprietario. Il frontespizio recitamentre la conclusione del manoscritto recita Soli Deo Sit Gloria (Sta solo alla gloria del Signore).un manoscritto della seconda metà del XVIII secolo comprendente un'ampia collezione di opere di Johann Sebastian Bach e una fuga di Carl Heinrich Graun, copiato da due differenti copisti anonimi appartenuto probabilmente a Johann Christoph Westphal, organista ed editore di Amburgo, conservato anch'esso preso la Staatsbibliothek di Berlino. Sul frontespizio è scrittoun manoscritto della fine del XVIII secolo, che si trova nella Biblioteca nazionale di Vienna.Viene usata come fonte secondaria anche la prima edizione stampata delle suites, curata da Louis Pierre Norblin e edita dalla casa editrice Janet et Cotelle a Parigi nel 1824, che, secondo il suo frontespizio, sarebbe basata su un manoscritto autografo trovato da Norblin stesso e dopo la sua morte perduto. Per la quinta suite, è possibile utilizzare come fonte la BWV 995, versione per liuto della stessa suite, pervenuta in un manoscritto autografo di Johann Sebastian Bach.
L'analisi di tutte queste fonti secondarie, in particolare della copia manoscritta di Anna Magdalena Bach, ha permesso di ricostruire un insieme di possibili archetipi, anche se mancanti di legature di portamento e altre articolazioni. Così esistono diverse interpretazioni delle suites, e non una sola edizione critica accettata, e anzi sono state scritte dal 1825 sino al 2000 più di 82 edizioni critiche diverse.In aggiunta, il professor Martin Jarvis della Charles Darwin University School of Music, a Darwin, Australia pone dubbi anche sulla paternità dell'opera: ritiene che Anna Magdalena Bach stessa abbia scritto le sei suites, anche se molti musicologi e musicisti, vista la difficoltà di trovare evidenti conferme a tali ipotesi, rimangono scettici su questo.
Le variazioni nel corso della storia dei significato attribuiti ai termini musicali rendono arduo determinare esattamente quali fossero gli strumenti "che Bach intendeva" che suonassero nelle suites, cosicché su quest'argomento vi sono numerose interpretazioni, tuttavia in genere si ritiene che di tali suite solo la sesta - scritta esplicitamente per uno strumento a cinque corde - possa essere stata scritta per strumenti diversi dal violoncello come il violoncello piccolo.
Nel passato, stando a ricostruzioni storiche, generalmente si riteneva che la sesta suite potesse essere stata scritta per la viola pomposa, che, secondo testimonianze storiche attribuite a Franz Benda, pare essere stato inventata da Bach stesso. Tuttavia gli stessi studi su questo strumento e sulla sua storia rendono più inverosimile tale tesi, tanto che Mark Mervyn Smith avanza l'ipotesi che verso la fine del XVIII secolo si sia cominciato a dare il nome di viola pomposa, che fino a quel momento era stato utilizzato per uno strumento di tessitura alta, al violoncello piccolo a 5 corde.Si avanzano anche ipotesi più originali, che però spesso trovano poche conferme in ambito musicologico; per esempio, Dmitry Badiarov ha ipotizzato che tutte le suites siano state scritte per violoncello da spalla o per violoncello piccolo; Sigiswald Kuijken e Ryo Terakado hanno registrato tutte le suites con un violoncello da spalla.
Nella musica anteriore al XIX secolo, la scrittura musicale non includeva molti dei segni di interpretazione che si ritrovano usualmente negli spartiti più recenti, in relazione a una diversa ripartizione dei ruoli fra compositore ed esecutore. I brani musicali manoscritti, d'altra parte, erano spesso trasmessi direttamente dal compositore all'esecutore a cui erano destinati (quando fosse diverso dal compositore stesso) o a copisti, che spesso nel caso di Bach erano la moglie e gli allievi. A differenza delle opere a stampa, le sporadiche indicazioni per l'esecuzione che si ritrovano sui manoscritti, assumono un valore completamente diverso quando si possono ritenere scritte dal compositore, rispetto a quando si tratta di aggiunte di altri esecutori o copisti. L'assenza di manoscritti autografi dell'autore assume dunque rilevanza in questo campo.
I violoncellisti odierni nella lettura delle suite bachiane sono chiamati a prendere autonomamente molte decisioni interpretative e talora tecniche, a cominciare dalle arcate e dalla dinamica, con esiti tutt'altro che scontati ed univoci, dato che l'originalità e la complessità di scrittura di queste opere non permettono facili confronti con altri modelli dell'epoca. Pertanto, le esecuzioni delle suites possono differire notevolmente fra loro, riflettendo in modo particolarmente evidente l'atteggiamento interpretativo e le concezioni estetico-musicali del violoncellista, e spesso la sua idea su come Bach intendesse che fossero suonate. Per questi motivi molti violoncellisti si accostano a quest'opera con molto rispetto e talvolta paura: per esempio, sia Casals sia Rostropovich si pentirono delle loro registrazioni delle suites e furono sempre riluttanti a farlo; addirittura Rostropovich in più occasioni disse che per lui era stato un errore registrarle. Nel medesimo modo, alcuni strumentisti, dalla seconda parte del XX secolo in poi, fra cui János Starker, Mischa Maisky, Yo-Yo Ma o Mario Brunello, hanno registrato più volte le suites per violoncello, non riconoscendosi più nell'interpretazione che avevano dato nella loro precedente registrazione.
Storicamente, i modi in cui le suites vengono suonate variano seguendo alcune tendenze che si rifanno a esecuzioni storicamente importanti:
Esecuzioni in cui viene enfatizzata l'intensità espressiva: è ritenuta esemplare quella di Pau Casals,
Esecuzioni in cui si persegue la fedeltà alla prassi esecutiva storica: la principale esecuzione di questo tipo è quella di Anner Bijlsma,
Esecuzioni in si ricerca la perfezione tecnica: si citano come esecutori di riferimento János Starker per la tecnica e Pierre Fournier per la cura del suono.Lo sviluppo e la definizione di questi approcci esecutivi sono stati fortemente determinati dalla storia delle suites per violoncello solo: dalla prima edizione stampata, che, come spiegato nella sezione Storia, le considerava come fossero studi e la prassi esecutiva predominante riteneva che il rispetto del testo bachiano si avesse solo attraverso un'esecuzione distaccata e asettica. La nuova interpretazione delle suites di Pau Casals non fu soltanto un recupero in termini di presenza nel repertorio o di popolarità ma si pone in generale contrasto con questa prassi esecutiva e portò poi le diverse prassi esecutive a ridefinirsi nell'interpretazione delle suites. Infatti, Casals pensava che poiché Bach, come molti musicisti suoi contemporanei, supervisionava direttamente l'esecuzione delle sue opere, egli così lasciasse scritto poco nei manoscritti per guidare l'interpretazione dell'esecutore e pertanto l'esecutore dovesse da solo trovare il carattere nascosto dietro l'opera che Bach non aveva annotato, rigettando l'ipotesi a lui contemporanea che Bach andasse suonato privo di espressione. In realtà, successivamente la ricerca musicologica dimostrò che entrambi questi punti di vista erano errati: i compositori del periodo barocco (Bach compreso) non avevano la possibilità di controllare l'esecuzione delle loro opere quando non fossero essi stessi esecutori, e che Bach, forse proprio per questo motivo, fu tra i compositori del suo periodo più accurati nella precisione di scrittura e ricchezza di dettagli dei manoscritti; la mancanza di precisione in queste suites non fu probabilmente intenzione del compositore, ma dovuta in gran parte alla mancanza del manoscritto autografo. Non essendo però noti questi sviluppi della musicologia al suo tempo, invero Casals credeva fermamente nel suo approccio e diceva:
Di queste suites, Casals insegnava ai suoi allievi che ognuna aveva un proprio carattere specifico dato dal preludio. Secondo lui, per ogni suite si possono rintracciare, analizzando il disegno ritmico, melodico e armonico dell'opera, alcuni caratteri specifici, fondamentali per l'interpretazione: così secondo l'interpretazione di Casals la prima suite risulta pervasa di ottimismo, mentre la seconda ha un carattere tragico, eroico invece per la terza, maestoso nella quarta, tempestoso e oscuro per la quinta e bucolico per la sesta.In questo modo Casals diventa anche l'iniziatore di una tradizione esecutiva (considerata spesso come "romantica") in cui l'elemento principale è l'intensità dell'espressione e si ricerca la maggior espressività possibile e la bellezza dell'opera. Questo approccio adottato da Casals è stata in gran parte mantenuto da molti esecutori successivi benché, dopo i successivi studi di musica antica, non corrispondesse alla prassi esecutiva storica: sebbene molto suggestiva, l'interpretazione di Casals fu ed è tuttora ampiamente criticata per le libertà che si era preso rispetto allo spartito e alle teorie sulla prassi esecutiva bachiana, la sua registrazione fu considerata più romantica che filologica, tuttavia non fu mai criticata ampiamente da un punto di vista estetico. Ciononostante, l'interpretazione di Casals è diventata storica e la tradizione esecutiva da lui iniziata permane ancora.
Alcune interpretazioni operate da Casals sulle suites furono successivamente supportate dalla ricerca filologica. È un caso presente in molte danze delle suites, come lo staccato nella corrente della III suite. Sulle danze, Casals riteneva che si dovessero suonare col più colore possibile; secondo lui, il naturale carattere ritmico che avrebbero in quanto danze non era da intendersi come un formalismo figurativo ma letteralmente. Questa impostazione, criticata dalla prassi esecutiva tecnicista predominante all'inizio del XX secolo, venne invece considerata corretta ed adottata dalle esecuzioni storicamente informate, come in quella di Anner Bijlsma. In ogni caso, benché si raffrontino nell'esecuzione di Casals degli elementi successivamente ritenuti filologici, la sua impostazione, che ricerca la maggior espressività possibile, è metodologicamente distante da un'esecuzione storicamente informata poiché considera prioritaria l'espressività delle suites rispetto all'aderenza alla filologia.
Per contro, le esecuzioni storicamente informate si basano su una ricerca musicologica che attinge direttamente allo studio delle fonti note cercando di riprodurre le suites nella maniera in cui Bach intendeva che fossero suonate e non nella maniera considerata esteticamente migliore, secondo la prassi della musica antica. Benché Nikolaus Harnoncourt avesse registrato le suites nel 1965 cercando di applicare una lettura filologica alla musica, la prima e principale registrazione storicamente informata viene considerata quella di Anner Bijlsma nel 1979, in cui per la prima volta fu utilizzato in una registrazione il violoncello piccolo per la sesta suite. Bijlsma ha scritto Bach, The Fencing Master, testo in cui spiega le ragioni che lo hanno portato ad adottare certe scelte nell'esecuzione in ragione dell'approccio filologico a queste opere.
In ogni caso, dagli ultimi decenni del XX secolo, numerosi violoncellisti spesso oscillano nelle loro esecuzioni fra tutti questi diversi approcci e in molti casi ciò è dovuto a una tendenza a sperimentare sempre nuove interpretazioni delle suites, sia dal punto di vista tecnico, sia espressivo, unendo sia elementi propri delle esecuzioni storicamente informate sia elementi originali e del tutto creativi, che talvolta si differenziano fondamentalmente dalle interpretazioni precedenti, come Mario Brunello che, nella sua ultima registrazione, sviluppa un colpo d'arco atto a riprodurre il suono della dulciana nel preludio della seconda suite, la quale invece secondo Casals avrebbe un carattere malinconico. Contemporaneamente altri esecutori aderiscono completamente ad una prassi esecutiva "romantica", scettici sulla determinatezza di un'esecuzione filologica delle suites, come Mischa Maisky.
Tuttavia, il virtuosismo e la grande mutevolezza di emozioni espresse, in relazione alle diverse interpretazioni e prassi esecutive, diedero così tanta popolarità alle suites al punto che divennero soggetto di un'ampia gamma di altre interpretazioni, non solamente o non completamente violoncellistiche, come il documentario Inspired by Bach, prodotto da Sony Classical, cui ha partecipato Yo-Yo Ma e numerose trascrizioni (vedi sezione più avanti). Vi sono anche performance più originali, come La tenzone fra Messer Violoncello e Madama Viola da Gamba, con il violoncellista Mario Brunello e il gambista Paolo Pandolfo, in cui i due immaginano un incontro immaginario fra due personaggi, la Viola da Gamba e il Violoncello, e poi suonano rispettivamente le suites e la loro trascrizione per viola da gamba.
Una danza galante – (Minuetti nelle suites 1 e 2, Bourrées nella 3 e nella 4, Gavotte nella 5 e nella 6)
GigaGli studiosi sostengono che Bach avesse concepito le suites come un unico insieme di opere, forse non con l'intenzione di essere un ciclo vero e proprio ma comunque un insieme di opere coerente, piuttosto che una serie arbitraria di brani musicali: comparate con le altre suites di Bach, le suites per violoncello risultano le più simili fra loro nella struttura dei movimenti. Inoltre, variando gli schemi della suite, Bach inserì movimenti di intermezzo o galanterie a coppie - in ogni caso da eseguire alternativamente - fra la Sarabanda e la Giga di ogni suite, sviluppando in questo modo una struttura simmetrica.
Di queste suites, Casals insegnava ai suoi allievi che ogni suite aveva un proprio carattere specifico dato dal preludio. Secondo lui per ogni suite si può rintracciare analizzando il disegno ritmico, melodico e armonico dell'opera alcuni caratteri specifici, fondamentali per l'interpretazione: così secondo l'interpretazione di Casals la prima suite risulta pervasa di ottimismo, mentre la seconda ha un carattere tragico, eroico invece per la terza, maestoso nella quarta, tempestoso e oscuro per la quinta e bucolico per la sesta.Solo cinque movimenti in tutto l'insieme delle suites sono privi di accordi, consistendo di una singola linea melodica: il secondo Minuetto della I Suite, il secondo Minuetto della II suite, la seconda Bourrée della III suite, la Giga della IV suite, e la Sarabanda della V Suite. In tutti gli altri movimenti vi sono degli accordi, prevalentemente arpeggiati: nelle suites rimane latente un carattere polifonico, ossia mediante l'uso degli arpeggi Bach realizza una forma di polifonia sottintesa (in inglese implied harmony) e di contrappunto melodizzato, infatti in molte parti delle suites si possono distinguere delle linee melodiche arpeggiate una sull'altra, con una voce al basso che sottolinea e sostiene la struttura armonica dell'opera.
La maggioranza degli studenti inizia studiando questa suite, infatti generalmente si ritiene che sia più semplice da suonare delle altre in relazione al tipo di preparazione tecnica richiesta. Si articola in 6 movimenti come le altre suites, il quinto movimento è un minuetto.
Il preludio ha un disegno ritmico-melodico in semicrome - una successione di accordi arpeggiati con nota di volta - che si mantiene inalterato per tutta la durata del pezzo e che viene trascinato dal disegno armonico. Tale disegno ritmico-melodico viene ripreso molte volte nelle restanti danze della suite e viene ripreso e sviluppato nel maestoso periodo finale del preludio; il preludio non ha dunque solo una funzione introduttiva alle danze ma contribuisce all'uniformità e unità della suite.
Il preludio si può suddividere in due parti: la prima, più lunga, con un forte tema ricorrente subito introdotto all'inizio e la seconda che consiste in un movimento che porta ad una successione di accordi in cadenza perfetta finale, dando a questo conclusione un fortissimo carattere polifonico. La successiva Allemanda contiene delle brevi cadenze, che allontanano il pezzo dagli schemi tradizionali di questa forma di danza. Eccetto il secondo Minuetto, l'intera danza ha un ritmo sostenuto e a volte incalzante, in particolare la Giga e il primo Minuetto, che contiene anche diversi passaggi di posizione e accordi, e che si contrappone al secondo minuetto. Casals classificò questa suite come una suite dal carattere tragico, malinconico ed espressivo in ogni nota.
Il preludio di questa suite si caratterizza per un inizio maestoso costruito sulla scala di Do maggiore, che si sviluppa in una parte arpeggiata molto energica, che arpeggia una progressione armonica imitata. Poi il fraseggio ritorna a delle scale per risolversi in una successione di potenti e sorprendenti accordi. Cruciale è la scelta della arcate affinché l'esecutore possa sottolineare al meglio la polifonia di questo brano e le diverse voci.
La Allemanda è il solo movimento con un inizio anacrusico di tre semicrome, anziché di una sola.
La seconda Bourrée, sebbene sia in do minore, ha solo due bemolli nell'armatura di chiave. Tale notazione non è inconsueta nella musica antica e nemmeno, per le scale minori, nella musica barocca. Le Bourrée di questa suite vengono spesso trascritte ed eseguite con altri strumenti come la tuba, l'euphonium, il trombone.
La quarta suite è una delle suites per violoncello solo tecnicamente più complesse per l'intonazione e per la presenza di numerosi allargamenti e cambi di posizione; il risultato infatti può risultare più scarno rispetto alle altre suites per la scarsa risonanza acustica sullo strumento, propria della tonalità di Mi bemolle Maggiore sul violoncello, poiché nessuna corda del violoncello è fra i gradi più importanti (tonica, sottodominante, dominante) della scala di Mi bemolle, differentemente dalle scale delle altre suites. Mario Brunello, per ovviare a questo problema, nella sua ultima registrazione delle suites ha cambiato l'accordatura del suo strumento abbassandola di due toni (cioè Fa- Si♭-Mi♭-La♭ anziché La-Re-Sol-Do). In molte interpretazioni, la suite è caratterizzata da un carattere tormentato,a tratti cupo e malinconico (fra cui, ad esempio, Pau Casals, Mstislav Rostropovich, Mischa Maisky). 
Il Preludio inizia con delle crome che impongono movimenti fra corde distanti fra loro che poi lasciano spazio ad una cadenza per poi ritornare al tema iniziale. Su questo preludio Pau Casals, insegnandone l'esecuzione e l'interpretazione ai suoi allievi, paragonava il suono delle crome all'effetto prodotto dai pedali di un grande organo.
Questa suite, in do minore, ha la particolarità di essere stata scritta in scordatura, con la corda più acuta abbassata di un tono rispetto all'accordatura convenzionale a quinte (da la a sol), ma risulta comunque possibile suonarla con l'accordatura standard, così molti accordi diventano più complicati, ma allo stesso modo si semplificano le linee melodiche. Bach non usò mai la scordatura se non solo in questa suite, questo non per facilitare l'esecuzione, che anzi diverrebbe in certi punti più difficoltosa, ma probabilmente per aumentare la risonanza acustica delle note sullo strumento (Sol, la nota cui è accordata la prima corda, è infatti la dominante di do minore) e dare un carattere timbricamente più scuro alla suite.Un manoscritto autografo di Bach della versione per liuto di questa suite esiste come BWV 995.
Il preludio inizia lentamente, esplorando i registri di tenore e basso, con uno stile di toccata, cioè quasi un'improvvisazione sui temi che seguiranno e si sviluppa in una fuga più veloce, che porta ad un finale accordale di grande effetto. La lentezza e la tonalità di Do minore imprime alla suite tensione che viene sottolineata dall'armonia sin dall'inizio, dove la prima frase comincia con un accordo di tonica (Do), prosegue in un moto per grado congiunto dalla dominante (Sol) ad un accordo di settima diminuita con pedale di tonica nella seconda battuta che viene fiorito e termina su un bicordo di tonica (Do-Mi♭). La fioritura dell'accordo di settima diminuita si muove su una linea melodica diversa da quella della prima battuta: mentre quest'ultima si conclude sulla nota centrale dell'accordo di settima diminuita, l'altra prende avvio dalla nota superiore dell'accordo, dando l'idea di essere come una voce diversa dalla prima; qui si evidenzia il carattere polifonico latente in tutte le suites. In modo analogo il preludio prosegue e continua in una fuga basata su una piccola frase musicale di tre note, alternata a frasi di risposta; il soggetto e la risposta, che vengono nelle varie entrate ripresi in registri diversi, si muovono modulando fra la tonalità della dominante (Sol minore) e della tonica (Do minore). Alla fine si ritorna ad un carattere accordale come quello iniziale che conclude il preludio in una serie di cadenze evitate che si compiono in una cadenza piccarda su un accordo di nona costruito sulla tonica. Secondo Casals, questo preludio deve essere suonato in modo da risultare oscuro, tempestoso: Casals, come traspare anche dalle sue interpretazioni, riteneva che questa suite dovesse essere il più possibile tempestosa ed appassionata.
Di questa suite è molto nota la Sarabanda per la sua bellezza ed è fra i pochi movimenti di tutte le sei suites in cui l'armonia si sviluppa solo linearmente, senza accordi. Poiché in questa sarabanda l'armonia è lineare, priva di accordi, ricca di appoggiature e con poche cadenze autentiche che sottolineino la tonalità del brano, appaiono sottolineate le dissonanze e in molte interpretazioni, come in quella di Maisky, nel complesso la sarabanda assume un carattere oscuro, a tratti indecifrabile, anche se comunque è rintracciabile un disegno armonico compiuto, che all'inizio modula da Do minore a Mi♭ maggiore e nel secondo episodio da Fa minore per tornare a Do minore, concludendosi in una cadenza plagale. Questa mancanza di conferme della tonalità insieme al suo continuo mutare aumenta l'atmosfera criptica dell'opera, che in tal senso risulta molto inusuale rispetto alla musica del tempo. Rostropovich la descrive come l'essenza del genio di Bach; Tortelier, come un'estensione del silenzio. Casals ritiene che sia una profonda espressione di dolore, come quella della Passione secondo Matteo e della Passione secondo Giovanni.
La quinta suite è anche eccezionale per la sua Corrente e la Giga in stile francese, rispetto alla forma italiana delle altre cinque suites.
È largamente ipotizzato che la sesta suite sia stata composta per un violoncello piccolo a cinque corde, come spiegato più sopra nella sezione Filologia e controversie interpretative.
I violoncellisti che vogliono suonare questa suite su un violoncello moderno a 4 corde riescono a farlo, ma incontrano più difficoltà essendo forzati a ricorrere a posizioni più alte per suonare molte delle note. Vista la difficoltà di esecuzione e di interpretazione rappresenta un punto di arrivo nella formazione violoncellistica. I musicisti di musica antica che seguono una rigorosa pratica esecutiva storica usano un violoncello piccolo a 5 corde per questa suite, fra questi si ricorda Anner Bijlsma, Pieter Wispelwey e Jaap ter Linden.
Questa suite è scritta in una forma molto più libera rispetto alle altre, avendo molti passaggi virtuosistici e/o simili a cadenze. È anche l'unica delle suites in cui vi siano parti che, in tutti i manoscritti, adottano la chiave di contralto, appunto per l'altezza delle note in essa, e infatti viene ampiamente esplorato, soprattutto nella Corrente, l'intero registro di questo strumento.
Mstislav Rostropovich chiamò questa suite "una sinfonia per violoncello solo", infatti la sua tonalità di re maggiore evoca gioia e trionfo e dà alla suite un carattere luminoso. Questa atmosfera raggiante e calorosa si manifesta sin dal Preludio, in cui il ritmo a terzine con due suoni ripetuti introduce ad una dimensione molto vivace e festosa rispetto alla quinta suite; tale atmosfera è ripresa nelle Gavotte e nella Giga. La Sarabanda e in particolar modo la Allemanda invece sono di un carattere più introspettivo e solitario ma manifestano una luminosa atmosfera di bellezza e purezza.
Pau Casals la descrive come la descrizione di una scena bucolica, in cui il preludio assomiglia a una grande scena di caccia.
Sono state fatte trascrizioni delle suites per numerosi strumenti, fra cui viola, contrabbasso, viola da gamba, mandolino, pianoforte, clavicembalo, marimba, chitarra classica, basso elettrico, ukulele, flauto dolce, corno francese, sassofono, clarinetto basso, fagotto, tromba, trombone, euphonium e tuba.
Fra i tentativi di comporre un accompagnamento pianistico alle suites si annovera un notevole lavoro da parte di Robert Schumann, mentre nel 1923 Leopold Godowsky arrangiò le suites 2, 3 e 5 in contrappunto per pianoforte solo.
Riedizioni delle registrazioni di Pau CasalsPau Casals, 1936, riedita in CD nel 1988 - 6 Suiten für Violoncello / Les 6 suites pour violoncelle / The 6 Cello Suites (EMI CHS 761027 2)
Pau Casals, 1936, riedita in CD nel 2003 - Cello Suites (EMI Classics 724356261723)Esecuzioni storicamente informate che fanno uso del violoncello piccoloAnner Bijlsma, 1992 - Suites for Violoncello Solo, BWV 1007 - 1012 (Sony Classical Vivarte Series S2K 48 047)
Pieter Wispelwey, 1998 - 6 Suites per Violoncello Solo Senza Basso (Channel Classics CCS 12298)
Jaap ter Linden, 2006 - Cello Suites Nos. 1 - 6 (Brilliant Classics 93132)Altre registrazioniAndré Navarra, 1977 - Les Six Suites pour Violoncelle (Calliope cal 9641.2)
Jacqueline du Pré: È possibile trovare alcune registrazioni delle suites (BWV 1007 e BWV 1008) suonate da Jacqueline du Pré nel CD edito nel 2005 - Jacqueline du Pré - The Early BBC Recordings (EMI Classics)
Mario Brunello, 2010 - Bach, Sei suites a violoncello solo senza basso (EVOLUTION, EAN 8015948001561)
Mischa Maisky, 1985 - Sechs Suiten Fur Violoncello Solo (Deutsche Grammophon 445 373-2)
Pierre Fournier, 1961 - Sechs Suiten für Violoncello Solo BWV 1007-1012 (Archiv Produktion 449 711-2)
Yo-Yo Ma, 1998 - Bach: The Cello Suites - Inspired by Bach (Sony Classical S2K 63203)TrascrizioniFlauto dolce: Mario Verbruggen (Harmonia Mundi HMT 7907071)
Trascrizione di Godowsky per pianoforte: Carol Grante, 1999 - Cello Suites Transcribed For Piano (Music & Arts Programs Of America 1046)
Viola: Patricia McCarty - Six Cello Suites performed on viola (Ashmont Music 6100)
Wikimedia Commons contiene immagini o altri file su Suites per violoncello solo
(EN) Spartiti di Suite per violoncello solo di Johann Sebastian Bach / Suite per violoncello solo di Johann Sebastian Bach (altra versione), su International Music Score Library Project, Project Petrucci LLC. 
(EN) Suite per violoncello solo di Johann Sebastian Bach, su MusicBrainz, MetaBrainz Foundation. 
(EN)  Trascrizione della IV suite per violoncello piccolo dal Werner Icking Music Archive

I Sum 41 sono un gruppo musicale canadese originario di Ajax, cittadina dell'Ontario, e formatosi nel 1996.
Dalla firma di un contratto discografico con la Island Records nel 1999, la band ha composto sei album in studio, due album dal vivo, una raccolta e tre EP. Sono inoltre divenuti famosi per i loro tour, che arrivano a toccare le più varie località del mondo e a protrarsi a volte oltre un anno.
La band è tra le più rilevanti nel genere pop punk a livello mondiale, grazie soprattutto all'album di successo All Killer No Filler. Il gruppo, però, ha collaudato altri generi avvicinandosi maggiormente al punk rock, all'hardcore punk e al metal, di cui sono prova gli album Does This Look Infected? e Chuck. Il quarto album, Underclass Hero, ha segnato il riavvicinamento da parte della band al pop punk.
Dal loro primo EP, Half Hour of Power, sino al terzo album Chuck i membri del gruppo sono stati Deryck "Bizzy D" Whibley (voce principale, chitarra e tastiera), Jason "Cone" McCaslin (basso e voce secondaria), Dave "Brownsound" Baksh (chitarra e voce secondaria) e Steve "Stevo" Jocz (batteria e voce secondaria). I componenti della band canadese hanno anche dato origine a un gruppo heavy metal, noto come Pain for Pleasure, le cui canzoni sono spesso presenti negli album dei Sum 41 come B side. In questo gruppo, considerabile un loro alter ego, Jocz e Whibley invertivano i loro compiti (Stevo32 era il cantante, Bizzy D il batterista).
L'11 maggio 2006 Dave Baksh, chitarrista del gruppo, ha deciso di lasciare la band; da allora i Sum non hanno ingaggiato altri chitarristi in sostituzione di Brownsound, ad eccezione di Thomas "Brown Tom" Thacker come turnista durante i concerti. Successivamente Tom è entrato a far parte del gruppo come membro ufficiale. Dopo 9 anni di separazione, tuttavia, Baksh ritorna ufficialmente nel gruppo nel 2015. Il 17 aprile 2013 anche il batterista Steve Jocz abbandona la band, dopo aver fatto parte della formazione per oltre 17 anni. Viene sostituito da Frank Zummo nella prima metà del 2015.
I Sum 41 sono stati eletti ai Kerrang! Awards come gruppo internazionale emergente nel 2002, mentre sono stati nominati per 7 Juno Award, di cui due vinti (nel 2002 e nel 2004).
I membri dei Sum 41 iniziano la loro carriera musicale in band rivali durante le scuole superiori; il loro incontro è avvenuto, come affermato da loro stessi, durante un concerto delle Hole, 41 giorni dopo l'inizio dell'estate del 1996 (da qui il nome "Sum 41"). La band aveva originariamente il nome di Kaspir, mutato poi nell'attuale a seguito di un concerto per la società di promozione ed organizzazione di concerti Supernova, il 28 settembre 1996. Fu Greig Nori dei Treble Charger, loro futuro manager e produttore, a scoprirli in uno show della stessa Supernova all'Opera House di Toronto il 24 febbraio 1996. Jay "Cone" McCaslin si aggregò al gruppo successivamente, nel 1999, in sostituzione di Mark Spicoluk.
L'EP di debutto dei Sum 41 esce il 27 giugno 2000 con il titolo Half Hour of Power. Il primo singolo estratto da quest'album è Makes No Difference, che è stato utilizzato come colonna sonora nei film Summer Catch, Bring It On, Maial College e Out Cold e nei videogiochi Dave Mirra Freestyle BMX 2 e NHL 2002. La canzone What I Believe è stata utilizzata nel film Fatti, strafatti e strafighe.
I membri del gruppo hanno inoltre affermato che agli inizi della loro carriera, dopo aver spedito degli EPK (Electronic Press Kit) di promozione a delle case discografiche, erano soliti incontrare i delegati delle stesse in strip club e bar, perfino dopo aver già firmato un contratto, solo per avere una cena gratis.
Il primo album dei Sum 41 viene messo in vendita l'8 maggio 2001 con il titolo di All Killer No Filler. Il primo singolo sul mercato, Fat Lip, ha scalato le classifiche di tutto il mondo durante l'estate; all'inizio del video della canzone i Sum cantano una parte di What We're All About in un negozio. Il singolo è stato scelto nella tracklist del videogioco della EA Sports NHL 2002, insieme a Makes No Difference, utilizzata anche come sigla conclusiva nello speciale di Dragon Ball, Le origini del mito.
La popolarità della band cresce notevolmente anche a seguito delle performance del Warped Tour svoltosi in quell'anno.
Dopo Fat Lip seguono altri tre singoli di successo: In Too Deep, nel cui video comico i componenti dei Sum 41 sono impegnati in una gara di tuffi, Motivation, un video più "semplice" in cui la band suona in una stanza ed Handle This, nel cui video appaiono i Sum durante i concerti e nella vita di tutti i giorni. I Sum 41 rimangono in tour per buona parte dell'anno, prima di tornare in studio di registrazione per la realizzazione di un nuovo album. Sia Fat Lip che In Too Deep compaiono nella colonna sonora del film American Pie 2; In Too Deep è parte anche della colonna sonora di Malcolm.
Nell'agosto del 2001 i Sum 41 suonano alla festa per il ventesimo anniversario di MTV insieme a Tommy Lee, batterista dei Mötley Crüe, e Rob Halford, cantante dei Judas Priest; i pezzi eseguiti sono Fat Lip, No Sleep Until Brooklyn dei Beastie Boys, Shout at the Devil dei Mötley Crüe e You've Got Another Thing Coming dei Judas Priest.Verso la fine dell'anno i Sum 41 producono una canzone natalizia con i Tenacious D, la band di Jack Black, intitolata Things I Want, in cui viene descritta la lista dei regali dell'attore americano.
Nel 2002, inoltre, il gruppo canta una cover degli Aerosmith e dei Run DMC, Walk This Way, con Ja Rule e Nelly.
La canzone What We're All About, presente come parte della nona traccia di Half Hour of Power, con il titolo di Dave's Possessed Hair/It's What We're All About, e in Motivation EP, con il titolo più conosciuto, viene inclusa nella colonna sonora del film Spiderman; la canzone, inoltre, presenta un assolo suonato da Kerry King degli Slayer, il quale compare anche nel videoclip della canzone.
Il 26 novembre 2002 i Sum 41 pubblicano il loro secondo album in studio, Does This Look Infected?, a poco più di un anno di distanza da All Killer No Filler. Con quest'album il gruppo inizia lentamente a cambiare la propria tipologia di musica, introducendo un nuovo stile ma mantenendo il suono armonioso che lo contraddistingue. Il primo singolo è Still Waiting, nel cui video un accattivante produttore convince (per finta) i ragazzi a cambiare il loro nome in "The Sums", risultando una parodia dei The Strokes; inoltre appare per una frazione di secondo un gioco dell'impiccato in cui la parola da indovinare è "Sum 41". Il brano compare anche come colonna sonora del videogioco Obscure.
La band fa seguire a Still Waiting un nuovo singolo, The Hell Song, nel cui video i ragazzi, con i costi di produzione ridotti all'osso, utilizzano pupazzi con le loro immagini oltre che quelle di altri personaggi famosi come Ozzy Osbourne, Gene Simmons, Pamela Anderson, Edward The Head e Gesù. Dal loro singolo successivo, Over My Head (Better Off Dead), è stato tratto un video trasmesso solo in Canada e sul sito web ufficiale negli USA, contenente immagini live della band, in cui appare anche la scritta: This video may contain: Nudity; Graphic violence; Sexual activity; Crude indecent language (Il video può contenere: Nudità; Violenza grafica; Attività sessuale; Linguaggio indecente e crudo). Tutto questo materiale è stato poi utilizzato anche per il DVD Sake Bombs and Happy Endings del 2004.
Un altro pezzo, Asshole, è dedicato ad Anna Nicole Smith e dura appena 37 secondi; durante un concerto, Deryck spiega che la canzone è una "special love song" (in realtà una serie di insulti) dedicata alla modella: da ciò deriva il nuovo titolo, A.N.I.C. (Anne Nicole Is a Cunt), con il quale il brano è stato incluso nell'album.
Yesterday.com è invece dedicata a un ex membro della band, che era solito dire "It's so yesterday.com" per indicare qualcosa ormai diventato vecchio, datato. Il nome della canzone è stato cambiato in seguito in No Brains.
Dopo un lungo tour per pubblicizzare l'album Does This Look Infected?, nel 2003 i Sum 41 interpretano una canzone dal titolo Little Know It All assieme a Iggy Pop; questa canzone è contenuta nell'ultimo album del cantante statunitense, Skull Ring, ed i Sum 41 hanno partecipato, assieme al leader degli Stooges, al Late Show with David Letterman per promuovere la canzone. Iggy Pop ha dichiarato di aver scelto i Sum 41 "perché hanno le palle".
Il 6 maggio dello stesso anno aprono inoltre l'MTV Icon in onore ai Metallica, suonando un medley di tre brani della band di Hetfield e Ulrich: For Whom the Bell Tolls, Enter Sandman e Master of Puppets..
All'inizio del 2004 la band partecipa alla prima edizione della manifestazione Rock Against Bush, registrando la canzone Moron, inclusa sia nel primo album della manifestazione che in alcune edizioni del CD Chuck (quella giapponese e quella europea) come bonus track.
A giugno del 2004 i Sum 41 si recano in Congo per girare un documentario per MTV chiamato Rocked: Sum 41 in Congo, trasmesso il 21 settembre dall'emittente musicale. Il documentario, incentrato sulla guerra in Congo, descrive il viaggio di dieci giorni della band attraverso le regioni orientali della Repubblica assieme a War Child, organizzazione di volontari canadesi che offre assistenza umanitaria ai bambini coinvolti nella guerra. Il documentario mostra anche la famosa fuga della band dal Congo sotto la supervisione di Chuck Pelletier, membro canadese dei caschi blu dell'ONU che salvò le vite della band e di altri 40 civili mentre la band stava registrando il documentario.
In seguito i Sum 41 intitolano in suo onore il loro ultimo album, Chuck. La War Child mise poi in vendita il video il 29 novembre 2005 negli Stati Uniti e in Canada.
Chuck esce quindi il 12 ottobre 2004 e il primo singolo, nonché l'unica canzone dell'album scritta dopo gli avvenimenti in Congo, è We're All to Blame, cui seguono Pieces e Some Say. Anche No Reason doveva essere pubblicata come singolo mondiale, ma fu poi scelta solamente per Stati Uniti ed Europa, mentre Some Say è stata venduta in Canada e Giappone.
We're All to Blame compare nel film Godzilla: Final Wars, come sottofondo nella battaglia tra il Godzilla giapponese e la sua controparte di Sydney, Australia, mentre Noots (traccia bonus di Chuck - Special Edition) nel film I Fantastici Quattro (2005). I Sum 41 sono anche stati scritturati per pubblicizzare i cellulari Motorola con in dotazione il software di iTunes.
L'ultimo album si differenzia notevolmente dai precedenti e contiene al suo interno melodie ed influenze molto diverse tra loro. Le sonorità spaziano dall'hardcore punk e il pop punk tipici dei dischi precedenti, all'alternative rock e al metal. Come dichiarato sul sito ufficiale, i Sum 41 lo considerano il loro Il 1º settembre 2004 il gruppo festeggia la nascita della stazione radio canadese CHOI all'Agora de Québec con altri gruppi, tra i quali Les Pistolets Roses e Wide Load.
I Sum 41 mettono in vendita anche un nuovo album, chiamato Happy Live Surprise, che contiene 22 canzoni, tutte registrate dal vivo a London, Ontario. Happy Live Surprise esce il 21 dicembre 2005 in Giappone. La band mette in vendita lo stesso CD sotto il nome di Go Chuck Yourself il 7 marzo 2006 in Canada.
Dopo il tour per Go Chuck Yourself, i Sum 41 partecipano anche ad un tour negli Stati Uniti con i Good Charlotte, con il supporto di Lola Ray e Hazen Street, dopodiché aprono i concerti dei Mötley Crüe per il loro Carnival of Sins Tour.
Dopo lo spettacolo dell'11 settembre 2005 a Quebec City, la band ferma temporaneamente i propri tour. Nonostante ciò, il 17 aprile 2006 i Sum 41 partecipano ad un tributo a Iggy Pop, raggiungendo Iggy sul palco per Little Know It All e Lust 4 Life. Partecipano anche a un album in onore di John Lennon, con una cover di una sua canzone. I proventi della vendita dell'album andarono ad Amnesty International. Nello stesso anno i Sum 41 hanno partecipato a Punk's Not Dead, un documentario sulla scena punk dal 1976 ad oggi, in cui vi sono anche partecipazioni di band come i NOFX, Bad Religion, Anti-Flag ed altri artisti. Il film è uscito nel 2006, anno del trentesimo anniversario del punk.
Nel pomeriggio dell'11 maggio 2006 Dave "Brownsound" Baksh, fino ad allora cofondatore e chitarrista solista della band, annuncia in un comunicato tramite il suo manager che avrebbe lasciato i Sum 41 per lavorare al suo nuovo gruppo, i Brown Brigade. Da allora i Sum hanno deciso di continuare in tre, senza un sostituto di Dave per le canzoni successive.
Come dichiarato dal chitarrista, "il gruppo aveva ormai perso l'ironia che li aveva contraddistinti e, nonostante l'attenzione da parte delle case discografiche fosse ancora alta, non sentiva più buone sensazioni da parte delle stazioni radiofoniche". Stanco delle critiche nei confronti della band di Ajax Brownsound ha deciso così che avrebbe concentrato i suoi sforzi unicamente sui Brown Brigade, gruppo con il quale avrebbe potuto da un lato dimostrare la sua abilità con la chitarra, omaggiando le tre band che l'avevano portato a suonare musica, Anthrax, Megadeth e Metallica, e, dall'altro, recuperare nei testi delle canzoni il senso dell'humor perso nei dieci anni di carriera con i Sum 41 (dieci anni festeggiati tra l'altro il 31 luglio 2006).
Dopo l'addio di Dave, Deryck pubblica il 12 maggio 2006 il suo primo commento ufficiale sulla partenza del chitarrista. Conferma così l'uscita dal gruppo del chitarrista indo-canadese ed annuncia intenzione della band di non sostituirlo, ma comunica allo stesso tempo la volontà di cercare un chitarrista di supporto per i concerti, che tuttavia non sarebbe apparso nei video, nei set fotografici, negli album e che non avrebbe avuto potere decisionale all'interno della band.
Durante la pausa del gruppo, Deryck concentra la sua attenzione della carriera di produttore, scrivendo e producendo due canzoni per l'album di Avril Lavigne, The Best Damn Thing. Stevo registra il suo primo video come regista per una band canadese chiamata The Midway State e Cone inizia un progetto con Todd Morse degli H2O e Juliette and the Licks. Il duo, chiamato The Operation M.D., all'inizio del 2007 pubblica l'album di debutto We Have an Emergency, in cui Cone è voce secondaria, voce principale in tre canzoni e suona basso e tastiere. L'album dei The Operation M.D. è co-prodotto e mixato da Deryck Whibley. Il primo video, Sayonara, è diretto da Stevo.
I Sum 41, a seguito dell'abbandono di Dave, hanno poi registrato il loro quinto album. Come annunciato agli inizi di maggio 2007, tra il 18 e il 27 luglio 2007 (a seconda delle nazioni) esce, per la prima volta senza la supervisione di Greig Nori (loro manager fino all'inizio del 2005), Underclass Hero, che condivide il titolo con il primo singolo del quale i Sum 41 hanno già pubblicato il video, messo nel loro sito ufficiale il 29 maggio 2007. Il video è ambientato di notte su una spiaggia a Los Angeles: i Sum 41 hanno annunciato, attraverso il loro sito ufficiale, che sarebbero serviti 200 ragazzi maggiorenni per girare questo video. Il cortometraggio vede infatti protagonisti delle cheerleader, una persona vestita con un costume raffigurante una A cerchiata (simbolo dell'anarchia), altri ragazzi e ovviamente i Sum 41 al centro della scena che suonano vicino ad un falò. Il ritornello di Underclass Hero è lo stesso di Subject to Change, canzone contenuta in Chuck. Deryck ha spiegato la sua scelta affermando che la canzone originale non ha mai avuto un vero e proprio argomento (da qui il nome della canzone, Subject to Change, ovvero argomento da cambiare), e perciò egli si era ripromesso di utilizzare il ritornello in un'altra canzone. Oltre ad Underclass Hero, la band ha pubblicato un altro singolo: March of the Dogs (disponibile su iTunes) canzone molto vicina a Chuck, ma anche l'unica del disco a parlare di politica. In essa è narrata tra l'altro la morte del Presidente degli Stati Uniti d'America, oltre a duri commenti nei confronti di George W. Bush, causando aspre critiche nei confronti della band, specie da parte degli ambienti conservatori, che additano i Sum 41 come irresponsabili che non hanno il diritto di criticare il Presidente statunitense poiché, tra l'altro, essi non sono statunitensi, ma canadesi.
March of the Dogs non è però il primo episodio di polemiche da parte dei Sum 41 sull'ex Capo di Stato statunitense, verso il quale sono sempre stati molto critici. Tra l'altro, già un'altra canzone, Moron (lett.: ritardato mentale, deficiente), bonus track di Chuck suonata a Rock Against Bush, è considerata come un duro biasimo nei confronti di Bush. Come dichiarato da Deryck Whibley in Go Chuck Yourself introducendo la canzone al pubblico in I Have a Question: Disponibili già da tempo illegalmente su YouTube, le canzoni del nuovo album sono state pubblicate in anteprima sul sito Mtv.com. Cone ha descritto il disco come veloce e duro, mentre a differenza Deryck lo considera molto più vicino a All Killer No Filler e Does This Look Infected? e dalle sonorità meno tendenti al metal. Nel sito ufficiale di MySpace, insieme alla notizia del completamento del nuovo album, è stato aggiunto un messaggio riguardante il nuovo chitarrista, Tom Thacker, membro dei Gob, che si aggiungerà alla band solo per le tournée.
In contemporanea all'uscita di Underclass Hero è stato trasmesso dalle radio il nuovo singolo, Walking Disaster, canzone che, come affermato da Deryck, tratta della sua infanzia e degli anni della crescita, periodo che non gli ha lasciato un buon ricordo. La canzone tratta però, d'altra parte, anche del suo rapporto con la moglie Avril Lavigne, con la quale, se dovessero arrivare dei figli, vorrebbe dar loro un'infanzia migliore della sua.
La canzone è stata presentata e suonata il 24 luglio al Tonight Show di Jay Leno sul canale americano NBC. La canzone Underclass Hero viene inclusa nel videogioco Madden NFL 08. Il terzo singolo estratto da Underclass Hero è With Me, pubblicato a fine febbraio 2008 contemporaneamente al video e presentato con una performance live al Jimmy Kimmel Show sul canale americano ABC; la canzone è stata inoltre colonna sonora di un episodio della serie americana Gossip Girl. La canzone So Long Goodbye è stata usata come parte della colonna sonora della serie Life of Ryan in onda su MTV Pulse. Il 26 novembre 2008 è stato pubblicato, per il solo mercato giapponese, la prima raccolta del gruppo, 8 Years of Blood, Sake and Tears.8 Years of Blood, Sake and Tears è stato pubblicato il 16 marzo anche per il resto del mondo, con il titolo All the Good Shit: 14 Solid Gold Hits 2001-2008..
Originariamente i Sum 41 avevano annunciato che il disco sarebbe stato prodotto da Gil Norton. La produzione era effettivamente iniziata il 1º novembre, ma in seguito Deryck afferma che non riuscendo ad essere soddisfatto della produzione in corso ha allontanato Norton per dedicarsi personalmente agli aspetti produttivi del CD.
Intanto in questo periodo viene annunciato che Tom Thacker è entrato ufficialmente nella band come membro fisso, anche se collaborerà al quinto album della band solo scrivendo il brano Screaming Bloody Murder.
Dopo continue posticipazioni dell'uscita del disco, i Sum 41 hanno comunicato che il loro nuovo CD uscirà in tutto il mondo il 29 marzo 2011, mentre in Giappone, dove alcuni fan hanno avuto la possibilità di ascoltare l'album in anteprima già a inizio 2011, la pubblicazione è prevista per il 23 marzo. Inoltre, dopo aver annunciato l'intenzione di organizzare 41 show in territorio francese, la band ha annunciato che Screaming Bloody Murder uscirà in Francia il 28 marzo.
Il 6 luglio 2010 è apparsa in internet una canzone tratta dal nuovo album, intitolata Skumfuk. Secondo il Twitter della band, la canzone è stata rubata e pubblicata senza il loro consenso, ma ciò non gli avrebbe causato fastidio e anzi, semmai, felicità per le numerose visualizzazioni su YouTube. In seguito a questo avvenimento la band ha cominciato a suonare regolarmente live la nuova canzone, nonostante non venga considerato un singolo del nuovo CD.
Il 14 gennaio 2011 viene trasmesso in anteprima mondiale il primo singolo del nuovo album, la title track Screaming Bloody Murder, sulla stazione radio Windsor 89X. La canzone uscirà poi in vendita su iTunes il 7 febbraio.
L'11 marzo sul sito altpress.com è stata pubblicata una terza canzone estratta dall'album, intitolata Blood in My Eyes.
Tra il 2010 e il 2011 i Sum 41 hanno toccato l'Italia in cinque diverse occasioni: il 17 aprile all'Alcatraz di Milano, il 4 settembre all'Arena Parco Nord partecipando all'I-Day Festival con blink-182, Simple Plan, All Time Low e Leeches, il 13 novembre nuovamente all'Alcatraz, il 10 febbraio 2011 al Teatro Della Concordia a Venaria Reale e il 13 all'Estragon a Bologna. In queste due ultime esibizioni la band ha eseguito live sia Skumfuck che Screaming Bloody Murder e Cone ha confermato che la band tornerà in estate per partecipare ad un festival.
L'album esce il 28 marzo in Italia, mentre negli Stati Uniti il 29 marzo, debuttando alla posizione numero 31 della Billboard 200.
Il 15 giugno 2011 viene estratto il secondo singolo da Screaming Bloody Murder, intitolato Baby You Don't Wanna Know, seguito il 3 agosto da un video ufficiale del brano, mentre il 9 agosto la band pubblica Live at the House of Blues: Cleveland 9.15.07, album live contenente un concerto del 2007 tenutosi a Cleveland, Ohio.
Nel 2012 la band pubblica come terzo singolo di Screaming Bloody Murder Blood in My Eyes.
Il 26 novembre 2012, durante un'intervista, Deryck dichiara che nel 2013 la band si dividerà tra vari tour e la registrazione del loro sesto album, di cui però non anticipa niente. Nel marzo 2013, aggiunge in un'altra intervista: Il 18 aprile 2013, tramite un comunicato sul sito ufficiale della band, Steve Jocz annuncia che lascia i Sum 41. Nel suo messaggio Stevo scrive:
Dopo l'uscita di Steve Jocz dai Sum 41, il gruppo non si esibisce più dal vivo e non rilascia alcuna notizia o aggiornamento sul suo futuro né sul sito ufficiale né nei principali social network. Dopo mesi di silenzio, nel febbraio 2014 Deryck Whibley concede un'intervista a Jay Hud dell'emittente radiofonica 89x, durante la quale annuncia che il gruppo è al lavoro su del nuovo materiale con un nuovo batterista.
Tra aprile e maggio 2014 Deryck viene ricoverato all'ospedale per via di un collasso avuto a causa del suo abuso di alcol. Durante il periodo in ospedale, il cantante lascia un messaggio sul suo sito ufficiale dicendo che si rimetterà presto e che ha finalmente ritrovato l'ispirazione e la voglia di fare per dedicarsi alla scrittura di un nuovo album dei Sum 41. Il 9 giugno Whibley pubblica un altro aggiornamento in cui annuncia che si è quasi completamente ripreso e che sta lavorando a dei nuovi brani nel suo studio.
Dopo oltre due anni di assenza dal palco, i Sum 41 tornano a suonare dal vivo in occasione della seconda edizione degli Alternative Press Music Awards, il 22 luglio 2015, durante il quale lo storico chitarrista della band Dave "Brownsound" Baksh sale sul palco a fianco degli ex compagni Deryck Whibley e Cone McLasin per suonare i brani di successo In Too Deep e Fat Lip e una cover di King of Rock dei Run DMC, cantata con DMC stesso. In concomitanza con il loro ritorno sulle scene, i Sum 41 aprono una campagna di raccolta fondi su Pledge Music per poter finanziare le registrazioni del loro sesto album in studio.
Ad agosto, in un'intervista rilasciata ad Alternative Press, Deryck e Dave confermano i rumour delle settimane precedenti affermando che "Brownsound" è definitivamente tornato a far parte dei Sum 41, e che registrerà insieme a loro e a Frank Zummo, il nuovo batterista, il nuovo album. Nel maggio 2016 i Sum 41 firmano un contratto discografico con l'etichetta statunitense Hopeless Records, e a giugno annunciano la prossima uscita del loro sesto album di inediti, intitolato 13 Voices, che avviene il 7 ottobre 2016. Il disco viene anticipato dai singoli Fake My Own Death e War, pubblicati in estate.
Il 23 aprile 2019, a sorpresa, viene annunciata la pubblicazione di un nuovo singolo intitolato Out for Blood. Altri due singoli inediti, A Death in the Family e Never There, vengono pubblicati a giugno. I tre brani vanno ad anticipare il settimo album di inediti del gruppo, Order in Decline, pubblicato il 19 luglio dello stesso anno dalla Hopeless Records. Poco prima dell'uscita ufficiale del disco un altro singolo, 45 (A Matter of Time), viene pubblicato nella prima settimana di luglio.
I Sum 41 hanno affermato che inizialmente (nel periodo di Half Hour of Power) la loro musica era stata profondamente influenzata dai NOFX (citati spesso anche nei ringraziamenti dei dischi). È inoltre importante l'influenza dei Beastie Boys, specialmente nelle sonorità hip hop in All Killer No Filler e lo stile rapcore, rintracciabile in Fat Lip e What We're All About.  Il secondo album Does This Look Infected? è stato invece condizionato dallo stile dei The Offspring, come affermato dalla stessa band. Sono inoltre presenti delle sonorità metal, provenienti da band come gli Iron Maiden. Gli stessi Sum 41, infatti, nel testo di Fat Lip, scrivono  riferendosi agli Iron Maiden e ai Judas Priest. Il loro album Chuck contiene una serie di canzoni che possono essere comparate al thrash metal, indicando i Metallica per la loro influenza nelle sonorità metal. Il gruppo canadese ha inoltre affermato di essersi ispirato ai Green Day sin dall'inizio, ciò è evidente soprattutto nello stile alternativo di Chuck, ma sono ravvisabili anche similitudini con i Rancid e i Descendents. Con Underclass Hero, il gruppo ha iniziato un ritorno alla musica del primo periodo, richiamando lo stile di Does This Look Infected? e All Killer No Filler. Quest'ultimo album è più "tagliente e veloce" di tutti i precedenti album, come afferma il bassista Cone McCaslin, ma è "più punk rock e meno metal" come ha dichiarato il frontman Deryck Whibley durante la produzione dell'album. A partire dal quinto album Screaming Bloody Murder il gruppo ha riabbracciato gradualmente le sonorità metal.
I Sum 41 hanno a loro volta ispirato gruppi, tra i quali si ricorda la band canadese Simple Plan che, all'inizio della propria carriera, aveva fatto da supporto live ai Sum 41. In una canzone cantano di loro, nella quale si riferiscono inoltre ai Good Charlotte, ai blink-182 e agli MxPx:
I Sum 41 nel corso degli anni hanno spaziato da canzoni ad alto contenuto sociale a canzoni "spensierate", canzoni politiche seguite o precedute da altre irriverenti senza soluzione di continuità. Se da un lato, specialmente nei primi album, le tracce esprimono serenità e voglia di divertirsi (Summer, presente nei primi due dischi, Heart Attack in All Killer No Filler, No Brains in Does This Look Infected?), negli album successivi sviluppano un impegno sociale maggiore, criticando la moderna società nei suoi aspetti più contraddittori. Insieme ad un atteggiamento anti-bellico e di condanna dell'azione di George W. Bush, come espresso in Still Waiting (incentrato sulla guerra in Afghanistan), We're All to Blame, Moron e March of the Dogs, altri brani trattano i temi più differenti. Così The Hell Song affronta (implicitamente) il problema dell'AIDS, My Direction il numero impressionante di suicidi giovanili nel Nord America, All Messed Up (questi ultimi tre brani sono tratti da Does This Look Infected?) le droghe, il DVD Rocked: Sum 41 in Congo le tante guerre dimenticate in Africa. Pieces (Chuck) e Walking Disaster (Underclass Hero), infine, trattano della solitudine nel mondo moderno e il rapporto conflittuale con la famiglia.
Deryck Whibley – voce (1998-presente), chitarra ritmica (1997-presente), tastiera (2004-presente), chitarra solista, cori (1996-1998)
Tom Thacker – chitarra solista, tastiera, voce secondaria (2009-presente), chitarra ritmica (2015-presente)
Mark McAdam – voce (1996-1997), chitarra ritmica (1996), basso (1996-1997, 1997), cori (1997)
2001: MuchMusic Video Awards, categoria People's Choice: Favorite Canadian Group per Makes No Difference
2012: Grammy Awards, categoria Best Hard Rock/Metal Performance per Blood in My Eyes
Steve McLean. Hot Canadian Bands. New Westminster (British Columbia, Canada), Trickle Rock Books, 2005. ISBN 978-1-894864-53-4.
Leigh Ann DeRemer. Contemporary Musicians. Profiles of the People in Music. Volume 38. Farmington Hills (Michigan, Stati Uniti), Thomson/Gale, 2003. ISBN 978-0-7876-6028-4.
AA. VV. Sum 41. Chuck. Milwaukee (Wisconsin, Stati Uniti), Hal Leonard Corp, 2005. ISBN 978-0-634-09513-9.
AA. VV. Sum 41. Does This Look Infected?. Milwaukee (Wisconsin, Stati Uniti), Hal Leonard Corp, 2003. ISBN 978-0-634-05790-8.
AA. VV. Best of Sum 41. Milwaukee (Wisconsin, Stati Uniti), Hal Leonard Corp, 2003. ISBN 978-0-634-06403-6.
AA. VV. Sum 41. All Killer No Filler. Milwaukee (Wisconsin, Stati Uniti), Hal Leonard Corp, 2001. ISBN 978-0-634-03650-7.

Sunny Isles Beach è un comune degli Stati Uniti d'America situato nella parte settentrionale della Contea di Miami-Dade dello Stato della Florida.
Secondo le stime del 2011, la città ha una popolazione di 21.327 abitanti su una superficie di 3,60 km².

Le suore ancelle del Cuore Immacolato di Maria, dette suore del Buon Pastore (in francese Sœurs Servantes du Cœur Immaculé de Marie), sono un istituto religioso femminile di diritto pontificio: le suore di questa congregazione pospongono al loro nome le sigla S.C.I.M.
La congregazione fu fondata a Québec l'11 gennaio 1850 da Marie-Josephe Fitzbach (1806-1885) con l'aiuto di sei insegnanti che accettarono di condurre vita comune con lei.Le giovani vennero iniziate alla vita religiosa dal gesuita Louis Saché, che redasse anche le prime regole della comunità. Vivente la fondatrice, la congregazione arrivò a contare oltre 200 suore sparse tra Canada e Stati Uniti d'America: nel 1935 venne aperta la prima missione in Basotholand e poco dopo altre in Ruanda e ad Haiti.L'istituto ricevette il pontificio decreto di lode il 25 luglio 1880 e le sue costituzioni vennero approvate definitivamente dalla Santa Sede il 16 agosto 1921.
Gli scopi delle suore del Buon Pastore sono istruire ed educare cristianamente la gioventù e aiutare le giovani traviate.Sono presenti in Canada, Haiti, Stati Uniti d'America, Brasile, Lesotho, Ruanda, Sudafrica;
la sede generalizia è a Sainte-Foy-Sillery-Cap-Rouge, presso Québec.Alla fine del 2008 la congregazione contava 582 religiose in 46 case.
Annuario Pontificio per l'anno 2010, Libreria Editrice Vaticana, Città del Vaticano 2010. ISBN 978-88-209-8355-0.
Guerrino Pelliccia e Giancarlo Rocca (curr.), Dizionario degli Istituti di Perfezione (DIP), 10 voll., Edizioni paoline, Milano 1974-2003.

Super Mario World, conosciuto in Giappone come Super Mario Bros. 4 - Super Mario World (スーパーマリオブラザーズ4 スーパーマリオワールド Sūpā Mario Burazāzu fō - Sūpā Mario Wārudo?), è un videogioco a piattaforme della Nintendo, pubblicato nel 1990 per la console Super Nintendo Entertainment System. È il quinto della serie principale di Mario (sesto se si considera anche Super Mario Bros.: The Lost Levels). Lo sviluppo del titolo è stato curato dalla Nintendo Entertainment Analysis and Development e diretto da Shigeru Miyamoto, con la collaborazione di Takashi Tezuka.
Si tratta del primo videogioco Nintendo in cui appare il dinosauro Yoshi. Con il suo aiuto, Mario e Luigi devono liberare Dinosaur Land e la Principessa Toadstool dalle grinfie di Bowser e dei Bowserotti. Il gameplay è lo stesso adottato nei titoli precedenti di Mario, con l'aggiunta di elementi e caratteristiche che saranno presenti anche nella maggior parte dei videogiochi successivi.Il titolo ha avuto successo di critica e commerciale, vendendo più di 20 milioni di copie in tutto il mondo. Nel corso degli anni è stato ripubblicato quattro volte: prima come parte della raccolta Super Mario All-Stars del 1994, poi come riadattamento per Game Boy Advance intitolato Super Mario Advance 2 (スーパーマリオアドバンス2 Sūpā Mario Adobansu Tsū?) in Giappone e Super Mario Advance 2: Super Mario World nel resto del mondo; successivamente è stato pubblicato su Virtual Console sia per la Wii, sia per Wii U. Il gioco è inoltre incluso in tutte le versioni del Nintendo Classic Mini: Super Nintendo Entertainment System.
Dal gioco è stata anche tratta un'omonima serie animata, prodotta dalla DiC Entertainment e andata in onda nel 1991.
Dopo aver salvato il Regno dei Funghi in Super Mario Bros 3, Mario e Luigi si recano su Dinosaur Land, un luogo abitato da dinosauri, per prendersi una vacanza. Mentre si trovano sulla spiaggia, però, scoprono che la principessa Toadstool è scomparsa e, perciò, i due fratelli si mettono alla ricerca per scoprire chi l'ha rapita. All'interno di una foresta s'imbattono in un grosso uovo che si schiude e da cui fuoriesce un giovane dinosauro chiamato Yoshi: quest'ultimo informa i due fratelli che altri amici dinosauri sono stati catturati e imprigionati dai Koopa Troopa. Mario e Luigi giungono alla conclusione che i responsabili sono Bowser e i suoi figli.I tre protagonisti partono alla ricerca degli amici di Yoshi, disseminati nei sette castelli dei figli di Bowser, fino a raggiungere il castello di Bowser dove, una volta sconfitto, quest'ultimo vola lontano con la sua Auto Clown Koopa e la principessa Toadstool viene salvata.
Super Mario World è un platform bidimensionale in cui il giocatore controlla il personaggio principale (Mario o Luigi). Il gioco riprende alcune caratteristiche del gameplay di titoli precedenti (Super Mario Bros., Super Mario Bros. 2, Super Mario Bros. 3) e introduce nuovi elementi, ad esempio la possibilità di roteare durante il salto.Il gioco è diviso in sette mondi che contengono diversi livelli ciascuno, comprendenti speciali, case stregate e fortezze. Il percorso di ogni mondo si conclude con un castello dentro il quale è presente un Bowserotto (figlio di Bowser). Diversamente da quanto accadeva in Super Mario Bros. 3, la mappa è un unico territorio, pur presentando diversi tipi di paesaggio. Essa presenta un vasto reticolato di strade e vicoli, che sono accessibili solo trovando tutte le uscite segrete dei livelli: di fatto, per la prima volta nella serie, Mario può ripetere più volte lo stesso livello, o per trovare le uscite segrete o per raccogliere le cinque monete di Yoshi presenti in ciascuno di esso.Ogni livello, contenente vari ostacoli e nemici, in cui Mario può saltare, correre o distruggere questi ultimi, termina con un traguardo, fatta eccezione per i livelli speciali, le case stregate e i castelli; alcuni livelli prevedono anche un'uscita segreta accessibile mediante una chiave solitamente ben nascosta. Il totale delle uscite "ordinarie" e di quelle segrete è di 96, pertanto il gioco non può considerarsi completo al 100% fino a quando nella schermata iniziale di selezione del blocco in cui si sta giocando (A, B, C) compare la cifra 96 affiancata da una stella (in alcune versioni compare 96 in azzurro); il numero in questione non indica la percentuale di completamento del gioco ma il totale delle uscite che si è stati in grado di trovare.
Per concludere la storia non è tuttavia necessario trovare tutte le 96 uscite, infatti è possibile sfruttare diverse scorciatoie (in particolare la Star Road, evoluzione delle Warp Zone dei precedenti titoli) per raggiungere direttamente l'ultimo livello, il castello di Bowser, senza quindi dover attraversare tutti i mondi. Inoltre, sbloccati tutti i sentieri della Star Road, è possibile accedere allo "Special World", un mondo speciale che, una volta completato, modifica la grafica di tutto il videogioco.
Super Mario World prevede anche una modalità multigiocatore: a turno, il primo giocatore controlla Mario e il secondo controlla Luigi. Si passa da un giocatore all'altro quando uno dei due perde una vita, oppure quando arriva al traguardo di un livello. Lo sprite che rappresenta Mario è identico a quello che rappresenta Luigi (cambiano soltanto i colori). Nella versione per Game Boy Advance è possibile scegliere uno dei due personaggi liberamente senza attendere il turno successivo.
Una delle novità introdotte in questo titolo è la Piuma, che trasforma Mario in Mario Cappa, donandogli un mantello con il quale può volare per pochi secondi (offre anche un tipo di volo che garantisce quota infinita, ma di difficile controllo): questo strumento è l'evoluzione del costume da procione denominato Mario Tanuki di Super Mario Bros. 3; oltre alla classica Super Stella che dona invincibilità, è stato inserito il palloncino P, che gonfia Mario come un palloncino, così da permettergli di galleggiare per un limite di tempo. Il giocatore può anche usare i gusci delle tartarughe scagliandoli contro i nemici come arma.Ad arricchire il gameplay c'è il dinosauro Yoshi, che fa da cavalcatura a Mario e funge anch'egli da potenziamento: Yoshi può camminare su nemici appuntiti, può ingoiare quasi tutti gli avversari e inoltre, se ingoia dei gusci Koopa, può volare, sputare fuoco o scatenare terremoti (a seconda del colore del guscio). Nella Terra dei Dinosauri ci sono razze di Yoshi di quattro colori (verde, rosso, blu, giallo), inoltre possono comparire cuccioli di Yoshi che vanno nutriti con frutta e nemici finché non crescono e diventano cavalcabili.
Super Mario World, sviluppato sotto la supervisione della Nintendo EAD, è stato diretto da Takashi Tezuka e prodotto da Shigeru Miyamoto, ideatore delle serie di Mario e The Legend of Zelda, insieme al graphic designer Shigefumi Hino.
Lo sviluppo durò tre anni con una squadra di sedici persone, in cui si cercò di dimostrare le capacità grafiche della console, come gli effetti di rotazione, la trasparenza delle nuvole (all'interno delle case stregate) e l'effetto lampo nel castello di Bowser, tuttavia Miyamoto ritenne il risultato insoddisfacente, derivato da uno sviluppo affrettato, e sperò che gli altri titoli per SNES fornissero maggiore esperienza di gioco, con più emozione e una storia più articolata.Miyamoto dichiarò che già dall'uscita di Super Mario Bros. voleva inserire un dinosauro che accompagnasse Mario nelle sue avventure, ma che gli ingegneri della Nintendo non ne furono in grado all'epoca, a causa delle limitazioni del Nintendo Entertainment System. Felice di aver realizzato infine il suo desiderio, Miyamoto commentò: «finalmente siamo stati in grado di tirare fuori Yoshi dagli schizzi con il SNES».Sebbene il titolo fu pubblicato nel 1991, in ritardo rispetto alle altre console a 16-bit, riuscí a soppiantare la nuova console Sega Mega Drive, nonostante il titolo Sonic the Hedgehog, che aveva ottenuto un rilevante successo commerciale, contribuendo alla popolarità del Mega Drive.
Nel 2002, Super Mario World fu trasposto dalla Nintendo EAD per la console Game Boy Advance con il titolo Super Mario World: Super Mario Advance 2. Alcuni livelli furono leggermente modificati, ma la novità sostanziale sta nel fatto di aver rimosso la modalità multigiocatore, pertanto è possibile giocare esclusivamente con Mario oppure con Luigi. Quest'ultimo è in grado di saltare più in alto rispetto al fratello, ma corre meno veloce. I personaggi inoltre hanno una voce, prestata da Charles Martinet; i loro sprite furono inoltre modificati, rendendoli più simili a quelli presenti in Super Mario Advance, basato a sua volta su Super Mario Bros. 2.
Kōji Kondō compose la colonna sonora di Super Mario World utilizzando solamente una tastiera elettronica. I brani presenti nei vari livelli, tranne che nell'introduzione, nei titoli di coda, sulla mappa, nei castelli e durante il combattimento con Bowser, rappresentano una variazione della stessa melodia. In base al livello, la velocità cambia: ad esempio aumenta in un bonus, diminuisce nei livelli subacquei. È presente anche il tema principale di Super Mario Bros. che può essere ascoltato nello Special World dopo alcuni minuti. Inoltre, cavalcando Yoshi, il bongo accompagna la musica.
Nel 1994 Super Mario World comparve anche nella raccolta per SNES Super Mario All Stars + Super Mario World, venduta negli Stati Uniti e in Europa con una speciale edizione della console sopracitata; questa versione di Super Mario World è ripresa parzialmente dal gioco Super Mario Advance 2 (che contempla a sua volta modifiche aggiuntive nelle opzioni).
Dal 2007 Super Mario World è scaricabile dal servizio on-line Virtual Console, per la console Wii: si tratta della stessa versione per SNES con l'aggiunta della possibilità di salvare in ogni momento (anche durante un livello), semplicemente uscendo dal gioco dal menu HOME.
Il 26 aprile 2013 è uscita una versione per la Virtual Console del Wii U, giocabile solo col Wii U GamePad, anche senza TV. Inoltre si può creare un punto di recupero (salvataggio) in ogni momento.Il 3 marzo 2016 è uscita una versione Virtual Console esclusiva per New Nintendo 3DS.
Il videogioco è stato acclamato dalla critica e considerato uno dei migliori della Nintendo, anche a distanza dalla sua pubblicazione. GameRankings gli ha attribuito un punteggio del 94,44%, mentre Allgame gli ha assegnato una valutazione di 5 su 5. La rivista Nintendo Power lo ha definito l'ottavo miglior videogioco per una console Nintendo nella classifica del "Top 200 Games"; l'Official Nintendo Magazine lo posiziona al settimo tra i 100 migliori videogiochi Nintendo di tutti i tempi.Nel 2009, a seguito di un sondaggio condotto dal mensile Empire, Super Mario World è stato considerato il miglior videogioco di tutti i tempi. Nella recensione per la versione su Virtual Console, il sito Imagine Games Network ha consigliato agli utenti di scaricare il titolo, specialmente a coloro che non hanno mai provato il port per GBA di cinque anni prima, che aveva riscosso successo, e ha dato un punteggio di 8,5 su 10.Del titolo sono stati elogiati diversi elementi, come la grafica colorata, il sistema di controllo, la colonna sonora e la longevità. L'introduzione di Yoshi è stata ben accolta dalla critica, anche se, secondo SpazioGames, tale personaggio poteva avere un «maggior peso» e rilevanza nella storia, anziché figurare come semplice comparsa.Super Mario World ha venduto più di 20 milioni di copie in tutto il mondo, diventando il videogioco più venduto di sempre della console SNES. Essendo il titolo di lancio della SNES, Super Mario World ha contribuito in maniera rilevante alla diffusione della stessa, con la vendita di 49,10 milioni di unità in tutto il mondo, tra cui 23,35 milioni in Nord America e 17,17 milioni in Giappone. In un sondaggio del 2008, Yoshi è stato scelto come il terzo personaggio videoludico preferito in Giappone, dopo Cloud Strife e Mario, rispettivamente al secondo e primo posto.
A meno di un mese dalla pubblicazione del titolo nel Nord America, la DiC Entertainment produsse una serie animata dal titolo omonimo, basata sul videogioco, sebbene alcune caratteristiche siano state modificate. La serie andò in onda tra settembre e dicembre del 1991 sulla rete televisiva NBC.Tra il 1992 e il 1993 era prevista la pubblicazione di uno spin-off di Super Mario World dal titolo Super Mario's Wacky Worlds, prodotto dalla NovaLogic per il lettore Philips CD-i, ma fu cancellato a causa del fallimento commerciale della console.Nel 1995 venne pubblicato un seguito del videogioco, intitolato Super Mario World 2: Yoshi's Island, che vede protagonista Yoshi.
(EN) Manuale di istruzioni di Super Mario World, Nintendo, 1991, ISBN non esistente.
(EN) Bryan Stratton, Super Mario World: Super Mario Advance 2: Prima's Official Strategy Guide, Prima Games, 2002, ISBN 0-7615-3913-1.
 Andrea Babich, I mondi di Super Mario: azioni, interazioni, esplorazioni, Unicopli, 2004, ISBN 88-400-0957-4.
 Super Mario World (JPG), in The Games Machine, nº 30, Edizioni Hobby, aprile 1991,  pp. 44-45, OCLC 955708482.
 Super Mario World (JPG), in Retro Gamer, nº 3, Formello, Play Media Company, ottobre/dicembre 2007,  pp. 28-29, ISSN 1971-3819.

Super Metroid (スーパーメトロイド Sūpā Metoroido?) è un videogioco di tipo avventura dinamica a tema fantascientifico progettato da Nintendo Research & Development 1, programmato da Intelligent Systems e pubblicato nel 1994 da Nintendo per il Super Nintendo Entertainment System.
Super Metroid è il terzo capitolo della saga di Metroid e con la sua cartuccia da 24 megabit è passato alla storia per essere stato per qualche tempo il più esteso e impegnativo videogioco in termini di memoria. La trama, che è cronologicamente situata tra Metroid II: Return of Samus (1991) e Metroid: Other M (2010), vede la protagonista Samus Aran rintracciare e recuperare il cucciolo di Metroid rapito dal suo rivale Ridley e portato sul pianeta Zebes. Super Metroid risulta essere un videogioco a piattaforme in 2D carico di elementi d'azione e d'avventura, dove Samus deve affrontare sul pianeta le forze ostili native e i pirati spaziali, suoi acerrimi nemici, potenziandosi con gli oggetti che trova lungo il cammino. È un videogioco che fa dello spirito esplorativo del giocatore un punto centrale poiché, a differenza degli altri platform che lo hanno preceduto, il suo sviluppo non è esclusivamente lineare ed è ricco di aree nascoste.Scritto e diretto da Yoshio Sakamoto e prodotto da Makoto Kano, con le musiche di Kenji Yamamoto e Minako Hamano, il videogioco venne pubblicato in Giappone il 19 marzo 1994. Grazie alla sua grafica evoluta, al suo gameplay caratteristico, all'atmosfera e all'efficace sonoro fu subito accolto positivamente, ricevette numerosi riconoscimenti e viene tuttora considerato un classico Nintendo e uno dei migliori videogiochi mai realizzati. Nonostante il gran successo alla sua uscita riscosse basse vendite in Giappone, ma raggiunse risultati migliori in Nord America e in Europa, tanto che è stato in seguito distribuito anche per le console Wii e Wii U. Il gioco è inoltre incluso in tutte le versioni del Nintendo Classic Mini: Super Nintendo Entertainment System.
In occasione del 20º anniversario dell'uscita del gioco, nel 2014 si è svolta una mostra apposita nella biblioteca Regenstein dell'Università di Chicago.
Dopo aver estinto i Metroid sul pianeta SR-388, la cacciatrice di taglie Samus Aran consegna l'unico cucciolo superstite ad alcuni scienziati in una colonia spaziale, in modo che possano esaminarlo e studiarlo. Samus torna sulla sua navicella quando improvvisamente riceve una chiamata di soccorso dalla colonia. Tornata sul luogo trova l'intero laboratorio distrutto e i membri della troupe addetta alla ricerca morti; investigando s'imbatte nel suo vecchio nemico Ridley, ma non riesce a impedire che scappi portando con sé la capsula con dentro il piccolo Metroid. La colonia spaziale entra in modalità "autodistruzione" e la ragazza è costretta a fuggire.
Ridley porta il Metroid sul pianeta Zebes, sede della base dei pirati spaziali (distrutta in passato da Samus, ma ora nuovamente operativa), i quali vogliono servirsene per produrre dei cloni. Per poter recuperare il Metroid, Samus deve prima eliminare le quattro potenti creature dominanti sul pianeta: Kraid, Phantoon, Draygon e infine Ridley. Solo dopo averli uccisi tutti potrà accedere a Tourian, il centro di controllo dove è rinchiuso il cucciolo.
Una volta eliminati i Boss e giunta sul posto, viene improvvisamente attaccata da un Metroid gigantesco che le risucchia l'energia vitale ma, poco prima di ucciderla, riconosce in lei la persona che l'aveva accudito e scappa via. Quel Metroid, ormai cresciuto a dimensioni anomale, si rivela essere quindi il cucciolo che stava cercando. Samus si riprende e continua la sua missione sino ad arrivare al covo della creatura che stava a capo dei pirati spaziali: il Cervello Madre, un cyborg che lei aveva distrutto in passato e che è stato successivamente ricostruito dai pirati. Samus abbatte le difese e lo affronta, ma il mostro assume una forma diversa dimostrandosi più potente che mai, tanto da ridurre Samus in fin di vita. Mentre sta per darle il colpo di grazia, improvvisamente interviene in soccorso il Super Metroid e si attacca alla testa del Cervello Madre, assorbendone parte dell'energia e trasferendola a Samus, ma essendo esausto viene finito dai raggi laser sparati dal mostro. Mentre crolla a terra, il Metroid dona a Samus un potere che le consente di sparare colpi altamente distruttivi. La ragazza annienta così il Cervello Madre, ma prontamente si attiva il processo di autodistruzione e Samus è ancora una volta costretta a scappare prima che l'intero globo salti in aria. Raggiunge dunque la navetta e si allontana mentre il pianeta Zebes esplode.
L'esperienza di gioco, ampliata e perfezionata, resta fedele ai due episodi precedenti della serie. Il giocatore controlla Samus Aran, la quale deve esplorare e accedere – tramite porte e ascensori – ad aree ignote del pianeta Zebes con l'ausilio di nuovi potenziamenti (power-up) che la protagonista ottiene durante il gioco e che aumentano le capacità della sua corazza e le abilità fisiche, oltre a permetterle di accedere a spazi altrimenti inaccessibili, spazi che il giocatore deve memorizzare per potervi fare ritorno una volta appresi i potenziamenti necessari. Il videogioco è privo di indicazioni sulla strada da prendere o su come interagire con i nemici: è difatti concepito per coinvolgere fino in fondo il giocatore, lasciandolo libero di esplorare/esaminare l'ambiente, d'imparare dai propri errori e di trovare da solo il modo per proseguire. Il gioco introduce un vasto numero di personaggi e armamenti, una spaziosa mappa e anche la possibilità di selezionare e associare fra loro le armi e le abilità apprese. A questo si aggiunge una maggiore agilità al personaggio e una capacità «Moon Walk», dal nome dell'omonima danza, che è attivabile tramite le opzioni speciali e che consente a Samus di camminare all'indietro durante il fuoco o il caricamento della propria arma.Il sistema di salvataggio è lo stesso di quello presente in Metroid II: Return of Samus; il giocatore può quindi salvare e riavviare la partita dalle stazioni di salvataggio disposte nel pianeta, con la novità che è possibile salvare direttamente dall'astronave di Samus, che ricarica completamente la sua salute e le sue munizioni. Samus è anche in grado di portare molti più serbatoi d'energia rispetto agli altri episodi (ce ne sono quattordici sparsi per il pianeta) e può usare alcune taniche di riserva che le offrono salute aggiuntiva (ce ne sono quattro in tutto). Inoltre può assorbire delle piccole quantità d'energia e munizioni rilasciate dai nemici uccisi e ha la possibilità di ripristinare completamente il vigore e alcuni proiettili collegandosi a specifiche unità situate in alcune stanze.
Crateria: è la superficie del pianeta, la zona è sotto un diluvio costante di piogge acide.
Brinstar: area tropicale del pianeta Zebes, un grande labirinto rivestito di folto fogliame. Dominata da Kraid.
Norfair: una zona prevalentemente magmatica e dalla temperatura elevatissima, si trova nel profondo della superficie. Dominata da Ridley.
Maridia: il mondo acquatico e paludoso di Zebes, è quasi completamente sommerso. Dominato da Draygon.
Wrecked Ship: nave spaziale naufragata anni fa, da allora è pervasa da un'atmosfera sinistra e negativa. Dominata da Phantoon.
Tourian: è il centro di controllo dei pirati spaziali Zebesian e il Cervello Madre. In principio era situato appena sotto la superficie, ma dopo che Samus ha sgominato le sue forze è stato ricostruito.
Il gioco presenta un elevato numero di specie aliene che variano in base al luogo del pianeta in cui si trova il giocatore, mentre alcune sono presenti in più sezioni. Certe creature sono più resistenti e offensive di altre, alcune invece risultano innocue e amichevoli e possono essere sfruttate a proprio vantaggio.
Come negli altri capitoli Samus può usufruire di taniche d'energia e di molti capi d'equipaggiamento che trova gradualmente durante il percorso: le tute le permettono di subire meno danni e di accedere ad aree ristrette; le bombe e i missili le consentono di aprirsi dei varchi, di superare le porte di sicurezza e di avere vantaggi sugli avversari, mentre alcuni particolari accessori possono esserle utili per guardare attraverso i muri o aggrapparsi al soffitto; i cosiddetti "raggi" possono congelare temporaneamente i nemici, trapassare le pareti o semplicemente caricare colpi più potenti. Altri potenziamenti riguardano invece le abilità fisiche: acquisendo componenti specifici, Samus può assumere una forma sferica, saltare molto più in alto del normale, correre a velocità elevata e addirittura distruggere nemici e ostacoli con una capriola. A seconda delle capacità e dell'esperienza del giocatore, alcuni elementi possono anche essere tralasciati o presi in un secondo momento. I raggi e i potenziamenti ottenuti si possono abilitare/disabilitare e combinare in qualsiasi circostanza tramite un'apposita schermata.
Il gioco contiene anche delle sequenze finali alternative: durante la fuga dal pianeta Zebes prima di tornare alla navicella spaziale si può scegliere di soffermarsi a Crateria per salvare alcune razze aliene bonarie. Ciò influenza il filmato conclusivo nel quale si può notare sullo sfondo un minuscolo granello che si allontana dal pianeta. Esistono poi tre versioni diverse del breve filmato che appare dopo i titoli di coda, le quali si possono visualizzare a seconda del tempo impiegato per terminare il gioco: se si finisce in meno di tre ore, appare Samus senza tuta; se si impiegano tra le tre e le dieci ore, la si può vedere con la tuta, ma con il volto scoperto; se si impiegano più di dieci ore, Samus appare invece con la tuta completa.
Dopo l'uscita del secondo capitolo di Metroid le cose erano cambiate per Nintendo R. & D.1. La squadra infatti era stata divisa in due parti: una di queste, l'Intelligent Systems, era responsabile della gestione dei giochi per console ed era guidata da Gunpei Yokoi. Intelligent Systems fu divisa a sua volta in più squadre di sviluppo da Yoshio Sakamoto per dare inizio nell'agosto del 1992 alla realizzazione del terzo episodio della saga: Super Metroid. In realtà la pianificazione iniziale era già cominciata nell'autunno del 1991, ma i tempi si prolungarono a causa della ritardataria approvazione da parte di Nintendo.Il gioco fu scritto e diretto da Sakamoto e prodotto da Makoto Kano, lo staff principale era formato da sole quindici persone (incluso Sakamoto), alcune anche molto giovani: cinque dipendenti formavano la squadra di progettazione del gioco e artistica, due erano incaricati del suono e sette erano occupati con la programmazione; alcuni liberi professionisti erano inoltre disponibili per alcuni incarichi speciali, come la traduzione o il testare un gioco per constatarne eventuali errori. Una volta distribuiti i vari compiti il problema iniziale era accordarsi sull'ambientazione da rappresentare; convennero così che l'idea migliore fosse di far svolgere la storia nuovamente sul pianeta Zebes, principalmente per rimanere conformi alla scenografia del primo capitolo — tra l'altro la troupe era composta anche da persone che avevano lavorato a entrambi i Metroid precedenti. Super Metroid contiene quindi aree già visitate, poiché l'obiettivo degli sviluppatori era infatti quello di aggiungere un senso di familiarità in modo da soddisfare gli amanti della serie, ma non per questo manca di nuove sezioni, che furono inserite soprattutto per aggiungere maggiore dramma all'esperienza di gioco.Con tutte le basi definite Sakamoto realizzò la rappresentazione grafica per le sequenze più importanti, poi vennero fatte le concept-art, che mostravano in modo minuzioso ogni aspetto del pianeta, così da avere una chiara idea dello scenario. Passarono poi alle creature e ai nemici: i diversi stili degli artisti però compromettevano in parte l'omogeneità delle illustrazioni e alcuni mostri apparivano persino gradevoli, come il miniboss Crocomire e il Cervello Madre. Perciò Tomoyoshi Yamane, supervisore creativo, si preoccupò di ritoccare ogni singola figura al fine di dare una forma più coerente e di alterare l'aspetto troppo grazioso dei boss, per esempio rivestendo di muco il corpo di Crocomire e aggiungendo della saliva e una nube d'alito cattivo sulla bocca del Cervello Madre. Una volta definito l'aspetto dei nemici studiarono i loro movimenti e la strategia per batterli; sembra anche che fra il già cospicuo numero di personaggi avessero ideato altre tre specie aliene da inserire: i Bang, i Refle e gli Stoke, che però furono scartate. Inserirono comunque nuove armi, tra cui il Grapple Beam, un raggio usato per far aggrappare e oscillare Samus a dei ganci sul soffitto, e poi incrementarono la fisicità della protagonista, introducendo anche la possibilità di farle sparare in tutte le direzioni mentre si muove in modo fluido e acrobatico. Successivamente si concentrarono sulla collocazione delle varie armi e potenziamenti. Tra le molte novità una piuttosto rilevante che la produzione aggiunse fu quella di avere a disposizione una mappa, alla quale si può accedere in qualsiasi momento e che mostra le strutture dell'ambiente (escluse le aree nascoste), i luoghi scoperti, l'ubicazione delle stanze interessanti e le varie zone di salvataggio. Inizialmente la mappa non era prevista perché non tutti i membri erano concordi nel realizzarla: infatti si ridussero all'ultimo momento e fu completata un mese prima dell'uscita del gioco. Si rivelò comunque un particolare importante, visto che il gioco fu uno dei primi a offrire al giocatore una pianta di riferimento.Per ciò che concerne il reparto sonoro, esso era costituito dai musicisti Minako Hamano e Kenji Yamamoto, che composero le musiche di sottofondo creando un notevole effetto di tensione grazie agli strumenti acustici (come il pianoforte) e ai sintetizzatori. Hamano compose i temi dei titoli iniziali, dei crediti finali, di Maridia, Wrecked Ship e degli scontri con i boss; Yamamoto invece curò i vari effetti sonori, la musica dei filmati di gioco, dell'introduzione, di Brinstar, Crateria e Norfair; in aggiunta registrò la propria voce per convertirla nel verso emesso dalla creatura Spore Spawn, il miniboss di Brinstar. Inoltre i versi di altri mostri furono campionati: quelli del Cervello Madre, ad esempio, vennero presi da Mothra, quelli di Ridley e Draygon invece risultano "rubati" da Angilas e quello di Kraid da Godzilla. Il comparto audio era essenziale per Super Metroid, dal momento che Sakamoto decise di non raccontare la storia utilizzando testi o voci (fatta eccezione per il prologo iniziale), ma puntando tutto sugli effetti sonori e sulla musica, con l'intenzione quindi di creare la giusta atmosfera solo con l'ausilio del suono, come un film muto. Difatti pure Samus non parla mai durante la storia, anche se la produzione aveva pensato di farle prestare la voce dalla giovane Hamano Minako per doppiarla nell'istante in cui il giocatore perde e Samus muore, ma l'intonazione di Minako appariva troppo sessuale, per cui non la utilizzarono.
Sakamoto osservò che Super Metroid fosse molto più profondo e più coinvolgente dei predecessori, che si distingueva dagli altri giochi d'azione e che fosse stato concepito per avere una connotazione decisamente più drammatica. Scopo principale degli sviluppatori era quello di renderlo un "buon gioco d'azione". Inizialmente prevedeva una capacità di 12 megabit, ma essendo stato sviluppato con una grafica a 16 bit e un sonoro d'alta qualità si estese a 24 megabit; sembra che sia stato il primo gioco in assoluto a disporre d'una cartuccia di tale dimensione. Richiese quindi un intenso lavoro e parecchio tempo per essere ultimato, nonostante i continui assilli da parte del produttore esecutivo Gunpei Yokoi che incitava ad accorciare i tempi. Gli ultimi sei mesi di lavoro furono i più faticosi e impegnativi: spesso la troupe dormì direttamente nello studio, e pare che si fosse creata pertanto un'aria statica che attirò l'attenzione degli altri uffici. In varie interviste Sakamoto raccontò che un dipendente li andò a svegliare una mattina dicendo che la camera puzzava come uno zoo.Prima del completamento Sakamoto portò una beta del gioco alla moglie e ai figli in modo da ottenere alcuni pareri esterni sulla qualità del prodotto, violando decisamente l'NDA di Nintendo. Lo sviluppo effettivo del gioco si concluse nel febbraio del 1994, ma prima di essere distribuito doveva essere provato e testato, oltre a essere perfezionato qualora ce ne fosse stato bisogno. Durante il controllo i membri del personale dovettero correggere lievi bug e revisionare il livello di difficoltà. In seguito dei liberi professionisti di tutte le età testarono il gioco per altre quattro settimane e furono invitati dai programmatori a esporre le loro opinioni riguardo alla difficoltà, alla qualità grafica e alla comodità dei comandi.
Dal momento che negli anni novanta era in continua crescita l'eccessiva violenza nei videogiochi come Mortal Kombat, la Entertainment Software Rating Board, che aveva e ha il compito di classificare i videogiochi in base al loro contenuto, volle informazioni al riguardo, in particolare se il gioco avrebbe potuto rappresentare un contraccolpo negativo per la saga Metroid. A tal proposito Sakamoto dichiarò: «Non pensiamo che ci sia troppa violenza nel gioco». Utilizzando Samus come esempio spiegò inoltre che il suo scopo era quello di mantenere la pace nella galassia e che perciò la violenza è in un certo senso giustificata. Poco prima del lancio Sakamoto espresse che c'era una possibilità di realizzare un nuovo capitolo della serie per la nuova console Nintendo, il Nintendo 64, ma precisò anche che si trattava di un'ipotesi e che non era una cosa certa. Super Metroid fu dunque distribuito in Giappone il 19 marzo 1994, in Nord America il 18 aprile 1994 e in Europa e Australia il 28 luglio 1994, quasi un decennio dopo il primo capitolo di Metroid.
La colonna sonora composta da Kenji Yamamoto ed Hamano venne pubblicata in una raccolta chiamata Sound in Action, nella quale erano incluse anche le musiche del primo Metroid, con trentotto tracce totali dalla durata complessiva di 59:13 minuti. Fu distribuita dalla Sony Records e diffusa in Giappone il 22 giugno 1994.
Super Metroid fu accolto con un ampio apprezzamento di critica, ricevendo un punteggio del 95.50% da GameRankings, diventando così uno dei giochi più quotati del sito. Tuttavia in Giappone non ebbe il successo sperato: pur essendo stato molto atteso dal pubblico, fu commercializzato proprio nel periodo in cui spopolavano o erano in procinto di essere distribuiti i videogiochi più ambiti e acclamati, come Donkey Kong Country; inoltre era l'anno dell'imminente lancio di nuove console, quali PlayStation e Sega Saturn; per questi motivi il videogioco registrò basse vendite nel Paese.
Grazie al commercializzazione da parte di Nintendo andò comunque meglio in Nord America e in Europa e visto l'apprezzamento della critica un anno dopo la sua uscita Nintendo lo mise sul "Nintendo Selects", un marchio utilizzato dall'azienda per promuovere videogiochi dalle alte vendite. Tuttavia visto che che nessuno dei tre capitoli della serie avrebbe raggiunto il livello di notorietà di videogiochi come Mario e The Legend of Zelda, non furono realizzati altri Metroid per otto anni, finché non uscirono nel 2002 Metroid Prime (per Nintendo GameCube) e Metroid Fusion (per Game Boy Advance).Chris Slate, della rivista di videogiochi Game Players, apprezzò il gioco sostenendo di essere soddisfatto del lavoro svolto dalla produzione, sia per quanto concerne la grafica e l'esperienza di gioco, sia il comparto sonoro; trovò interessante e necessaria anche la funzione automatica della mappa, che evidenzia le aree esplorate e quindi i progressi fatti dal giocatore durante il gioco, sottolineando che era l'unica caratteristica in Super Metroid che l'originale Metroid avrebbe dovuto avere. Affermò inoltre che ogni fan dei giochi d'azione non avrebbe dovuto lasciarsi sfuggire il titolo, che li avrebbe convinti a giocarci e rigiocarci più volte. La rivista Nintendo Power invece dichiarò: «Questo potrebbe essere il miglior gioco d'azione e avventura di sempre», definendolo "l'onda del futuro" ed elogiando il comparto grafico, l'audio e i controlli. Electronic Gaming Monthly lo definì "incredibile", gradì moltissimo soprattutto il lato grafico e la trama profondamente drammatica, complimentandosi per i controlli "nitidi e chiari" e per le molte armi a disposizione, lodò la longevità e osservò che il gioco rendeva certamente giustizia a Metroid e che probabilmente non avrebbe deluso i videogiocatori.Fu apprezzato anche dalla redazione britannica di Super Play: uno dei collaboratori, Zy Nicholson, dichiarò che Super Metroid era migliore del suo videogioco preferito, Mega Man X, descrivendolo come "molto più di un'esperienza di gioco" e lo confrontò con il film del 1986 Aliens di James Cameron, ritenendo che la già notevole esperienza di gioco sarebbe aumentata se fosse stato giocato al buio e con il volume alzato, affermando inoltre che lui stesso fu tentato dal giocarci di seguito addirittura senza mangiare e dormire. Un altro, Tony Mott, nella sua recensione dichiarò che il punto forte del gioco era la sua atmosfera e la definì un miscuglio di Turrican (1990), Aliens, Exile (1989) e Nodes of Yesod (1985); apprezzò poi i controlli e l'esperienza di gioco raffinata, applaudendo il buon lavoro svolto da Nintendo; concluse affermando: «È senza dubbio il miglior gioco di quest'anno che ho giocato finora». Un altro revisore, James Leach, fu concorde con Nicholson e Mott e aggiunse che Super Metroid era quello che Mega Man X avrebbe dovuto essere, esprimendo che conteneva tutto ciò che cercava in un videogioco: giocabilità, trucchi e zone nascoste, armi potenti e nemici efficacemente realizzati. Dopo aver sommato le recensioni il verdetto della rivista inglese fu unanime nell'apprezzare Super Metroid, che lo descrisse come "un gioco meraviglioso che tutti dovrebbero possedere". In seguito Andy Robinson di GamesRadar ha commentato la colonna sonora, definendola "fenomenale" e una delle migliori del mondo videoludico.
Super Metroid ricevette molti riconoscimenti e viene ancora oggi citato come uno dei più grandi videogiochi di sempre. La rivista Electronic Gaming Monthly lo nominò il miglior videogioco del mese di maggio 1994, premiandolo poi come il più bel gioco d'azione dell'anno; nel 1997 lo mise al sesto posto nella classifica dei cento migliori giochi di sempre e successivamente nel 2002 lo considerò il miglior videogioco di tutti i tempi. Nel 1997 fu posizionato al sesto posto anche da Nintendo Power e poi nel 2006 al dodicesimo fra i duecento migliori giochi di sempre; l'ha anche considerato come il miglior capitolo della serie Metroid, seguito da Metroid Prime (2º posto) e Metroid: Zero Mission (3º posto).La rivista Game Informer nel 2001 lo collocò al ventinovesimo posto nella lista dei cento più grandi giochi di sempre, mentre nel 2009 lo inserì al ventunesimo in quella dei duecento. Sul sito Imagine Games Network (IGN) nella classifica annuale dei cento giochi più belli di sempre Super Metroid si guadagnò nel 2003 il terzo posto, scese al decimo nel 2005, al quarto nel 2006, e al settimo nel 2007. GamePro l'ha poi elencato
al settimo posto tra i trenta più grandi giochi in grafica 16 bit, e tra i quindici «retro games» per la Wii che meritano di essere giocati.Nel 2011 è stato posizionato al nono posto da GamesRadar nell'elenco dei cento migliori giochi di sempre; in seguito è salito al terzo e per di più al primo tra i più grandi videogiochi per Super Nintendo di tutti i tempi, Chrono Trigger al secondo posto e The Legend of Zelda: A Link to the Past al terzo. In cima alla lista anche nell'elenco dei venti migliori giochi per Super Nintendo secondo il sito ScrewAttack, dove è seguito da The Legend of Zelda: A Link to the Past (2º posto) e Final Fantasy VI (3º posto). Inoltre è stato considerato il miglior gioco per Super Nintendo anche dal Classic Game Room, sorpassando The Legend of Zelda: A Link to the Past (2º posto) e Super Mario World (3º posto). Tra i migliori giochi di sempre anche secondo la rivista Empire.
Super Metroid ebbe un effetto notevole e duraturo nel settore dei videogiochi, già a partire dal 1997 proprio con il gioco Castlevania: Symphony of the Night, che trasse alcune spunti, in particolare il fatto di rendere alcune aree inaccessibili senza l'ausilio di speciali poteri. Negli anni è risultato essere fonte d'ispirazione per numerosi videogiochi, come Cave Story (2004), Portal, BioShock (2007), Shadow Complex (2009), Outland, Insanely Twisted Shadow Planet (2011) e molti altri. Col passare del tempo è diventato uno dei titoli più contesi e uno dei primi a essere giocati e completati in speedrun, un fenomeno che consiste nel completamento del gioco nel minor tempo possibile, trascurando i potenziamenti non necessari, cercando la via più veloce, avvantaggiandosi sfruttando alcuni errori di gioco e raccogliendo solo le armi e le taniche di energia indispensabili; a questo proposito il gioco risulta essere terminabile dai giocatori più agguerriti in quarantacinque minuti circa. Un'altra sfida consiste invece nello scovare e raccogliere tutti i potenziamenti, le armi, i proiettili e i serbatoi d'energia, completando il gioco al 100% e sempre il prima possibile.
Nel 2007 Nintendo ha reso il titolo disponibile per il download sul canale Virtual Console del Wii. Il sito IGN ha commentato che, nonostante sia stato distribuito nove mesi dopo il lancio ufficiale della Wii, ne è valsa la pena di aspettare. Per i giocatori che non ci hanno mai giocato IGN afferma che "finalmente possono scoprire cosa si sono persi in tutti questi anni". Lo stesso ha dichiarato Frank Povo su GameSpot, che ha trovato assolutamente sorprendente il lungo lasso di tempo trascorso tra la prima pubblicazione del gioco e quella successiva per la Wii, ma ha anche sottolineato che la cosa più importante è che "i giocatori possono giocare a questo capolavoro senza dover recuperare la cartuccia Super Nintendo Entertainment System originale o armeggiare con emulatori giuridicamente discutibili". Ha espresso comunque un parere non proprio positivo sulla versione emulata di Virtual Console, ma nonostante tutto ha ribadito la sua convinzione che Super Metroid è "uno dei migliori giochi di avventura dinamica 2D mai prodotto".
Successivamente nel maggio del 2013 è stato distribuito per un periodo limitato sulla console Wii U, e sembra che molti giovani videogiocatori abbiano riscontrato grosse difficoltà nel cimentarsi in un prodotto nettamente più impegnativo della maggior parte dei titoli attuali. Sono state pubblicate sul web anche diverse mod, tra le quali: modifiche della velocità nelle animazioni dei personaggi, cambiamenti dei toni e dei colori dell'ambiente e nuove caratterizzazioni totali di Samus (disponibile senza la tuta) e del pianeta Zebes.
Super Metroid (JPG), in Retro Gamer, nº 1, supplemento a Guide digitali n° 12, Roma, Play Press Publishing, 2006,  pp. 82-85, ISSN 1724-4722.
 Matteo Bittanti, Intermedialità: videogiochi, cinema, televisione, fumetti, Unicopli, 2008, ISBN 978-88-400-1270-4.
(EN) Nintendo, Super Metroid instruction booklet, Nintendo of America, Inc, 1994.

In astronomia si definisce super Terra un pianeta extrasolare di tipo roccioso che abbia una massa compresa tra 1,9–5 e 10 masse terrestri (M⊕); questa classe di pianeti è dunque una via di mezzo tra i giganti gassosi di massa simile ad Urano e Nettuno ed i pianeti rocciosi di dimensioni simili alla Terra. Il nostro sistema solare non contiene pianeti classificabili in questa categoria, in quanto il pianeta roccioso più grande è proprio la Terra e il pianeta immediatamente di massa maggiore, Urano, è un gigante gassoso con una massa pari a circa 14 volte quella terrestre.
Il termine "super Terra" si riferisce esclusivamente alla massa del pianeta, e non considera altre proprietà quali condizioni in superficie o eventuale abitabilità. Per evitare potenziali ambiguità sono stati coniati anche altri termini, di utilizzo meno diffuso, per enfatizzare alcune probabili caratteristiche di certe super Terre individuate: nano gassoso, per i pianeti più massicci di questa categoria e probabilmente costituiti da grandi quantità di gas; super Venere o super Plutone, per sottolineare le altissime o viceversa bassissime temperature superficiali che caratterizzerebbero il pianeta in oggetto.
I primi pianeti appartenenti a questa categoria furono scoperti nel 1992 attorno ad una pulsar; fu però a partire dal 2005 che si iniziarono ad individuare delle super Terre attorno a stelle di sequenza principale, con la scoperta di Gliese 876 d.
In generale, la definizione di super Terra si basa esclusivamente sulla massa, e non comprende altre caratteristiche, come temperatura, composizione, parametri orbitali o ambiente, affini a quelli della Terra. Mentre le fonti generalmente sono concordi nell'indicare 10 masse terrestri (~69% della massa di Urano) come il limite superiore perché una super Terra possa ancora considerarsi tale, il limite inferiore varia tra 1–1,9 e 5 M⊕. Secondo altri autori il termine andrebbe limitato ai pianeti privi di un involucro atmosferico significativo. I pianeti che eccedono le 10 M⊕ entrano nel novero dei giganti gassosi. Al 2019, la carenza di pianeti cosiddetti intermedi (Fulton gap, dal nome dell'astronomo che ha rilevato il fenomeno), cioè aventi raggio tra 1,5 e due volte quello terrestre, trova spiegazione, oltre ad una insufficiente base statistica, alla possibilità di ulteriori scenari nell'evoluzione della formazione esoplanetaria.
La scoperta delle prime super Terre coincide con la scoperta dei primi pianeti extrasolari: nel 1992 Aleksander Wolszczan e Dale Frail scoprirono tre pianeti attorno alla pulsar millisecondo PSR B1257+12 la cui massa era compresa tra 0,025 e 4,3 volte la massa terrestre: valori troppo bassi per considerarli dei giganti gassosi. Dal momento che sino ad allora l'esistenza degli esopianeti era solamente oggetto di discussione e speculazione, la scoperta suscitò un grande interesse nella comunità scientifica, dal momento che si trattava dei primi esopianeti confermati e per di più orbitavano attorno ad una pulsar, fatto sorprendente per l'epoca in quanto si supponeva che solo le stelle di sequenza principale potessero avere dei pianeti.Bisognerà però attendere sino al 2005 prima che venga individuata la prima super Terra intorno ad una stella di sequenza principale: si trattava di Gliese 876 d, scoperta da un gruppo guidato dal ricercatore Eugenio Rivera in orbita attorno alla nana rossa Gliese 876 (in precedenza erano stati scoperti nel sistema due giganti gassosi di dimensioni simili a Giove). Ha una massa stimata tra 5,8 e 7,5 masse terrestri ed un periodo orbitale di soli due giorni; la grande vicinanza del pianeta alla sua stella madre fa sì che la sua temperatura superficiale sia piuttosto elevata, tra 430 e 650 kelvin.
Nel 2006 sono state scoperte altre due super Terre: OGLE-2005-BLG-390Lb, con una massa 5,5 volte quella terrestre, scoperta grazie all'effetto lente gravitazionale, e HD 69830 b, con una massa pari a 10 volte quella terrestre.
Nell'aprile 2007 un gruppo svizzero, guidato da Stéphane Udry, ha annunciato la scoperta di due super Terre attorno alla nana rossa Gliese 581, denominate Gliese 581 c e d ed entrambe ritenute al limite della zona abitabile del sistema. Al momento della scoperta si riteneva che Gliese 581 c, che possiede una massa pari a 5 volte quella terrestre e dista mediamente dalla stella madre 0,073 unità astronomiche (UA) (11 milioni di km), si trovasse nel bordo più interno e caldo della zona abitabile. Pertanto si ritenne inizialmente che la temperatura del pianeta variasse tra un minimo di −3 °C (270 K), con un'albedo paragonabile a Venere, ed un massimo di 40 °C (313 K), con un'albedo paragonabile a quella terrestre. Ricerche successive hanno però mostrato che Gliese 581 c si troverebbe ben più internamente rispetto alla zona abitabile del sistema e soffrirebbe inoltre di un importante effetto serra, simile a quello che affligge Venere. Gliese 581 d, con una massa 7,7 volte quella terrestre, orbita invece all'interno della zona abitabile, in corrispondenza del suo limite esterno.Nel giugno 2008 è stata scoperta una delle super Terre di massa più piccola, MOA-2007-BLG-192Lb; individuato grazie all'effetto lente gravitazionale, il pianeta possiede una massa di circa 3,3 M⊕ ed orbita attorno ad una nana bruna.Nello stesso mese è stata annunciata la scoperta di tre super Terre orbitanti attorno ad una stella un po' più piccola del Sole, HD 40307. Il primo pianeta ha una massa pari a 4,2 masse terrestri, il secondo 6,7 e il terzo 9,4. I tre pianeti sono stati individuati grazie al metodo della velocità radiale calcolata mediante lo spettrografo HARPS situato in Cile, presso l'osservatorio di La Silla. Lo stesso team ha annunciato la scoperta di un ulteriore pianeta con 7,5 masse terrestri intorno alla stella HD 181433, attorno alla quale orbita un pianeta simile a Giove già noto con un periodo di tre anni.
Nel febbraio 2009 è stata annunciata la scoperta di CoRoT-7 b, con una massa stimata di 4,8 M⊕ ed un periodo orbitale di appena 0,853 giorni; la densità stimata sembra indicare una composizione molto simile a quella dei pianeti del sistema solare interno, dunque con prevalenza di silicati. CoRoT-7 b, scoperto appena dopo HD 7924 b, è la prima super Terra individuata attorno ad una stella di sequenza principale che non sia una nana rossa.Il 21 aprile 2009 è stata annunciata la scoperta di un'altra super Terra intorno a Gliese 581: Gliese 581 e. Con una massa di circa 1,9 M⊕, è il più piccolo esopianeta sinora individuato attorno ad una stella di sequenza principale; orbita intorno alla sua stella in 3,15 giorni ad una distanza media di 0,03 UA. Si ritiene che il pianeta sperimenti un riscaldamento mareale almeno 100 volte superiore a quello che il satellite Io subisce da Giove.Nel dicembre 2009 è stata annunciata la scoperta di GJ 1214 b, 2,7 volte più massiccio della Terra, la cui densità è compatibile con quella ipotizzata per un pianeta oceano.Delle 32 super Terre scoperte nel 2009, 24 sono state scoperte tramite lo strumento HARPS montato sui telescopi Keck.Nel gennaio 2010 è stato individuato il pianeta HD 156668 b; la sua massa minima di 4,15 masse terrestri lo rende il secondo pianeta meno massiccio ad esser stato scoperto mediante il metodo della velocità radiale, dopo Gliese 581 e.
Il 24 agosto è stata annunciata la scoperta di un sistema planetario costituito da almeno sette pianeti, non tutti confermati, in orbita attorno alla nana gialla HD 10180; uno dei pianeti non confermati, HD 10180 b, possiederebbe una massa di 1,35 ± 0,23 masse terrestri, che lo renderebbe, qualora venisse confermato, l'esopianeta meno massiccio scoperto attorno ad una stella di sequenza principale; vi è però una probabilità del 98,6% che il pianeta sia realmente esistente.
Il 29 settembre è stata annunciata la scoperta, tramite la misurazione della velocità radiale, di una quarta super Terra attorno a Gliese 581; denominato Gliese 581 g o, amichevolmente, Zarmina, il pianeta presenta una massa 3,1 volte quella terrestre ed orbita secondo una traiettoria quasi circolare ad una distanza media dalla stella di 0,146 UA, che lo colloca nella zona abitabile. La scoperta del pianeta, assieme a quella contemporanea di Gliese 581 f, è stata messa in dubbio da una successiva analisi dei dati, dalla quale non si è avuto un riscontro preciso dell'effettiva presenza di questi ultimi due pianeti; l'Extrasolar Planet Encyclopedia li classifica, a dicembre 2011, come non confermati.
Il 2 febbraio 2011 il telescopio spaziale Kepler ha trasmesso una lista di 1235 probabili pianeti extrasolari, che comprende 68 possibili pianeti di dimensioni simili alla Terra (R < 1,25 R⊕) e altre 288 possibili super Terre (1,25 R⊕ < R < 2 R⊕). Inoltre, 54 probabili pianeti sono stati individuati nella zona abitabile del loro sistema; sei di questi hanno dimensioni inferiori al doppio di quelle terrestri: KOI 326.01 (R = 0,85 R⊕), KOI 701.03 (R = 1,73 R⊕), KOI 268.01 (R = 1,75 R⊕), KOI 1026.01 (R = 1,77 R⊕), KOI 854.01 (R = 1,91 R⊕), KOI 70.03 (R = 1,96 R⊕). Degno di nota è anche un sistema costituito da ben sei pianeti, denominati da "b" a "g", in orbita attorno a Kepler-11, una nana gialla molto simile al Sole. Tutti e sei i pianeti, le cui masse sono comprese tra 2,3 e 13,5 M⊕, sono transitanti sulla superficie della stella, in virtù della loro inclinazione rispetto alla nostra linea di vista inferiore al grado. Questa proprietà ha reso possibile una misurazione diretta dei diametri e dei periodi orbitali semplicemente monitorando le eclissi della stella da parte dei pianeti. Il sistema è il più compatto conosciuto: le orbite dei pianeti da "b" ad "f" infatti giacciono tutte ad una distanza inferiore a quella che separa Mercurio dal Sole, mentre l'orbita di "g" è più larga del 20% rispetto all'orbita di Mercurio.
Sulla base di queste ultime scoperte, gli astronomi ipotizzano che possano trovarsi almeno 30.000 probabili pianeti abitabili entro mille anni luce dalla Terra, almeno 50 miliardi di pianeti di tipo roccioso solamente nella Via Lattea, 500 milioni dei quali probabilmente orbitanti nella zona abitabile del loro sistema.
La scoperta di altre quattro super Terre (Gliese 370 b e le tre orbitanti attorno al sistema di HD 20794) tramite lo spettrografo HARPS dell'ESO è stata annunciata il 17 agosto 2011; tra queste, Gliese 370 b giacerebbe al limite interno della zona abitabile del sistema e sarebbe potenzialmente abitabile qualora possedesse una copertura nuvolosa in grado di coprire più del 50% della superficie planetaria. Altre 10 super Terre, su 41 nuovi esopianeti scoperti, sono state confermate il 12 settembre.Il 5 dicembre 2011 è stata annunciata e confermata la scoperta, tramite il telescopio Kepler, della prima super Terra inequivocabilmente orbitante nella zona abitabile del suo sistema planetario: si tratta di Kepler-22 b, un pianeta di raggio 2,4 volte quello terrestre, che orbita ad una distanza dalla sua stella (una nana gialla lievemente più piccola del Sole) di circa 0,89 UA.Nel settembre 2012 è stata annunciata la scoperta di due pianeti in orbita attorno a Gliese 163, dei quali uno, Gliese 163 c, con una massa pari a 6,9 volte la massa della Terra e probabilmente orbitante nella zona abitabile del sistema. Nell'ottobre dello stesso anno è stato dato l'annuncio della probabile scoperta di una super terra anche attorno ad α Centauri B, facente parte del sistema stellare più vicino al Sole, mentre a dicembre sono state annunciate cinque super Terre in orbita attorno alla vicina τ Ceti, uno dei quali, e, sarebbe all'interno della zona abitabile.Nel gennaio 2013 è stata poi annunciata la scoperta, in seguito all'analisi dei dati forniti dal telescopio spaziale Kepler, di un possibile pianeta, denominato KOI-172.02, molto simile alla Terra (R=1,5 R⊕) che orbita nella zona abitabile del sistema di una nana gialla simile al Sole; questo pianeta è ritenuto un possibile candidato ad ospitare forme di vita extraterrestri. Nell'aprile dello stesso anno è stata annunciata la scoperta di cinque pianeti orbitanti all'interno della zona abitabile della stella Kepler-62, distante 1 200 anni luce dal sistema solare. Altre tre super Terre sono state individuate attorno alla nana rossa Gliese 667 C e sono parte di un sistema più ampio comprendente altri quattro pianeti.
A causa della loro massa maggiore rispetto a quella terrestre, le caratteristiche fisiche delle super Terre differiscono in maniera sostanziale da quelle del nostro pianeta.
Caratteristica principale delle super Terre è l'alto valore della gravità superficiale, in genere maggiore di quella di Nettuno e Saturno (e in certi casi anche di quella di Giove), che dipende strettamente dal valore della massa e dalle dimensioni di questi pianeti. Un gruppo di astronomi ha sviluppato dei modelli fisico-matematici per dedurre le dimensioni di quattordici diverse tipologie di pianeti che si ritiene possano esistere nella nostra Galassia; tra questi, pianeti composti da sostanze pure, quali acqua e/o ghiaccio (pianeti oceano), carbonio, ferro, silicati, monossido di carbonio, carburo di silicio, e da miscele di queste sostanze. Il gruppo ha calcolato in che modo la gravità arrivi a comprimere questi pianeti, consentendo di predire un preciso valore del diametro a seconda della composizione e della massa presa in esame. Ad esempio, un pianeta di massa terrestre composto da acqua e/o ghiaccio avrebbe un diametro di circa 15.700 km, mentre un pianeta ferroso di egual massa avrebbe un diametro di appena 4800 km; per raffronto, la Terra, costituita prevalentemente da silicati con un nucleo ferroso, ha un diametro equatoriale di 12.756 km. Se ne deduce dunque che i pianeti a prevalenza d'acqua e ghiaccio siano i meno densi, mentre i pianeti ferrosi siano quelli con la densità maggiore; bisogna comunque tenere presente che, a parità di composizione, un pianeta massiccio è più denso di un pianeta meno massiccio.Uno studio condotto sul pianeta Gliese 876 d ha reso noto che sarebbe teoricamente possibile dedurre la composizione di una super Terra calcolando la densità a partire dal raggio, misurabile durante il transito sulla superficie della stella, e dalla massa del pianeta, deducibile tramite le misurazioni astrometriche. Nel caso specifico, dal momento che Gliese 876 d non è un pianeta transitante e dato che l'unico valore noto è la sua massa (5,88 ± 0,99 M⊕), il suo raggio teorico calcolato è compreso tra 9.200 km (1,4 raggi terrestri), assumendo che si tratti di un pianeta di silicati con un grande nucleo ferroso, e 12.500 km (2,0 raggi terrestri), assumendolo un pianeta oceano. La gravità superficiale stimata per un pianeta il cui raggio è compreso entro questo range andrebbe tra 1,9 e 3,3 g (19 e 32 m/s²).
La struttura di una super Terra riflette le modalità che hanno condotto alla sua formazione. A seconda della regione del sistema planetario in cui il pianeta si è formato, è possibile riconoscere due tipi principali di super Terre: le super Terre ricche in acqua e ghiaccio, che si sono formate al di là della frost line del sistema e che daranno luogo ai pianeti oceano, e le super Terre povere d'acqua, simili grossomodo ai pianeti del sistema solare interno e formatesi internamente alla frost line.
La formazione di una super Terra povera d'acqua ricalca sostanzialmente la formazione dei pianeti rocciosi del sistema solare. L'urto e l'aggregazione dei planetesimi, frammenti di roccia ricchi di ferro e silicati presenti nel disco circumstellare residuato dalla nascita della stella madre, determina la formazione di un certo numero di protopianeti, che, in virtù dell'enorme attrito causato dalle plurime collisioni, si presentano come delle caldissime sfere di roccia fusa che irradiano calore nello spazio circostante. Il raffreddamento della parte più superficiale del magma determina la formazione di strutture cristalline di silicati di ferro, da cui avranno origine dei minerali. A seconda della quantità di ossigeno dei silicati, una parte del ferro può non venire incorporata nei minerali nascenti; questa frazione libera di ferro, a causa della sua densità maggiore rispetto al resto del magma silicatico, sprofonda verso il centro del pianeta nascente, andando a costituire un nucleo circondato da un mantello di magma prevalentemente silicatico; l'interno del pianeta in formazione assume così un aspetto pluristratificato, simile a quello della Terra. Ciò che differenzia il nucleo di una super Terra da quello terrestre è il fatto che il primo, nonostante le elevatissime temperature (~10.000 K), si presenterebbe completamente solido a causa delle elevate pressioni che gravano su di esso; il nucleo terrestre è composto invece da una frazione solida, detta nucleo interno, circondata da un involucro fluido, il nucleo esterno, percorso da correnti convettive che sarebbero responsabili del campo geomagnetico.
Tra le super Terre povere d'acqua sono annoverati gli ipotetici pianeti di carbonio, che orbiterebbero attorno a stelle originatesi a partire da nebulose particolarmente ricche di questo elemento e povere in ossigeno. La loro struttura interna comprende un nucleo ferroso, circondato da un mantello interno di carburi e un mantello esterno di grafite, sovrastato a sua volta da una sottile crosta e, in alcuni casi, da un'atmosfera secondaria, ricca di composti del carbonio. Si ritiene che se nel mantello esterno si raggiungessero opportune condizioni di pressione, alcuni strati di grafite, dello spessore anche di diversi chilometri, potrebbero cristallizzare in diamante.Notevolmente differente è la formazione dei pianeti ricchi d'acqua, rappresentati dai pianeti oceano: come già accennato, questi pianeti si formano al di là della frost line, che corrisponde ad una distanza dalla stella tale che la temperatura è sufficientemente bassa da permettere ai composti volatili contenenti idrogeno, come l'acqua, l'ammoniaca e il metano, di raggiungere lo stato di ghiaccio. La loro struttura è assai peculiare: questi pianeti sono infatti caratterizzati da grandissime quantità d'acqua, che danno luogo ad un oceano superficiale profondo diverse centinaia di chilometri. Negli strati inferiori di questo immenso oceano l'acqua, per effetto della grandissima pressione, raggiunge lo stato solido: si crea così un secondo mantello, più superficiale rispetto a quello di roccia, costituito da ghiaccio. Non si tratta però del comune ghiaccio visibile nelle regioni fredde del nostro pianeta, il ghiaccio Ih, ma delle calde forme cristalline note come ghiaccio VII, X e XI, che si formano a seguito di pressioni elevatissime.
Alcuni modelli teorici indicano che alcune super Terre possono manifestare un'attività geologica affine a quella del nostro pianeta, caratterizzata forse da una tettonica a placche.L'attività geologica della Terra è alimentata dai moti convettivi che il magma del mantello compie in virtù del calore endogeno, che è in parte un residuo del processo di formazione planetaria e in parte è dovuto al decadimento degli elementi radioattivi presenti nel mantello. Supponendo che possieda una concentrazione di tali elementi affine a quella del nostro pianeta, dal momento che questi hanno una diffusione uniforme nella Galassia, è ragionevole pensare che una super Terra, in virtù della sua grande massa, abbia una quantità maggiore di elementi radioattivi e dunque sviluppi un maggior calore endogeno, che alimenterebbe dunque nel mantello dei moti convettivi più energici. La conseguenza sarebbe una tettonica a placche più violenta di quella terrestre, caratterizzata dalla presenza di zolle più sottili rispetto a quelle terrestri a causa di un più rapido turn-over della crosta planetaria, che si riflette in un minor tempo a sua disposizione per raffreddarsi ed ispessirsi. Nonostante il ridotto spessore della crosta, le faglie dovrebbero presentare una resistenza analoga a quella terrestre a causa della maggiore forza di gravità che esercita una maggiore pressione su di esse.I modelli suggeriscono che sorprendentemente la massa della Terra sia appena superiore al limite necessario per poter avere una tettonica attiva; questo spiega come mai Venere, che è appena meno massiccio della Terra, possieda una tettonica appena accennata, mentre Marte, con una massa circa un decimo di quella terrestre, si presenta geologicamente inattivo.
L'attività geologica, e in particolare il vulcanismo, immette nell'atmosfera del nostro pianeta grandi quantità di gas, come l'anidride carbonica, che reagisce con il silicato di calcio delle rocce producendo carbonato di calcio e silice, solidi insolubili che sedimentano nei fondali oceanici. Il processo di subduzione della sottile crosta oceanica trasporta questi sedimenti nel mantello; la subduzione rifornisce così il mantello di carbonio, il quale, riconvertito in anidride carbonica, torna nell'atmosfera consentendo a questo ciclo di reazioni di riprendere. L'importanza di questo ciclo carbonio-silicio consiste nel fatto che il tutto agisce come un termostato che mantiene stabile la temperatura terrestre, contribuendo a mantenere l'acqua allo stato liquido e dunque rendendo il pianeta adatto a sviluppare la vita come la conosciamo. L'ipotetica maggiore efficienza della tettonica di una super Terra accelererebbe i tempi di questo ciclo, rendendo per certi versi questi pianeti più adatti allo sviluppo di forme di vita.La grande massa consente inoltre alla super Terra di trattenere un'atmosfera sufficientemente spessa in maniera più efficiente ed impedisce alle molecole d'acqua di sfuggire nello spazio. Tuttavia non possediamo informazioni precise sulle atmosfere delle super Terre e non si conoscono con esattezza le temperature superficiali di questi pianeti né l'eventuale presenza di un effetto serra, anche se è possibile stimare una temperatura di equilibrio in relazione dal grado di insolazione ricevuta dal pianeta e dall'albedo del pianeta. Ad esempio, per la Terra questa temperatura è di 254,3 K (−19 °C), ben al di sotto della temperatura media del pianeta; è la presenza di importanti quantità di gas serra e del suddetto ciclo carbonio-silicio a far sì che la Terra mantenga una temperatura media tale da mantenere l'acqua allo stato liquido. Allo stesso modo, Venere ha una temperatura di equilibrio di 231,7 K  (−41 °C), nonostante l'imponente effetto serra che affligge l'atmosfera venusiana faccia sì che il pianeta abbia una temperatura reale di 737 K (464 °C).

La Supercoppa italiana 2004 di calcio femminile si è disputata venerdì 4 settembre 2004 allo Stadio Mariotti di Alghero. La sfida avrebbe dovuto vedere in un primo momento contrapposte il Foroni Verona, vincitore della Serie A 2003-2004, contro la Lazio, vincitrice della Coppa Italia 2002-2003.
Tuttavia, poche settimane prima della gara, la società veronese rinunciò all'iscrizione al campionato per problemi economici, rinunciando anche a ripartire da una serie inferiore e dichiarando il fallimento. Lo scioglimento della squadra (e lo svincolo delle sue atlete), comportò diversi problemi nello scegliere quale squadra ammettere nella finalissima di Supercoppa. Il Foroni Verona, infatti, era stato anche finalista della Coppa Italia, mentre la Torres, già ammessa di diritto, era stata la seconda classificata in campionato dietro le biancoverdi. Fu così deciso che a disputare l'insolita edizione della Supercoppa, fosse il Milan, terzo nel campionato precedente. Da regolamento, inoltre, proprio alle rossonere è toccato essere indicata come squadra "ospitante" (come avrebbe dovuto essere il disciolto Foroni Verona). Ad oggi, il Milan è l'unica squadra ad aver disputato la Supercoppa senza avere né vinto lo Scudetto, né essere arrivata in finale di Coppa Italia l'anno prima.
Sul campo, la gara è stata a senso unico: le torresine hanno vinto per 5-0, forti anche della presenza nelle loro file di diverse giocatrici provenienti proprio dal disciolto Foroni Verona: tra queste Chiara Gazzoli, autrice di una doppietta (nell'edizione precedente aveva marcato una tripletta proprio con le scaligere). Le altre tre reti per le sarde sono state di Rita Guarino, Gioia Masia e della danese Merete Pedersen.
Per la seconda edizione consecutiva, la vincitrice della Supercoppa impone uno scarto di cinque reti all'avversaria. Per la prima volta, invece, la Supercoppa si disputa nella regione di appartenenza di una delle due squadre: la Sardegna.

La superficie di Marte apparve sin dalle prime esplorazioni mediante sonde automatiche come complessa e geologicamente diversificata. Mentre l'emisfero sud è ricoperto da vasti crateri e si eleva al di sopra del livello topografico di riferimento, l'emisfero nord è scarsamente craterizzato e situato generalmente al di sotto di tale livello (che corrisponde per convenzione alla quota ove la pressione atmosferica vale 6,1 hPa).
L'emisfero sud si compone principalmente di una crosta rocciosa molto antica, attraversata da numerosi canali naturali lunghi centinaia di km e larghi qualche decina; localmente si possono individuare pianure di origine più recente.
L'emisfero nord, al contrario, presenta formazioni vulcaniche di notevoli dimensioni, con flussi di lava solidificati, numerose scarpate e canyon, ed un paesaggio generalmente diversificato e vario. La regione principale dell'emisfero, sede dei grandi vulcani, è Tharsis, un vasto rigonfiamento che si eleva fino a 10 km sul terreno circostante e che dà origine alla maggior parte delle fratture superficiali visibili.
Lo strato superficiale del pianeta è composto principalmente da silicati ed ossido di ferro, oltre ad altri composti chimicamente reattivi che possono rapidamente danneggiare eventuali materiali organici.
La presenza di ghiaccio d'acqua su Marte è largamente testimoniata nei sedimenti delle regioni polari, sotto le calotte di anidride carbonica e sotto forma di permafrost, fino a 3 km di profondità. Le analisi delle sonde automatiche sul pianeta hanno confermato che per lunghi periodi il pianeta fu percorso da fiumi e che ampie distese furono sommerse, forse anche per un miliardo d'anni. Le analisi svolte dalla sonda Mars Express hanno rivelato che il ghiaccio presente al polo sud, se sciolto, potrebbe coprire la superficie del pianeta con nove metri d'acqua. Il ghiaccio presente al polo sud, comunque, non è sufficiente a spiegare le estese erosioni della superficie e quindi gli scienziati stanno ricercando altri depositi d'acqua o altri fenomeni che possano spiegarle.
La temperatura superficiale media è di circa 210 K (~-60 °C), e varia con la stagione e la latitudine da minime di 140 K (~-130 °C) nelle regioni polari, in inverno, a massime di 300 K (~20 °C) nelle regioni equatoriali, in estate.

La Superliga 2013-2014 è l'83ª edizione della massima serie del campionato rumeno maschile di pallanuoto. La regular season è iniziata il 17 settembre 2013 con la prima fase a girone unico per concludersi il 21 ottobre. La seconda fase a girone doppio è iniziata il 29 novembre con il primo dei tre tornei previsti, per concludersi il 26 gennaio 2014 con il terzo e ultimo torneo. I playoff hanno avuto inizio il 1º febbraio e si sono conclusi il 26 aprile con gara-3 della finale scudetto.
Le otto squadre vengono incluse in un girone unico all'italiana dove le squadre si affrontano in un doppio girone di andata e ritorno, per un totale di 14 giornate. Tutte le squadre vengono ammesse alla seconda fase. La fase ha avuto inizio il 17 settembre e si è conclusa il 21 ottobre 2013.
Tutti gli incontri del girone di andata si sono disputati nella Piscina Olimpica Ioan Alexandrescu di Oradea tra il 17 e il 22 settembre, mentre quelli del girone di ritorno hanno avuto luogo nella Piscina Steaua a Bucarest tra il 17 e il 21 ottobre.
Nella seconda fase le otto squadre vengono suddivise in due gironi da quattro ciascuno, in base alla classifica finale della prima fase: le prime quattro vengono inserite nel gruppo A, le restanti quattro nel gruppo B. Le squadre del gruppo A hanno assicurato il passaggio ai playoff scudetto, con le prime due che hanno accesso diretto alle semifinali, terza e quarta che partono da un turno preliminare a cui si aggiungono anche le prime due del gruppo B.La seconda fase viene disputata in tre tornei diversi, dove ciascuna squadra incontra le altre del proprio girone per tre volte.
Gli incontri conclusi dopo i tiri di rigore indicano il risultato di parità alla fine dei tempi supplementari e il risultato dei tiri di rigore in apice.
In caso di risoluzione ai tempi supplementari viene mostrato in apice il risultato di parità alla fine dei tempi regolamentari. In caso di risoluzione ai tiri di rigore viene mostrato in apice il risultato dei tiri di rigore.

Una supernova di tipo Ia è una tipologia di supernova originata dall'esplosione di una nana bianca. Una nana bianca è ciò che resta di una stella di massa medio-piccola che ha completato il suo ciclo vitale e al cui interno la fusione nucleare è cessata; tuttavia, le nane bianche al carbonio-ossigeno, le più comuni dell'Universo, sono in grado, se le loro temperature salgono a sufficienza, di far perdurare le reazioni di fusione, che rilasciano una gran quantità di energia.
Da un punto di vista fisico, le nane bianche a lenta rotazione possiedono una massa limite, definita limite di Chandrasekhar, che equivale a circa 1,44 masse solari (M☉). Questa è la massa più elevata che può essere supportata dalla pressione esercitata dagli elettroni degenerati; oltre questo limite le nane bianche tendono a collassare. Se una nana bianca aumenta gradualmente la propria massa accrescendola da una compagna in un sistema binario, si ritiene che, nel momento in cui si approssima al limite, il suo nucleo possa raggiungere la temperatura richiesta per la fusione del carbonio. Se la nana bianca si fonde poi con un'altra stella (un evento in realtà molto raro), essa potrebbe persino superare il limite e iniziare a collassare, riaumentando la temperatura fino al punto di fusione. Entro pochi secondi dall'inizio della fusione, una sostanziale frazione della materia della nana bianca subisce una reazione termonucleare incontrollata che rilascia un'energia sufficiente (1-2 × 1044 J) a disgregare la stella in una violenta esplosione.Questa categoria di supernovae produce un picco notevole di luminosità assoluta, che si presenta pressoché simile in tutte le esplosioni di questo tipo a causa della relativa uniformità delle masse delle nane bianche che esplodono in seguito ai processi di accrescimento. Per tale ragione le supernovae di tipo Ia sono utilizzate come candele standard per misurare la distanza della loro galassia ospitante, poiché la loro magnitudine apparente dipende quasi esclusivamente dalla distanza a cui si trovano.
La tipologia Ia (uno a) è una sottocategoria della classificazione delle supernovae formulata dai due astronomi statunitensi Rudolph Minkowski e Fritz Zwicky.
I differenti scenari che possono condurre alla formazione di una supernova di questo tipo condividono un medesimo meccanismo di base. Quando una nana bianca al carbonio-ossigeno in lenta rotazione accresce materia a partire da un'altra stella, essa non può superare il limite imposto dalla massa di Chandrasekhar, dal momento che gli elettroni degenerati non sarebbero più in grado di sorreggere la massa stessa dell'oggetto compatto; quest'ultimo, in mancanza di un meccanismo di compenso, collassa in una stella di neutroni, fenomeno che normalmente si verifica nel caso di una nana bianca composta essenzialmente di magnesio, neon e ossigeno.Gli astronomi che si occupano di formulare dei modelli sulle esplosioni delle supernovae di tipo Ia convengono però sul fatto che tale limite non venga mai realmente raggiunto, sicché il collasso non avrebbe mai inizio; tuttavia, l'incremento di pressione e densità dovuto all'aumento di massa determina un rialzo della temperatura del nucleo della nana bianca; quando quest'ultima si avvicina al 99% del limite di Chandrasekhar, si attuano dei moti di convezione, che durano per circa un migliaio di anni. Ad un certo punto di questa fase, si innesca un fronte di combustione potenziato dalla fusione del carbonio (detonazione del carbonio); i dettagli di questo fenomeno sono sconosciuti, compresi l'esatta localizzazione del fronte e i punti da cui questo ha origine. Poco dopo si innesca anche la fusione dell'ossigeno, che procede a ritmi inferiori a quella del carbonio.Non appena ha avuto inizio la fusione, la temperatura interna della nana bianca subisce un ulteriore incremento. Se una normale stella, per effetto della pressione termica, tende a espandersi e raffreddarsi per controbilanciare un incremento di temperatura, in una nana bianca la pressione degli elettroni degeneri è indipendente dalla temperatura; ne consegue che la nana bianca non è in grado di regolare i processi termonucleari come fanno le stelle normali, risultando vulnerabile al runaway termico. Le reazioni subiscono un'accelerazione drammatica, in parte dovuta all'instabilità di Rayleigh-Taylor e alle interazioni con le turbolenze interne. È ancora materia di vivace dibattito se questo fronte di combustione si trasformi da una deflagrazione subsonica in una detonazione supersonica.
Senza indulgere nei dettagli dei processi nucleari, è generalmente accettato che una sostanziale frazione del carbonio e dell'ossigeno venga convertito in elementi più pesanti in appena pochi secondi, innalzando la temperatura del nucleo fino a miliardi di kelvin. L'energia rilasciata dalla fusione (1–2 × 1044 J) è più che sufficiente a determinare lo smembramento della stella; la violenta esplosione rilascia un'onda d'urto che viaggia ad una velocità compresa tra 5.000 e 20.000 km/s, circa il 6% della velocità della luce. L'energia rilasciata durante l'esplosione determina anche un enorme aumento della luminosità; una tipica supernova di tipo Ia raggiunge valori di magnitudine assoluta pari a −19,3, quasi 5 miliardi di volte più brillante del Sole, con minime variazioni tra una supernova e l'altra. Il fatto che l'eventuale stella di neutroni originata dalla supernova resti vincolata alla compagna o meno dipende dal quantitativo di materia espulsa nel resto di supernova e dalla velocità a cui essa è stata espulsa.
I meccanismi che portano all'esplosione di una supernova di tipo Ia sono simili a quelli che innescano le novae, variabili cataclismiche in cui la nana bianca accresce materia da una stella compagna ad un tasso inferiore e non raggiunge il limite di Chandrasekhar. Nelle novae, la materia in caduta sulla superficie della nana bianca subisce un rialzo termico tale da innescare la fusione dell'idrogeno, che causa un'esplosione superficiale che però non è in grado di distruggere la nana bianca.Le supernovae di tipo Ia differiscono dalle supernovae a collasso nucleare per il fatto che l'esplosione di queste ultime è la conseguenza del collasso del nucleo di una stella massiccia.
Diversi modelli sono stati proposti per spiegare la formazione di una supernova di tipo Ia. Uno di questi è costituito dall'evoluzione di un sistema binario stretto. Il sistema è inizialmente costituito da due stelle di sequenza principale, con la componente primaria lievemente più massiccia della secondaria; possedendo una massa superiore, la primaria subisce un'evoluzione più rapida, giungendo per prima alla fase di gigante del ramo asintotico, stadio in cui il volume della stella si espande enormemente rispetto a quello posseduto quando essa si trovava all'interno della sequenza principale. Se le due stelle sono sufficientemente vicine da condividere un comune involucro di gas esterno, la primaria può perdere una significativa frazione della sua massa, cedendo inoltre una certa quantità di momento angolare, che causa un decadimento della sua orbita che si riflette in una riduzione del semiasse maggiore e del periodo di rivoluzione, determinando un avvicinamento delle due stelle. La componente primaria infine espelle i suoi strati più esterni in una nebulosa planetaria, mentre il nucleo collassa in una tenue nana bianca.
In un secondo momento anche la componente secondaria inizia ad affrontare la fase post-sequenza principale, espandendosi in gigante rossa e inglobando la nana bianca. In questa fase, le due stelle condividono nuovamente un comune involucro gassoso e continuano ad avvicinarsi man mano che perdono momento angolare; il risultato sarà un'orbita così stretta che essa potrà essere completata in poche ore. Durante questa fase si attivano dei meccanismi di trasferimento di massa dalla gigante verso la nana bianca; se questo meccanismo dura per un tempo sufficiente, la nana bianca può avvicinarsi alla massa limite di Chandrasekhar, pari a circa 1,44 M☉. La durata del trasferimento di materia dalla secondaria alla nana bianca può durare per alcuni milioni di anni (durante i quali può andare incontro a ripetute esplosioni di nova) prima che si raggiungano le condizioni idonee all'esplosione in supernova di tipo Ia.
La nana bianca può sottrarre materia anche da compagne appartenenti non necessariamente alla tipologia delle giganti, come le subgiganti o persino, se l'orbita è sufficientemente stretta, da una stella di sequenza principale. I processi realmente in atto durante la fase di accrescimento rimangono oggetto di incertezze, dal momento che dipendono sia dal tasso di accrescimento della materia sia dal trasferimento di momento angolare verso la nana bianca.Un secondo possibile meccanismo, anche se meno probabile, è costituito dalla fusione di due nane bianche le cui masse, sommate, eccedono la massa di Chandrasekhar. Inizialmente le due nane bianche si trovano ad una distanza piuttosto piccola l'una dall'altra. Nel corso di migliaia di anni, l'orbita delle due stelle attorno al comune baricentro inizia a restringersi e a decadere a causa della progressiva perdita di momento angolare, dovuta sia alle interazioni magnetiche tra le due stelle e le loro atmosfere, sia all'emissione di onde gravitazionali. La progressiva diminuzione dell'ampiezza dell'orbita e il conseguente aumento dell'attrazione gravitazionale tra le due componenti provoca lo smembramento di una delle due nane bianche; il processo di rottura è estremamente complesso e porterebbe alla formazione di un disco di plasma quasi degenere in orbita attorno alla nana superstite. Man mano che perdono il loro momento angolare, le particelle del disco precipitano sulla superficie della nana superstite, accrescendone la massa; il superamento della massa di Chandrasekhar comporta il collasso della nana bianca e la successiva esplosione. Tale ipotesi è stata formulata per spiegare l'anomala massa (2 M☉) del progenitore di SN 2003fg.Le collisioni tra le singole stelle all'interno della nostra galassia sono un evento piuttosto raro, con una cadenza stimata in una ogni 107-1013 anni, di gran lunga meno frequente della comparsa delle novae; la frequenza di simili eventi incrementa tuttavia in regioni a densità stellare particolarmente elevata, come le regioni centrali degli ammassi globulari, conducendo alla formazione di un particolare tipo stellare noto come vagabonda blu (blue straggler). Per quanto riguarda le nane bianche, un probabile scenario è costituito dalla collisione tra una singola nana bianca e una stella binaria oppure tra due binarie contenenti delle nane bianche; il risultato è la formazione di una binaria stretta di nane bianche, che, secondo le modalità sopra esposte, può fondersi e dar luogo all'esplosione.A dispetto degli altri tipi di supernovae, le supernovae di tipo Ia generalmente sono ospitate in tutti i tipi di galassia, comprese le ellittiche, non mostrando preferenze per particolari regioni galattiche.
Le supernovae di tipo Ia possiedono delle caratteristiche curve di luce, vale a dire dei grafici che mostrano il variare della luminosità in funzione del tempo trascorso dall'esplosione. In corrispondenza del massimo di luminosità, lo spettro mostra le linee degli elementi di massa intermedia compresi tra l'ossigeno e il calcio, che sono i principali costituenti degli strati più esterni della nana bianca. Diversi mesi dopo l'esplosione, quando questi strati si sono espansi fino a divenire trasparenti, lo spettro è dominato dalle linee degli elementi presenti in profondità, sintetizzati durante l'esplosione, per la gran parte isotopi di massa atomica 56 e numero atomico differente (appartenenti al picco del ferro), che vanno incontro a decadimento radioattivo. Il decadimento del nichel-56 in cobalto-56 e di quest'ultimo in ferro-56 produce fotoni ad alta energia che dominano l'emissione energetica del materiale espulso per scale temporali medio-lunghe.La somiglianza nei profili di luminosità assoluta di quasi tutte le supernovae di tipo Ia conosciute le rende utilizzabili come candele standard secondarie. La causa di tale uniformità nella curva luminosa è oggetto di speculazioni.
Fu proprio l'osservazione di alcune supernovae di tipo Ia distanti, nel 1998, a mostrare che, sorprendentemente, l'universo sembrava soggetto ad un'espansione accelerata.
(EN) Martin Schwarzschild, Structure and Evolution of the Stars, Princeton University Press, 1958, ISBN 0-691-08044-5.
(EN) Cliff Pickover, The Stars of Heaven, Oxford, Oxford University Press, 2001, ISBN 0-19-514874-6.
(EN) John Gribbin, Mary Gribbin, Stardust: Supernovae and Life—The Cosmic Connection, Yale University Press, 2001, ISBN 0-300-09097-8.
 A. De Blasi, Le stelle: nascita, evoluzione e morte, Bologna, CLUEB, 2002, ISBN 88-491-1832-5.
 AA.VV, L'Universo - Grande enciclopedia dell'astronomia, Novara, De Agostini, 2002.
 J. Gribbin, Enciclopedia di astronomia e cosmologia, Milano, Garzanti, 2005, ISBN 88-11-50517-8.
 W. Owen, et al, Atlante illustrato dell'Universo, Milano, Il Viaggiatore, 2006, ISBN 88-365-3679-4.
 J. Lindstrom, Stelle, galassie e misteri cosmici, Trieste, Editoriale Scienza, 2006, ISBN 88-7307-326-3.
 C. Abbondi, Universo in evoluzione dalla nascita alla morte delle stelle, Sandit, 2007, ISBN 88-89150-32-7.
(EN) J. Craig Wheeler, Cosmic Catastrophes: Exploding Stars, Black Holes, and Mapping the Universe, 2ª ed., Cambridge, Cambridge University Press, 2007,  pagine 339, ISBN 0-521-85714-7.
(EN) Martin Mobberley, Supernovae and How to Observe Them, New York, Springer, 2007,  pagine 209, ISBN 0-387-35257-0.
(EN) Novae and Supernovae, su peripatus.gen.nz. URL consultato il 16 ottobre 2011 (archiviato dall'url originale il 15 agosto 2007).